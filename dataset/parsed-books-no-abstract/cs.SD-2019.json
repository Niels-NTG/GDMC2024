{title:'Van Eyndhoven et al. (§72019§r)', author: 'Simon Van Eyndhoven; Tom Francart; Alexander Bertrand', display:{Lore:['[{"text": "arXiv:1602.05702", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.SY\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEEG-informed attended speaker extraction from recorded speech mixtures with application in neuro-steered hearing prostheses\\u00a7r\\n\\n\\u00a78\\u00a7oSimon Van Eyndhoven\\nTom Francart\\nAlexander Bertrand\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1602.05702\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TBME.2016.2587382\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Transactions on Biomedical Engineering, vol. 64, no. 5, pp.\\n  1045-1056, 2017\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 5 Feb 2019 15:53:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper is published in IEEE Transactions on Biomedical Engineering (2016) and is under copyright. Please cite this paper as: S. Van Eyndhoven, T. Francart, and A. Bertrand, \\"EEG-informed attended speaker "}','{"text": "extraction from recorded speech mixtures with application in neuro-steered hearing prostheses\\", IEEE Transactions on Biomedical Engineering, vol. 64, no. 5, pp. 1045-1056, 2017\\u00a7r"}']}
{title:'Bell et al. (§72019§r)', author: 'Shawn Bell; Liane Gabora', display:{Lore:['[{"text": "arXiv:1610.02475", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a75nlin.AO\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Music-generating System Inspired by the Science of Complex Adaptive Systems\\u00a7r\\n\\n\\u00a78\\u00a7oShawn Bell\\nLiane Gabora\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1610.02475\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIn Proceedings of the 4th International Workshop on Musical\\n  Meta-creation (MUME 2016). Palo Alto: Association for the Advancement of\\n  Artificial Intelligence (AAAI) Press. ISBN: 978-0-86491-397-5\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 15 Jul 2019 21:07:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, In Proceedings of the 4th InternationalWorkshop on Musical Meta-creation. Palo Alto: Association for the Advancement of Artificial Intelligence (AAAI) Press. ISBN: 978-0-86491-397-5 (2016)\\u00a7r"}']}
{title:'Sarkar et al. (§72019§r)', author: 'Achintya Kr. Sarkar; Zheng-Hua Tan', display:{Lore:['[{"text": "arXiv:1704.02373", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTime-Contrastive Learning Based DNN Bottleneck Features for Text-Dependent Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oAchintya Kr. Sarkar\\nZheng-Hua Tan\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1704.02373\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nNIPS Time Series Workshop 2017, Long Beach, CA, USA\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 11 May 2019 16:19:20 GMT)\\u00a7r"}']}
{title:'Bammer et al. (§72019§r)', author: 'Roswitha Bammer; Monika Dörfler; Pavol Harar', display:{Lore:['[{"text": "arXiv:1706.08818", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGabor frames and deep scattering networks in audio processing\\u00a7r\\n\\n\\u00a78\\u00a7oRoswitha Bammer\\nMonika D\\u00f6rfler\\nPavol Harar\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1706.08818\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3390/axioms8040106\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nAxioms 2019, 8(4), 106\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 1 Oct 2019 12:48:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o26 pages, 8 figures, 4 tables. Repository for reproducibility: https://gitlab.com/hararticles/gs-gt . Keywords: machine learning; scattering transform; Gabor transform; deep learning; time-frequency analysis; CNN. "}','{"text": "Accepted and publishedafter peer revision\\u00a7r"}']}
{title:'Schmidmeier (§72019§r)', author: 'Markus Schmidmeier', display:{Lore:['[{"text": "arXiv:1709.00375", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l2:3:4-Harmony within the Tritave\\u00a7r\\n\\n\\u00a78\\u00a7oMarkus Schmidmeier\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1709.00375\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1080/17459737.2019.1605626\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJournal of Mathematics and Music (2019)\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 7 Apr 2019 20:11:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn the new version, some remarks about music perception have been added. 29 pages, 22 figures, 7 tables. To appear in the Journal of Mathematics and Music\\u00a7r"}']}
{title:'Briot et al. (§72019§r)', author: 'Jean-Pierre Briot; Gaëtan Hadjeres; François-David Pachet', display:{Lore:['[{"text": "arXiv:1709.01620", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning Techniques for Music Generation \\u2013 A Survey\\u00a7r\\n\\n\\u00a78\\u00a7oJean-Pierre Briot\\nGa\\u00ebtan Hadjeres\\nFran\\u00e7ois-David Pachet\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1709.01620\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 7 Aug 2019 19:25:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o209 pages. This paper is a simplified version of thebook: J.-P. Briot, G. Hadjeres and F.-D. Pachet, Deep Learning Techniques for Music Generation, Computational Synthesis and Creative Systems, Springer, 2019\\u00a7r"}']}
{title:'Phan et al. (§72019§r)', author: 'Huy Phan; Philipp Koch; Ian McLoughlin; Alfred Mertins', display:{Lore:['[{"text": "arXiv:1712.02116", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnabling Early Audio Event Detection with Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oHuy Phan\\nPhilipp Koch\\nIan McLoughlin\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1712.02116\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2018.8461859\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPublished in Proceedings of 43rd IEEE International Conference on\\n  Acoustics, Speech, and Signal Processing (ICASSP), pp. 141-145, 2018\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 6 Apr 2019 22:20:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished version available at https://ieeexplore.ieee.org/document/8461859\\u00a7r"}']}
{title:'Kim et al. (§72019§r)', author: 'Chanwoo Kim; Ehsan Variani; Arun Narayanan; Michiel Bacchiani', display:{Lore:['[{"text": "arXiv:1712.03439", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Implementation of the Room Simulator for Training Deep Neural Network Acoustic Models\\u00a7r\\n\\n\\u00a78\\u00a7oChanwoo Kim\\nEhsan Variani\\nArun Narayanan\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1712.03439\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 1 Jan 2019 00:56:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at INTERSPEECH 2018. (https://www.isca-speech.org/archive/Interspeech_2018/abstracts/2566.html)\\u00a7r"}']}
{title:'Medhat et al. (§72019§r)', author: 'Fady Medhat; David Chesmore; John Robinson', display:{Lore:['[{"text": "arXiv:1801.05504", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Classification of Music Genre using Masked Conditional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oFady Medhat\\nDavid Chesmore\\nJohn Robinson\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1801.05504\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICDM.2017.125\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE International Conference on Data Mining (ICDM) Year: 2017\\n  Pages: 979 - 984\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 28 Apr 2019 09:00:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oRestricted Boltzmann Machine; RBM;Conditional RBM; CRBM; Deep Belief Net; DBN; Conditional Neural Network; CLNN; Masked Conditional Neural Network; MCLNN; Music Information Retrieval; MIR. IEEE International Conference "}','{"text": "on Data Mining (ICDM), 2017\\u00a7r"}']}
{title:'Donahue et al. (§72019§r)', author: 'Chris Donahue; Julian McAuley; Miller Puckette', display:{Lore:['[{"text": "arXiv:1802.04208", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Audio Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oChris Donahue\\nJulian McAuley\\nMiller Puckette\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1802.04208\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 9 Feb 2019 00:51:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished as a conference paper at ICLR 2019\\u00a7r"}']}
{title:'Jati et al. (§72019§r)', author: 'Arindam Jati; Panayiotis Georgiou', display:{Lore:['[{"text": "arXiv:1802.07860", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Predictive Coding using Convolutional Neural Networks towards Unsupervised Learning of Speaker Characteristics\\u00a7r\\n\\n\\u00a78\\u00a7oArindam Jati\\nPanayiotis Georgiou\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1802.07860\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2019.2921890\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol. 27, no. 10, pp. 1577-1589, Oct. 2019\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 Apr 2019 23:27:08 GMT)\\u00a7r"}']}
{title:'Elowsson (§72019§r)', author: 'Anders Elowsson', display:{Lore:['[{"text": "arXiv:1804.02918", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPolyphonic Pitch Tracking with Deep Layered Learning\\u00a7r\\n\\n\\u00a78\\u00a7oAnders Elowsson\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1804.02918\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 18 Mar 2019 15:22:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis is a distilled version (14 pages) from my PhD thesis \\"A. Elowsson; Modeling Music: Studies of Music Transcription, Music Perception and Music Production; 2018\\". This specific version added the learned active bin "}','{"text": "indices in the sparse kernel and the associated computed weights, which can be used to compute the Tentogram\\u00a7r"}']}
{title:'Kong et al. (§72019§r)', author: 'Qiuqiang Kong; Yong Xu; Iwona Sobieraj; Wenwu Wang; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:1804.04715", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Event Detection and Time-Frequency Segmentation from Weakly Labelled Data\\u00a7r\\n\\n\\u00a78\\u00a7oQiuqiang Kong\\nYong Xu\\nIwona Sobieraj\\nWenwu Wang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1804.04715\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2019.2895254\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing\\n  (Volume: 27, Issue: 4, April 2019)\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 2 Mar 2019 21:24:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 8 figures\\u00a7r"}']}
{title:'Goyal et al. (§72019§r)', author: 'Mohit Goyal; Varun Srivastava; Prathosh A. P', display:{Lore:['[{"text": "arXiv:1804.10147", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetection of Glottal Closure Instants from Raw Speech using Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oMohit Goyal\\nVarun Srivastava\\nPrathosh A. P\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1804.10147\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 9 Jul 2019 20:20:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUpdated submission. Figures Added. Accepted in Interspeech 2019\\u00a7r"}']}
{title:'Pons et al. (§72019§r)', author: 'Jordi Pons; Xavier Serra', display:{Lore:['[{"text": "arXiv:1805.00237", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRandomly weighted CNNs for (music) audio classification\\u00a7r\\n\\n\\u00a78\\u00a7oJordi Pons\\nXavier Serra\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1805.00237\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 14 Feb 2019 19:14:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn proceedings of the 44th IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP2019). Code: https://github.com/jordipons/elmarc\\u00a7r"}']}
{title:'Zhang et al. (§72019§r)', author: 'Yuanyuan Zhang; Jun Du; Zirui Wang; Jianshu Zhang', display:{Lore:['[{"text": "arXiv:1806.01506", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention Based Fully Convolutional Network for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYuanyuan Zhang\\nJun Du\\nZirui Wang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1806.01506\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.23919/APSIPA.2018.8659587\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2018 Asia-Pacific Signal and Information Processing Association\\n  Annual Summit and Conference (APSIPA ASC)\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 2 May 2019 09:10:24 GMT)\\u00a7r"}']}
{title:'Dong et al. (§72019§r)', author: 'Linhao Dong; Shiyu Zhou; Wei Chen; Bo Xu', display:{Lore:['[{"text": "arXiv:1806.06342", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExtending Recurrent Neural Aligner for Streaming End-to-End Speech Recognition in Mandarin\\u00a7r\\n\\n\\u00a78\\u00a7oLinhao Dong\\nShiyu Zhou\\nWei Chen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1806.06342\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 26 Feb 2019 02:51:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Interspeech 2018\\u00a7r"}']}
{title:'Lattner et al. (§72019§r)', author: 'Stefan Lattner; Maarten Grachten; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:1806.08236", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Transposition-Invariant Interval Features from Symbolic Music and Audio\\u00a7r\\n\\n\\u00a78\\u00a7oStefan Lattner\\nMaarten Grachten\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1806.08236\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 4 Feb 2019 17:09:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper accepted at the 19th International Society for Music Information Retrieval Conference, ISMIR 2018, Paris, France, September 23-27; 8 pages, 5 figures\\u00a7r"}']}
{title:'Lin et al. (§72019§r)', author: 'Zhiyu Lin; Kyle Xiao; Mark Riedl', display:{Lore:['[{"text": "arXiv:1806.11170", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerationMania: Learning to Semantically Choreograph\\u00a7r\\n\\n\\u00a78\\u00a7oZhiyu Lin\\nKyle Xiao\\nMark Riedl\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1806.11170\\u00a7r\\n\\nVersion:\\u00a77v5 (Mon, 12 Aug 2019 17:58:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in AIIDE 2019\\u00a7r"}']}
{title:'Liao et al. (§72019§r)', author: 'Chien-Feng Liao; Yu Tsao; Hung-Yi Lee; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:1807.07501", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoise Adaptive Speech Enhancement using Domain Adversarial Training\\u00a7r\\n\\n\\u00a78\\u00a7oChien-Feng Liao\\nYu Tsao\\nHung-Yi Lee\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1807.07501\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 1 Jul 2019 09:02:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2019\\u00a7r"}']}
{title:'Andén et al. (§72019§r)', author: 'Joakim Andén; Vincent Lostanlen; Stéphane Mallat', display:{Lore:['[{"text": "arXiv:1807.08869", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Time-Frequency Scattering\\u00a7r\\n\\n\\u00a78\\u00a7oJoakim And\\u00e9n\\nVincent Lostanlen\\nSt\\u00e9phane Mallat\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1807.08869\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TSP.2019.2918992\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Transactions on Signal Processing, vol. 67, no. 14, pp.\\n  3704-3718, July 15, 2019\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 12 Jul 2019 21:16:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages, 10 figures\\u00a7r"}']}
{title:'Gabrielli et al. (§72019§r)', author: 'Leonardo Gabrielli; Stefano Tomassetti; Stefano Squartini; Carlo Zinato; Stefano Guaiana', display:{Lore:['[{"text": "arXiv:1809.05483", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Multi-Stage Algorithm for Acoustic Physical Model Parameters Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oLeonardo Gabrielli\\nStefano Tomassetti\\nStefano Squartini\\nCarlo Zinato\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1809.05483\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 12 Feb 2019 14:39:56 GMT)\\u00a7r"}']}
{title:'Makris et al. (§72019§r)', author: 'Dimos Makris; Maximos Kaliakatsos-Papakostas; Katia Lida Kermanidis', display:{Lore:['[{"text": "arXiv:1809.06127", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeepDrum: An Adaptive Conditional Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oDimos Makris\\nMaximos Kaliakatsos-Papakostas\\nKatia Lida Kermanidis\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1809.06127\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 21 Jan 2019 21:32:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2018 Joint Workshop on Machine Learning for Music. The Federated Artificial Intelligence Meeting (FAIM), a joint workshop program of ICML, IJCAI/ECAI, and AAMAS\\u00a7r"}']}
{title:'Luo et al. (§72019§r)', author: 'Yi Luo; Nima Mesgarani', display:{Lore:['[{"text": "arXiv:1809.07454", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking for Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oYi Luo\\nNima Mesgarani\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1809.07454\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2019.2915167\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 15 May 2019 07:40:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE/ACM Transactionson Audio, Speech and Language Processing. This version is the authors\' version and may vary from the final publication in details\\u00a7r"}']}
{title:'Kataoka et al. (§72019§r)', author: 'Hidetomo Kataoka; Takashi Ijiri; Kohei Matsumura; Jeremy White; Akira Hirabayashi', display:{Lore:['[{"text": "arXiv:1809.10581", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic Probing for Estimating the Storage Time and Firmness of Tomatoes and Mandarin Oranges\\u00a7r\\n\\n\\u00a78\\u00a7oHidetomo Kataoka\\nTakashi Ijiri\\nKohei Matsumura\\nJeremy White\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1809.10581\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 Apr 2019 13:25:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 9 figures. After submitting the first version, we have continued measurements and found some results indicating a possibility that the conditions of our measurement devices had an influence to the estimation "}','{"text": "results. We are still continuing experiments\\u00a7r"}']}
{title:'Li et al. (§72019§r)', author: 'Xiaofei Li; Yutong Ban; Laurent Girin; Xavier Alameda-Pineda; Radu Horaud', display:{Lore:['[{"text": "arXiv:1809.10936", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOnline Localization and Tracking of Multiple Moving Speakers in Reverberant Environments\\u00a7r\\n\\n\\u00a78\\u00a7oXiaofei Li\\nYutong Ban\\nLaurent Girin\\nXavier Alameda-Pineda\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1809.10936\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JSTSP.2019.2903472\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 26 Feb 2019 19:40:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE Journal of Selected Topics in Signal Processing, 2019\\u00a7r"}']}
{title:'Roux et al. (§72019§r)', author: 'Jonathan Le Roux; Gordon Wichern; Shinji Watanabe; Andy Sarroff; John R. Hershey', display:{Lore:['[{"text": "arXiv:1810.01395", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhasebook and Friends: Leveraging Discrete Representations for Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oJonathan Le Roux\\nGordon Wichern\\nShinji Watanabe\\nAndy Sarroff\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.01395\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JSTSP.2019.2904183\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 7 Mar 2019 16:26:58 GMT)\\u00a7r"}']}
{title:'Lostanlen (§72019§r)', author: 'Vincent Lostanlen', display:{Lore:['[{"text": "arXiv:1810.04506", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Time-frequency Scattering and Computer Music\\u00a7r\\n\\n\\u00a78\\u00a7oVincent Lostanlen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.04506\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 20 May 2019 18:41:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages. Published as a chapter in the book: \\"Florian Hecker: Halluzination, Perspektive, Synthese\\",pp. 97\\u2013102. Nicolaus Schafhausen, Vanessa Joan M\\u00fcller, editors. Sternberg Press, Berlin, 2019\\u00a7r"}']}
{title:'Wang et al. (§72019§r)', author: 'Yun Wang; Juncheng Li; Florian Metze', display:{Lore:['[{"text": "arXiv:1810.09050", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparison of Five Multiple Instance Learning Pooling Functions for Sound Event Detection with Weak Labeling\\u00a7r\\n\\n\\u00a78\\u00a7oYun Wang\\nJuncheng Li\\nFlorian Metze\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.09050\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 19 Feb 2019 17:07:43 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72019§r)', author: 'Yun Wang; Florian Metze', display:{Lore:['[{"text": "arXiv:1810.09052", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConnectionist Temporal Localization for Sound Event Detection with Sequential Labeling\\u00a7r\\n\\n\\u00a78\\u00a7oYun Wang\\nFlorian Metze\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.09052\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 19 Feb 2019 17:13:16 GMT)\\u00a7r"}']}
{title:'Lluís et al. (§72019§r)', author: 'Francesc Lluís; Jordi Pons; Xavier Serra', display:{Lore:['[{"text": "arXiv:1810.12187", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end music source separation: is it possible in the waveform domain?\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesc Llu\\u00eds\\nJordi Pons\\nXavier Serra\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.12187\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 28 Jun 2019 09:41:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn proceedings of INTERSPEECH 2019. Code: https://github.com/francesclluis/source-separation-wavenet and demo: http://jordipons.me/apps/end-to-end-music-source-separation/\\u00a7r"}']}
{title:'Hawthorne et al. (§72019§r)', author: 'Curtis Hawthorne; Andriy Stasyuk; Adam Roberts; Ian Simon; Cheng-Zhi Anna Huang; Sander Dieleman; Erich Elsen; Jesse Engel; Douglas Eck', display:{Lore:['[{"text": "arXiv:1810.12247", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oCurtis Hawthorne\\nAndriy Stasyuk\\nAdam Roberts\\n+ 5 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.12247\\u00a7r\\n\\nVersion:\\u00a77v5 (Thu, 17 Jan 2019 19:45:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oExamples available at https://goo.gl/magenta/maestro-examples\\u00a7r"}']}
{title:'Phaye et al. (§72019§r)', author: 'Sai Samarth R Phaye; Emmanouil Benetos; Ye Wang', display:{Lore:['[{"text": "arXiv:1810.12642", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSubSpectralNet - Using Sub-Spectrogram based Convolutional Neural Networks for Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oSai Samarth R Phaye\\nEmmanouil Benetos\\nYe Wang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.12642\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 25 Feb 2019 13:40:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEInternational Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2019\\u00a7r"}']}
{title:'Karamatlı et al. (§72019§r)', author: 'Ertuğ Karamatlı; Ali Taylan Cemgil; Serap Kırbız', display:{Lore:['[{"text": "arXiv:1810.13104", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Source Separation Using Variational Autoencoders and Weak Class Supervision\\u00a7r\\n\\n\\u00a78\\u00a7oErtu\\u011f Karamatl\\u0131\\nAli Taylan Cemgil\\nSerap K\\u0131rb\\u0131z\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.13104\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2019.2929440\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Signal Processing Letters 26 (2019) 1349-1353\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 4 Aug 2019 14:09:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted version\\u00a7r"}']}
{title:'Mokrý et al. (§72019§r)', author: 'Ondřej Mokrý; Pavel Záviška; Pavel Rajmic; Vítězslav Veselý', display:{Lore:['[{"text": "arXiv:1810.13137", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a72math.OC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntroducing SPAIN (SParse Audio INpainter)\\u00a7r\\n\\n\\u00a78\\u00a7oOnd\\u0159ej Mokr\\u00fd\\nPavel Z\\u00e1vi\\u0161ka\\nPavel Rajmic\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.13137\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.23919/EUSIPCO.2019.8902560\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2019 27th European Signal Processing Conference (EUSIPCO)\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 18 Jun 2019 10:54:41 GMT)\\u00a7r"}']}
{title:'Phan et al. (§72019§r)', author: 'Huy Phan; Oliver Y. Chén; Philipp Koch; Lam Pham; Ian McLoughlin; Alfred Mertins; Maarten De Vos', display:{Lore:['[{"text": "arXiv:1811.01095", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBeyond Equal-Length Snippets: How Long is Sufficient to Recognize an Audio Scene?\\u00a7r\\n\\n\\u00a78\\u00a7oHuy Phan\\nOliver Y. Ch\\u00e9n\\nPhilipp Koch\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.01095\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 8 May 2019 19:00:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to 2019 AES Conference on Audio Forensics\\u00a7r"}']}
{title:'Hung et al. (§72019§r)', author: 'Yun-Ning Hung; Yi-An Chen; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:1811.01143", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultitask learning for frame-level instrument recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYun-Ning Hung\\nYi-An Chen\\nYi-Hsuan Yang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.01143\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 18 Feb 2019 08:42:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis is a pre-print version of an ICASSP 2019 paper\\u00a7r"}']}
{title:'Casebeer et al. (§72019§r)', author: 'Jonah Casebeer; Zhepei Wang; Paris Smaragdis', display:{Lore:['[{"text": "arXiv:1811.01251", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-View Networks For Multi-Channel Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oJonah Casebeer\\nZhepei Wang\\nParis Smaragdis\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.01251\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 26 Feb 2019 23:49:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 7 figures, Accepted to ICASSP 2019\\u00a7r"}']}
{title:'Slizovskaia et al. (§72019§r)', author: 'Olga Slizovskaia; Leo Kim; Gloria Haro; Emilia Gomez', display:{Lore:['[{"text": "arXiv:1811.01850", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Sound Source Separation Conditioned On Instrument Labels\\u00a7r\\n\\n\\u00a78\\u00a7oOlga Slizovskaia\\nLeo Kim\\nGloria Haro\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.01850\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 9 May 2019 16:55:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 2 tables, ICASSP 2019\\u00a7r"}']}
{title:'Kim et al. (§72019§r)', author: 'Sungwon Kim; Sang-gil Lee; Jongyoon Song; Jaehyeon Kim; Sungroh Yoon', display:{Lore:['[{"text": "arXiv:1811.02155", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFloWaveNet : A Generative Flow for Raw Audio\\u00a7r\\n\\n\\u00a78\\u00a7oSungwon Kim\\nSang-gil Lee\\nJongyoon Song\\nJaehyeon Kim\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.02155\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 20 May 2019 06:37:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, ICML\'2019\\u00a7r"}']}
{title:'Nolasco et al. (§72019§r)', author: 'Inês Nolasco; Alessandro Terenzi; Stefania Cecchi; Simone Orcioni; Helen L. Bear; Emmanouil Benetos', display:{Lore:['[{"text": "arXiv:1811.06330", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-based identification of beehive states\\u00a7r\\n\\n\\u00a78\\u00a7oIn\\u00eas Nolasco\\nAlessandro Terenzi\\nStefania Cecchi\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.06330\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 15 Feb 2019 16:07:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP 2019\\u00a7r"}']}
{title:'Leglaive et al. (§72019§r)', author: 'Simon Leglaive; Laurent Girin; Radu Horaud', display:{Lore:['[{"text": "arXiv:1811.06713", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-supervised multichannel speech enhancement with variational autoencoders and non-negative matrix factorization\\u00a7r\\n\\n\\u00a78\\u00a7oSimon Leglaive\\nLaurent Girin\\nRadu Horaud\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.06713\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8683704\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE International Conference on Acoustics Speech and Signal\\n  Processing (ICASSP), Brighton, UK, May 2019, pp. 101-105\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 30 Apr 2019 13:57:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, audio examples and code available online at https://team.inria.fr/perception/icassp-2019-mvae/\\u00a7r"}']}
{title:'Thickstun et al. (§72019§r)', author: 'John Thickstun; Zaid Harchaoui; Dean P. Foster; Sham M. Kakade', display:{Lore:['[{"text": "arXiv:1811.08045", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCoupled Recurrent Models for Polyphonic Music Composition\\u00a7r\\n\\n\\u00a78\\u00a7oJohn Thickstun\\nZaid Harchaoui\\nDean P. Foster\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.08045\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 26 Nov 2019 21:55:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages; long version of the paper appearing in ISMIR 2019\\u00a7r"}']}
{title:'Chen et al. (§72019§r)', author: 'Ke Chen; Weilin Zhang; Shlomo Dubnov; Gus Xia; Wei Li', display:{Lore:['[{"text": "arXiv:1811.08380", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Effect of Explicit Structure Encoding of Deep Neural Networks for Symbolic Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oKe Chen\\nWeilin Zhang\\nShlomo Dubnov\\nGus Xia\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.08380\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/MMRP.2019.00022\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2019 International Workshop on Multilayer Music Representation and\\n  Processing (MMRP)\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 24 Jan 2019 12:42:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 13 figures\\u00a7r"}']}
{title:'Zhou et al. (§72019§r)', author: 'Jianfeng Zhou; Tao Jiang; Lin Li; Qingyang Hong; Zhe Wang; Bingyin Xia', display:{Lore:['[{"text": "arXiv:1811.09355", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTraining Multi-Task Adversarial Network for Extracting Noise-Robust Speaker Embedding\\u00a7r\\n\\n\\u00a78\\u00a7oJianfeng Zhou\\nTao Jiang\\nLin Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.09355\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 12 May 2019 07:26:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by ICASSP2019\\u00a7r"}']}
{title:'Kumar et al. (§72019§r)', author: 'Anurag Kumar; Ankit Shah; Bhiksha Raj; Alex Hauptmann', display:{Lore:['[{"text": "arXiv:1811.09967", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Sound Events From Webly Labeled Data\\u00a7r\\n\\n\\u00a78\\u00a7oAnurag Kumar\\nAnkit Shah\\nBhiksha Raj\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.09967\\u00a7r\\n\\nVersion:\\u00a77v4 (Sun, 14 Jul 2019 06:21:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted IJCAI 2019\\u00a7r"}']}
{title:'Chou et al. (§72019§r)', author: 'Szu-Yu Chou; Kai-Hsiang Cheng; Jyh-Shing Roger Jang; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:1812.01269", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning to match transient sound events using attentional similarity for few-shot sound recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSzu-Yu Chou\\nKai-Hsiang Cheng\\nJyh-Shing Roger Jang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1812.01269\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 18 Feb 2019 07:58:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis is a pre-print version of an ICASSP 2019 paper\\u00a7r"}']}
{title:'Das et al. (§72019§r)', author: 'Sagnik Das; Nisha Gandhi; Tejas Naik; Roy Shilkrot', display:{Lore:['[{"text": "arXiv:1812.03415", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIncrease Apparent Public Speaking Fluency By Speech Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oSagnik Das\\nNisha Gandhi\\nTejas Naik\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1812.03415\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8682937\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2019 IEEE International Conference on Acoustics, Speech and Signal\\n  Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 3 Aug 2019 22:10:07 GMT)\\u00a7r"}']}
{title:'Michelashvili et al. (§72019§r)', author: 'Michael Michelashvili; Sagie Benaim; Lior Wolf', display:{Lore:['[{"text": "arXiv:1812.06087", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-Supervised Monaural Singing Voice Separation With a Masking Network Trained on Synthetic Mixtures\\u00a7r\\n\\n\\u00a78\\u00a7oMichael Michelashvili\\nSagie Benaim\\nLior Wolf\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1812.06087\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 6 May 2019 14:12:23 GMT)\\u00a7r"}']}
{title:'Barkan et al. (§72019§r)', author: 'Oren Barkan; David Tsiris; Ori Katz; Noam Koenigstein', display:{Lore:['[{"text": "arXiv:1812.06349", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInverSynth: Deep Estimation of Synthesizer Parameter Configurations from Audio Signals\\u00a7r\\n\\n\\u00a78\\u00a7oOren Barkan\\nDavid Tsiris\\nOri Katz\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1812.06349\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 21 Nov 2019 06:47:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in IEEE/ACM Transactions on Audio Speech and Language Processing\\u00a7r"}']}
{title:'Colombo et al. (§72019§r)', author: 'Florian Colombo; Johanni Brea; Wulfram Gerstner', display:{Lore:['[{"text": "arXiv:1812.06669", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning to Generate Music with BachProp\\u00a7r\\n\\n\\u00a78\\u00a7oFlorian Colombo\\nJohanni Brea\\nWulfram Gerstner\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1812.06669\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5281/zenodo.3249394\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nin Proceedings of the 16th Sound and Music Computing Conference.\\n  2019. p. 380-386\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 12 Jun 2019 10:42:50 GMT)\\u00a7r"}']}
{title:'RezezadehAzar et al. (§72019§r)', author: 'Shahla RezezadehAzar; Ali Ahmadi; Saber Malekzadeh; Maryam Samami', display:{Lore:['[{"text": "arXiv:1812.07017", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInstrument-Independent Dastgah Recognition of Iranian Classical Music Using AzarNet\\u00a7r\\n\\n\\u00a78\\u00a7oShahla RezezadehAzar\\nAli Ahmadi\\nSaber Malekzadeh\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1812.07017\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.13140/RG.2.2.18688.89602\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 9 Jan 2019 11:26:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to the 27th Iranian Conference on Electrical Engineering (ICEE 2019)\\u00a7r"}']}
{title:'Ban et al. (§72019§r)', author: 'Yutong Ban; Xavier Alameda-PIneda; Christine Evers; Radu Horaud', display:{Lore:['[{"text": "arXiv:1812.08246", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTracking Multiple Audio Sources with the von Mises Distribution and Variational EM\\u00a7r\\n\\n\\u00a78\\u00a7oYutong Ban\\nXavier Alameda-PIneda\\nChristine Evers\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1812.08246\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2019.2908376\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 10 Apr 2019 13:32:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE Signal Processing Letters, 2019\\u00a7r"}']}
{title:'Zeng et al. (§72019§r)', author: 'Qiang Zeng; Jianhai Su; Chenglong Fu; Golam Kayas; Lannan Luo', display:{Lore:['[{"text": "arXiv:1812.10199", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Multiversion Programming Inspired Approach to Detecting Audio Adversarial Examples\\u00a7r\\n\\n\\u00a78\\u00a7oQiang Zeng\\nJianhai Su\\nChenglong Fu\\nGolam Kayas\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1812.10199\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nThe AAAI-19 Workshop on Artificial Intelligence for Cyber Security\\n  (AICS), 2019\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 3 Dec 2019 19:51:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 4 figures, AICS 2019, The AAAI-19 Workshop on Artificial Intelligence for Cyber Security (AICS), 2019\\u00a7r"}']}
{title:'Du et al. (§72019§r)', author: 'Xingjian Du; Mengyao Zhu; Xuan Shi; Xinpeng Zhang; Wen Zhang; Jingdong Chen', display:{Lore:['[{"text": "arXiv:1901.00295", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Model for Speech Enhancement by Consistent Spectrogram Masking\\u00a7r\\n\\n\\u00a78\\u00a7oXingjian Du\\nMengyao Zhu\\nXuan Shi\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.00295\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Jan 2019 08:39:05 GMT)\\u00a7r"}']}
{title:'Ribas et al. (§72019§r)', author: 'Dayana Ribas; Jorge Llombart; Antonio Miguel; Luis Vicente', display:{Lore:['[{"text": "arXiv:1901.00660", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Speech Enhancement for Reverberated and Noisy Signals using Wide Residual Networks\\u00a7r\\n\\n\\u00a78\\u00a7oDayana Ribas\\nJorge Llombart\\nAntonio Miguel\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.00660\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 3 Jan 2019 09:41:25 GMT)\\u00a7r"}']}
{title:'Ming et al. (§72019§r)', author: 'Huaiping Ming; Lei He; Haohan Guo; Frank K. Soong', display:{Lore:['[{"text": "arXiv:1901.00707", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFeature reinforcement with word embedding and parsing information in neural TTS\\u00a7r\\n\\n\\u00a78\\u00a7oHuaiping Ming\\nLei He\\nHaohan Guo\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.00707\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Mar 2019 15:24:38 GMT)\\u00a7r"}']}
{title:'Sahidullah et al. (§72019§r)', author: 'Md Sahidullah; Hector Delgado; Massimiliano Todisco; Tomi Kinnunen; Nicholas Evans; Junichi Yamagishi; Kong-Aik Lee', display:{Lore:['[{"text": "arXiv:1901.01085", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntroduction to Voice Presentation Attack Detection and Recent Advances\\u00a7r\\n\\n\\u00a78\\u00a7oMd Sahidullah\\nHector Delgado\\nMassimiliano Todisco\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.01085\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPublished in Handbook of Biometric Anti-Spoofing Presentation\\n  Attack Detection (Second Edition eBook ISBN 978-3-319-92627-8), 2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Jan 2019 13:31:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished as a book-chapter in Handbook of Biometric Anti-Spoofing Presentation Attack Detection (Second Edition)\\u00a7r"}']}
{title:'Fonseca et al. (§72019§r)', author: 'Eduardo Fonseca; Manoj Plakal; Daniel P. W. Ellis; Frederic Font; Xavier Favory; Xavier Serra', display:{Lore:['[{"text": "arXiv:1901.01189", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Sound Event Classifiers from Web Audio with Noisy Labels\\u00a7r\\n\\n\\u00a78\\u00a7oEduardo Fonseca\\nManoj Plakal\\nDaniel P. W. Ellis\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.01189\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 7 Mar 2019 22:37:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInternational Conference on Acoustics, Speech, and Signal Processing (ICASSP 2019)\\u00a7r"}']}
{title:'Wu et al. (§72019§r)', author: 'Yuzhong Wu; Tan Lee', display:{Lore:['[{"text": "arXiv:1901.01502", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Sound Texture in CNN-Based Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oYuzhong Wu\\nTan Lee\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.01502\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 6 Jan 2019 05:21:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2019\\u00a7r"}']}
{title:'Park et al. (§72019§r)', author: 'Sangwook Park; David K. Han; Hanseok Ko', display:{Lore:['[{"text": "arXiv:1901.02050", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSinusoidal wave generating network based on adversarial learning and its application: synthesizing frog sounds for data augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oSangwook Park\\nDavid K. Han\\nHanseok Ko\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.02050\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Jan 2019 20:23:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper has been revised from our previous manuscripts as following reviewer\'s comments in ICML, NIP, and IEEE TSP\\u00a7r"}']}
{title:'Terneux et al. (§72019§r)', author: 'Andrés Estrella Terneux; Damián Nicolalde; Daniel Nicolalde; Andrés Merino-Viteri', display:{Lore:['[{"text": "arXiv:1901.02495", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPresence-absence estimation in audio recordings of tropical frog communities\\u00a7r\\n\\n\\u00a78\\u00a7oAndr\\u00e9s Estrella Terneux\\nDami\\u00e1n Nicolalde\\nDaniel Nicolalde\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.02495\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Jan 2019 20:08:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o27 pages, 13 figures\\u00a7r"}']}
{title:'Pellegrini et al. (§72019§r)', author: 'Thomas Pellegrini; Léo Cances', display:{Lore:['[{"text": "arXiv:1901.03146", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCosine-similarity penalty to discriminate sound classes in weakly-supervised sound event detection\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Pellegrini\\nL\\u00e9o Cances\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.03146\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 1 Apr 2019 12:24:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, accepted at IJCNN 2019. Code: https://github.com/topel/ijcnn19_submission\\u00a7r"}']}
{title:'Seth et al. (§72019§r)', author: 'Harshita Seth; Pulkit Kumar; Muktabh Mayank Srivastava', display:{Lore:['[{"text": "arXiv:1901.03860", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrototypical Metric Transfer Learning for Continuous Speech Keyword Spotting With Limited Training Data\\u00a7r\\n\\n\\u00a78\\u00a7oHarshita Seth\\nPulkit Kumar\\nMuktabh Mayank Srivastava\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.03860\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 12 Jan 2019 13:20:54 GMT)\\u00a7r"}']}
{title:'Crangle et al. (§72019§r)', author: 'Colleen E. Crangle; Rui Wang; Marcos Perreau-Guimaraes; Michelle U. Nguyen; Duc T. Nguyen; Patrick Suppes', display:{Lore:['[{"text": "arXiv:1901.04110", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMachine learning for the recognition of emotion in the speech of couples in psychotherapy using the Stanford Suppes Brain Lab Psychotherapy Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oColleen E. Crangle\\nRui Wang\\nMarcos Perreau-Guimaraes\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.04110\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Jan 2019 02:21:08 GMT)\\u00a7r"}']}
{title:'Tits et al. (§72019§r)', author: 'Noé Tits; Kevin El Haddad; Thierry Dutoit', display:{Lore:['[{"text": "arXiv:1901.04276", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Transfer Learning for Low Resource Emotional TTS\\u00a7r\\n\\n\\u00a78\\u00a7oNo\\u00e9 Tits\\nKevin El Haddad\\nThierry Dutoit\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.04276\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Jan 2019 13:05:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at IntelliSys 2019\\u00a7r"}']}
{title:'Nasrullah et al. (§72019§r)', author: 'Zain Nasrullah; Yue Zhao', display:{Lore:['[{"text": "arXiv:1901.04555", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Artist Classification with Convolutional Recurrent Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oZain Nasrullah\\nYue Zhao\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.04555\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Mar 2019 20:29:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of the 2019 International Joint Conference on Neural Networks (IJCNN)\\u00a7r"}']}
{title:'Malekzadeh et al. (§72019§r)', author: 'Saber Malekzadeh; Maryam Samami; Shahla RezazadehAzar; Maryam Rayegan', display:{Lore:['[{"text": "arXiv:1901.04696", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lClassical Music Generation in Distinct Dastgahs with AlimNet ACGAN\\u00a7r\\n\\n\\u00a78\\u00a7oSaber Malekzadeh\\nMaryam Samami\\nShahla RezazadehAzar\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.04696\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.13140/RG.2.2.32101.65765\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Jan 2019 08:00:27 GMT)\\u00a7r"}']}
{title:'Malekzadeh (§72019§r)', author: 'Saber Malekzadeh', display:{Lore:['[{"text": "arXiv:1901.04699", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhoneme-Based Persian Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSaber Malekzadeh\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.04699\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.13140/RG.2.2.32856.96007\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Jan 2019 08:07:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oin Farsi\\u00a7r"}']}
{title:'Sahai et al. (§72019§r)', author: 'Abhimanyu Sahai; Romann Weber; Brian McWilliams', display:{Lore:['[{"text": "arXiv:1901.05061", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpectrogram Feature Losses for Music Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oAbhimanyu Sahai\\nRomann Weber\\nBrian McWilliams\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.05061\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 26 Jun 2019 19:15:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for presentation at the 27th European Signal Processing Conference (EUSIPCO 2019)\\u00a7r"}']}
{title:'Radfar et al. (§72019§r)', author: 'Martin H. Radfar; Richard M. Dansereau; Willy Wong', display:{Lore:['[{"text": "arXiv:1901.07604", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Separation Using Gain-Adapted Factorial Hidden Markov Models\\u00a7r\\n\\n\\u00a78\\u00a7oMartin H. Radfar\\nRichard M. Dansereau\\nWilly Wong\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.07604\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Jan 2019 20:17:07 GMT)\\u00a7r"}']}
{title:'Li et al. (§72019§r)', author: 'Xinyu Li; Venkata Chebiyyam; Katrin Kirchhoff', display:{Lore:['[{"text": "arXiv:1901.08608", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-stream Network With Temporal Attention For Environmental Sound Classification\\u00a7r\\n\\n\\u00a78\\u00a7oXinyu Li\\nVenkata Chebiyyam\\nKatrin Kirchhoff\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.08608\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Jan 2019 19:02:17 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72019§r)', author: 'Caifeng Liu; Lin Feng; Guochao Liu; Huibing Wang; Shenglan Liu', display:{Lore:['[{"text": "arXiv:1901.08928", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBottom-up Broadcast Neural Network For Music Genre Classification\\u00a7r\\n\\n\\u00a78\\u00a7oCaifeng Liu\\nLin Feng\\nGuochao Liu\\nHuibing Wang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.08928\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Jan 2019 02:01:00 GMT)\\u00a7r"}']}
{title:'Qian et al. (§72019§r)', author: 'Xinyuan Qian; Andrea Cavallaro; Alessio Brutti; Maurizio Omologo', display:{Lore:['[{"text": "arXiv:1901.08983", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLOCATA challenge: speaker localization with a planar array\\u00a7r\\n\\n\\u00a78\\u00a7oXinyuan Qian\\nAndrea Cavallaro\\nAlessio Brutti\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.08983\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Jan 2019 17:00:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of the LOCATA ChallengeWorkshop - a satellite event of IWAENC 2018 (arXiv:1811.08482 )\\u00a7r"}']}
{title:'Huzaifah et al. (§72019§r)', author: 'M. Huzaifah; L. Wyse', display:{Lore:['[{"text": "arXiv:1901.10240", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lApplying Visual Domain Style Transfer and Texture Synthesis Techniques to Audio - Insights and Challenges\\u00a7r\\n\\n\\u00a78\\u00a7oM. Huzaifah\\nL. Wyse\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.10240\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s00521-019-04053-8\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nNeural Computing and Applications, 32(4):1051-1065, 2020\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Jan 2019 11:59:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPost-peer-review, pre-copyedit version of an article to be published in Neural Computing and Applications. 11 figures\\u00a7r"}']}
{title:'Le et al. (§72019§r)', author: 'Thanh-Ha Le; Philippe Gilberton; Ngoc Q. K. Duong', display:{Lore:['[{"text": "arXiv:1901.11291", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiscriminate natural versus loudspeaker emitted speech\\u00a7r\\n\\n\\u00a78\\u00a7oThanh-Ha Le\\nPhilippe Gilberton\\nNgoc Q. K. Duong\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.11291\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 17 Feb 2019 14:34:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure\\u00a7r"}']}
{title:'Mingote et al. (§72019§r)', author: 'Victoria Mingote; Antonio Miguel; Alfonso Ortega; Eduardo Lleida', display:{Lore:['[{"text": "arXiv:1901.11332", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimization of the Area Under the ROC Curve using Neural Network Supervectors for Text-Dependent Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oVictoria Mingote\\nAntonio Miguel\\nAlfonso Ortega\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.11332\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 Apr 2019 14:41:41 GMT)\\u00a7r"}']}
{title:'Shi et al. (§72019§r)', author: 'Ziqiang Shi; Huibin Lin; Liu Liu; Rujie Liu; Jiqing Han', display:{Lore:['[{"text": "arXiv:1902.00631", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIs CQT more suitable for monaural speech separation than STFT? an empirical study\\u00a7r\\n\\n\\u00a78\\u00a7oZiqiang Shi\\nHuibin Lin\\nLiu Liu\\nRujie Liu\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.00631\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Feb 2019 03:01:13 GMT)\\u00a7r"}']}
{title:'Shi et al. (§72019§r)', author: 'Ziqiang Shi; Huibin Lin; Liu Liu; Rujie Liu; Shoji Hayakawa; Shouji Harada; Jiqing Han', display:{Lore:['[{"text": "arXiv:1902.00651", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFurcaNet: An end-to-end deep gated convolutional, long short-term memory, deep neural networks for single channel speech separation\\u00a7r\\n\\n\\u00a78\\u00a7oZiqiang Shi\\nHuibin Lin\\nLiu Liu\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.00651\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 18 Mar 2019 02:44:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:1902.00631\\u00a7r"}']}
{title:'Imoto et al. (§72019§r)', author: 'Keisuke Imoto; Seisuke Kyochi', display:{Lore:['[{"text": "arXiv:1902.00816", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Event Detection Using Graph Laplacian Regularization Based on Event Co-occurrence\\u00a7r\\n\\n\\u00a78\\u00a7oKeisuke Imoto\\nSeisuke Kyochi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.00816\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 18 Feb 2019 10:06:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2019\\u00a7r"}']}
{title:'Wager et al. (§72019§r)', author: 'Sanna Wager; George Tzanetakis; Cheng-i Wang; Lijiang Guo; Aswin Sivaraman; Minje Kim', display:{Lore:['[{"text": "arXiv:1902.00956", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Autotuner: A Data-Driven Approach to Natural-Sounding Pitch Correction for Singing Voice in Karaoke Performances\\u00a7r\\n\\n\\u00a78\\u00a7oSanna Wager\\nGeorge Tzanetakis\\nCheng-i Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.00956\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 3 Feb 2019 19:05:24 GMT)\\u00a7r"}']}
{title:'Dey et al. (§72019§r)', author: 'Jayanta Dey; Md Sanzid Bin Hossain; Mohammad Ariful Haque', display:{Lore:['[{"text": "arXiv:1902.01544", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Ensemble SVM-based Approach for Voice Activity Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJayanta Dey\\nMd Sanzid Bin Hossain\\nMohammad Ariful Haque\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.01544\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Feb 2019 04:48:17 GMT)\\u00a7r"}']}
{title:'Leglaive et al. (§72019§r)', author: 'Simon Leglaive; Laurent Girin; Radu Horaud', display:{Lore:['[{"text": "arXiv:1902.01605", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA variance modeling framework based on variational autoencoders for speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oSimon Leglaive\\nLaurent Girin\\nRadu Horaud\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.01605\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/MLSP.2018.8516711\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. of the IEEE International Workshop on Machine Learning for\\n  Signal Processing (MLSP), Aalborg, Denmark, September 2018\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Feb 2019 09:36:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 3 figures\\u00a7r"}']}
{title:'Kumar et al. (§72019§r)', author: 'Harish Kumar; Balaraman Ravindran', display:{Lore:['[{"text": "arXiv:1902.01973", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPolyphonic Music Composition with LSTM Neural Networks and Reinforcement Learning\\u00a7r\\n\\n\\u00a78\\u00a7oHarish Kumar\\nBalaraman Ravindran\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.01973\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 3 Mar 2019 20:10:05 GMT)\\u00a7r"}']}
{title:'Tamaru et al. (§72019§r)', author: 'Hiroki Tamaru; Yuki Saito; Shinnosuke Takamichi; Tomoki Koriyama; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:1902.03389", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerative Moment Matching Network-based Random Modulation Post-filter for DNN-based Singing Voice Synthesis and Neural Double-tracking\\u00a7r\\n\\n\\u00a78\\u00a7oHiroki Tamaru\\nYuki Saito\\nShinnosuke Takamichi\\nTomoki Koriyama\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.03389\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 9 Feb 2019 07:49:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, to appear in IEEE ICASSP 2019 (Paper Code: SLP-P22.11, Session: Speech Synthesis III)\\u00a7r"}']}
{title:'Leglaive et al. (§72019§r)', author: 'Simon Leglaive; Umut Simsekli; Antoine Liutkus; Laurent Girin; Radu Horaud', display:{Lore:['[{"text": "arXiv:1902.03926", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech enhancement with variational autoencoders and alpha-stable distributions\\u00a7r\\n\\n\\u00a78\\u00a7oSimon Leglaive\\nUmut Simsekli\\nAntoine Liutkus\\nLaurent Girin\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.03926\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8682546\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE International Conference on Acoustics Speech and Signal\\n  Processing (ICASSP), Brighton, UK, May 2019, pp. 541-545\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Feb 2019 14:50:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, audio examples and code available online : https://team.inria.fr/perception/research/icassp2019-asvae/. arXiv admin note: text overlapwith arXiv:1811.06713\\u00a7r"}']}
{title:'Marafioti et al. (§72019§r)', author: 'Andrés Marafioti; Nicki Holighaus; Nathanaël Perraudin; Piotr Majdak', display:{Lore:['[{"text": "arXiv:1902.04072", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Generation of Time-Frequency Features with application in audio synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oAndr\\u00e9s Marafioti\\nNicki Holighaus\\nNathana\\u00ebl Perraudin\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.04072\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 16 May 2019 13:35:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at ICML 2019\\u00a7r"}']}
{title:'Kelz et al. (§72019§r)', author: 'Rainer Kelz; Sebastian Böck; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:1902.04390", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultitask Learning for Polyphonic Piano Transcription, a Case Study\\u00a7r\\n\\n\\u00a78\\u00a7oRainer Kelz\\nSebastian B\\u00f6ck\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.04390\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Feb 2019 14:00:59 GMT)\\u00a7r"}']}
{title:'Jain (§72019§r)', author: 'Royal Jain', display:{Lore:['[{"text": "arXiv:1902.05069", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving performance and inference on audio classification tasks using capsule networks\\u00a7r\\n\\n\\u00a78\\u00a7oRoyal Jain\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.05069\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Feb 2019 08:36:19 GMT)\\u00a7r"}']}
{title:'Jorge et al. (§72019§r)', author: 'Jorge; Davila-Chacon; Jindong; Liu; Stefan; Wermter', display:{Lore:['[{"text": "arXiv:1902.05446", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhanced Robot Speech Recognition Using Biomimetic Binaural Sound Source Localization\\u00a7r\\n\\n\\u00a78\\u00a7oJorge\\nDavila-Chacon\\nJindong\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.05446\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TNNLS.2018.2830119\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Transactions on Neural Networks and Learning Systems (Volume:\\n  30, Issue: 1, Jan. 2019)\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Feb 2019 14:09:11 GMT)\\u00a7r"}']}
{title:'Ribas et al. (§72019§r)', author: 'Dayana Ribas; Emmanuel Vincent', display:{Lore:['[{"text": "arXiv:1902.05761", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn improved uncertainty propagation method for robust i-vector based speaker recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDayana Ribas\\nEmmanuel Vincent\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.05761\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n44th International Conference on Acoustics, Speech, and Signal\\n  Processing (ICASSP 2019), May 2019, Brighton, United Kingdom\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 19 Feb 2019 12:18:14 GMT)\\u00a7r"}']}
{title:'Stoller et al. (§72019§r)', author: 'Daniel Stoller; Simon Durand; Sebastian Ewert', display:{Lore:['[{"text": "arXiv:1902.06797", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end Lyrics Alignment for Polyphonic Music Using an Audio-to-Character Recognition Model\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Stoller\\nSimon Durand\\nSebastian Ewert\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.06797\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Feb 2019 20:53:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages (1 for references), 2 figures,2 tables. Camera-readyversion, accepted at the International Conference on Acoustics, Speech, and Signal Processing 2019(ICASSP)\\u00a7r"}']}
{title:'Rungta et al. (§72019§r)', author: 'Atul Rungta; Nicholas Rewkowski; Roberta Klatzky; Dinesh Manocha', display:{Lore:['[{"text": "arXiv:1902.06880", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lP-Reverb: Perceptual Characterization of Early and Late Reflections for Auditory Displays\\u00a7r\\n\\n\\u00a78\\u00a7oAtul Rungta\\nNicholas Rewkowski\\nRoberta Klatzky\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.06880\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Feb 2019 03:53:30 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72019§r)', author: 'Shanshan Wang; Gaurav Naithani; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:1902.07033", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Latency Deep Clustering For Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oShanshan Wang\\nGaurav Naithani\\nTuomas Virtanen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.07033\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Feb 2019 13:00:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP 2019\\u00a7r"}']}
{title:'Blaauw et al. (§72019§r)', author: 'Merlijn Blaauw; Jordi Bonada; Ryunosuke Daido', display:{Lore:['[{"text": "arXiv:1902.07292", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData Efficient Voice Cloning for Neural Singing Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oMerlijn Blaauw\\nJordi Bonada\\nRyunosuke Daido\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.07292\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Feb 2019 21:31:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2019\\u00a7r"}']}
{title:'Haque et al. (§72019§r)', author: 'Albert Haque; Michelle Guo; Prateek Verma; Li Fei-Fei', display:{Lore:['[{"text": "arXiv:1902.07817", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-Linguistic Embeddings for Spoken Sentences\\u00a7r\\n\\n\\u00a78\\u00a7oAlbert Haque\\nMichelle Guo\\nPrateek Verma\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.07817\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Feb 2019 23:58:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInternational Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2019\\u00a7r"}']}
{title:'Engel et al. (§72019§r)', author: 'Jesse Engel; Kumar Krishna Agrawal; Shuo Chen; Ishaan Gulrajani; Chris Donahue; Adam Roberts', display:{Lore:['[{"text": "arXiv:1902.08710", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGANSynth: Adversarial Neural Audio Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oJesse Engel\\nKumar Krishna Agrawal\\nShuo Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.08710\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 15 Apr 2019 01:37:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oColab Notebook: http://goo.gl/magenta/gansynth-demo\\u00a7r"}']}
{title:'Bjorck et al. (§72019§r)', author: 'Johan Bjorck; Brendan H. Rappazzo; Di Chen; Richard Bernstein; Peter H. Wrege; Carla P. Gomes', display:{Lore:['[{"text": "arXiv:1902.09069", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Detection and Compression for Passive Acoustic Monitoring of the African Forest Elephant\\u00a7r\\n\\n\\u00a78\\u00a7oJohan Bjorck\\nBrendan H. Rappazzo\\nDi Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.09069\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Feb 2019 02:48:54 GMT)\\u00a7r"}']}
{title:'An et al. (§72019§r)', author: 'Inkyu An; Doheon Lee; Byeongho Jo; Jung-Woo Choi; Sung-Eui Yoon', display:{Lore:['[{"text": "arXiv:1902.09179", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Sound Source Localization considering Similarity of Back-Propagation Signals\\u00a7r\\n\\n\\u00a78\\u00a7oInkyu An\\nDoheon Lee\\nByeongho Jo\\nJung-Woo Choi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.09179\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Feb 2019 10:18:22 GMT)\\u00a7r"}']}
{title:'Wu et al. (§72019§r)', author: 'Mengyue Wu; Heinrich Dinkel; Kai Yu', display:{Lore:['[{"text": "arXiv:1902.09254", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Caption: Listen and Tell\\u00a7r\\n\\n\\u00a78\\u00a7oMengyue Wu\\nHeinrich Dinkel\\nKai Yu\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.09254\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8682377\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 31 May 2019 03:56:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by ICASSP2019\\u00a7r"}']}
{title:'Zhang et al. (§72019§r)', author: 'Liwen Zhang; Jiqing Han', display:{Lore:['[{"text": "arXiv:1902.10063", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic scene classification using multi-layer temporal pooling based on convolutional neural network\\u00a7r\\n\\n\\u00a78\\u00a7oLiwen Zhang\\nJiqing Han\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.10063\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 3 Apr 2019 11:10:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o(0) thetitle for this version is inappropriate; (1) the introduction part aboutthe discusses about the handcrafted methods are not precise; (2) the Fig. 1 in section 2 is not correct; (3) the experiments about the CNN "}','{"text": "part are insufficient\\u00a7r"}']}
{title:'Spratley et al. (§72019§r)', author: 'Steven Spratley; Daniel Beck; Trevor Cohn', display:{Lore:['[{"text": "arXiv:1903.00142", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Unified Neural Architecture for Instrumental Audio Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oSteven Spratley\\nDaniel Beck\\nTrevor Cohn\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.00142\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Mar 2019 03:28:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proc. ICASSP 2019, May 12-17, Brighton, UK\\u00a7r"}']}
{title:'Kong et al. (§72019§r)', author: 'Qiuqiang Kong; Changsong Yu; Turab Iqbal; Yong Xu; Wenwu Wang; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:1903.00765", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeakly Labelled AudioSet Tagging with Attention Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oQiuqiang Kong\\nChangsong Yu\\nTurab Iqbal\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.00765\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2019.2930913\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol. 27, no. 11, pp. 1791-1802, Nov. 2019\\u00a7r\\n\\nVersion:\\u00a77v6 (Tue, 10 Dec 2019 15:32:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages\\u00a7r"}']}
{title:'Drugman et al. (§72019§r)', author: 'Thomas Drugman; Goeric Huybrechts; Viacheslav Klimkov; Alexis Moinet', display:{Lore:['[{"text": "arXiv:1903.01290", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTraditional Machine Learning for Pitch Detection\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Drugman\\nGoeric Huybrechts\\nViacheslav Klimkov\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.01290\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2018.2874155\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Signal Processing Letters, Vol. 25, Issue 11, pp. 1745-1749,\\n  2018\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Mar 2019 14:53:26 GMT)\\u00a7r"}']}
{title:'Cohen-Hadria et al. (§72019§r)', author: 'Alice Cohen-Hadria; Axel Roebel; Geoffroy Peeters', display:{Lore:['[{"text": "arXiv:1903.01415", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving singing voice separation using Deep U-Net and Wave-U-Net with data augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oAlice Cohen-Hadria\\nAxel Roebel\\nGeoffroy Peeters\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.01415\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPublished in Proceedings of the 27th European Signal Processing\\n  Conference (EUSIPCO), 2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Mar 2019 18:17:28 GMT)\\u00a7r"}']}
{title:'Jacques et al. (§72019§r)', author: 'Celine Jacques; Axel Roebel', display:{Lore:['[{"text": "arXiv:1903.01416", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData Augmentation for Drum Transcription with Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oCeline Jacques\\nAxel Roebel\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.01416\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPublished in Proceedings of the 27th European Signal Processing\\n  Conference (EUSIPCO), 2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Mar 2019 18:17:29 GMT)\\u00a7r"}']}
{title:'Yela et al. (§72019§r)', author: 'Delia Fano Yela; Dan Stowell; Mark Sandler', display:{Lore:['[{"text": "arXiv:1903.01976", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpectral Visibility Graphs: Application to Similarity of Harmonic Signals\\u00a7r\\n\\n\\u00a78\\u00a7oDelia Fano Yela\\nDan Stowell\\nMark Sandler\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.01976\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 20 Jun 2019 08:47:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oEuropean Signal Processing Conference (EUSIPCO)\\u00a7r"}']}
{title:'Lee et al. (§72019§r)', author: 'Donmoon Lee; Jaejun Lee; Jeongsoo Park; Kyogu Lee', display:{Lore:['[{"text": "arXiv:1903.02794", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Music Features by Knowledge Transfer from User-item Log Data\\u00a7r\\n\\n\\u00a78\\u00a7oDonmoon Lee\\nJaejun Lee\\nJeongsoo Park\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.02794\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Mar 2019 09:51:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, and 1 table. Accepted paper at the International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2019\\u00a7r"}']}
{title:'Drugman et al. (§72019§r)', author: 'Thomas Drugman; Yannis Stylianou; Yusuke Kida; Masami Akamine', display:{Lore:['[{"text": "arXiv:1903.02844", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Activity Detection: Merging Source and Filter-based Information\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Drugman\\nYannis Stylianou\\nYusuke Kida\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.02844\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2015.2495219\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Signal Processing Letters, Volume 23, Issue 2, pp. 252-256,\\n  2015\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Mar 2019 11:26:10 GMT)\\u00a7r"}']}
{title:'Choi et al. (§72019§r)', author: 'Hyeong-Seok Choi; Jang-Hyun Kim; Jaesung Huh; Adrian Kim; Jung-Woo Ha; Kyogu Lee', display:{Lore:['[{"text": "arXiv:1903.03107", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhase-aware Speech Enhancement with Deep Complex U-Net\\u00a7r\\n\\n\\u00a78\\u00a7oHyeong-Seok Choi\\nJang-Hyun Kim\\nJaesung Huh\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.03107\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 2 Apr 2019 08:11:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSignificant errorwas found in data processing step, therefore will be retracted from International Conference on Learning Representations (ICLR)2019. It is not recommended to read current version\\u00a7r"}']}
{title:'Mydlarz et al. (§72019§r)', author: 'Charlie Mydlarz; Mohit Sharma; Yitzchak Lockerman; Ben Steers; Claudio Silva; Juan Pablo Bello', display:{Lore:['[{"text": "arXiv:1903.03195", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe life of a New York City noise sensor network\\u00a7r\\n\\n\\u00a78\\u00a7oCharlie Mydlarz\\nMohit Sharma\\nYitzchak Lockerman\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.03195\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3390/s19061415\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nSensors 2019, 19, 1415\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 26 Mar 2019 14:02:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis article belongs to the Section Intelligent Sensors, 24 pages, 15 figures, 3 tables, 45 references\\u00a7r"}']}
{title:'Sekiguchi et al. (§72019§r)', author: 'Kouhei Sekiguchi; Aditya Arie Nugraha; Yoshiaki Bando; Kazuyoshi Yoshii', display:{Lore:['[{"text": "arXiv:1903.03237", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast Multichannel Source Separation Based on Jointly Diagonalizable Spatial Covariance Matrices\\u00a7r\\n\\n\\u00a78\\u00a7oKouhei Sekiguchi\\nAditya Arie Nugraha\\nYoshiaki Bando\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.03237\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Mar 2019 01:17:23 GMT)\\u00a7r"}']}
{title:'Nugraha et al. (§72019§r)', author: 'Aditya Arie Nugraha; Kouhei Sekiguchi; Kazuyoshi Yoshii', display:{Lore:['[{"text": "arXiv:1903.03269", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Deep Generative Model of Speech Complex Spectrograms\\u00a7r\\n\\n\\u00a78\\u00a7oAditya Arie Nugraha\\nKouhei Sekiguchi\\nKazuyoshi Yoshii\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.03269\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8682797\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Mar 2019 03:57:30 GMT)\\u00a7r"}']}
{title:'Masuyama et al. (§72019§r)', author: 'Yoshiki Masuyama; Kohei Yatabe; Yuma Koizumi; Yasuhiro Oikawa; Noboru Harada', display:{Lore:['[{"text": "arXiv:1903.03971", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Griffin-Lim Iteration\\u00a7r\\n\\n\\u00a78\\u00a7oYoshiki Masuyama\\nKohei Yatabe\\nYuma Koizumi\\nYasuhiro Oikawa\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.03971\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 10 Mar 2019 11:27:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, to appear in IEEE ICASSP 2019 (Paper Code: AASP-L3.1, Session: Source Separation and Speech Enhancement I)\\u00a7r"}']}
{title:'Oza et al. (§72019§r)', author: 'Manan Oza; Himanshu Vaghela; Kriti Srivastava', display:{Lore:['[{"text": "arXiv:1903.04722", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProgressive Generative Adversarial Binary Networks for Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oManan Oza\\nHimanshu Vaghela\\nKriti Srivastava\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.04722\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Mar 2019 04:16:20 GMT)\\u00a7r"}']}
{title:'Chandna et al. (§72019§r)', author: 'Pritish Chandna; Merlijn Blaauw; Jordi Bonada; Emilia Gomez', display:{Lore:['[{"text": "arXiv:1903.07554", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Vocoder Based Method For Singing Voice Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oPritish Chandna\\nMerlijn Blaauw\\nJordi Bonada\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.07554\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8683323\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2019 - 2019 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 23 Apr 2019 16:13:33 GMT)\\u00a7r"}']}
{title:'Roy et al. (§72019§r)', author: 'Pierre Roy; Francois Pachet', display:{Lore:['[{"text": "arXiv:1903.08459", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSmart Edition of MIDI Files\\u00a7r\\n\\n\\u00a78\\u00a7oPierre Roy\\nFrancois Pachet\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.08459\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Mar 2019 11:51:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o20 pages, 16 figures, 14 audio (MIID and MP3) examples\\u00a7r"}']}
{title:'Gibbon (§72019§r)', author: 'Dafydd Gibbon', display:{Lore:['[{"text": "arXiv:1903.08718", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCRAFT: A multifunction online platform for speech prosody visualisation\\u00a7r\\n\\n\\u00a78\\u00a7oDafydd Gibbon\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.08718\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Mar 2019 11:35:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, 2 tables; ICPhS 2019\\u00a7r"}']}
{title:'Harar et al. (§72019§r)', author: 'Pavol Harar; Roswitha Bammer; Anna Breger; Monika Dörfler; Zdenek Smekal', display:{Lore:['[{"text": "arXiv:1903.08950", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Machine Hearing on Limited Data Sets\\u00a7r\\n\\n\\u00a78\\u00a7oPavol Harar\\nRoswitha Bammer\\nAnna Breger\\nMonika D\\u00f6rfler\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.08950\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 12 Jul 2019 13:20:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 3 figures, 2 tables. Repository for reproducibility: https://gitlab.com/hararticles/gs-ms-mt/. Keywords: audio, CNN, limited data, Mel scattering, mel-spectrogram, augmented target loss function. Rewritten and"}','{"text": "restructured after peer revision. Recomputed and added new experiments and visualizations. Changedthe presentation of theresults\\u00a7r"}']}
{title:'Kim et al. (§72019§r)', author: 'Sung Kim; Visvesh Sathe', display:{Lore:['[{"text": "arXiv:1903.09027", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBandwidth Extension on Raw Audio via Generative Adversarial Networks\\u00a7r\\n\\n\\u00a78\\u00a7oSung Kim\\nVisvesh Sathe\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.09027\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Mar 2019 14:32:02 GMT)\\u00a7r"}']}
{title:'Shimada et al. (§72019§r)', author: 'Kazuki Shimada; Yoshiaki Bando; Masato Mimura; Katsutoshi Itoyama; Kazuyoshi Yoshii; Tatsuya Kawahara', display:{Lore:['[{"text": "arXiv:1903.09341", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Speech Enhancement Based on Multichannel NMF-Informed Beamforming for Noise-Robust Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oKazuki Shimada\\nYoshiaki Bando\\nMasato Mimura\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.09341\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2019.2907015\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 31 Mar 2019 14:53:47 GMT)\\u00a7r"}']}
{title:'Shahin (§72019§r)', author: 'Ismail Shahin', display:{Lore:['[{"text": "arXiv:1903.09803", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotion Recognition based on Third-Order Circular Suprasegmental Hidden Markov Model\\u00a7r\\n\\n\\u00a78\\u00a7oIsmail Shahin\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.09803\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 23 Mar 2019 11:24:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at The 2019 IEEE Jordan International Joint Conference on Electrical Engineering and Information Technology (JEEIT), Jordan\\u00a7r"}']}
{title:'Wyse et al. (§72019§r)', author: 'Lonce Wyse; Muhammad Huzaifah', display:{Lore:['[{"text": "arXiv:1903.10703", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConditioning a Recurrent Neural Network to synthesize musical instrument transients\\u00a7r\\n\\n\\u00a78\\u00a7oLonce Wyse\\nMuhammad Huzaifah\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.10703\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Mar 2019 06:33:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSound and Music Computing Conference. Malaga, Spain, May 2019\\u00a7r"}']}
{title:'Chandna et al. (§72019§r)', author: 'Pritish Chandna; Merlijn Blaauw; Jordi Bonada; Emilia Gomez', display:{Lore:['[{"text": "arXiv:1903.10729", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWGANSing: A Multi-Voice Singing Voice Synthesizer Based on the Wasserstein-GAN\\u00a7r\\n\\n\\u00a78\\u00a7oPritish Chandna\\nMerlijn Blaauw\\nJordi Bonada\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.10729\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.23919/EUSIPCO.2019.8903099\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2019 27th European Signal Processing Conference (EUSIPCO)\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 19 Jun 2019 10:35:17 GMT)\\u00a7r"}']}
{title:'Schreiber et al. (§72019§r)', author: 'Hendrik Schreiber; Meinard Müller', display:{Lore:['[{"text": "arXiv:1903.10839", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusical Tempo and Key Estimation using Convolutional Neural Networks with Directional Filters\\u00a7r\\n\\n\\u00a78\\u00a7oHendrik Schreiber\\nMeinard M\\u00fcller\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.10839\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Mar 2019 12:43:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSound Music Computing Conference (SMC), M\\u00e1laga, Spain, May 2019\\u00a7r"}']}
{title:'Jaiswal et al. (§72019§r)', author: 'Mimansa Jaiswal; Zakaria Aldeneh; Cristian-Paul Bara; Yuanhang Luo; Mihai Burzo; Rada Mihalcea; Emily Mower Provost', display:{Lore:['[{"text": "arXiv:1903.11672", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMuSE-ing on the Impact of Utterance Ordering On Crowdsourced Emotion Annotations\\u00a7r\\n\\n\\u00a78\\u00a7oMimansa Jaiswal\\nZakaria Aldeneh\\nCristian-Paul Bara\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.11672\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Mar 2019 19:49:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, ICASSP 2019\\u00a7r"}']}
{title:'He et al. (§72019§r)', author: 'Ke-Xin He; Yu-Han Shen; Wei-Qiang Zhang', display:{Lore:['[{"text": "arXiv:1903.11791", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierarchical Pooling Structure for Weakly Labeled Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oKe-Xin He\\nYu-Han Shen\\nWei-Qiang Zhang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.11791\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 28 Nov 2019 01:41:49 GMT)\\u00a7r"}']}
{title:'Trowitzsch et al. (§72019§r)', author: 'Ivo Trowitzsch; Christopher Schymura; Dorothea Kolossa; Klaus Obermayer', display:{Lore:['[{"text": "arXiv:1904.00055", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoining Sound Event Detection and Localization Through Spatial Segregation\\u00a7r\\n\\n\\u00a78\\u00a7oIvo Trowitzsch\\nChristopher Schymura\\nDorothea Kolossa\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.00055\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2019.2958408\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 21 Dec 2019 20:22:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in IEEE/ACM Transactions on Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Zhang et al. (§72019§r)', author: 'Jingyang Zhang; Wenhao Ding; Jintao Kang; Liang He', display:{Lore:['[{"text": "arXiv:1904.00063", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Scale Time-Frequency Attention for Acoustic Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJingyang Zhang\\nWenhao Ding\\nJintao Kang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.00063\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 9 Sep 2019 07:06:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2019\\u00a7r"}']}
{title:'Swietojanski et al. (§72019§r)', author: 'Pawel Swietojanski; Ondrej Miksik', display:{Lore:['[{"text": "arXiv:1904.00202", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStatic Visual Spatial Priors for DoA Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oPawel Swietojanski\\nOndrej Miksik\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.00202\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 30 Mar 2019 11:34:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 6 figures, 3 tables\\u00a7r"}']}
{title:'Drude et al. (§72019§r)', author: 'Lukas Drude; Jahn Heymann; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:1904.01578", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised training of neural mask-based beamforming\\u00a7r\\n\\n\\u00a78\\u00a7oLukas Drude\\nJahn Heymann\\nReinhold Haeb-Umbach\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.01578\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Apr 2019 12:00:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCorrection to Eq.11: Hermite symbol was on the wrong variable. Replaces y with the normalized version\\u00a7r"}']}
{title:'Vecchiotti et al. (§72019§r)', author: 'Paolo Vecchiotti; Ning Ma; Stefano Squartini; Guy J. Brown', display:{Lore:['[{"text": "arXiv:1904.01916", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end Binaural Sound Localisation from the Raw Waveform\\u00a7r\\n\\n\\u00a78\\u00a7oPaolo Vecchiotti\\nNing Ma\\nStefano Squartini\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.01916\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Apr 2019 11:07:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2019\\u00a7r"}']}
{title:'Scheibler et al. (§72019§r)', author: 'Robin Scheibler; Nobutaka Ono', display:{Lore:['[{"text": "arXiv:1904.02334", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-modal Blind Source Separation with Microphones and Blinkies\\u00a7r\\n\\n\\u00a78\\u00a7oRobin Scheibler\\nNobutaka Ono\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.02334\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8682594\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Apr 2019 03:34:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at IEEEICASSP 2019, Brighton, UK. 5 pages. 3 figures\\u00a7r"}']}
{title:'Zen et al. (§72019§r)', author: 'Heiga Zen; Viet Dang; Rob Clark; Yu Zhang; Ron J. Weiss; Ye Jia; Zhifeng Chen; Yonghui Wu', display:{Lore:['[{"text": "arXiv:1904.02882", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oHeiga Zen\\nViet Dang\\nRob Clark\\n+ 4 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.02882\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Apr 2019 06:05:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted for Interspeech 2019, 7 pages\\u00a7r"}']}
{title:'Tanaka et al. (§72019§r)', author: 'Kou Tanaka; Hirokazu Kameoka; Takuhiro Kaneko; Nobukatsu Hojo', display:{Lore:['[{"text": "arXiv:1904.02892", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWaveCycleGAN2: Time-domain Neural Post-filter for Speech Waveform Generation\\u00a7r\\n\\n\\u00a78\\u00a7oKou Tanaka\\nHirokazu Kameoka\\nTakuhiro Kaneko\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.02892\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 Apr 2019 01:15:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH2019\\u00a7r"}']}
{title:'Takahashi et al. (§72019§r)', author: 'Naoya Takahashi; Sudarsanam Parthasaarathy; Nabarun Goswami; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:1904.03065", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRecursive speech separation for unknown number of speakers\\u00a7r\\n\\n\\u00a78\\u00a7oNaoya Takahashi\\nSudarsanam Parthasaarathy\\nNabarun Goswami\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.03065\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 2 Sep 2019 00:41:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2019 (oral)\\u00a7r"}']}
{title:'Pascual et al. (§72019§r)', author: 'Santiago Pascual; Joan Serrà; Antonio Bonafonte', display:{Lore:['[{"text": "arXiv:1904.03418", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Generalized Speech Enhancement with Generative Adversarial Networks\\u00a7r\\n\\n\\u00a78\\u00a7oSantiago Pascual\\nJoan Serr\\u00e0\\nAntonio Bonafonte\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.03418\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 6 Apr 2019 10:58:17 GMT)\\u00a7r"}']}
{title:'Kong et al. (§72019§r)', author: 'Qiuqiang Kong; Yin Cao; Turab Iqbal; Yong Xu; Wenwu Wang; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:1904.03476", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-task learning for audio tagging, sound event detection and spatial localization: DCASE 2019 baseline systems\\u00a7r\\n\\n\\u00a78\\u00a7oQiuqiang Kong\\nYin Cao\\nTurab Iqbal\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.03476\\u00a7r\\n\\nVersion:\\u00a77v4 (Sun, 9 Jun 2019 04:08:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Liu et al. (§72019§r)', author: 'Yi Liu; Liang He; Jia Liu', display:{Lore:['[{"text": "arXiv:1904.03479", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLarge Margin Softmax Loss for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oYi Liu\\nLiang He\\nJia Liu\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.03479\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 6 Apr 2019 15:53:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to Interspeech 2019. The code and models have been released\\u00a7r"}']}
{title:'Phan et al. (§72019§r)', author: 'Huy Phan; Oliver Y. Chén; Lam Pham; Philipp Koch; Maarten De Vos; Ian McLoughlin; Alfred Mertins', display:{Lore:['[{"text": "arXiv:1904.03543", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatio-Temporal Attention Pooling for Audio Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oHuy Phan\\nOliver Y. Ch\\u00e9n\\nLam Pham\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.03543\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 28 Jun 2019 12:32:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at the 20th Annual Conference of the International Speech Communication Association (INTERSPEECH 2019)\\u00a7r"}']}
{title:'Zhang et al. (§72019§r)', author: 'Yang Zhang; Lantian Li; Dong Wang', display:{Lore:['[{"text": "arXiv:1904.03617", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVAE-based regularization for deep speaker embedding\\u00a7r\\n\\n\\u00a78\\u00a7oYang Zhang\\nLantian Li\\nDong Wang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.03617\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 7 Apr 2019 10:11:58 GMT)\\u00a7r"}']}
{title:'Narisetty et al. (§72019§r)', author: 'Chaitanya Narisetty; Tatsuya Komatsu; Reishi Kondo', display:{Lore:['[{"text": "arXiv:1904.03787", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBayesian Non-Parametric Multi-Source Modelling Based Determined Blind Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oChaitanya Narisetty\\nTatsuya Komatsu\\nReishi Kondo\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.03787\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Apr 2019 00:39:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures. Accepted at ICASSP 2019\\u00a7r"}']}
{title:'Choi et al. (§72019§r)', author: 'Seungwoo Choi; Seokjun Seo; Beomjun Shin; Hyeongmin Byun; Martin Kersner; Beomsu Kim; Dongyoung Kim; Sungjoo Ha', display:{Lore:['[{"text": "arXiv:1904.03814", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTemporal Convolution for Real-time Keyword Spotting on Mobile Devices\\u00a7r\\n\\n\\u00a78\\u00a7oSeungwoo Choi\\nSeokjun Seo\\nBeomjun Shin\\n+ 4 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.03814\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 18 Nov 2019 06:16:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn INTERSPEECH 2019\\u00a7r"}']}
{title:'Kameoka et al. (§72019§r)', author: 'Hirokazu Kameoka; Kou Tanaka; Aaron Valero Puche; Yasunori Ohishi; Takuhiro Kaneko', display:{Lore:['[{"text": "arXiv:1904.04540", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCrossmodal Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oHirokazu Kameoka\\nKou Tanaka\\nAaron Valero Puche\\nYasunori Ohishi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.04540\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Apr 2019 08:53:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech2019\\u00a7r"}']}
{title:'Kaneko et al. (§72019§r)', author: 'Takuhiro Kaneko; Hirokazu Kameoka; Kou Tanaka; Nobukatsu Hojo', display:{Lore:['[{"text": "arXiv:1904.04631", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCycleGAN-VC2: Improved CycleGAN-based Non-parallel Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oTakuhiro Kaneko\\nHirokazu Kameoka\\nKou Tanaka\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.04631\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Apr 2019 12:55:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2019. Project page: http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/cyclegan-vc2/index.html\\u00a7r"}']}
{title:'Zhang et al. (§72019§r)', author: 'Wei Zhang; Xiaodong Cui; Ulrich Finkler; Brian Kingsbury; George Saon; David Kung; Michael Picheny', display:{Lore:['[{"text": "arXiv:1904.04956", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDistributed Deep Learning Strategies For Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oWei Zhang\\nXiaodong Cui\\nUlrich Finkler\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.04956\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Apr 2019 01:00:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in ICASSP\'19\\u00a7r"}']}
{title:'Martin et al. (§72019§r)', author: 'Charles P Martin; Jim Torresen', display:{Lore:['[{"text": "arXiv:1904.05009", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Interactive Musical Prediction System with Mixture Density Recurrent Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oCharles P Martin\\nJim Torresen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.05009\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Apr 2019 05:50:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for presentation at the International Conference on New Interfaces for Musical Expression (NIME), June 2019\\u00a7r"}']}
{title:'Verma et al. (§72019§r)', author: 'Prateek Verma; Chris Chafe; Jonathan Berger', display:{Lore:['[{"text": "arXiv:1904.05073", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeuralogram: A Deep Neural Network Based Representation for Audio Signals\\u00a7r\\n\\n\\u00a78\\u00a7oPrateek Verma\\nChris Chafe\\nJonathan Berger\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.05073\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Apr 2019 09:04:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to DAFx 2019, the 22nd International Conference on Digital Audio Effects, Birmingham, United Kingdom\\u00a7r"}']}
{title:'Cuesta et al. (§72019§r)', author: 'Helena Cuesta; Emilia Gómez; Pritish Chandna', display:{Lore:['[{"text": "arXiv:1904.05086", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Framework for Multi-f0 Modeling in SATB Choir Recordings\\u00a7r\\n\\n\\u00a78\\u00a7oHelena Cuesta\\nEmilia G\\u00f3mez\\nPritish Chandna\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.05086\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Apr 2019 09:35:50 GMT)\\u00a7r"}']}
{title:'Song et al. (§72019§r)', author: 'Hongwei Song; Jiqing Han; Shiwen Deng; Zhihao Du', display:{Lore:['[{"text": "arXiv:1904.05204", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic Scene Classification by Implicitly Identifying Distinct Sound Events\\u00a7r\\n\\n\\u00a78\\u00a7oHongwei Song\\nJiqing Han\\nShiwen Deng\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.05204\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2019-2231\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2019, 3860-3864\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 27 Apr 2019 02:49:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ocode URL typo, code is available at https://github.com/hackerekcah/distinct-events-asc.git\\u00a7r"}']}
{title:'Song et al. (§72019§r)', author: 'Hongwei Song; Jiqing Han; Shiwen Deng', display:{Lore:['[{"text": "arXiv:1904.05243", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Compact and Discriminative Feature Based on Auditory Summary Statistics for Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oHongwei Song\\nJiqing Han\\nShiwen Deng\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.05243\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2018-1299\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nin Proceedings of the Annual Conference of the International\\n  Speech Communication Association, INTERSPEECH, vol. 2018-September, 2018, pp.\\n  3294-3298\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Apr 2019 15:31:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted as a conference paper of Interspeech 2018\\u00a7r"}']}
{title:'Li et al. (§72019§r)', author: 'Xiaofei Li; Laurent Girin; Radu Horaud', display:{Lore:['[{"text": "arXiv:1904.05249", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExpectation-Maximization for Speech Source Separation Using Convolutive Transfer Function\\u00a7r\\n\\n\\u00a78\\u00a7oXiaofei Li\\nLaurent Girin\\nRadu Horaud\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.05249\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1049/trit.2018.1061\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nCAAI Transactions on Intelligent Technologies, 2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Apr 2019 15:38:43 GMT)\\u00a7r"}']}
{title:'Gosztolya et al. (§72019§r)', author: 'Gábor Gosztolya; Ádám Pintér; László Tóth; Tamás Grósz; Alexandra Markó; Tamás Gábor Csapó', display:{Lore:['[{"text": "arXiv:1904.05259", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutoencoder-Based Articulatory-to-Acoustic Mapping for Ultrasound Silent Speech Interfaces\\u00a7r\\n\\n\\u00a78\\u00a7oG\\u00e1bor Gosztolya\\n\\u00c1d\\u00e1m Pint\\u00e9r\\nL\\u00e1szl\\u00f3 T\\u00f3th\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.05259\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Apr 2019 15:57:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 6 figures, Accepted to IJCNN 2019\\u00a7r"}']}
{title:'Lavrentyeva et al. (§72019§r)', author: 'Galina Lavrentyeva; Sergey Novoselov; Andzhukaev Tseren; Marina Volkova; Artem Gorlanov; Alexandr Kozlov', display:{Lore:['[{"text": "arXiv:1904.05576", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSTC Antispoofing Systems for the ASVspoof2019 Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oGalina Lavrentyeva\\nSergey Novoselov\\nAndzhukaev Tseren\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.05576\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Apr 2019 08:37:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2019, Graz, Austria\\u00a7r"}']}
{title:'Kong et al. (§72019§r)', author: 'Qiuqiang Kong; Yin Cao; Turab Iqbal; Yong Xu; Wenwu Wang; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:1904.05635", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-task learning for audio tagging, sound event detection spatial localization: DCASE 2019 baseline systems\\u00a7r\\n\\n\\u00a78\\u00a7oQiuqiang Kong\\nYin Cao\\nTurab Iqbal\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.05635\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 14 Apr 2019 08:39:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWe want to replace but create this submission by mistake. See arXiv:1904.03476 instead\\u00a7r"}']}
{title:'Al-Radhi et al. (§72019§r)', author: 'Mohammed Salah Al-Radhi; Tamás Gábor Csapó; Géza Németh', display:{Lore:['[{"text": "arXiv:1904.06075", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRNN-based speech synthesis using a continuous sinusoidal model\\u00a7r\\n\\n\\u00a78\\u00a7oMohammed Salah Al-Radhi\\nTam\\u00e1s G\\u00e1bor Csap\\u00f3\\nG\\u00e9za N\\u00e9meth\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.06075\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Apr 2019 07:23:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 4 figures, Accepted to IJCNN 2019\\u00a7r"}']}
{title:'Porras et al. (§72019§r)', author: 'Dagoberto Porras; Alexander Sepúlveda-Sepúlveda; Tamás Gábor Csapó', display:{Lore:['[{"text": "arXiv:1904.06083", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.TO\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDNN-based Acoustic-to-Articulatory Inversion using Ultrasound Tongue Imaging\\u00a7r\\n\\n\\u00a78\\u00a7oDagoberto Porras\\nAlexander Sep\\u00falveda-Sep\\u00falveda\\nTam\\u00e1s G\\u00e1bor Csap\\u00f3\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.06083\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Apr 2019 07:42:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 5 figures, Accepted to IJCNN 2019\\u00a7r"}']}
{title:'Novoselov et al. (§72019§r)', author: 'Sergey Novoselov; Aleksei Gusev; Artem Ivanov; Timur Pekhovsky; Andrey Shulipa; Galina Lavrentyeva; Vladimir Volokhov; Alexandr Kozlov', display:{Lore:['[{"text": "arXiv:1904.06093", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSTC Speaker Recognition Systems for the VOiCES From a Distance Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oSergey Novoselov\\nAleksei Gusev\\nArtem Ivanov\\n+ 4 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.06093\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Apr 2019 08:23:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2019, Graz, Austria\\u00a7r"}']}
{title:'Bitton et al. (§72019§r)', author: 'Adrien Bitton; Philippe Esling; Antoine Caillon; Martin Fouilleul', display:{Lore:['[{"text": "arXiv:1904.06215", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAssisted Sound Sample Generation with Musical Conditioning in Adversarial Auto-Encoders\\u00a7r\\n\\n\\u00a78\\u00a7oAdrien Bitton\\nPhilippe Esling\\nAntoine Caillon\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.06215\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 22 Jun 2019 13:53:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7othis article has been accepted for presentation to the 22nd International Conference on Digital Audio Effects (DAFx 2019) ; we provide additional content on this companion repository https://github.com/acids-ircam/Expressive_WAE_FADER\\u00a7"}','{"text": "r"}']}
{title:'Yang et al. (§72019§r)', author: 'Gene-Ping Yang; Chao-I Tuan; Hung-Yi Lee; Lin-shan Lee', display:{Lore:['[{"text": "arXiv:1904.07845", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Speech Separation with Time-and-Frequency Cross-domain Joint Embedding and Clustering\\u00a7r\\n\\n\\u00a78\\u00a7oGene-Ping Yang\\nChao-I Tuan\\nHung-Yi Lee\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.07845\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Apr 2019 17:48:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2019\\u00a7r"}']}
{title:'Neekhara et al. (§72019§r)', author: 'Paarth Neekhara; Chris Donahue; Miller Puckette; Shlomo Dubnov; Julian McAuley', display:{Lore:['[{"text": "arXiv:1904.07944", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExpediting TTS Synthesis with Adversarial Vocoding\\u00a7r\\n\\n\\u00a78\\u00a7oPaarth Neekhara\\nChris Donahue\\nMiller Puckette\\nShlomo Dubnov\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.07944\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 26 Jul 2019 03:36:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished as a conference paper at INTERSPEECH 2019\\u00a7r"}']}
{title:'Xue et al. (§72019§r)', author: 'Jiabin Xue; Jiqing Han; Tieran Zheng; Jiaxing Guo; Boyong Wu', display:{Lore:['[{"text": "arXiv:1904.08031", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHard Sample Mining for the Improved Retraining of Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJiabin Xue\\nJiqing Han\\nTieran Zheng\\nJiaxing Guo\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.08031\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Apr 2019 00:39:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2019;\\u00a7r"}']}
{title:'Xue et al. (§72019§r)', author: 'Jiabin Xue; Jiqing Han; Tieran Zheng; Xiang Gao; Jiaxing Guo', display:{Lore:['[{"text": "arXiv:1904.08039", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Multi-Task Learning Framework for Overcoming the Catastrophic Forgetting in Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJiabin Xue\\nJiqing Han\\nTieran Zheng\\nXiang Gao\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.08039\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Apr 2019 00:55:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2019;\\u00a7r"}']}
{title:'Mack et al. (§72019§r)', author: 'Wolfgang Mack; Emanuël A. P. Habets', display:{Lore:['[{"text": "arXiv:1904.08369", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Filtering: Signal Extraction and Reconstruction Using Complex Time-Frequency Filters\\u00a7r\\n\\n\\u00a78\\u00a7oWolfgang Mack\\nEmanu\\u00ebl A. P. Habets\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.08369\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2019.2955818\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 9 Dec 2019 09:34:04 GMT)\\u00a7r"}']}
{title:'Tang et al. (§72019§r)', author: 'Zhenyu Tang; John D. Kanu; Kevin Hogan; Dinesh Manocha', display:{Lore:['[{"text": "arXiv:1904.08452", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRegression and Classification for Direction-of-Arrival Estimation with Convolutional Recurrent Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oZhenyu Tang\\nJohn D. Kanu\\nKevin Hogan\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.08452\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2019-1111\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2019, 654-658\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 10 Jul 2019 03:48:07 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72019§r)', author: 'Ruihan Yang; Tianyao Chen; Yiyi Zhang; Gus Xia', display:{Lore:['[{"text": "arXiv:1904.08842", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInspecting and Interacting with Meaningful Music Representations using VAE\\u00a7r\\n\\n\\u00a78\\u00a7oRuihan Yang\\nTianyao Chen\\nYiyi Zhang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.08842\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Apr 2019 15:22:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for poster at the International Conference on New Interfaces for Musical Expression (NIME), June 2019\\u00a7r"}']}
{title:'Chhetri et al. (§72019§r)', author: 'Amit Chhetri; Mohamed Mansour; Wontak Kim; Guangdong Pan', display:{Lore:['[{"text": "arXiv:1904.08971", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Acoustic Modeling for Broadband Beamforming\\u00a7r\\n\\n\\u00a78\\u00a7oAmit Chhetri\\nMohamed Mansour\\nWontak Kim\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.08971\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nEuropean Signal Processing Conference (EUSIPCO 2019)\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Apr 2019 18:38:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, conference\\u00a7r"}']}
{title:'Abdoli et al. (§72019§r)', author: 'Sajjad Abdoli; Patrick Cardinal; Alessandro Lameiras Koerich', display:{Lore:['[{"text": "arXiv:1904.08990", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Environmental Sound Classification using a 1D Convolutional Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oSajjad Abdoli\\nPatrick Cardinal\\nAlessandro Lameiras Koerich\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.08990\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Apr 2019 20:07:03 GMT)\\u00a7r"}']}
{title:'Rengaswamy et al. (§72019§r)', author: 'Pradeep Rengaswamy; Gurunath Reddy M; Krothapalli Sreenivasa Rao', display:{Lore:['[{"text": "arXiv:1904.09765", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lhf0: A hybrid pitch extraction method for multimodal voice\\u00a7r\\n\\n\\u00a78\\u00a7oPradeep Rengaswamy\\nGurunath Reddy M\\nKrothapalli Sreenivasa Rao\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.09765\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Apr 2019 08:08:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPitch Extraction, F0 extraction, harmonic signals, speech, monophonic songs, Convolutional Neural Network, 5 pages, 5 figures\\u00a7r"}']}
{title:'Huang et al. (§72019§r)', author: 'Feng Huang; Peter Balazs', display:{Lore:['[{"text": "arXiv:1904.10380", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a72math.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHarmonic-aligned Frame Mask Based on Non-stationary Gabor Transform with Application to Content-dependent Speaker Comparison\\u00a7r\\n\\n\\u00a78\\u00a7oFeng Huang\\nPeter Balazs\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.10380\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Apr 2019 15:21:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech2019\\u00a7r"}']}
{title:'Parthasarathi et al. (§72019§r)', author: 'Sree Hari Krishnan Parthasarathi; Nitin Sivakrishnan; Pranav Ladkat; Nikko Strom', display:{Lore:['[{"text": "arXiv:1904.10584", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRealizing Petabyte Scale Acoustic Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oSree Hari Krishnan Parthasarathi\\nNitin Sivakrishnan\\nPranav Ladkat\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.10584\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JETCAS.2019.2912353\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Apr 2019 00:39:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2156-3357 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications standards/publications/rights/index.html for more information\\u00a7r"}']}
{title:'Drossos et al. (§72019§r)', author: 'Konstantinos Drossos; Paul Magron; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:1904.10678", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Adversarial Domain Adaptation Based On The Wasserstein Distance For Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oKonstantinos Drossos\\nPaul Magron\\nTuomas Virtanen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.10678\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Nov 2019 11:56:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUpdated indices at Eq 6\\u00a7r"}']}
{title:'Arronte-Alvarez et al. (§72019§r)', author: 'Aitor Arronte-Alvarez; Francisco Gomez-Martin', display:{Lore:['[{"text": "arXiv:1904.11074", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Attentional Neural Network Architecture for Folk Song Classification\\u00a7r\\n\\n\\u00a78\\u00a7oAitor Arronte-Alvarez\\nFrancisco Gomez-Martin\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.11074\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Apr 2019 21:27:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICMC 2019\\u00a7r"}']}
{title:'Liu et al. (§72019§r)', author: 'Yuzhou Liu; DeLiang Wang', display:{Lore:['[{"text": "arXiv:1904.11148", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDivide and Conquer: A Deep CASA Approach to Talker-independent Monaural Speaker Separation\\u00a7r\\n\\n\\u00a78\\u00a7oYuzhou Liu\\nDeLiang Wang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.11148\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 Apr 2019 03:57:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 5 figures\\u00a7r"}']}
{title:'Senoussaoui et al. (§72019§r)', author: 'Mohammed Senoussaoui; Patrick Cardinal; Najim Dehak; Alessandro Lameiras Koerich', display:{Lore:['[{"text": "arXiv:1904.11641", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Sincerity Detection based on Covariance Feature Vectors and Ensemble Methods\\u00a7r\\n\\n\\u00a78\\u00a7oMohammed Senoussaoui\\nPatrick Cardinal\\nNajim Dehak\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.11641\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Apr 2019 01:42:41 GMT)\\u00a7r"}']}
{title:'Hou et al. (§72019§r)', author: 'Yuanbo Hou; Qiuqiang Kong; Shengchen Li; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:1904.12102", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Event Detection with Sequentially Labelled Data Based on Connectionist Temporal Classification and Unsupervised Clustering\\u00a7r\\n\\n\\u00a78\\u00a7oYuanbo Hou\\nQiuqiang Kong\\nShengchen Li\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.12102\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8683627\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2019 - 2019 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 27 Apr 2019 03:54:24 GMT)\\u00a7r"}']}
{title:'Tonami et al. (§72019§r)', author: 'Noriyuki Tonami; Keisuke Imoto; Masahiro Niitsuma; Ryosuke Yamanishi; Yoichi Yamashita', display:{Lore:['[{"text": "arXiv:1904.12146", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Analysis of Acoustic Events and Scenes Based on Multitask Learning\\u00a7r\\n\\n\\u00a78\\u00a7oNoriyuki Tonami\\nKeisuke Imoto\\nMasahiro Niitsuma\\nRyosuke Yamanishi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.12146\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 19 Jul 2019 00:08:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to WASPAA 2019\\u00a7r"}']}
{title:'Deolekar et al. (§72019§r)', author: 'Subodh Deolekar; Siby Abraham', display:{Lore:['[{"text": "arXiv:1904.12194", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Automation of Creativity: A Machine Intelligence Approach\\u00a7r\\n\\n\\u00a78\\u00a7oSubodh Deolekar\\nSiby Abraham\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.12194\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 27 Apr 2019 18:54:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o31 pages, 24 figures, 12 tables\\u00a7r"}']}
{title:'Teyhouee et al. (§72019§r)', author: 'Aydin Teyhouee; Nathaniel D. Osgood', display:{Lore:['[{"text": "arXiv:1904.12354", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCough Detection Using Hidden Markov Models\\u00a7r\\n\\n\\u00a78\\u00a7oAydin Teyhouee\\nNathaniel D. Osgood\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.12354\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 28 Apr 2019 17:47:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSBP-BRiMS 2019\\u00a7r"}']}
{title:'Kim et al. (§72019§r)', author: 'Joshua Y. Kim; Chunfeng Liu; Rafael A. Calvo; Kathryn McCabe; Silas C. R. Taylor; Björn W. Schuller; Kaihang Wu', display:{Lore:['[{"text": "arXiv:1904.12403", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparison of Online Automatic Speech Recognition Systems and the Nonverbal Responses to Unintelligible Speech\\u00a7r\\n\\n\\u00a78\\u00a7oJoshua Y. Kim\\nChunfeng Liu\\nRafael A. Calvo\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.12403\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 Apr 2019 00:00:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 2 figures\\u00a7r"}']}
{title:'Meng et al. (§72019§r)', author: 'Zhong Meng; Yong Zhao; Jinyu Li; Yifan Gong', display:{Lore:['[{"text": "arXiv:1904.12406", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oZhong Meng\\nYong Zhao\\nJinyu Li\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.12406\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8682488\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2019 IEEE International Conference on Acoustics, Speech and Signal\\n  Processing (ICASSP), Brighton, United Kingdom\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 Apr 2019 00:37:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, ICASSP 2019\\u00a7r"}']}
{title:'Adavanne et al. (§72019§r)', author: 'Sharath Adavanne; Archontis Politis; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:1904.12769", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLocalization, Detection and Tracking of Multiple Moving Sound Sources with a Convolutional Recurrent Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oSharath Adavanne\\nArchontis Politis\\nTuomas Virtanen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.12769\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 Apr 2019 15:26:47 GMT)\\u00a7r"}']}
{title:'Castro (§72019§r)', author: 'Pablo Samuel Castro', display:{Lore:['[{"text": "arXiv:1904.13285", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerforming Structured Improvisations with pre-trained Deep Learning Models\\u00a7r\\n\\n\\u00a78\\u00a7oPablo Samuel Castro\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.13285\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Apr 2019 14:50:12 GMT)\\u00a7r"}']}
{title:'Purwins et al. (§72019§r)', author: 'Hendrik Purwins; Bo Li; Tuomas Virtanen; Jan Schlüter; Shuo-yiin Chang; Tara Sainath', display:{Lore:['[{"text": "arXiv:1905.00078", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning for Audio Signal Processing\\u00a7r\\n\\n\\u00a78\\u00a7oHendrik Purwins\\nBo Li\\nTuomas Virtanen\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.00078\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JSTSP.2019.2908700\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJournal of Selected Topics of Signal Processing 14, No. 8 (2019)\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 25 May 2019 09:02:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 2 pdf figures\\u00a7r"}']}
{title:'Venkataramani et al. (§72019§r)', author: 'Shrikant Venkataramani; Efthymios Tzinis; Paris Smaragdis', display:{Lore:['[{"text": "arXiv:1905.00151", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Style Transfer Approach to Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oShrikant Venkataramani\\nEfthymios Tzinis\\nParis Smaragdis\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.00151\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 9 May 2019 15:46:38 GMT)\\u00a7r"}']}
{title:'Cao et al. (§72019§r)', author: 'Yin Cao; Qiuqiang Kong; Turab Iqbal; Fengyan An; Wenwu Wang; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:1905.00268", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPolyphonic Sound Event Detection and Localization using a Two-Stage Strategy\\u00a7r\\n\\n\\u00a78\\u00a7oYin Cao\\nQiuqiang Kong\\nTurab Iqbal\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.00268\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.33682/4jhy-bj81\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 5 Nov 2019 10:18:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 2 figures, conference\\u00a7r"}']}
{title:'Pariente et al. (§72019§r)', author: 'Manuel Pariente; Antoine Deleforge; Emmanuel Vincent', display:{Lore:['[{"text": "arXiv:1905.01209", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Statistically Principled and Computationally Efficient Approach to Speech Enhancement using Variational Autoencoders\\u00a7r\\n\\n\\u00a78\\u00a7oManuel Pariente\\nAntoine Deleforge\\nEmmanuel Vincent\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.01209\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 14 May 2019 08:30:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2019\\u00a7r"}']}
{title:'Casebeer et al. (§72019§r)', author: 'Jonah Casebeer; Michael Colomb; Paris Smaragdis', display:{Lore:['[{"text": "arXiv:1905.01391", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Tensor Factorization for Spatially-Aware Scene Decomposition\\u00a7r\\n\\n\\u00a78\\u00a7oJonah Casebeer\\nMichael Colomb\\nParis Smaragdis\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.01391\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 26 Sep 2019 23:39:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, accepted to WASPAA 2019\\u00a7r"}']}
{title:'Nardelli (§72019§r)', author: 'Marco Buongiorno Nardelli', display:{Lore:['[{"text": "arXiv:1905.01842", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTopology of Networks in Generalized Musical Spaces\\u00a7r\\n\\n\\u00a78\\u00a7oMarco Buongiorno Nardelli\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.01842\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 May 2019 06:59:24 GMT)\\u00a7r"}']}
{title:'Fu et al. (§72019§r)', author: 'Szu-Wei Fu; Chien-Feng Liao; Yu Tsao', display:{Lore:['[{"text": "arXiv:1905.01898", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning with Learned Loss Function: Speech Enhancement with Quality-Net to Improve Perceptual Evaluation of Speech Quality\\u00a7r\\n\\n\\u00a78\\u00a7oSzu-Wei Fu\\nChien-Feng Liao\\nYu Tsao\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.01898\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2019.2953810\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 14 Nov 2019 06:20:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE Signal Processing Letters (SPL)\\u00a7r"}']}
{title:'Lordelo et al. (§72019§r)', author: 'Carlos Lordelo; Emmanouil Benetos; Simon Dixon; Sven Ahlbäck', display:{Lore:['[{"text": "arXiv:1905.01899", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating kernel shapes and skip connections for deep learning-based harmonic-percussive separation\\u00a7r\\n\\n\\u00a78\\u00a7oCarlos Lordelo\\nEmmanouil Benetos\\nSimon Dixon\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.01899\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 Jul 2019 13:45:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at WASPAA 2019, 5 pages, 5 figures\\u00a7r"}']}
{title:'Levin (§72019§r)', author: 'David N. Levin', display:{Lore:['[{"text": "arXiv:1905.03278", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ME\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the representation of speech and music\\u00a7r\\n\\n\\u00a78\\u00a7oDavid N. Levin\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.03278\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 8 May 2019 18:11:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 6 figures\\u00a7r"}']}
{title:'Kavalerov et al. (§72019§r)', author: 'Ilya Kavalerov; Scott Wisdom; Hakan Erdogan; Brian Patton; Kevin Wilson; Jonathan Le Roux; John R. Hershey', display:{Lore:['[{"text": "arXiv:1905.03330", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUniversal Sound Separation\\u00a7r\\n\\n\\u00a78\\u00a7oIlya Kavalerov\\nScott Wisdom\\nHakan Erdogan\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.03330\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 2 Aug 2019 20:44:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted to WASPAA 2019\\u00a7r"}']}
{title:'Menne et al. (§72019§r)', author: 'Tobias Menne; Ilya Sklyar; Ralf Schlüter; Hermann Ney', display:{Lore:['[{"text": "arXiv:1905.03500", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalysis of Deep Clustering as Preprocessing for Automatic Speech Recognition of Sparsely Overlapping Speech\\u00a7r\\n\\n\\u00a78\\u00a7oTobias Menne\\nIlya Sklyar\\nRalf Schl\\u00fcter\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.03500\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2019-1728\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of INTERSPEECH 2019\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 25 Sep 2019 12:07:45 GMT)\\u00a7r"}']}
{title:'Malek et al. (§72019§r)', author: 'Jiri Malek; Zbynek Koldovsky; Marek Bohac', display:{Lore:['[{"text": "arXiv:1905.03632", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.SY\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBlock-Online Multi-Channel Speech Enhancement Using DNN-Supported Relative Transfer Function Estimates\\u00a7r\\n\\n\\u00a78\\u00a7oJiri Malek\\nZbynek Koldovsky\\nMarek Bohac\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.03632\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1049/iet-spr.2019.0304\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIET Signal Processing, vol. 14, no. 3, pp. 124-133, May 2020\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 11 Dec 2019 15:35:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 8 figures, 4 tables. Modified versionof the article accepted for publication in IET Signal Processing journal. Original results unchanged, additional experiments presented, refined discussion and conclusions\\u00a7r"}','{"text": ""}']}
{title:'Caracalla et al. (§72019§r)', author: 'Hugo Caracalla; Axel Roebel', display:{Lore:['[{"text": "arXiv:1905.03637", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound texture synthesis using convolutional neural networks\\u00a7r\\n\\n\\u00a78\\u00a7oHugo Caracalla\\nAxel Roebel\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.03637\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 May 2019 13:51:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to Digital Audio Conference (DAFx 2019)\\u00a7r"}']}
{title:'Revay et al. (§72019§r)', author: 'Shauna Revay; Matthew Teschke', display:{Lore:['[{"text": "arXiv:1905.04348", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulticlass Language Identification using Deep Learning on Spectral Images of Audio Signals\\u00a7r\\n\\n\\u00a78\\u00a7oShauna Revay\\nMatthew Teschke\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.04348\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 10 May 2019 19:15:59 GMT)\\u00a7r"}']}
{title:'Sarkar et al. (§72019§r)', author: 'Achintya kr. Sarkar; Zheng-Hua Tan; Hao Tang; Suwon Shon; James Glass', display:{Lore:['[{"text": "arXiv:1905.04554", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTime-Contrastive Learning Based Deep Bottleneck Features for Text-Dependent Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oAchintya kr. Sarkar\\nZheng-Hua Tan\\nHao Tang\\nSuwon Shon\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.04554\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2019.2915322\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 11 May 2019 17:20:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCopyright (c) 2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for "}','{"text": "advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Fu et al. (§72019§r)', author: 'Szu-Wei Fu; Chien-Feng Liao; Yu Tsao; Shou-De Lin', display:{Lore:['[{"text": "arXiv:1905.04874", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMetricGAN: Generative Adversarial Networks based Black-box Metric Scores Optimization for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oSzu-Wei Fu\\nChien-Feng Liao\\nYu Tsao\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.04874\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 May 2019 06:21:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Thirty-sixth International Conference on MachineLearning (ICML) 2019\\u00a7r"}']}
{title:'Lu et al. (§72019§r)', author: 'Yu-Ding Lu; Hsin-Ying Lee; Hung-Yu Tseng; Ming-Hsuan Yang', display:{Lore:['[{"text": "arXiv:1905.05375", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervised Audio Spatialization with Correspondence Classifier\\u00a7r\\n\\n\\u00a78\\u00a7oYu-Ding Lu\\nHsin-Ying Lee\\nHung-Yu Tseng\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.05375\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 May 2019 03:20:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICIP 2019\\u00a7r"}']}
{title:'Gillick et al. (§72019§r)', author: 'Jon Gillick; Adam Roberts; Jesse Engel; Douglas Eck; David Bamman', display:{Lore:['[{"text": "arXiv:1905.06118", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning to Groove with Inverse Sequence Transformations\\u00a7r\\n\\n\\u00a78\\u00a7oJon Gillick\\nAdam Roberts\\nJesse Engel\\nDouglas Eck\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.06118\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 36th International Conference on Machine\\n  Learning, PMLR 97:2269-2279, 2019\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 26 Jul 2019 17:24:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oBlog post and links: https://g.co/magenta/groovae\\u00a7r"}']}
{title:'Gu et al. (§72019§r)', author: 'Rongzhi Gu; Jian Wu; Shi-Xiong Zhang; Lianwu Chen; Yong Xu; Meng Yu; Dan Su; Yuexian Zou; Dong Yu', display:{Lore:['[{"text": "arXiv:1905.06286", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Multi-Channel Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oRongzhi Gu\\nJian Wu\\nShi-Xiong Zhang\\n+ 5 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.06286\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 28 May 2019 02:02:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to interspeech 2019\\u00a7r"}']}
{title:'Favory et al. (§72019§r)', author: 'Xavier Favory; Xavier Serra', display:{Lore:['[{"text": "arXiv:1905.06717", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti Web Audio Sequencer: Collaborative Music Making\\u00a7r\\n\\n\\u00a78\\u00a7oXavier Favory\\nXavier Serra\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.06717\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 May 2019 13:10:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 4 figures, short paper of the Web Audio Conference 2018\\u00a7r"}']}
{title:'Bahmaninezhad et al. (§72019§r)', author: 'Fahimeh Bahmaninezhad; Jian Wu; Rongzhi Gu; Shi-Xiong Zhang; Yong Xu; Meng Yu; Dong Yu', display:{Lore:['[{"text": "arXiv:1905.07497", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA comprehensive study of speech separation: spectrogram vs waveform separation\\u00a7r\\n\\n\\u00a78\\u00a7oFahimeh Bahmaninezhad\\nJian Wu\\nRongzhi Gu\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.07497\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 23 Jul 2019 20:35:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2019\\u00a7r"}']}
{title:'Scheibler et al. (§72019§r)', author: 'Robin Scheibler; Nobutaka Ono', display:{Lore:['[{"text": "arXiv:1905.07880", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIndependent Vector Analysis with more Microphones than Sources\\u00a7r\\n\\n\\u00a78\\u00a7oRobin Scheibler\\nNobutaka Ono\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.07880\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 7 Aug 2019 06:28:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to WASPAA 2019, 5 pages, 3figures\\u00a7r"}']}
{title:'herremans et al. (§72019§r)', author: 'Dorien herremans; David Martens; Kenneth Sörensen', display:{Lore:['[{"text": "arXiv:1905.08076", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDance Hit Song Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oDorien herremans\\nDavid Martens\\nKenneth S\\u00f6rensen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.08076\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1080/09298215.2014.881888\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJournal of New music Research. 43:302 (2014)\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 17 May 2019 17:01:10 GMT)\\u00a7r"}']}
{title:'Lostanlen et al. (§72019§r)', author: 'Vincent Lostanlen; Justin Salamon; Andrew Farnsworth; Steve Kelling; Juan Pablo Bello', display:{Lore:['[{"text": "arXiv:1905.08352", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust sound event detection in bioacoustic sensor networks\\u00a7r\\n\\n\\u00a78\\u00a7oVincent Lostanlen\\nJustin Salamon\\nAndrew Farnsworth\\nSteve Kelling\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.08352\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1371/journal.pone.0214168\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 30 Oct 2019 01:31:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o32 pages, in English. Submitted to PLOS ONE journal in February 2019; revised August 2019; published October 2019\\u00a7r"}']}
{title:'Adavanne et al. (§72019§r)', author: 'Sharath Adavanne; Archontis Politis; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:1905.08546", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA multi-room reverberant dataset for sound event localization and detection\\u00a7r\\n\\n\\u00a78\\u00a7oSharath Adavanne\\nArchontis Politis\\nTuomas Virtanen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.08546\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 24 May 2019 07:33:42 GMT)\\u00a7r"}']}
{title:'Shi et al. (§72019§r)', author: 'Liming Shi; Jesper Kjaer Nielsen; Jesper Rindom Jensen; Max A. Little; Mads Graesboll Christensen', display:{Lore:['[{"text": "arXiv:1905.08557", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBayesian Pitch Tracking Based on the Harmonic Model\\u00a7r\\n\\n\\u00a78\\u00a7oLiming Shi\\nJesper Kjaer Nielsen\\nJesper Rindom Jensen\\nMax A. Little\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.08557\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 May 2019 11:24:16 GMT)\\u00a7r"}']}
{title:'Lostanlen (§72019§r)', author: 'Vincent Lostanlen', display:{Lore:['[{"text": "arXiv:1905.08601", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUne ou deux composantes ? La r\\u00e9ponse de la diffusion en ondelettes\\u00a7r\\n\\n\\u00a78\\u00a7oVincent Lostanlen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.08601\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 1 Jul 2019 04:05:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, in French. Submitted to the GRETSI workshop\\u00a7r"}']}
{title:'Wen et al. (§72019§r)', author: 'Yandong Wen; Rita Singh; Bhiksha Raj', display:{Lore:['[{"text": "arXiv:1905.10604", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReconstructing faces from voices\\u00a7r\\n\\n\\u00a78\\u00a7oYandong Wen\\nRita Singh\\nBhiksha Raj\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.10604\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 31 May 2019 19:53:01 GMT)\\u00a7r"}']}
{title:'Mobin et al. (§72019§r)', author: 'Shariq Mobin; Bruno Olshausen', display:{Lore:['[{"text": "arXiv:1905.10751", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAuditory Separation of a Conversation from Background via Attentional Gating\\u00a7r\\n\\n\\u00a78\\u00a7oShariq Mobin\\nBruno Olshausen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.10751\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 26 May 2019 07:38:35 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72019§r)', author: 'Yu-Hua Chen; Bryan Wang; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:1905.11689", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDemonstration of PerformanceNet: A Convolutional Neural Network Model for Score-to-Audio Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oYu-Hua Chen\\nBryan Wang\\nYi-Hsuan Yang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.11689\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 May 2019 09:00:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o3 pages, 2 figures, IJCAI Demo 2019 camera-ready version\\u00a7r"}']}
{title:'Sarfati et al. (§72019§r)', author: 'Marc Sarfati; Anthony Hu; Jonathan Donier', display:{Lore:['[{"text": "arXiv:1905.11700", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnsemble-based cover song detection\\u00a7r\\n\\n\\u00a78\\u00a7oMarc Sarfati\\nAnthony Hu\\nJonathan Donier\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.11700\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 May 2019 09:29:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 4 figures, 7 tables\\u00a7r"}']}
{title:'Haunschmid et al. (§72019§r)', author: 'Verena Haunschmid; Shreyan Chowdhury; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:1905.11760", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTwo-level Explanations in Music Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oVerena Haunschmid\\nShreyan Chowdhury\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.11760\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 May 2019 12:08:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oML4MD Workshop of the 36th International Conference on MachineLearning\\u00a7r"}']}
{title:'Foleiss et al. (§72019§r)', author: 'Juliano H. Foleiss; Tiago F. Tavares', display:{Lore:['[{"text": "arXiv:1905.11959", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTexture Selection for Automatic Music Genre Classification\\u00a7r\\n\\n\\u00a78\\u00a7oJuliano H. Foleiss\\nTiago F. Tavares\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.11959\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.asoc.2020.106127\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 May 2019 17:30:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Pattern Recognition (may, 2019)\\u00a7r"}']}
{title:'Muñoz-Montoro et al. (§72019§r)', author: 'A. J. Muñoz-Montoro; P. Vera-Candeas; D. Suarez-Dou; R. Cortina', display:{Lore:['[{"text": "arXiv:1905.12324", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA new definition of the distortion matrix for an audio-to-score alignment system\\u00a7r\\n\\n\\u00a78\\u00a7oA. J. Mu\\u00f1oz-Montoro\\nP. Vera-Candeas\\nD. Suarez-Dou\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.12324\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nComputational and Mathematical Methods, Wiley Online Library. 2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 29 May 2019 11:02:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCMMSE 2019\\u00a7r"}']}
{title:'BT et al. (§72019§r)', author: 'Balamurali BT; Kin Wah Edward Lin; Simon Lui; Jer-Ming Chen; Dorien Herremans', display:{Lore:['[{"text": "arXiv:1905.12439", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards robust audio spoofing detection: a detailed comparison of traditional and learned features\\u00a7r\\n\\n\\u00a78\\u00a7oBalamurali BT\\nKin Wah Edward Lin\\nSimon Lui\\nJer-Ming Chen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.12439\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Access. 2019\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 19 Jun 2019 01:27:28 GMT)\\u00a7r"}']}
{title:'da Silva et al. (§72019§r)', author: 'Angelo C. Mendes da Silva; Mauricio A. Nunes; Raul Fonseca Neto', display:{Lore:['[{"text": "arXiv:1905.12804", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Music Classification Model based on Metric Learning and Feature Extraction from MP3 Audio Files\\u00a7r\\n\\n\\u00a78\\u00a7oAngelo C. Mendes da Silva\\nMauricio A. Nunes\\nRaul Fonseca Neto\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.12804\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 17 Sep 2019 22:29:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn a review process, I foundsome errors and made some changes in methodology that improved my results. Once I finish the experiments, I will upload the new version\\u00a7r"}']}
{title:'Álvarez et al. (§72019§r)', author: 'David Álvarez; Santiago Pascual; Antonio Bonafonte', display:{Lore:['[{"text": "arXiv:1906.00733", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProblem-Agnostic Speech Embeddings for Multi-Speaker Text-to-Speech with SampleRNN\\u00a7r\\n\\n\\u00a78\\u00a7oDavid \\u00c1lvarez\\nSantiago Pascual\\nAntonio Bonafonte\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.00733\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 22 Sep 2019 17:56:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at 10thISCA Speech Synthesis Workshop\\u00a7r"}']}
{title:'Guan et al. (§72019§r)', author: 'Melody Y. Guan; Gregory Valiant', display:{Lore:['[{"text": "arXiv:1906.01040", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Surprising Density of Illusionable Natural Speech\\u00a7r\\n\\n\\u00a78\\u00a7oMelody Y. Guan\\nGregory Valiant\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.01040\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 19 Aug 2019 05:52:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCogSci2019\\u00a7r"}']}
{title:'Liu et al. (§72019§r)', author: 'Jen-Yu Liu; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:1906.01203", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDilated Convolution with Dilated GRU for Music Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oJen-Yu Liu\\nYi-Hsuan Yang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.01203\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 4 Jun 2019 05:39:43 GMT)\\u00a7r"}']}
{title:'Kubo et al. (§72019§r)', author: 'Yuki Kubo; Norihiro Takamune; Daichi Kitamura; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:1906.02482", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Full-Rank Spatial Covariance Estimation Using Independent Low-Rank Matrix Analysis for Blind Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oYuki Kubo\\nNorihiro Takamune\\nDaichi Kitamura\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.02482\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 18 Jun 2019 04:26:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, To appear in the Proceedings of the 27th European Signal Processing Conference (EUSIPCO 2019)\\u00a7r"}']}
{title:'Prétet et al. (§72019§r)', author: 'Laure Prétet; Romain Hennequin; Jimena Royo-Letelier; Andrea Vaglio', display:{Lore:['[{"text": "arXiv:1906.02618", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSinging voice separation: a study on training data\\u00a7r\\n\\n\\u00a78\\u00a7oLaure Pr\\u00e9tet\\nRomain Hennequin\\nJimena Royo-Letelier\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.02618\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8683555\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2019 - 2019 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 6 Jun 2019 14:44:36 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72019§r)', author: 'Ruihan Yang; Dingsu Wang; Ziyu Wang; Tianyao Chen; Junyan Jiang; Gus Xia', display:{Lore:['[{"text": "arXiv:1906.03626", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Music Analogy Via Latent Representation Disentanglement\\u00a7r\\n\\n\\u00a78\\u00a7oRuihan Yang\\nDingsu Wang\\nZiyu Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.03626\\u00a7r\\n\\nVersion:\\u00a77v4 (Sun, 20 Oct 2019 03:57:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the International Society for Music Information Retrieval (ISMIR), 2019\\u00a7r"}']}
{title:'Choi et al. (§72019§r)', author: 'Keunwoo Choi; Kyunghyun Cho', display:{Lore:['[{"text": "arXiv:1906.03697", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Unsupervised Drum Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oKeunwoo Choi\\nKyunghyun Cho\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.03697\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 28 Jun 2019 22:59:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oISMIR 2019 camera-ready\\u00a7r"}']}
{title:'Won et al. (§72019§r)', author: 'Minz Won; Sanghyuk Chun; Xavier Serra', display:{Lore:['[{"text": "arXiv:1906.04972", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lToward Interpretable Music Tagging with Self-Attention\\u00a7r\\n\\n\\u00a78\\u00a7oMinz Won\\nSanghyuk Chun\\nXavier Serra\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.04972\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 12 Jun 2019 07:08:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 12 figures; code:https://github.com/minzwon/self-attention-music-tagging\\u00a7r"}']}
{title:'Glickman et al. (§72019§r)', author: 'Mark E. Glickman; Jason I. Brown; Ryan B. Song', display:{Lore:['[{"text": "arXiv:1906.05427", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l(A) Data in the Life: Authorship Attribution of Lennon-McCartney Songs\\u00a7r\\n\\n\\u00a78\\u00a7oMark E. Glickman\\nJason I. Brown\\nRyan B. Song\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.05427\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1162/99608f92.130f856e\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 12 Jun 2019 23:52:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o44 pages, 5 figures\\u00a7r"}']}
{title:'Shi et al. (§72019§r)', author: 'Zhengshan Shi; Carlos Cancino-Chacón; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:1906.06428", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUser Curated Shaping of Expressive Performances\\u00a7r\\n\\n\\u00a78\\u00a7oZhengshan Shi\\nCarlos Cancino-Chac\\u00f3n\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.06428\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 Jun 2019 22:59:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, ICML 2019 Machine Learning for Music Discovery Workshop\\u00a7r"}']}
{title:'Costa (§72019§r)', author: 'Luciano da Fontoura Costa', display:{Lore:['[{"text": "arXiv:1906.06559", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CY\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.pop-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModeling Consonance and its Relationships with Temperament, Harmony, and Electronic Amplification\\u00a7r\\n\\n\\u00a78\\u00a7oLuciano da Fontoura Costa\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.06559\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 15 Jun 2019 13:19:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 14 figures, 2 tables. A working manuscript\\u00a7r"}']}
{title:'Arslan (§72019§r)', author: 'Yuksel Arslan', display:{Lore:['[{"text": "arXiv:1906.06586", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA New Approach to Real Time Impulsive Sound Detection for Surveillance Applications\\u00a7r\\n\\n\\u00a78\\u00a7oYuksel Arslan\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.06586\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 15 Jun 2019 16:20:25 GMT)\\u00a7r"}']}
{title:'Hamidi et al. (§72019§r)', author: 'Nima Hamidi; Mohsen Vahidzadeh; Stephen Baek', display:{Lore:['[{"text": "arXiv:1906.06746", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-scale Embedded CNN for Music Tagging (MsE-CNN)\\u00a7r\\n\\n\\u00a78\\u00a7oNima Hamidi\\nMohsen Vahidzadeh\\nStephen Baek\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.06746\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 16 Jun 2019 18:16:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of the 36th International Conference on MachineLearning (ICML)\\u00a7r"}']}
{title:'Maiti et al. (§72019§r)', author: 'Soumi Maiti; Michael I Mandel', display:{Lore:['[{"text": "arXiv:1906.06762", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParametric Resynthesis with neural vocoders\\u00a7r\\n\\n\\u00a78\\u00a7oSoumi Maiti\\nMichael I Mandel\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.06762\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Nov 2019 18:44:12 GMT)\\u00a7r"}']}
{title:'Elowsson et al. (§72019§r)', author: 'Anders Elowsson; Anders Friberg', display:{Lore:['[{"text": "arXiv:1906.07145", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModeling Music Modality with a Key-Class Invariant Pitch Chroma CNN\\u00a7r\\n\\n\\u00a78\\u00a7oAnders Elowsson\\nAnders Friberg\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.07145\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Jun 2019 17:33:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in ISMIR, 2019\\u00a7r"}']}
{title:'Gu et al. (§72019§r)', author: 'Yue Gu; Zhihao Du; Hui Zhang; Xueliang Zhang', display:{Lore:['[{"text": "arXiv:1906.08415", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Monaural Speech Enhancement Method for Robust Small-Footprint Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oYue Gu\\nZhihao Du\\nHui Zhang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.08415\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 20 Jun 2019 02:25:45 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72019§r)', author: 'Jong Wook Kim; Juan Pablo Bello', display:{Lore:['[{"text": "arXiv:1906.08512", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Learning for Improved Onsets and Frames Music Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oJong Wook Kim\\nJuan Pablo Bello\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.08512\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 20 Jun 2019 09:17:36 GMT)\\u00a7r"}']}
{title:'Tripathi et al. (§72019§r)', author: 'Suraj Tripathi; Abhiram Ramesh; Abhay Kumar; Chirag Singh; Promod Yenigalla', display:{Lore:['[{"text": "arXiv:1906.08873", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Discriminative features using Center Loss and Reconstruction as Regularizer for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSuraj Tripathi\\nAbhiram Ramesh\\nAbhay Kumar\\nChirag Singh\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.08873\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 31 Aug 2019 05:19:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, Accepted in IJCAI Affective Computing Workshop 2019\\u00a7r"}']}
{title:'Vidwans et al. (§72019§r)', author: 'Amruta Vidwans; Prateek Verma; Preeti Rao', display:{Lore:['[{"text": "arXiv:1906.08916", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnderstanding and Classifying Cultural Music Using Melodic Features Case Of Hindustani, Carnatic And Turkish Music\\u00a7r\\n\\n\\u00a78\\u00a7oAmruta Vidwans\\nPrateek Verma\\nPreeti Rao\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.08916\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 Jun 2019 02:06:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe work appeared in the 3rd CompMusic Workshop for Developing Computational models for the Discovery of the Worlds Music held atIIT Madras at Chennai in2013\\u00a7r"}']}
{title:'Yi et al. (§72019§r)', author: 'Yuan-Hao Yi; Yang Ai; Zhen-Hua Ling; Li-Rong Dai', display:{Lore:['[{"text": "arXiv:1906.08977", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSinging Voice Synthesis Using Deep Autoregressive Neural Networks for Acoustic Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oYuan-Hao Yi\\nYang Ai\\nZhen-Hua Ling\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.08977\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 Jun 2019 06:40:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech2019\\u00a7r"}']}
{title:'Dubnov (§72019§r)', author: 'Shlomo Dubnov', display:{Lore:['[{"text": "arXiv:1906.09155", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQuery-based Deep Improvisation\\u00a7r\\n\\n\\u00a78\\u00a7oShlomo Dubnov\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.09155\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n7th International Workshop on Musical Metacreation, International\\n  Conference on Computational Creativity 2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 Jun 2019 14:09:22 GMT)\\u00a7r"}']}
{title:'Kelz et al. (§72019§r)', author: 'Rainer Kelz; Sebastian Böck; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:1906.09165", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Polyphonic ADSR Piano Note Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oRainer Kelz\\nSebastian B\\u00f6ck\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.09165\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8683582\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2019 - 2019 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP), Brighton, United Kingdom, 2019, pp.\\n  246-250\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 Jun 2019 14:38:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, published as ICASSP\'19\\u00a7r"}']}
{title:'Lostanlen et al. (§72019§r)', author: 'Vincent Lostanlen; Florian Hecker', display:{Lore:['[{"text": "arXiv:1906.09334", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Shape of RemiXXXes to Come: Audio Texture Synthesis with Time-frequency Scattering\\u00a7r\\n\\n\\u00a78\\u00a7oVincent Lostanlen\\nFlorian Hecker\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.09334\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 29 Jun 2019 14:25:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 3 figures. To appear in the proceedings of the International Conference on Digital Audio Effects (DAFX-19), Birmingham, UK\\u00a7r"}']}
{title:'López-Espejo et al. (§72019§r)', author: 'Iván López-Espejo; Zheng-Hua Tan; Jesper Jensen', display:{Lore:['[{"text": "arXiv:1906.09417", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKeyword Spotting for Hearing Assistive Devices Robust to External Speakers\\u00a7r\\n\\n\\u00a78\\u00a7oIv\\u00e1n L\\u00f3pez-Espejo\\nZheng-Hua Tan\\nJesper Jensen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.09417\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 26 Jun 2019 09:48:25 GMT)\\u00a7r"}']}
{title:'Csapó et al. (§72019§r)', author: 'Tamás Gábor Csapó; Mohammed Salah Al-Radhi; Géza Németh; Gábor Gosztolya; Tamás Grósz; László Tóth; Alexandra Markó', display:{Lore:['[{"text": "arXiv:1906.09885", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUltrasound-based Silent Speech Interface Built on a Continuous Vocoder\\u00a7r\\n\\n\\u00a78\\u00a7oTam\\u00e1s G\\u00e1bor Csap\\u00f3\\nMohammed Salah Al-Radhi\\nG\\u00e9za N\\u00e9meth\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.09885\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jun 2019 12:34:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted for publication at Interspeech 2019\\u00a7r"}']}
{title:'India et al. (§72019§r)', author: 'Miquel India; Pooyan Safari; Javier Hernando', display:{Lore:['[{"text": "arXiv:1906.09890", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf Multi-Head Attention for Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMiquel India\\nPooyan Safari\\nJavier Hernando\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.09890\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 1 Jul 2019 22:02:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4+1 pages. 4 Figures. Accepted for Interspeech 2009\\u00a7r"}']}
{title:'Rivero et al. (§72019§r)', author: 'Daniel Rivero; Enrique Fernandez-Blanco; Alejandro Pazos', display:{Lore:['[{"text": "arXiv:1906.09972", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lClassical Music Prediction and Composition by means of Variational Autoencoders\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Rivero\\nEnrique Fernandez-Blanco\\nAlejandro Pazos\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.09972\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 Jun 2019 15:44:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 4 figures\\u00a7r"}']}
{title:'Liu et al. (§72019§r)', author: 'Shuo Liu; Gil Keren; Björn Schuller', display:{Lore:['[{"text": "arXiv:1906.09997", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSingle-Channel Speech Separation with Auxiliary Speaker Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oShuo Liu\\nGil Keren\\nBj\\u00f6rn Schuller\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.09997\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jun 2019 14:35:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages including reference\\u00a7r"}']}
{title:'Chung et al. (§72019§r)', author: 'Joon Son Chung; Bong-Jin Lee; Icksang Han', display:{Lore:['[{"text": "arXiv:1906.10042", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWho said that?: Audio-visual speaker diarisation of real-world meetings\\u00a7r\\n\\n\\u00a78\\u00a7oJoon Son Chung\\nBong-Jin Lee\\nIcksang Han\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.10042\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jun 2019 16:06:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2019\\u00a7r"}']}
{title:'Simonetta et al. (§72019§r)', author: 'Federico Simonetta; Carlos Cancino-Chacón; Stavros Ntalampiras; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:1906.10547", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Convolutional Approach to Melody Line Identification in Symbolic Scores\\u00a7r\\n\\n\\u00a78\\u00a7oFederico Simonetta\\nCarlos Cancino-Chac\\u00f3n\\nStavros Ntalampiras\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.10547\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5281/zenodo.3527965\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jun 2019 13:07:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of 20th International Society for Music Information Retrieval Conference\\u00a7r"}']}
{title:'Chung (§72019§r)', author: 'Joon Son Chung', display:{Lore:['[{"text": "arXiv:1906.10555", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNaver at ActivityNet Challenge 2019 \\u2013 Task B Active Speaker Detection (AVA)\\u00a7r\\n\\n\\u00a78\\u00a7oJoon Son Chung\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.10555\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Jun 2019 14:11:14 GMT)\\u00a7r"}']}
{title:'Naranjo-Alcazar et al. (§72019§r)', author: 'Javier Naranjo-Alcazar; Sergi Perez-Castanos; Irene Martin-Morato; Pedro Zuccarello; Maximo Cobos', display:{Lore:['[{"text": "arXiv:1906.10891", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the performance of residual block design alternatives in convolutional neural networks for end-to-end audio classification\\u00a7r\\n\\n\\u00a78\\u00a7oJavier Naranjo-Alcazar\\nSergi Perez-Castanos\\nIrene Martin-Morato\\nPedro Zuccarello\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.10891\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 26 Sep 2019 07:06:48 GMT)\\u00a7r"}']}
{title:'Lee et al. (§72019§r)', author: 'Kyungyun Lee; Juhan Nam', display:{Lore:['[{"text": "arXiv:1906.11139", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning a Joint Embedding Space of Monophonic and Mixed Music Signals for Singing Voice\\u00a7r\\n\\n\\u00a78\\u00a7oKyungyun Lee\\nJuhan Nam\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.11139\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Jun 2019 14:55:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oISMIR 2019, Delft, Netherlands\\u00a7r"}']}
{title:'Wichern et al. (§72019§r)', author: 'Gordon Wichern; Joe Antognini; Michael Flynn; Licheng Richard Zhu; Emmett McQuinn; Dwight Crow; Ethan Manilow; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:1907.01160", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWHAM!: Extending Speech Separation to Noisy Environments\\u00a7r\\n\\n\\u00a78\\u00a7oGordon Wichern\\nJoe Antognini\\nMichael Flynn\\n+ 4 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.01160\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Jul 2019 04:27:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at Interspeech 2019\\u00a7r"}']}
{title:'Nguyen et al. (§72019§r)', author: 'Linh Nguyen; Jaime Valls Miro; Xiaojun Qiu', display:{Lore:['[{"text": "arXiv:1907.01169", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCan a Robot Hear the Shape and Dimensions of a Room?\\u00a7r\\n\\n\\u00a78\\u00a7oLinh Nguyen\\nJaime Valls Miro\\nXiaojun Qiu\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.01169\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Jul 2019 05:08:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS2019)\\u00a7r"}']}
{title:'Oneata et al. (§72019§r)', author: 'Dan Oneata; Horia Cucu', display:{Lore:['[{"text": "arXiv:1907.01195", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKite: Automatic speech recognition for unmanned aerial vehicles\\u00a7r\\n\\n\\u00a78\\u00a7oDan Oneata\\nHoria Cucu\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.01195\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Jul 2019 06:50:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted at Interspeech 2019\\u00a7r"}']}
{title:'Reddy et al. (§72019§r)', author: 'Chandan K A Reddy; Ross Cutler; Johannes Gehrke', display:{Lore:['[{"text": "arXiv:1907.01742", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSupervised Classifiers for Audio Impairments with Noisy Labels\\u00a7r\\n\\n\\u00a78\\u00a7oChandan K A Reddy\\nRoss Cutler\\nJohannes Gehrke\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.01742\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Jul 2019 05:21:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in INTERSPEECH 2019\\u00a7r"}']}
{title:'Slizovskaia et al. (§72019§r)', author: 'Olga Slizovskaia; Emilia Gómez; Gloria Haro', display:{Lore:['[{"text": "arXiv:1907.01813", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Case Study of Deep-Learned Activations via Hand-Crafted Audio Features\\u00a7r\\n\\n\\u00a78\\u00a7oOlga Slizovskaia\\nEmilia G\\u00f3mez\\nGloria Haro\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.01813\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Jul 2019 09:32:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe 2018 Joint Workshop onMachine Learning for Music, The Federated Artificial Intelligence Meeting (FAIM), Joint workshop program of ICML, IJCAI/ECAI, and AAMAS, Stockholm, Sweden, Saturday, July 14th, 2018\\u00a7r"}']}
{title:'Doras et al. (§72019§r)', author: 'Guillaume Doras; Geoffroy Peeters', display:{Lore:['[{"text": "arXiv:1907.01824", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCover Detection using Dominant Melody Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oGuillaume Doras\\nGeoffroy Peeters\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.01824\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n20th International Society for Music Information Retrieval\\n  Conference, Delft, The Netherlands, 2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Jul 2019 09:56:58 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72019§r)', author: 'Zhichao Zhang; Shugong Xu; Tianhao Qiao; Shunqing Zhang; Shan Cao', display:{Lore:['[{"text": "arXiv:1907.02230", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention based Convolutional Recurrent Neural Network for Environmental Sound Classification\\u00a7r\\n\\n\\u00a78\\u00a7oZhichao Zhang\\nShugong Xu\\nTianhao Qiao\\nShunqing Zhang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.02230\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Jul 2019 05:41:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Chinese Conference onPattern Recognition and Computer Vision (PRCV) 2019\\u00a7r"}']}
{title:'Cífka et al. (§72019§r)', author: 'Ondřej Cífka; Umut Şimşekli; Gaël Richard', display:{Lore:['[{"text": "arXiv:1907.02265", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSupervised Symbolic Music Style Translation Using Synthetic Data\\u00a7r\\n\\n\\u00a78\\u00a7oOnd\\u0159ej C\\u00edfka\\nUmut \\u015eim\\u015fekli\\nGa\\u00ebl Richard\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.02265\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5281/zenodo.3527878\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 20th International Society for Music\\n  Information Retrieval Conference (2019) 588-595\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Jul 2019 08:16:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oISMIR 2019 camera-ready\\u00a7r"}']}
{title:'Mamun et al. (§72019§r)', author: 'Nursadul Mamun; Soheil Khorram; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:1907.02526", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConvolutional Neural Network-based Speech Enhancement for Cochlear Implant Recipients\\u00a7r\\n\\n\\u00a78\\u00a7oNursadul Mamun\\nSoheil Khorram\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.02526\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Jul 2019 21:25:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2019\\u00a7r"}']}
{title:'Aouameur et al. (§72019§r)', author: 'Cyran Aouameur; Philippe Esling; Gaëtan Hadjeres', display:{Lore:['[{"text": "arXiv:1907.02637", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Drum Machine : An Interactive System for Real-time Synthesis of Drum Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oCyran Aouameur\\nPhilippe Esling\\nGa\\u00ebtan Hadjeres\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.02637\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 13 Nov 2019 12:51:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, accepted at the International Conference on Computational Creativity 2019\\u00a7r"}']}
{title:'Park et al. (§72019§r)', author: 'Jonggwon Park; Kyoyun Choi; Sungwook Jeon; Dokyun Kim; Jonghun Park', display:{Lore:['[{"text": "arXiv:1907.02698", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Bi-directional Transformer for Musical Chord Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJonggwon Park\\nKyoyun Choi\\nSungwook Jeon\\nDokyun Kim\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.02698\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Jul 2019 07:00:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o20th International Society for Music Information Retrieval Conference (ISMIR), Delft, The Netherlands, 2019\\u00a7r"}']}
{title:'Elsner et al. (§72019§r)', author: 'Daniel Elsner; Stefan Langer; Fabian Ritz; Robert Müller; Steffen Illium', display:{Lore:['[{"text": "arXiv:1907.02864", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Neural Baselines for Computational Paralinguistics\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Elsner\\nStefan Langer\\nFabian Ritz\\nRobert M\\u00fcller\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.02864\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2019-2478\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2019, 2388-2392\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Jul 2019 14:43:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures; This paper was accepted at INTERSPEECH 2019, Graz, 15-19th September 2019. DOI willbe added after publishment of the accepted paper\\u00a7r"}']}
{title:'Chowdhury et al. (§72019§r)', author: 'Shreyan Chowdhury; Andreu Vall; Verena Haunschmid; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:1907.03572", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Explainable Music Emotion Recognition: The Route via Mid-level Features\\u00a7r\\n\\n\\u00a78\\u00a7oShreyan Chowdhury\\nAndreu Vall\\nVerena Haunschmid\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.03572\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Jul 2019 12:58:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInternational Society for Music Information Retrieval Conference, Delft, The Netherlands, 2019\\u00a7r"}']}
{title:'Meade et al. (§72019§r)', author: 'Nicholas Meade; Nicholas Barreyre; Scott C. Lowe; Sageev Oore', display:{Lore:['[{"text": "arXiv:1907.04352", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Conditioning for Generative Music Systems with Human-Interpretable Controls\\u00a7r\\n\\n\\u00a78\\u00a7oNicholas Meade\\nNicholas Barreyre\\nScott C. Lowe\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.04352\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInternational Conference on Computational Creativity, 2019\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 4 Aug 2019 00:40:23 GMT)\\u00a7r"}']}
{title:'Donahue et al. (§72019§r)', author: 'Chris Donahue; Huanru Henry Mao; Yiting Ethan Li; Garrison W. Cottrell; Julian McAuley', display:{Lore:['[{"text": "arXiv:1907.04868", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLakhNES: Improving multi-instrumental music generation with cross-domain pre-training\\u00a7r\\n\\n\\u00a78\\u00a7oChris Donahue\\nHuanru Henry Mao\\nYiting Ethan Li\\nGarrison W. Cottrell\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.04868\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Jul 2019 18:00:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished as a conference paper at ISMIR 2019\\u00a7r"}']}
{title:'Masuyama et al. (§72019§r)', author: 'Yoshiki Masuyama; Masahito Togami; Tatsuya Komatsu', display:{Lore:['[{"text": "arXiv:1907.04984", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultichannel Loss Function for Supervised Speech Source Separation by Mask-based Beamforming\\u00a7r\\n\\n\\u00a78\\u00a7oYoshiki Masuyama\\nMasahito Togami\\nTatsuya Komatsu\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.04984\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Jul 2019 03:41:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, Accepted at INTERSPEECH 2019\\u00a7r"}']}
{title:'Genchel et al. (§72019§r)', author: 'Benjamin Genchel; Ashis Pati; Alexander Lerch', display:{Lore:['[{"text": "arXiv:1907.05208", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExplicitly Conditioned Melody Generation: A Case Study with Interdependent RNNs\\u00a7r\\n\\n\\u00a78\\u00a7oBenjamin Genchel\\nAshis Pati\\nAlexander Lerch\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.05208\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Jul 2019 00:05:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of the 7th International Workshopon Musical Meta-creation (MUME). Charlotte, North Carolina 2019\\u00a7r"}']}
{title:'Dubey et al. (§72019§r)', author: 'Harishchandra Dubey; Abhijeet Sangwan; John Hansen', display:{Lore:['[{"text": "arXiv:1907.05584", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lToeplitz Inverse Covariance based Robust Speaker Clustering for Naturalistic Audio Streams\\u00a7r\\n\\n\\u00a78\\u00a7oHarishchandra Dubey\\nAbhijeet Sangwan\\nJohn Hansen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.05584\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Jul 2019 05:54:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 Pages, 3 Fiigures, 5 Equations\\u00a7r"}']}
{title:'Lattner et al. (§72019§r)', author: 'Stefan Lattner; Monika Dörfler; Andreas Arzt', display:{Lore:['[{"text": "arXiv:1907.05982", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Complex Basis Functions for Invariant Representations of Audio\\u00a7r\\n\\n\\u00a78\\u00a7oStefan Lattner\\nMonika D\\u00f6rfler\\nAndreas Arzt\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.05982\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 13 Jul 2019 00:23:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper accepted at the 20th International Society for Music Information Retrieval Conference, ISMIR 2019, Delft, The Netherlands, November4-8; 8 pages, 4 figures, 4 tables\\u00a7r"}']}
{title:'Harar et al. (§72019§r)', author: 'Pavol Harar; Zoltan Galaz; Jesus B. Alonso-Hernandez; Jiri Mekyska; Radim Burget; Zdenek Smekal', display:{Lore:['[{"text": "arXiv:1907.06129", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Robust Voice Pathology Detection\\u00a7r\\n\\n\\u00a78\\u00a7oPavol Harar\\nZoltan Galaz\\nJesus B. Alonso-Hernandez\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.06129\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s00521-018-3464-7\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nNeural Computing and Applications (2018): 1-11\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 13 Jul 2019 21:09:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 1 figure, 10 tables. Keywords: Voicepathology detection, deep learning, gradientboosting, anomaly detection\\u00a7r"}']}
{title:'Huang et al. (§72019§r)', author: 'Cheng-Zhi Anna Huang; Curtis Hawthorne; Adam Roberts; Monica Dinculescu; James Wexler; Leon Hong; Jacob Howcroft', display:{Lore:['[{"text": "arXiv:1907.06637", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Bach Doodle: Approachable music composition with machine learning at scale\\u00a7r\\n\\n\\u00a78\\u00a7oCheng-Zhi Anna Huang\\nCurtis Hawthorne\\nAdam Roberts\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.06637\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 14 Jul 2019 23:39:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2019\\u00a7r"}']}
{title:'Shi et al. (§72019§r)', author: 'Ziqiang Shi; Liu Liu; Huibin Lin; Rujie Liu; Anyan Shi', display:{Lore:['[{"text": "arXiv:1907.07398", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHODGEPODGE: Sound event detection based on ensemble of semi-supervised learning methods\\u00a7r\\n\\n\\u00a78\\u00a7oZiqiang Shi\\nLiu Liu\\nHuibin Lin\\nRujie Liu\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.07398\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Jul 2019 08:55:51 GMT)\\u00a7r"}']}
{title:'Drossos et al. (§72019§r)', author: 'Konstantinos Drossos; Shayan Gharib; Paul Magron; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:1907.08506", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLanguage Modelling for Sound Event Detection with Teacher Forcing and Scheduled Sampling\\u00a7r\\n\\n\\u00a78\\u00a7oKonstantinos Drossos\\nShayan Gharib\\nPaul Magron\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.08506\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 6 Nov 2019 11:49:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oFixed the display of URLs at footnote, updated the results\\u00a7r"}']}
{title:'Ramires et al. (§72019§r)', author: 'António Ramires; Xavier Serra', display:{Lore:['[{"text": "arXiv:1907.08520", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData Augmentation for Instrument Classification Robust to Audio Effects\\u00a7r\\n\\n\\u00a78\\u00a7oAnt\\u00f3nio Ramires\\nXavier Serra\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.08520\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Jul 2019 14:04:32 GMT)\\u00a7r"}']}
{title:'Epure et al. (§72019§r)', author: 'Elena V. Epure; Anis Khlif; Romain Hennequin', display:{Lore:['[{"text": "arXiv:1907.08698", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Knowledge Bases And Parallel Annotations For Music Genre Translation\\u00a7r\\n\\n\\u00a78\\u00a7oElena V. Epure\\nAnis Khlif\\nRomain Hennequin\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.08698\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 27 Jul 2019 20:31:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in ISMIR 2019\\u00a7r"}']}
{title:'Lipping et al. (§72019§r)', author: 'Samuel Lipping; Konstantinos Drossos; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:1907.09238", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCrowdsourcing a Dataset of Audio Captions\\u00a7r\\n\\n\\u00a78\\u00a7oSamuel Lipping\\nKonstantinos Drossos\\nTuomas Virtanen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.09238\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Jul 2019 11:21:08 GMT)\\u00a7r"}']}
{title:'Fan et al. (§72019§r)', author: 'Cunhang Fan; Bin Liu; Jianhua Tao; Jiangyan Yi; Zhengqi Wen', display:{Lore:['[{"text": "arXiv:1907.09884", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiscriminative Learning for Monaural Speech Separation Using Deep Embedding Features\\u00a7r\\n\\n\\u00a78\\u00a7oCunhang Fan\\nBin Liu\\nJianhua Tao\\nJiangyan Yi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.09884\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Jul 2019 14:00:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, accepted by INTERSPEECH 2019\\u00a7r"}']}
{title:'Wedekind et al. (§72019§r)', author: 'Stephen Wedekind; P. Fraundorf', display:{Lore:['[{"text": "arXiv:1907.09936", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLog Complex Color for Visual Pattern Recognition of Total Sound\\u00a7r\\n\\n\\u00a78\\u00a7oStephen Wedekind\\nP. Fraundorf\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.09936\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nAudio Engineering Society Convention 141 (2016) paper 9647;\\n  Subject of US patent 10,341,795\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Jul 2019 15:03:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 5 figures, 28 references, cf. http://tinyurl.com/color-rtsm\\u00a7r"}']}
{title:'Grzywalski et al. (§72019§r)', author: 'Tomasz Grzywalski; Riccardo Belluzzo; Szymon Drgas; Agnieszka Cwalinska; Honorata Hafke-Dys', display:{Lore:['[{"text": "arXiv:1907.11238", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInteractive Lungs Auscultation with Reinforcement Learning Agent\\u00a7r\\n\\n\\u00a78\\u00a7oTomasz Grzywalski\\nRiccardo Belluzzo\\nSzymon Drgas\\nAgnieszka Cwalinska\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.11238\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 Jul 2019 11:04:08 GMT)\\u00a7r"}']}
{title:'Gong et al. (§72019§r)', author: 'Shuyu Gong; Zhewei Wang; Tao Sun; Yuanhang Zhang; Charles D. Smith; Li Xu; Jundong Liu', display:{Lore:['[{"text": "arXiv:1907.11956", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDilated FCN: Listening Longer to Hear Better\\u00a7r\\n\\n\\u00a78\\u00a7oShuyu Gong\\nZhewei Wang\\nTao Sun\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.11956\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 27 Jul 2019 17:52:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages; will appear in WASPAA conference\\u00a7r"}']}
{title:'Kaneko et al. (§72019§r)', author: 'Takuhiro Kaneko; Hirokazu Kameoka; Kou Tanaka; Nobukatsu Hojo', display:{Lore:['[{"text": "arXiv:1907.12279", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStarGAN-VC2: Rethinking Conditional Methods for StarGAN-Based Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oTakuhiro Kaneko\\nHirokazu Kameoka\\nKou Tanaka\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.12279\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 7 Aug 2019 10:21:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2019. Project page: http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/stargan-vc2/index.html\\u00a7r"}']}
{title:'Thomas et al. (§72019§r)', author: 'Mark Thomas; Bruce Martin; Katie Kowarski; Briand Gaudet; Stan Matwin', display:{Lore:['[{"text": "arXiv:1907.13188", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMarine Mammal Species Classification using Convolutional Neural Networks and a Novel Acoustic Representation\\u00a7r\\n\\n\\u00a78\\u00a7oMark Thomas\\nBruce Martin\\nKatie Kowarski\\nBriand Gaudet\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.13188\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Jul 2019 19:17:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages, To appear in ECML-PKDD 2019\\u00a7r"}']}
{title:'Mamun et al. (§72019§r)', author: 'Nursadul Mamun; Ria Ghosh; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:1908.00031", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQuantifying Cochlear Implant Users\' Ability for Speaker Identification using CI Auditory Stimuli\\u00a7r\\n\\n\\u00a78\\u00a7oNursadul Mamun\\nRia Ghosh\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.00031\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 Jul 2019 18:17:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2019\\u00a7r"}']}
{title:'Lattner et al. (§72019§r)', author: 'Stefan Lattner; Maarten Grachten', display:{Lore:['[{"text": "arXiv:1908.00948", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHigh-Level Control of Drum Track Generation Using Learned Patterns of Rhythmic Interaction\\u00a7r\\n\\n\\u00a78\\u00a7oStefan Lattner\\nMaarten Grachten\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.00948\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Aug 2019 16:39:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper accepted at the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA 2019), New Paltz, New York, U.S.A., October 20-23; 6 pages, 3 figures, 1 table\\u00a7r"}']}
{title:'Mangal et al. (§72019§r)', author: 'Sanidhya Mangal; Rahul Modak; Poorva Joshi', display:{Lore:['[{"text": "arXiv:1908.01080", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLSTM Based Music Generation System\\u00a7r\\n\\n\\u00a78\\u00a7oSanidhya Mangal\\nRahul Modak\\nPoorva Joshi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.01080\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.17148/IARJSET.2019.6508\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIARJSET: Vol. 6, Issue 5 (2019) 47-54\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Aug 2019 22:10:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 11 figures\\u00a7r"}']}
{title:'Nakamura et al. (§72019§r)', author: 'Taiki Nakamura; Yuki Saito; Shinnosuke Takamichi; Yusuke Ijima; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:1908.01454", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lV2S attack: building DNN-based voice conversion from automatic speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oTaiki Nakamura\\nYuki Saito\\nShinnosuke Takamichi\\nYusuke Ijima\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.01454\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Aug 2019 03:28:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, accepted for The 10th ISCA Speech Synthesis Workshop (SSW10)\\u00a7r"}']}
{title:'Baird et al. (§72019§r)', author: 'Alice Baird; Bjoern Schuller', display:{Lore:['[{"text": "arXiv:1908.01671", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic Sounds for Wellbeing: A Novel Dataset and Baseline Results\\u00a7r\\n\\n\\u00a78\\u00a7oAlice Baird\\nBjoern Schuller\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.01671\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 21 Oct 2019 20:50:51 GMT)\\u00a7r"}']}
{title:'Lee et al. (§72019§r)', author: 'Juheon Lee; Hyeong-Seok Choi; Chang-Bin Jeon; Junghyun Koo; Kyogu Lee', display:{Lore:['[{"text": "arXiv:1908.01919", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarially Trained End-to-end Korean Singing Voice Synthesis System\\u00a7r\\n\\n\\u00a78\\u00a7oJuheon Lee\\nHyeong-Seok Choi\\nChang-Bin Jeon\\nJunghyun Koo\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.01919\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Aug 2019 01:17:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, INTERSPEECH 2019 (oral presentation)\\u00a7r"}']}
{title:'Kubo et al. (§72019§r)', author: 'Yuki Kubo; Norihiro Takamune; Daichi Kitamura; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:1908.01964", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcceleration of rank-constrained spatial covariance matrix estimation for blind speech extraction\\u00a7r\\n\\n\\u00a78\\u00a7oYuki Kubo\\nNorihiro Takamune\\nDaichi Kitamura\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.01964\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Aug 2019 05:36:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 3 figures, To appear in the Proceedings of Asia-Pacific Signal and Information ProcessingAssociation Annual Summit and Conference 2019 (APSIPA 2019)\\u00a7r"}']}
{title:'Hatala et al. (§72019§r)', author: 'Zulkarnaen Hatala; Victor Puturuhu', display:{Lore:['[{"text": "arXiv:1908.03143", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lViterbi Extraction tutorial with Hidden Markov Toolkit\\u00a7r\\n\\n\\u00a78\\u00a7oZulkarnaen Hatala\\nVictor Puturuhu\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.03143\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Aug 2019 03:38:22 GMT)\\u00a7r"}']}
{title:'Doire et al. (§72019§r)', author: 'Clement S. J. Doire; Olumide Okubadejo', display:{Lore:['[{"text": "arXiv:1908.05182", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInterleaved Multitask Learning for Audio Source Separation with Independent Databases\\u00a7r\\n\\n\\u00a78\\u00a7oClement S. J. Doire\\nOlumide Okubadejo\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.05182\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Aug 2019 15:56:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 2 figures\\u00a7r"}']}
{title:'S et al. (§72019§r)', author: 'Bhavana V. S; Pradip K. Das', display:{Lore:['[{"text": "arXiv:1908.05553", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Verification Using Simple Temporal Features and Pitch Synchronous Cepstral Coefficients\\u00a7r\\n\\n\\u00a78\\u00a7oBhavana V. S\\nPradip K. Das\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.05553\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Aug 2019 14:05:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 3 figures\\u00a7r"}']}
{title:'Qiao et al. (§72019§r)', author: 'Tianhao Qiao; Shunqing Zhang; Zhichao Zhang; Shan Cao; Shugong Xu', display:{Lore:['[{"text": "arXiv:1908.05863", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSub-Spectrogram Segmentation for Environmental Sound Classification via Convolutional Recurrent Neural Network and Score Level Fusion\\u00a7r\\n\\n\\u00a78\\u00a7oTianhao Qiao\\nShunqing Zhang\\nZhichao Zhang\\nShan Cao\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.05863\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 16 Aug 2019 06:39:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted in the 2019 IEEE InternationalWorkshop on Signal Processing Systems (SiPS2019)\\u00a7r"}']}
{title:'Takamichi et al. (§72019§r)', author: 'Shinnosuke Takamichi; Kentaro Mitsui; Yuki Saito; Tomoki Koriyama; Naoko Tanji; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:1908.06248", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJVS corpus: free Japanese multi-speaker voice corpus\\u00a7r\\n\\n\\u00a78\\u00a7oShinnosuke Takamichi\\nKentaro Mitsui\\nYuki Saito\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.06248\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 17 Aug 2019 06:04:46 GMT)\\u00a7r"}']}
{title:'Lee et al. (§72019§r)', author: 'Jie Hwan Lee; Hyeong-Seok Choi; Kyogu Lee', display:{Lore:['[{"text": "arXiv:1908.06593", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio query-based music source separation\\u00a7r\\n\\n\\u00a78\\u00a7oJie Hwan Lee\\nHyeong-Seok Choi\\nKyogu Lee\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.06593\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Aug 2019 04:56:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 7 figures, Appearing in the proceedings of the 20th International Society for Music Information Retrieval Conference (ISMIR 2019) (camera-ready version)\\u00a7r"}']}
{title:'Rana et al. (§72019§r)', author: 'Aakanksha Rana; Cagri Ozcinar; Aljoscha Smolic', display:{Lore:['[{"text": "arXiv:1908.06752", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Generating Ambisonics Using Audio-Visual Cue for Virtual Reality\\u00a7r\\n\\n\\u00a78\\u00a7oAakanksha Rana\\nCagri Ozcinar\\nAljoscha Smolic\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.06752\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8683318\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 16 Aug 2019 14:49:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2019 - 2019 IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP)\\u00a7r"}']}
{title:'Liu et al. (§72019§r)', author: 'Yuan Liu; Zhongwei Cheng; Jie Liu; Bourhan Yassin; Zhe Nan; Jiebo Luo', display:{Lore:['[{"text": "arXiv:1908.07517", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.DB\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAI for Earth: Rainforest Conservation by Acoustic Surveillance\\u00a7r\\n\\n\\u00a78\\u00a7oYuan Liu\\nZhongwei Cheng\\nJie Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.07517\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Aug 2019 03:50:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to KDD2019 Workshop on Data Mining and AI for Conservation\\u00a7r"}']}
{title:'Yao et al. (§72019§r)', author: 'Jian Yao; Ahmad Al-Dahle', display:{Lore:['[{"text": "arXiv:1908.08044", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCoarse-to-fine Optimization for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oJian Yao\\nAhmad Al-Dahle\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.08044\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInterspeech 2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Aug 2019 17:51:29 GMT)\\u00a7r"}']}
{title:'Sun et al. (§72019§r)', author: 'Xuecong Sun; Han Jia; Zhe Zhang; Yuzhen Yang; Zhaoyong Sun; Jun Yang', display:{Lore:['[{"text": "arXiv:1908.08160", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.app-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Localization and Separation in Three-dimensional Space Using a Single Microphone with a Metamaterial Enclosure\\u00a7r\\n\\n\\u00a78\\u00a7oXuecong Sun\\nHan Jia\\nZhe Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.08160\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 7 Nov 2019 06:42:26 GMT)\\u00a7r"}']}
{title:'Hung et al. (§72019§r)', author: 'Hsiao-Tzu Hung; Chung-Yang Wang; Yi-Hsuan Yang; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:1908.09484", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Automatic Jazz Melody Generation by Transfer Learning Techniques\\u00a7r\\n\\n\\u00a78\\u00a7oHsiao-Tzu Hung\\nChung-Yang Wang\\nYi-Hsuan Yang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.09484\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Aug 2019 05:57:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, Accepted to APSIPA ASC(Asia-Pacific Signal and Information Processing AssociationAnnual Summit and Conference ) 2019\\u00a7r"}']}
{title:'Okamoto et al. (§72019§r)', author: 'Yuki Okamoto; Keisuke Imoto; Tatsuya Komatsu; Shinnosuke Takamichi; Takumi Yagyu; Ryosuke Yamanishi; Yoichi Yamashita', display:{Lore:['[{"text": "arXiv:1908.10055", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOverview of Tasks and Investigation of Subjective Evaluation Methods in Environmental Sound Synthesis and Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oYuki Okamoto\\nKeisuke Imoto\\nTatsuya Komatsu\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.10055\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Aug 2019 07:19:37 GMT)\\u00a7r"}']}
{title:'Perez-Lopez et al. (§72019§r)', author: 'Andres Perez-Lopez; Eduardo Fonseca; Xavier Serra', display:{Lore:['[{"text": "arXiv:1908.10133", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA hybrid parametric-deep learning approach for sound event localization and detection\\u00a7r\\n\\n\\u00a78\\u00a7oAndres Perez-Lopez\\nEduardo Fonseca\\nXavier Serra\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.10133\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Aug 2019 11:20:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, submitted to DCASE2019Workshop\\u00a7r"}']}
{title:'Bando et al. (§72019§r)', author: 'Yoshiaki Bando; Yoko Sasaki; Kazuyoshi Yoshii', display:{Lore:['[{"text": "arXiv:1908.11307", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Bayesian Unsupervised Source Separation Based on a Complex Gaussian Mixture Model\\u00a7r\\n\\n\\u00a78\\u00a7oYoshiaki Bando\\nYoko Sasaki\\nKazuyoshi Yoshii\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.11307\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 29 Aug 2019 15:45:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 2 figures, accepted for publication in 2019 IEEEInternational Workshopon Machine Learning for Signal Processing (MLSP)\\u00a7r"}']}
{title:'Baumann et al. (§72019§r)', author: 'Roland Baumann; Khalid Mahmood Malik; Ali Javed; Andersen Ball; Brandon Kujawa; Hafiz Malik', display:{Lore:['[{"text": "arXiv:1909.00935", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Spoofing Detection Corpus for Single and Multi-order Audio Replays\\u00a7r\\n\\n\\u00a78\\u00a7oRoland Baumann\\nKhalid Mahmood Malik\\nAli Javed\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.00935\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 Sep 2019 03:26:26 GMT)\\u00a7r"}']}
{title:'Défossez et al. (§72019§r)', author: 'Alexandre Défossez; Nicolas Usunier; Léon Bottou; Francis Bach', display:{Lore:['[{"text": "arXiv:1909.01174", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDemucs: Deep Extractor for Music Sources with extra unlabeled data remixed\\u00a7r\\n\\n\\u00a78\\u00a7oAlexandre D\\u00e9fossez\\nNicolas Usunier\\nL\\u00e9on Bottou\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.01174\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 Sep 2019 13:41:56 GMT)\\u00a7r"}']}
{title:'Campo et al. (§72019§r)', author: 'Damian Campo; Manuela Bastidas; Olga Lucía Quintero', display:{Lore:['[{"text": "arXiv:1909.01265", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.AP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultiresolution analysis (discrete wavelet transform) through Daubechies family for emotion recognition in speech\\u00a7r\\n\\n\\u00a78\\u00a7oDamian Campo\\nManuela Bastidas\\nOlga Luc\\u00eda Quintero\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.01265\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.13140/RG.2.1.5089.1608\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 Sep 2019 16:00:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in: Conference, XX Congreso Argentino de Bioingenier\\u00eda, SABI 2015, Octubre 28-30, 2015\\u00a7r"}']}
{title:'Pan et al. (§72019§r)', author: 'Zihan Pan; Yansong Chua; Jibin Wu; Malu Zhang; Haizhou Li; Eliathamby Ambikairajah', display:{Lore:['[{"text": "arXiv:1909.01302", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn efficient and perceptually motivated auditory neural encoding and decoding algorithm for spiking neural networks\\u00a7r\\n\\n\\u00a78\\u00a7oZihan Pan\\nYansong Chua\\nJibin Wu\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.01302\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 4 Sep 2019 06:02:57 GMT)\\u00a7r"}']}
{title:'Kelz et al. (§72019§r)', author: 'Rainer Kelz; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:1909.01622", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Interpretable Polyphonic Transcription with Invertible Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oRainer Kelz\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.01622\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 4 Sep 2019 08:46:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at the 20th International Society for Music Information Retrieval Conference, Delft, The Netherlands, 2019\\u00a7r"}']}
{title:'Wei et al. (§72019§r)', author: 'Xizi Wei; Melvyn Hunt; Adrian Skilling', display:{Lore:['[{"text": "arXiv:1909.03030", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Network-Based Modeling of Phonetic Durations\\u00a7r\\n\\n\\u00a78\\u00a7oXizi Wei\\nMelvyn Hunt\\nAdrian Skilling\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.03030\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 Sep 2019 17:37:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures\\u00a7r"}']}
{title:'Bryan (§72019§r)', author: 'Nicholas J. Bryan', display:{Lore:['[{"text": "arXiv:1909.03642", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImpulse Response Data Augmentation and Deep Neural Networks for Blind Room Acoustic Parameter Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oNicholas J. Bryan\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.03642\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 21 Oct 2019 19:57:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder Review\\u00a7r"}']}
{title:'Kawahara et al. (§72019§r)', author: 'Hideki Kawahara; Ken-Ichi Sakakibara; Eri Haneishi; Kaori Hagiwara', display:{Lore:['[{"text": "arXiv:1909.03650", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-time and interactive tools for vocal training based on an analytic signal with a cosine series envelope\\u00a7r\\n\\n\\u00a78\\u00a7oHideki Kawahara\\nKen-Ichi Sakakibara\\nEri Haneishi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.03650\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/APSIPAASC47483.2019.9023094\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2019 Asia-Pacific Signal and Information Processing Association\\n  Annual Summit and Conference (APSIPA ASC), Lanzhou, China, 2019, pp. 907-910\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 9 Sep 2019 06:30:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 6 figures, APSIPA ASC 2019\\u00a7r"}']}
{title:'Serra et al. (§72019§r)', author: 'O. M. Serra; F. P. R. Martins; L. R. Padovese', display:{Lore:['[{"text": "arXiv:1909.04425", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic detection of estuarine dolphin whistles in spectrogram images\\u00a7r\\n\\n\\u00a78\\u00a7oO. M. Serra\\nF. P. R. Martins\\nL. R. Padovese\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.04425\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Sep 2019 12:00:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages; 18 figures\\u00a7r"}']}
{title:'Koneputugodage et al. (§72019§r)', author: 'Chamin Hewa Koneputugodage; Rhys Healy; Sean Lamont; Ian Mallett; Matt Brown; Matt Walters; Ushini Attanayake; Libo Zhang; Roger T. Dean; Alexander Hunter; Charles Gretton; Christian Walder', display:{Lore:['[{"text": "arXiv:1909.05030", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComputer Assisted Composition in Continuous Time\\u00a7r\\n\\n\\u00a78\\u00a7oChamin Hewa Koneputugodage\\nRhys Healy\\nSean Lamont\\n+ 8 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.05030\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Sep 2019 05:57:58 GMT)\\u00a7r"}']}
{title:'Cañón et al. (§72019§r)', author: 'Juan Sebastián Gómez Cañón; Perfecto Herrera; Emilia Gómez; Estefanía Cano', display:{Lore:['[{"text": "arXiv:1909.05882", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe emotions that we perceive in music: the influence of language and lyrics comprehension on agreement\\u00a7r\\n\\n\\u00a78\\u00a7oJuan Sebasti\\u00e1n G\\u00f3mez Ca\\u00f1\\u00f3n\\nPerfecto Herrera\\nEmilia G\\u00f3mez\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.05882\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 25 Oct 2019 08:31:02 GMT)\\u00a7r"}']}
{title:'Pons et al. (§72019§r)', author: 'Jordi Pons; Xavier Serra', display:{Lore:['[{"text": "arXiv:1909.06654", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lmusicnn: Pre-trained convolutional neural networks for music audio tagging\\u00a7r\\n\\n\\u00a78\\u00a7oJordi Pons\\nXavier Serra\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.06654\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 14 Sep 2019 18:52:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to be presented at the Late-Breaking/Demo session of ISMIR 2019\\u00a7r"}']}
{title:'Reddy et al. (§72019§r)', author: 'Chandan K. A. Reddy; Ebrahim Beyrami; Jamie Pool; Ross Cutler; Sriram Srinivasan; Johannes Gehrke', display:{Lore:['[{"text": "arXiv:1909.08050", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA scalable noisy speech dataset and online subjective test framework\\u00a7r\\n\\n\\u00a78\\u00a7oChandan K. A. Reddy\\nEbrahim Beyrami\\nJamie Pool\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.08050\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 17 Sep 2019 19:40:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterSpeech 2019\\u00a7r"}']}
{title:'Manilow et al. (§72019§r)', author: 'Ethan Manilow; Gordon Wichern; Prem Seetharaman; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:1909.08494", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCutting Music Source Separation Some Slakh: A Dataset to Study the Impact of Training Data Quality and Quantity\\u00a7r\\n\\n\\u00a78\\u00a7oEthan Manilow\\nGordon Wichern\\nPrem Seetharaman\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.08494\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 18 Sep 2019 15:14:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at WASPAA 2019\\u00a7r"}']}
{title:'Qu et al. (§72019§r)', author: 'Ante Qu; Doug L. James', display:{Lore:['[{"text": "arXiv:1909.09235", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Impact of Ground Sound\\u00a7r\\n\\n\\u00a78\\u00a7oAnte Qu\\nDoug L. James\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.09235\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 19 Sep 2019 21:06:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 11 figures. In Proceedings of the 22nd International Conference on Digital Audio Effects (DAFx-19), Birmingham, UK, September 2-6, 2019. Audio examples can be downloaded publicly at http://graphics.stanford.edu/pap"}','{"text": "ers/ground/\\u00a7r"}']}
{title:'Purohit et al. (§72019§r)', author: 'Harsh Purohit; Ryo Tanabe; Kenji Ichige; Takashi Endo; Yuki Nikaido; Kaori Suefusa; Yohei Kawaguchi', display:{Lore:['[{"text": "arXiv:1909.09347", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMIMII Dataset: Sound Dataset for Malfunctioning Industrial Machine Investigation and Inspection\\u00a7r\\n\\n\\u00a78\\u00a7oHarsh Purohit\\nRyo Tanabe\\nKenji Ichige\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.09347\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 20 Sep 2019 07:17:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, to appear in DCASE 2019 Workshop\\u00a7r"}']}
{title:'Mohapatra et al. (§72019§r)', author: 'Debasish Ray Mohapatra; Victor Zappi; Sidney Fels', display:{Lore:['[{"text": "arXiv:1909.09585", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn extended two-dimensional vocal tract model for fast acoustic simulation of single-axis symmetric three-dimensional tubes\\u00a7r\\n\\n\\u00a78\\u00a7oDebasish Ray Mohapatra\\nVictor Zappi\\nSidney Fels\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.09585\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2019-1764\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 19 Sep 2019 03:29:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, Interspeech 2019 submission\\u00a7r"}']}
{title:'Gogate et al. (§72019§r)', author: 'Mandar Gogate; Kia Dashtipour; Ahsan Adeel; Amir Hussain', display:{Lore:['[{"text": "arXiv:1909.10407", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCochleaNet: A Robust Language-independent Audio-Visual Model for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oMandar Gogate\\nKia Dashtipour\\nAhsan Adeel\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.10407\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 23 Sep 2019 14:59:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o34 pages, 11 figures, Submitted to Information Fusion\\u00a7r"}']}
{title:'Fujii et al. (§72019§r)', author: 'Kazuki Fujii; Yuki Saito; Shinnosuke Takamichi; Yukino Baba; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:1909.11391", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHumanGAN: generative adversarial network with human-based discriminator and its evaluation in speech perception modeling\\u00a7r\\n\\n\\u00a78\\u00a7oKazuki Fujii\\nYuki Saito\\nShinnosuke Takamichi\\nYukino Baba\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.11391\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 25 Sep 2019 10:32:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEEICASSP 2020\\u00a7r"}']}
{title:'Bińkowski et al. (§72019§r)', author: 'Mikołaj Bińkowski; Jeff Donahue; Sander Dieleman; Aidan Clark; Erich Elsen; Norman Casagrande; Luis C. Cobo; Karen Simonyan', display:{Lore:['[{"text": "arXiv:1909.11646", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHigh Fidelity Speech Synthesis with Adversarial Networks\\u00a7r\\n\\n\\u00a78\\u00a7oMiko\\u0142aj Bi\\u0144kowski\\nJeff Donahue\\nSander Dieleman\\n+ 4 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.11646\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 26 Sep 2019 21:12:19 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72019§r)', author: 'Natalie Yu-Hsien Wang; Hsiao-Lan Sharon Wang; Tao-Wei Wang; Szu-Wei Fu; Xugan Lu; Yu Tsao; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:1909.11912", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving the Intelligibility of Electric and Acoustic Stimulation Speech Using Fully Convolutional Networks Based Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oNatalie Yu-Hsien Wang\\nHsiao-Lan Sharon Wang\\nTao-Wei Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.11912\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 26 Sep 2019 05:55:38 GMT)\\u00a7r"}']}
{title:'Adapa (§72019§r)', author: 'Sainath Adapa', display:{Lore:['[{"text": "arXiv:1909.12699", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUrban Sound Tagging using Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oSainath Adapa\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.12699\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 27 Sep 2019 14:14:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Shahin et al. (§72019§r)', author: 'Ismail Shahin; Ali Bou Nassif', display:{Lore:['[{"text": "arXiv:1909.13070", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmirati-Accented Speaker Identification in Stressful Talking Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oIsmail Shahin\\nAli Bou Nassif\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.13070\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 29 Oct 2019 10:27:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, this work has been accepted in The International Conference on Electrical and Computing Technologiesand Applications, 2019 (ICECTA 2019)\\u00a7r"}']}
{title:'Shahin et al. (§72019§r)', author: 'Ismail Shahin; Ali Bou Nassif', display:{Lore:['[{"text": "arXiv:1909.13244", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Verification in Emotional Talking Environments based on Third-Order Circular Suprasegmental Hidden Markov Model\\u00a7r\\n\\n\\u00a78\\u00a7oIsmail Shahin\\nAli Bou Nassif\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.13244\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 29 Oct 2019 10:26:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, accepted in The International Conference on Electrical and Computing Technologiesand Applications, 2019 (ICECTA 2019). arXiv admin note: text overlapwith arXiv:1903.09803\\u00a7r"}']}
{title:'Gogate et al. (§72019§r)', author: 'Mandar Gogate; Ahsan Adeel; Kia Dashtipour; Peter Derleth; Amir Hussain', display:{Lore:['[{"text": "arXiv:1910.00424", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAV Speech Enhancement Challenge using a Real Noisy Corpus\\u00a7r\\n\\n\\u00a78\\u00a7oMandar Gogate\\nAhsan Adeel\\nKia Dashtipour\\nPeter Derleth\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.00424\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 Sep 2019 10:58:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:1909.10407\\u00a7r"}']}
{title:'Cheuk et al. (§72019§r)', author: 'Kin Wai Cheuk; Balamurali B. T.; Gemma Roig; Dorien Herremans', display:{Lore:['[{"text": "arXiv:1910.01463", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLatent space representation for multi-target speaker detection and identification with a sparse dataset using Triplet neural networks\\u00a7r\\n\\n\\u00a78\\u00a7oKin Wai Cheuk\\nBalamurali B. T.\\nGemma Roig\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.01463\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of IEEE Automatic Speech Recognition and Understanding\\n  Workshop (ASRU 2019). Singapore. 2019\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 4 Oct 2019 01:30:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ASRU 2019\\u00a7r"}']}
{title:'Remaggi et al. (§72019§r)', author: 'Luca Remaggi; Philip J. B. Jackson; Wenwu Wang', display:{Lore:['[{"text": "arXiv:1910.02127", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModeling the Comb Filter Effect and Interaural Coherence for Binaural Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oLuca Remaggi\\nPhilip J. B. Jackson\\nWenwu Wang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.02127\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2019.2946043\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Oct 2019 20:04:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE Copyright. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2019\\u00a7r"}']}
{title:'Konar (§72019§r)', author: 'Sushan Konar', display:{Lore:['[{"text": "arXiv:1910.06375", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Sounds of Music : Science of Musical Scales III \\u2013 Indian Classical\\u00a7r\\n\\n\\u00a78\\u00a7oSushan Konar\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.06375\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nResonance - Journal of Science Education, 24(10), 1125 (2019)\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Oct 2019 18:41:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oFinal part of a 3-article series on Musical Scales, see arXiv:1908.07940, arXiv:1909.06259\\u00a7r"}']}
{title:'Ahmed et al. (§72019§r)', author: 'Asad Ahmed; Pratham Tangri; Anirban Panda; Dhruv Ramani; Samarjit Karmakar', display:{Lore:['[{"text": "arXiv:1910.06697", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVFNet: A Convolutional Architecture for Accent Classification\\u00a7r\\n\\n\\u00a78\\u00a7oAsad Ahmed\\nPratham Tangri\\nAnirban Panda\\nDhruv Ramani\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.06697\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Oct 2019 13:04:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at IEEEINDICON 2019\\u00a7r"}']}
{title:'Cho et al. (§72019§r)', author: 'Janghoon Cho; Sungrack Yun; Hyoungwoo Park; Jungyun Eum; Kyuwoong Hwang', display:{Lore:['[{"text": "arXiv:1910.06784", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic Scene Classification Based on a Large-margin Factorized CNN\\u00a7r\\n\\n\\u00a78\\u00a7oJanghoon Cho\\nSungrack Yun\\nHyoungwoo Park\\nJungyun Eum\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.06784\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Oct 2019 07:47:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, DCASE 2019 Workshop\\u00a7r"}']}
{title:'Park et al. (§72019§r)', author: 'Hyoungwoo Park; Sungrack Yun; Jungyun Eum; Janghoon Cho; Kyuwoong Hwang', display:{Lore:['[{"text": "arXiv:1910.06790", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeakly Labeled Sound Event Detection Using Tri-training and Adversarial Learning\\u00a7r\\n\\n\\u00a78\\u00a7oHyoungwoo Park\\nSungrack Yun\\nJungyun Eum\\nJanghoon Cho\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.06790\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Oct 2019 07:47:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, DCASE 2019 Workshop\\u00a7r"}']}
{title:'Yadav et al. (§72019§r)', author: 'Sarthak Yadav; Atul Rai', display:{Lore:['[{"text": "arXiv:1910.07364", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrequency and temporal convolutional attention for text-independent speaker recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSarthak Yadav\\nAtul Rai\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.07364\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 19 Oct 2019 20:05:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, 3 tables, submitted to ICASSP 2020\\u00a7r"}']}
{title:'Geng et al. (§72019§r)', author: 'Chuang Geng; Lei Wang', display:{Lore:['[{"text": "arXiv:1910.07840", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end speech enhancement based on discrete cosine transform\\u00a7r\\n\\n\\u00a78\\u00a7oChuang Geng\\nLei Wang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.07840\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 22 Oct 2019 06:35:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, ICASSP 2020\\u00a7r"}']}
{title:'Wang et al. (§72019§r)', author: 'Zehao Wang; Jingru Li; Xiaoou Chen; Zijin Li; Shicheng Zhang; Baoqiang Han; Deshun Yang', display:{Lore:['[{"text": "arXiv:1910.09021", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusical Instrument Playing Technique Detection Based on FCN: Using Chinese Bowed-Stringed Instrument as an Example\\u00a7r\\n\\n\\u00a78\\u00a7oZehao Wang\\nJingru Li\\nXiaoou Chen\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.09021\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 20 Oct 2019 16:50:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Grais et al. (§72019§r)', author: 'Emad M. Grais; Fei Zhao; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:1910.09266", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Band Multi-Resolution Fully Convolutional Neural Networks for Singing Voice Separation\\u00a7r\\n\\n\\u00a78\\u00a7oEmad M. Grais\\nFei Zhao\\nMark D. Plumbley\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.09266\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Oct 2019 11:29:29 GMT)\\u00a7r"}']}
{title:'Drossos et al. (§72019§r)', author: 'Konstantinos Drossos; Samuel Lipping; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:1910.09387", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lClotho: An Audio Captioning Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oKonstantinos Drossos\\nSamuel Lipping\\nTuomas Virtanen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.09387\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Oct 2019 14:06:01 GMT)\\u00a7r"}']}
{title:'Caracalla et al. (§72019§r)', author: 'Hugo Caracalla; Axel Roebel', display:{Lore:['[{"text": "arXiv:1910.09497", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound texture synthesis using RI spectrograms\\u00a7r\\n\\n\\u00a78\\u00a7oHugo Caracalla\\nAxel Roebel\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.09497\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Oct 2019 16:40:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to IEEEInternational Conference on Acoustics, Speech and Signal Processing (ICASSP 2020)\\u00a7r"}']}
{title:'Scheibler et al. (§72019§r)', author: 'Robin Scheibler; Nobutaka Ono', display:{Lore:['[{"text": "arXiv:1910.10654", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast Independent Vector Extraction by Iterative SINR Maximization\\u00a7r\\n\\n\\u00a78\\u00a7oRobin Scheibler\\nNobutaka Ono\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.10654\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 23 Oct 2019 16:24:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, Submitted to ICASSP 2020\\u00a7r"}']}
{title:'Yang et al. (§72019§r)', author: 'Ziye Yang; Xiao-Lei Zhang', display:{Lore:['[{"text": "arXiv:1910.10912", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-channel Speech Separation Using Deep Embedding Model with Multilayer Bootstrap Networks\\u00a7r\\n\\n\\u00a78\\u00a7oZiye Yang\\nXiao-Lei Zhang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.10912\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Oct 2019 04:35:11 GMT)\\u00a7r"}']}
{title:'Dokania et al. (§72019§r)', author: 'Shubham Dokania; Vasudev Singh', display:{Lore:['[{"text": "arXiv:1910.11117", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGraph Representation learning for Audio     Music genre Classification\\u00a7r\\n\\n\\u00a78\\u00a7oShubham Dokania\\nVasudev Singh\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.11117\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 23 Oct 2019 13:59:23 GMT)\\u00a7r"}']}
{title:'Seetharaman et al. (§72019§r)', author: 'Prem Seetharaman; Gordon Wichern; Jonathan Le Roux; Bryan Pardo', display:{Lore:['[{"text": "arXiv:1910.11133", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBootstrapping deep music separation from primitive auditory grouping principles\\u00a7r\\n\\n\\u00a78\\u00a7oPrem Seetharaman\\nGordon Wichern\\nJonathan Le Roux\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.11133\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 23 Oct 2019 17:44:13 GMT)\\u00a7r"}']}
{title:'Rajapakshe et al. (§72019§r)', author: 'Thejan Rajapakshe; Rajib Rana; Siddique Latif; Sara Khalifa; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:1910.11256", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPre-training in Deep Reinforcement Learning for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oThejan Rajapakshe\\nRajib Rana\\nSiddique Latif\\nSara Khalifa\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.11256\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 26 Oct 2019 14:20:20 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72019§r)', author: 'Jisung Wang; Jihwan Kim; Sangki Kim; Yeha Lee', display:{Lore:['[{"text": "arXiv:1910.11590", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Lexicon-Free Modeling Units for End-to-End Korean and Korean-English Code-Switching Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJisung Wang\\nJihwan Kim\\nSangki Kim\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.11590\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Oct 2019 09:23:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Luu et al. (§72019§r)', author: 'Chau Luu; Peter Bell; Steve Renals', display:{Lore:['[{"text": "arXiv:1910.11643", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChannel adversarial training for speaker verification and diarization\\u00a7r\\n\\n\\u00a78\\u00a7oChau Luu\\nPeter Bell\\nSteve Renals\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.11643\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP40776.2020.9053323\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Oct 2019 12:14:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEEICASSP 2020\\u00a7r"}']}
{title:'Valle et al. (§72019§r)', author: 'Rafael Valle; Jason Li; Ryan Prenger; Bryan Catanzaro', display:{Lore:['[{"text": "arXiv:1910.11997", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMellotron: Multispeaker expressive voice synthesis by conditioning on rhythm, pitch and global style tokens\\u00a7r\\n\\n\\u00a78\\u00a7oRafael Valle\\nJason Li\\nRyan Prenger\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.11997\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 26 Oct 2019 04:28:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 1 table\\u00a7r"}']}
{title:'Fonseca et al. (§72019§r)', author: 'Eduardo Fonseca; Frederic Font; Xavier Serra', display:{Lore:['[{"text": "arXiv:1910.12004", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModel-agnostic Approaches to Handling Noisy Labels When Training Sound Event Classifiers\\u00a7r\\n\\n\\u00a78\\u00a7oEduardo Fonseca\\nFrederic Font\\nXavier Serra\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.12004\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 26 Oct 2019 05:53:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWASPAA 2019\\u00a7r"}']}
{title:'Román et al. (§72019§r)', author: 'Miguel A. Román; Antonio Pertusa; Jorge Calvo-Zaragoza', display:{Lore:['[{"text": "arXiv:1910.12086", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA holistic approach to polyphonic music transcription with neural networks\\u00a7r\\n\\n\\u00a78\\u00a7oMiguel A. Rom\\u00e1n\\nAntonio Pertusa\\nJorge Calvo-Zaragoza\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.12086\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 26 Oct 2019 15:19:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSource code available at https://github.com/mangelroman/audio2score\\u00a7r"}']}
{title:'Hsu et al. (§72019§r)', author: 'Jui-Yang Hsu; Yuan-Jui Chen; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:1910.12094", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMeta Learning for End-to-End Low-Resource Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJui-Yang Hsu\\nYuan-Jui Chen\\nHung-yi Lee\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.12094\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 26 Oct 2019 16:00:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, submitted to ICASSP 2020\\u00a7r"}']}
{title:'Yang et al. (§72019§r)', author: 'Gene-Ping Yang; Szu-Lin Wu; Yao-Wen Mao; Hung-yi Lee; Lin-shan Lee', display:{Lore:['[{"text": "arXiv:1910.12706", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInterrupted and cascaded permutation invariant training for speech separation\\u00a7r\\n\\n\\u00a78\\u00a7oGene-Ping Yang\\nSzu-Lin Wu\\nYao-Wen Mao\\nHung-yi Lee\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.12706\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Oct 2019 14:28:12 GMT)\\u00a7r"}']}
{title:'Lee et al. (§72019§r)', author: 'Juheon Lee; Hyeong-Seok Choi; Junghyun Koo; Kyogu Lee', display:{Lore:['[{"text": "arXiv:1910.13069", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDisentangling Timbre and Singing Style with Multi-singer Singing Synthesis System\\u00a7r\\n\\n\\u00a78\\u00a7oJuheon Lee\\nHyeong-Seok Choi\\nJunghyun Koo\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.13069\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Oct 2019 03:40:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, Submitted to ICASSP2020\\u00a7r"}']}
{title:'Sun et al. (§72019§r)', author: 'Haoran Sun; Yunqi Cai; Lantian Li; Dong Wang', display:{Lore:['[{"text": "arXiv:1910.13288", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Investigation of Unsupervised Speech Factorization Based on Normalization Flow\\u00a7r\\n\\n\\u00a78\\u00a7oHaoran Sun\\nYunqi Cai\\nLantian Li\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.13288\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Oct 2019 14:26:22 GMT)\\u00a7r"}']}
{title:'Boeddeker et al. (§72019§r)', author: 'Christoph Boeddeker; Tomohiro Nakatani; Keisuke Kinoshita; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:1910.13707", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJointly optimal dereverberation and beamforming\\u00a7r\\n\\n\\u00a78\\u00a7oChristoph Boeddeker\\nTomohiro Nakatani\\nKeisuke Kinoshita\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.13707\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Oct 2019 08:11:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Drude et al. (§72019§r)', author: 'Lukas Drude; Jens Heitkaemper; Christoph Boeddeker; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:1910.13934", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSMS-WSJ: Database, performance measures, and baseline recipe for multi-channel source separation and recognition\\u00a7r\\n\\n\\u00a78\\u00a7oLukas Drude\\nJens Heitkaemper\\nChristoph Boeddeker\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.13934\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Oct 2019 15:39:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Venkataramani et al. (§72019§r)', author: 'Shrikant Venkataramani; Efthymios Tzinis; Paris Smaragdis', display:{Lore:['[{"text": "arXiv:1911.00102", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end Non-Negative Autoencoders for Sound Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oShrikant Venkataramani\\nEfthymios Tzinis\\nParis Smaragdis\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.00102\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Oct 2019 20:55:08 GMT)\\u00a7r"}']}
{title:'Lostanlen et al. (§72019§r)', author: 'Vincent Lostanlen; Kaitlin Palmer; Elly Knight; Christopher Clark; Holger Klinck; Andrew Farnsworth; Tina Wong; Jason Cramer; Juan Pablo Bello', display:{Lore:['[{"text": "arXiv:1911.00417", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLong-distance Detection of Bioacoustic Events with Per-channel Energy Normalization\\u00a7r\\n\\n\\u00a78\\u00a7oVincent Lostanlen\\nKaitlin Palmer\\nElly Knight\\n+ 5 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.00417\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.33682/ts6e-sn53\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Nov 2019 14:50:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures. Presented at the 3rd International Workshopon Detection and Classification of Acoustic Scenes and Events (DCASE). 25\\u201326 October 2019, New York,NY, USA\\u00a7r"}']}
{title:'Morrison et al. (§72019§r)', author: 'Max Morrison; Bryan Pardo', display:{Lore:['[{"text": "arXiv:1911.02073", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOtoMechanic: Auditory Automobile Diagnostics via Query-by-Example\\u00a7r\\n\\n\\u00a78\\u00a7oMax Morrison\\nBryan Pardo\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.02073\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Nov 2019 20:25:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Workshop on Detection and Classification of Acoustic Scenes and Events 2019 (DCASE2019)\\u00a7r"}']}
{title:'Sarfjoo et al. (§72019§r)', author: 'Seyyed Saeed Sarfjoo; Xin Wang; Gustav Eje Henter; Jaime Lorenzo-Trueba; Shinji Takaki; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:1911.03952", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransformation of low-quality device-recorded speech to high-quality speech using improved SEGAN model\\u00a7r\\n\\n\\u00a78\\u00a7oSeyyed Saeed Sarfjoo\\nXin Wang\\nGustav Eje Henter\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.03952\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.7488/ds/1994\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Nov 2019 11:17:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis study was conducted during an internship of the first author at NII, Japan in 2017\\u00a7r"}']}
{title:'Won et al. (§72019§r)', author: 'Minz Won; Sanghyuk Chun; Xavier Serra', display:{Lore:['[{"text": "arXiv:1911.04385", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisualizing and Understanding Self-attention based Music Tagging\\u00a7r\\n\\n\\u00a78\\u00a7oMinz Won\\nSanghyuk Chun\\nXavier Serra\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.04385\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Nov 2019 16:52:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMachine Learning for Music Discovery Workshop (ML4MD) at ICML 2019\\u00a7r"}']}
{title:'Coto-Jimenez (§72019§r)', author: 'Marvin Coto-Jimenez', display:{Lore:['[{"text": "arXiv:1911.04580", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSupervised Initialization of LSTM Networks for Fundamental Frequency Detection in Noisy Speech Signals\\u00a7r\\n\\n\\u00a78\\u00a7oMarvin Coto-Jimenez\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.04580\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Nov 2019 21:57:29 GMT)\\u00a7r"}']}
{title:'Foleiss et al. (§72019§r)', author: 'Juliano Henrique Foleiss; Tiago Fernandes Tavares', display:{Lore:['[{"text": "arXiv:1911.04660", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRandom Projections of Mel-Spectrograms as Low-Level Features for Automatic Music Genre Classification\\u00a7r\\n\\n\\u00a78\\u00a7oJuliano Henrique Foleiss\\nTiago Fernandes Tavares\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.04660\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Nov 2019 03:58:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEESignal Processing Letters\\u00a7r"}']}
{title:'Yin et al. (§72019§r)', author: 'Dacheng Yin; Chong Luo; Zhiwei Xiong; Wenjun Zeng', display:{Lore:['[{"text": "arXiv:1911.04697", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPHASEN: A Phase-and-Harmonics-Aware Speech Enhancement Network\\u00a7r\\n\\n\\u00a78\\u00a7oDacheng Yin\\nChong Luo\\nZhiwei Xiong\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.04697\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Nov 2019 06:16:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by AAAI\'20\\u00a7r"}']}
{title:'Carsault et al. (§72019§r)', author: 'Tristan Carsault; Jérôme Nika; Philippe Esling', display:{Lore:['[{"text": "arXiv:1911.04973", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing musical relationships between chord labels in automatic chord extraction tasks\\u00a7r\\n\\n\\u00a78\\u00a7oTristan Carsault\\nJ\\u00e9r\\u00f4me Nika\\nPhilippe Esling\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.04973\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Nov 2019 15:32:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in ISMIR, 2018\\u00a7r"}']}
{title:'Koutini et al. (§72019§r)', author: 'Khaled Koutini; Shreyan Chowdhury; Verena Haunschmid; Hamid Eghbal-zadeh; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:1911.05833", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotion and Theme Recognition in Music with Frequency-Aware RF-Regularized CNNs\\u00a7r\\n\\n\\u00a78\\u00a7oKhaled Koutini\\nShreyan Chowdhury\\nVerena Haunschmid\\nHamid Eghbal-zadeh\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.05833\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Oct 2019 10:19:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMediaEval`19, 27-29 October 2019, Sophia Antipolis, France\\u00a7r"}']}
{title:'Jansen et al. (§72019§r)', author: 'Aren Jansen; Daniel P. W. Ellis; Shawn Hershey; R. Channing Moore; Manoj Plakal; Ashok C. Popat; Rif A. Saurous', display:{Lore:['[{"text": "arXiv:1911.05894", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCoincidence, Categorization, and Consolidation: Learning to Recognize Sounds with Minimal Supervision\\u00a7r\\n\\n\\u00a78\\u00a7oAren Jansen\\nDaniel P. W. Ellis\\nShawn Hershey\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.05894\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Nov 2019 02:07:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis extended version of a ICASSP 2020 submissionunder same title has an added figure and additional discussion for easier consumption\\u00a7r"}']}
{title:'Maiti et al. (§72019§r)', author: 'Soumi Maiti; Michael I Mandel', display:{Lore:['[{"text": "arXiv:1911.06266", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker independence of neural vocoders and their effect on parametric resynthesis speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oSoumi Maiti\\nMichael I Mandel\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.06266\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Nov 2019 17:45:44 GMT)\\u00a7r"}']}
{title:'Chang et al. (§72019§r)', author: 'Ya-Liang Chang; Kuan-Ying Lee; Po-Yu Wu; Hung-yi Lee; Winston Hsu', display:{Lore:['[{"text": "arXiv:1911.06476", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Long Audio Inpainting\\u00a7r\\n\\n\\u00a78\\u00a7oYa-Liang Chang\\nKuan-Ying Lee\\nPo-Yu Wu\\nHung-yi Lee\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.06476\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Nov 2019 04:42:29 GMT)\\u00a7r"}']}
{title:'Raissi et al. (§72019§r)', author: 'Tina Raissi; Santiago Pascual; Maurizio Omologo', display:{Lore:['[{"text": "arXiv:1911.06713", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSample Drop Detection for Distant-speech Recognition with Asynchronous Devices Distributed in Space\\u00a7r\\n\\n\\u00a78\\u00a7oTina Raissi\\nSantiago Pascual\\nMaurizio Omologo\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.06713\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.23919/Eusipco47968.2020.9287791\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Nov 2019 15:56:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Sukhavasi et al. (§72019§r)', author: 'Manoj Sukhavasi; Sainath Adapa', display:{Lore:['[{"text": "arXiv:1911.07041", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic theme recognition using CNN and self-attention\\u00a7r\\n\\n\\u00a78\\u00a7oManoj Sukhavasi\\nSainath Adapa\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.07041\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 16 Nov 2019 14:53:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMediaEval 2019, 27-29 October 2019, Sophia Antipolis, France\\u00a7r"}']}
{title:'Liu et al. (§72019§r)', author: 'Shuo Liu; Gil Keren; Björn Schuller', display:{Lore:['[{"text": "arXiv:1911.07062", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lN-HANS: Introducing the Augsburg Neuro-Holistic Audio-eNhancement System\\u00a7r\\n\\n\\u00a78\\u00a7oShuo Liu\\nGil Keren\\nBj\\u00f6rn Schuller\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.07062\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 27 Nov 2019 19:59:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages including references\\u00a7r"}']}
{title:'Gharib et al. (§72019§r)', author: 'Shayan Gharib; Konstantinos Drossos; Eemi Fagerlund; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:1911.07098", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVOICe: A Sound Event Detection Dataset For Generalizable Domain Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oShayan Gharib\\nKonstantinos Drossos\\nEemi Fagerlund\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.07098\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 25 Nov 2019 11:40:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oFixed the footnote at the abstract\\u00a7r"}']}
{title:'Smith (§72019§r)', author: 'III Julius O. Smith', display:{Lore:['[{"text": "arXiv:1911.07575", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Spatial Sampling Approach to Wave Field Synthesis: PBAP and Huygens Arrays\\u00a7r\\n\\n\\u00a78\\u00a7oIII Julius O. Smith\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.07575\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Nov 2019 12:08:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o42 pages\\u00a7r"}']}
{title:'Tzinis et al. (§72019§r)', author: 'Efthymios Tzinis; Scott Wisdom; John R. Hershey; Aren Jansen; Daniel P. W. Ellis', display:{Lore:['[{"text": "arXiv:1911.07951", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Universal Sound Separation Using Sound Classification\\u00a7r\\n\\n\\u00a78\\u00a7oEfthymios Tzinis\\nScott Wisdom\\nJohn R. Hershey\\nAren Jansen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.07951\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP40776.2020.9053921\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2020 - 2020 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Nov 2019 20:56:26 GMT)\\u00a7r"}']}
{title:'Bishop et al. (§72019§r)', author: 'Laura Bishop; Carlos Cancino-Chacón; Werner Goebl', display:{Lore:['[{"text": "arXiv:1911.09018", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMoving to Communicate, Moving to Interact: Patterns of Body Motion in Musical Duo Performance\\u00a7r\\n\\n\\u00a78\\u00a7oLaura Bishop\\nCarlos Cancino-Chac\\u00f3n\\nWerner Goebl\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.09018\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1525/mp.2019.37.1.1\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nMusic Perception 37 (2019) 1-25\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Nov 2019 16:46:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o32 pages, 10 figures. This version is a final preprint of the paper prepared by the authors. Please cite as Bishop, L., Cancino-Chac\\u00f3n, C., and Goebl, W. (2019). Moving to communicate, moving to interact: Patterns of "}','{"text": "body motion in musical duo performance. MusicPerception, 37, 1-25\\u00a7r"}']}
{title:'Voisin (§72019§r)', author: 'Frédéric Voisin', display:{Lore:['[{"text": "arXiv:1911.09459", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDesigning Virtual Soundscapes for Alzheimer\'s Disease Care\\u00a7r\\n\\n\\u00a78\\u00a7oFr\\u00e9d\\u00e9ric Voisin\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.09459\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n14th International Symposium on Computer Music Multidisciplinary\\n  Research, Oct 2019, Marseille, France\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Nov 2019 13:23:12 GMT)\\u00a7r"}']}
{title:'Peracha et al. (§72019§r)', author: 'Omar Peracha; Shawn Head', display:{Lore:['[{"text": "arXiv:1911.10119", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGANkyoku: a Generative Adversarial Network for Shakuhachi Music\\u00a7r\\n\\n\\u00a78\\u00a7oOmar Peracha\\nShawn Head\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.10119\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Nov 2019 16:19:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of the 2019 International Computer Music Conference, ICMC\\u00a7r"}']}
{title:'Uranga et al. (§72019§r)', author: 'Beñat Mencia Uranga; Austen Lamacraft', display:{Lore:['[{"text": "arXiv:1911.11879", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a75cond-mat.stat-mech\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSchr\\u00f6dingeRNN: Generative Modeling of Raw Audio as a Continuously Observed Quantum State\\u00a7r\\n\\n\\u00a78\\u00a7oBe\\u00f1at Mencia Uranga\\nAusten Lamacraft\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.11879\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Nov 2019 23:33:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o32 pages, 20 figures, under review for MSML 2020\\u00a7r"}']}
{title:'Ramírez et al. (§72019§r)', author: 'Jaime Ramírez; M. Julia Flores', display:{Lore:['[{"text": "arXiv:1911.12618", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMachine learning for music genre: multifaceted review and experimentation with audioset\\u00a7r\\n\\n\\u00a78\\u00a7oJaime Ram\\u00edrez\\nM. Julia Flores\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.12618\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s10844-019-00582-9\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Nov 2019 09:57:28 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72019§r)', author: 'Bo-Wen Chen; Yen-Min Hsu; Hung-Yi Lee', display:{Lore:['[{"text": "arXiv:1911.12926", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJ-Net: Randomly weighted U-Net for audio source separation\\u00a7r\\n\\n\\u00a78\\u00a7oBo-Wen Chen\\nYen-Min Hsu\\nHung-Yi Lee\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.12926\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Nov 2019 02:24:05 GMT)\\u00a7r"}']}
{title:'Ziemer et al. (§72019§r)', author: 'Tim Ziemer; Holger Schultheis', display:{Lore:['[{"text": "arXiv:1912.00766", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThree Orthogonal Dimensions for Psychoacoustic Sonification\\u00a7r\\n\\n\\u00a78\\u00a7oTim Ziemer\\nHolger Schultheis\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.00766\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21785/icad2019.018\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Nov 2019 13:19:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oKeywords: Auditory Display, Audition, Noise/acoustics, SoundDesign, Interpretability\\u00a7r"}']}
{title:'Chung et al. (§72019§r)', author: 'Joon Son Chung; Arsha Nagrani; Ernesto Coto; Weidi Xie; Mitchell McLaren; Douglas A Reynolds; Andrew Zisserman', display:{Lore:['[{"text": "arXiv:1912.02522", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoxSRC 2019: The first VoxCeleb Speaker Recognition Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oJoon Son Chung\\nArsha Nagrani\\nErnesto Coto\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.02522\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 5 Dec 2019 12:00:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oISCA Archive\\u00a7r"}']}
{title:'Li et al. (§72019§r)', author: 'Andong Li; Chengshi Zheng; Xiaodong Li', display:{Lore:['[{"text": "arXiv:1912.03679", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Supervised Speech enhancement Approach with Residual Noise Control for Voice Communication\\u00a7r\\n\\n\\u00a78\\u00a7oAndong Li\\nChengshi Zheng\\nXiaodong Li\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.03679\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 8 Dec 2019 13:50:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, Submitted to Signal Processing Letters\\u00a7r"}']}
{title:'Tuan et al. (§72019§r)', author: 'Chao-I Tuan; Yuan-Kuei Wu; Hung-yi Lee; Yu Tsao', display:{Lore:['[{"text": "arXiv:1912.03884", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMITAS: A Compressed Time-Domain Audio Separation Network with Parameter Sharing\\u00a7r\\n\\n\\u00a78\\u00a7oChao-I Tuan\\nYuan-Kuei Wu\\nHung-yi Lee\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.03884\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 9 Dec 2019 07:44:32 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72019§r)', author: 'Xi Chen; Shouyi Yin; Dandan Song; Peng Ouyang; Leibo Liu; Shaojun Wei', display:{Lore:['[{"text": "arXiv:1912.05124", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSmall-footprint Keyword Spotting with Graph Convolutional Network\\u00a7r\\n\\n\\u00a78\\u00a7oXi Chen\\nShouyi Yin\\nDandan Song\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.05124\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 11 Dec 2019 05:44:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the IEEE Automatic Speech Recognition and Understanding Workshop(ASRU 2019)\\u00a7r"}']}
{title:'Verma et al. (§72019§r)', author: 'Prateek Verma; Jonathan Berger', display:{Lore:['[{"text": "arXiv:1912.05683", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning to Model Aspects of Hearing Perception Using Neural Loss Functions\\u00a7r\\n\\n\\u00a78\\u00a7oPrateek Verma\\nJonathan Berger\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.05683\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 11 Dec 2019 23:00:13 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72019§r)', author: 'Liqiang Zhang; Chengzhu Yu; Heng Lu; Chao Weng; Yusong Wu; Xiang Xie; Zijin Li; Dong Yu', display:{Lore:['[{"text": "arXiv:1912.10128", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Singing From Speech\\u00a7r\\n\\n\\u00a78\\u00a7oLiqiang Zhang\\nChengzhu Yu\\nHeng Lu\\n+ 4 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.10128\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 20 Dec 2019 22:45:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP-2020\\u00a7r"}']}
{title:'Tian et al. (§72019§r)', author: 'Yapeng Tian; Chenliang Xu; Dingzeyu Li', display:{Lore:['[{"text": "arXiv:1912.10292", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Audio Prior\\u00a7r\\n\\n\\u00a78\\u00a7oYapeng Tian\\nChenliang Xu\\nDingzeyu Li\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.10292\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 21 Dec 2019 16:35:54 GMT)\\u00a7r"}']}
{title:'Venkataramanan et al. (§72019§r)', author: 'Kannan Venkataramanan; Haresh Rengaraj Rajamohan', display:{Lore:['[{"text": "arXiv:1912.10458", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotion Recognition from Speech\\u00a7r\\n\\n\\u00a78\\u00a7oKannan Venkataramanan\\nHaresh Rengaraj Rajamohan\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.10458\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 22 Dec 2019 14:43:14 GMT)\\u00a7r"}']}
{title:'Dorobek (§72019§r)', author: 'Mateusz Dorobek', display:{Lore:['[{"text": "arXiv:1912.10815", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWykorzystanie sztucznej inteligencji do generowania tre\\u015bci muzycznych\\u00a7r\\n\\n\\u00a78\\u00a7oMateusz Dorobek\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.10815\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.13140/RG.2.2.23824.66564\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 15 Dec 2019 02:48:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oBachelor Thesis,in Polish\\u00a7r"}']}
{title:'Liu et al. (§72019§r)', author: 'Yi Liu; Tianyu Liang; Can Xu; Xianwei Zhang; Xianhong Chen; Wei-Qiang Zhang; Liang He; Dandan song; Ruyun Li; Yangcheng Wu; Peng Ouyang; Shouyi Yin', display:{Lore:['[{"text": "arXiv:1912.11585", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTHUEE system description for NIST 2019 SRE CTS Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oYi Liu\\nTianyu Liang\\nCan Xu\\n+ 8 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.11585\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 25 Dec 2019 03:44:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis is the system description of THUEE submitted to NIST SRE 2019\\u00a7r"}']}
{title:'Huang et al. (§72019§r)', author: 'Lu Huang; Gaofeng Cheng; Pengyuan Zhang; Yi Yang; Shumin Xu; Jiasong Sun', display:{Lore:['[{"text": "arXiv:1912.11613", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUtterance-level Permutation Invariant Training with Latency-controlled BLSTM for Single-channel Multi-talker Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oLu Huang\\nGaofeng Cheng\\nPengyuan Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.11613\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 25 Dec 2019 07:40:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of APSIPA Annual Summit and Conference 2019, 18-21 November 2019, Lanzhou, China\\u00a7r"}']}
{title:'Chang et al. (§72019§r)', author: 'Yu-Tao Chang; Yuan-Hong Yang; Yu-Huai Peng; Syu-Siang Wang; Tai-Shih Chi; Yu Tsao; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:1912.11984", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMoEVC: A Mixture-of-experts Voice Conversion System with Sparse Gating Mechanism for Accelerating Online Computation\\u00a7r\\n\\n\\u00a78\\u00a7oYu-Tao Chang\\nYuan-Hong Yang\\nYu-Huai Peng\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.11984\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 27 Dec 2019 04:50:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Drugman et al. (§72019§r)', author: 'Thomas Drugman; Baris Bozkurt; Thierry Dutoit', display:{Lore:['[{"text": "arXiv:1912.12602", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComplex Cepstrum-based Decomposition of Speech for Glottal Source Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Drugman\\nBaris Bozkurt\\nThierry Dutoit\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.12602\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 29 Dec 2019 07:58:18 GMT)\\u00a7r"}']}
{title:'Drugman et al. (§72019§r)', author: 'Thomas Drugman; Paavo Alku; Abeer Alwan; Bayya Yegnanarayana', display:{Lore:['[{"text": "arXiv:1912.12604", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGlottal Source Processing: from Analysis to Applications\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Drugman\\nPaavo Alku\\nAbeer Alwan\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.12604\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 29 Dec 2019 08:13:58 GMT)\\u00a7r"}']}
{title:'Babacan et al. (§72019§r)', author: "Onur Babacan; Thomas Drugman; Nicolas d'Alessandro; Nathalie Henrich; Thierry Dutoit", display:{Lore:['[{"text": "arXiv:1912.12609", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparative Study of Pitch Extraction Algorithms on a Large Variety of Singing Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oOnur Babacan\\nThomas Drugman\\nNicolas d\'Alessandro\\nNathalie Henrich\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.12609\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 29 Dec 2019 08:45:08 GMT)\\u00a7r"}']}
{title:'Drugman et al. (§72019§r)', author: 'Thomas Drugman; Baris Bozkurt; Thierry Dutoit', display:{Lore:['[{"text": "arXiv:1912.12843", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCausal-Anticausal Decomposition of Speech using Complex Cepstrum for Glottal Source Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Drugman\\nBaris Bozkurt\\nThierry Dutoit\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.12843\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 Dec 2019 08:12:03 GMT)\\u00a7r"}']}
{title:'Drugman et al. (§72019§r)', author: 'Thomas Drugman; Alexis Moinet; Thierry Dutoit; Geoffrey Wilfart', display:{Lore:['[{"text": "arXiv:1912.12887", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing a Pitch-Synchronous Residual Codebook for Hybrid HMM/Frame Selection Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Drugman\\nAlexis Moinet\\nThierry Dutoit\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.12887\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 Dec 2019 11:34:39 GMT)\\u00a7r"}']}
{title:'Drugman et al. (§72019§r)', author: 'Thomas Drugman; Abeer Alwan', display:{Lore:['[{"text": "arXiv:2001.00459", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Robust Voicing Detection and Pitch Estimation Based on Residual Harmonics\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Drugman\\nAbeer Alwan\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2001.00459\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 28 Dec 2019 13:45:29 GMT)\\u00a7r"}']}
{title:'Drugman et al. (§72019§r)', author: 'Thomas Drugman; Mark Thomas; Jon Gudnason; Patrick Naylor; Thierry Dutoit', display:{Lore:['[{"text": "arXiv:2001.00473", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetection of Glottal Closure Instants from Speech Signals: a Quantitative Review\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Drugman\\nMark Thomas\\nJon Gudnason\\nPatrick Naylor\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2001.00473\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 28 Dec 2019 14:12:16 GMT)\\u00a7r"}']}
{title:'Drugman et al. (§72019§r)', author: 'Thomas Drugman; Baris Bozkurt; Thierry Dutoit', display:{Lore:['[{"text": "arXiv:2001.00840", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparative Study of Glottal Source Estimation Techniques\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Drugman\\nBaris Bozkurt\\nThierry Dutoit\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2001.00840\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 28 Dec 2019 20:40:08 GMT)\\u00a7r"}']}
{title:'Drugman et al. (§72019§r)', author: 'Thomas Drugman; Thierry Dutoit', display:{Lore:['[{"text": "arXiv:2001.00841", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGlottal Closure and Opening Instant Detection from Speech Signals\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Drugman\\nThierry Dutoit\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2001.00841\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 28 Dec 2019 19:27:45 GMT)\\u00a7r"}']}
{title:'Drugman et al. (§72019§r)', author: 'Thomas Drugman; Geoffrey Wilfart; Thierry Dutoit', display:{Lore:['[{"text": "arXiv:2001.00842", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Deterministic plus Stochastic Model of the Residual Signal for Improved Parametric Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Drugman\\nGeoffrey Wilfart\\nThierry Dutoit\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2001.00842\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 29 Dec 2019 07:26:47 GMT)\\u00a7r"}']}
{title:'Drugman et al. (§72019§r)', author: 'Thomas Drugman; Thierry Dutoit', display:{Lore:['[{"text": "arXiv:2001.01000", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Deterministic plus Stochastic Model of the Residual Signal and its Applications\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Drugman\\nThierry Dutoit\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2001.01000\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 29 Dec 2019 07:52:37 GMT)\\u00a7r"}']}
