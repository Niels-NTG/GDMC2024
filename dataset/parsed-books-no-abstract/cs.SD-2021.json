{title:'Sanyal et al. (§72021§r)', author: 'Shankha Sanyal; Archi Banerjee; Souparno Roy; Sayan Nag; Ranjan Sengupta; Dipak Ghosh', display:{Lore:['[{"text": "arXiv:1604.02243", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRagas in Bollywood music A microscopic view through multrifractal cross-correlation method\\u00a7r\\n\\n\\u00a78\\u00a7oShankha Sanyal\\nArchi Banerjee\\nSouparno Roy\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1604.02243\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nThe Journal of Acoustical Society of India (ISSN: 0973-3302) Vol.\\n  48, No. 1-2, 2021 (pp. 91-97)\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 26 May 2021 17:34:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 5 figures\\u00a7r"}']}
{title:'Banerjee et al. (§72021§r)', author: 'Archi Banerjee; Shankha Sanyal; Ranjan Sengupta; Dipak Ghosh', display:{Lore:['[{"text": "arXiv:1604.02250", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVariation of singing styles within a particular Gharana of Hindustani classical music A nonlinear multifractal study\\u00a7r\\n\\n\\u00a78\\u00a7oArchi Banerjee\\nShankha Sanyal\\nRanjan Sengupta\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1604.02250\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nThe Journal of Acoustical Society of India (ISSN: 0973-3302) :\\n  Vol. 48, No. 1-2, 2021 (pp. 35-45)\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 26 May 2021 17:41:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 8 figures\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Zixing Zhang; Ding Liu; Jing Han; Kun Qian; Björn Schuller', display:{Lore:['[{"text": "arXiv:1707.08729", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning audio sequence representations for acoustic event classification\\u00a7r\\n\\n\\u00a78\\u00a7oZixing Zhang\\nDing Liu\\nJing Han\\nKun Qian\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1707.08729\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 19 Jun 2021 10:13:12 GMT)\\u00a7r"}']}
{title:'Choi et al. (§72021§r)', author: 'Keunwoo Choi; György Fazekas; Kyunghyun Cho; Mark Sandler', display:{Lore:['[{"text": "arXiv:1709.01922", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparison of Audio Signal Preprocessing Methods for Deep Neural Networks on Music Tagging\\u00a7r\\n\\n\\u00a78\\u00a7oKeunwoo Choi\\nGy\\u00f6rgy Fazekas\\nKyunghyun Cho\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1709.01922\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 22 Feb 2021 13:21:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages. EUSIPCO 2018 camera-ready. arXiv:1706.02361 does not have the overlapped part with this submission anymore\\u00a7r"}']}
{title:'Kankanahalli (§72021§r)', author: 'Srihari Kankanahalli', display:{Lore:['[{"text": "arXiv:1710.09064", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Optimized Speech Coding with Deep Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oSrihari Kankanahalli\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1710.09064\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 8 Jul 2021 15:43:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted and presented at ICASSP 2018. Samples availablehere: http://srik.tk/speech-coding/\\u00a7r"}']}
{title:'Zheng et al. (§72021§r)', author: 'Weiping Zheng; Zhenyao Mo; Jiantao Yi', display:{Lore:['[{"text": "arXiv:1807.04073", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA punishment voting algorithm based on super categories construction for acoustic scene classification\\u00a7r\\n\\n\\u00a78\\u00a7oWeiping Zheng\\nZhenyao Mo\\nJiantao Yi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1807.04073\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 Mar 2021 05:14:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThere is a minor mistake found in the voting process. So, We are very sorry about this mistake and request to withdraw this manuscript\\u00a7r"}']}
{title:'Peng et al. (§72021§r)', author: 'Xutan Peng; Chen Li; Zhi Cai; Faqiang Shi; Yidan Liu; Jianxin Li', display:{Lore:['[{"text": "arXiv:1810.01248", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Lightweight Music Texture Transfer System\\u00a7r\\n\\n\\u00a78\\u00a7oXutan Peng\\nChen Li\\nZhi Cai\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.01248\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 4 Aug 2021 15:20:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis version (v3) is identicalwith v1; v2 should no longer be cited in the literature due to incorrect author list\\u00a7r"}']}
{title:'Zhang (§72021§r)', author: 'Xiao-Lei Zhang', display:{Lore:['[{"text": "arXiv:1811.01233", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Ad-hoc Beamforming\\u00a7r\\n\\n\\u00a78\\u00a7oXiao-Lei Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.01233\\u00a7r\\n\\nVersion:\\u00a77v7 (Tue, 9 Feb 2021 04:37:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Computer Speech and Language\\u00a7r"}']}
{title:'Nolasco et al. (§72021§r)', author: 'Inês Nolasco; Emmanouil Benetos', display:{Lore:['[{"text": "arXiv:1811.06016", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTo bee or not to bee: Investigating machine learning approaches for beehive sound recognition\\u00a7r\\n\\n\\u00a78\\u00a7oIn\\u00eas Nolasco\\nEmmanouil Benetos\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.06016\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the Detection and Classification of Acoustic Scenes\\n  and Events 2018 Workshop (DCASE2018)\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 2 Dec 2021 13:46:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at Detection and Classification of Acoustic Scenes and Events (DCASE) workshop 2018\\u00a7r"}']}
{title:'Cai et al. (§72021§r)', author: 'Chao Cai; Rong Zheng; Jun Luo', display:{Lore:['[{"text": "arXiv:1901.03450", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUbiquitous Acoustic Sensing on Commodity IoT Devices: A Survey\\u00a7r\\n\\n\\u00a78\\u00a7oChao Cai\\nRong Zheng\\nJun Luo\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.03450\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 12 Aug 2021 04:30:13 GMT)\\u00a7r"}']}
{title:'Lo et al. (§72021§r)', author: 'Chen-Chou Lo; Szu-Wei Fu; Wen-Chin Huang; Xin Wang; Junichi Yamagishi; Yu Tsao; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:1904.08352", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMOSNet: Deep Learning based Objective Assessment for Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oChen-Chou Lo\\nSzu-Wei Fu\\nWen-Chin Huang\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.08352\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2019-2003\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 14 Jul 2021 07:32:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech2019\\u00a7r"}']}
{title:'Paolizzo et al. (§72021§r)', author: 'Fabio Paolizzo; Natalia Pichierri; Daniele Casali; Daniele Giardino; Marco Matta; Giovanni Costantini', display:{Lore:['[{"text": "arXiv:1905.12629", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA New Multilabel System for Automatic Music Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oFabio Paolizzo\\nNatalia Pichierri\\nDaniele Casali\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.12629\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 2 Jun 2021 21:55:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2 tables. Research supported by the EU through the MUSICAL-MOODS project funded by the Marie Sklodowska-CurieActions Individual Fellowships Global Fellowships (MSCA-IF-GF) of the Horizon 2020 Programme "}','{"text": "H2020/2014-2020, REA grant agreement n.659434\\u00a7r"}']}
{title:'Perez-Castanos et al. (§72021§r)', author: 'Sergi Perez-Castanos; Javier Naranjo-Alcazar; Pedro Zuccarello; Maximo Cobos; Frances J. Ferri', display:{Lore:['[{"text": "arXiv:1906.04591", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCNN depth analysis with different channel inputs for Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oSergi Perez-Castanos\\nJavier Naranjo-Alcazar\\nPedro Zuccarello\\nMaximo Cobos\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.04591\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 13 Aug 2021 11:04:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at URSI2020, Malaga, Spain\\u00a7r"}']}
{title:'Nakamura et al. (§72021§r)', author: 'Eita Nakamura; Kazuyoshi Yoshii', display:{Lore:['[{"text": "arXiv:1908.06969", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusical Rhythm Transcription Based on Bayesian Piece-Specific Score Models Capturing Repetitions\\u00a7r\\n\\n\\u00a78\\u00a7oEita Nakamura\\nKazuyoshi Yoshii\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.06969\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 Feb 2021 18:27:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTitle changed; change in organizations of sections; appendix added; some explanations added; 14 pages, 9 figures (supplemental material: 11 pages)\\u00a7r"}']}
{title:'Défossez et al. (§72021§r)', author: 'Alexandre Défossez; Nicolas Usunier; Léon Bottou; Francis Bach', display:{Lore:['[{"text": "arXiv:1911.13254", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Source Separation in the Waveform Domain\\u00a7r\\n\\n\\u00a78\\u00a7oAlexandre D\\u00e9fossez\\nNicolas Usunier\\nL\\u00e9on Bottou\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.13254\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 28 Apr 2021 14:37:48 GMT)\\u00a7r"}']}
{title:'Latif et al. (§72021§r)', author: 'Siddique Latif; Rajib Rana; Sara Khalifa; Raja Jurdak; Junaid Qadir; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2001.00378", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Representation Learning in Speech Processing: Challenges, Recent Advances, and Future Trends\\u00a7r\\n\\n\\u00a78\\u00a7oSiddique Latif\\nRajib Rana\\nSara Khalifa\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2001.00378\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 24 Sep 2021 05:09:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPart of this work is acceptedin IEEE Transactions onAffective Computing 2021. https://ieeexplore.ieee.org/document/9543566\\u00a7r"}']}
{title:'Yeh et al. (§72021§r)', author: 'Yin-Cheng Yeh; Wen-Yi Hsiao; Satoru Fukayama; Tetsuro Kitahara; Benjamin Genchel; Hao-Min Liu; Hao-Wen Dong; Yian Chen; Terence Leong; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2001.02360", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Melody Harmonization with Triad Chords: A Comparative Study\\u00a7r\\n\\n\\u00a78\\u00a7oYin-Cheng Yeh\\nWen-Yi Hsiao\\nSatoru Fukayama\\n+ 6 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2001.02360\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 27 Apr 2021 10:03:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o20 pages, 6 figures, published in Journal of New Music Research (JNMR), Volume 50 Issue1\\u00a7r"}']}
{title:'Shi et al. (§72021§r)', author: 'Yanpei Shi; Thomas Hain', display:{Lore:['[{"text": "arXiv:2001.06397", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSupervised Speaker Embedding De-Mixing in Two-Speaker Environment\\u00a7r\\n\\n\\u00a78\\u00a7oYanpei Shi\\nThomas Hain\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2001.06397\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 5 Feb 2021 15:46:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at SLT2021\\u00a7r"}']}
{title:'Tsiang (§72021§r)', author: 'Elaine Y L Tsiang', display:{Lore:['[{"text": "arXiv:2002.00791", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOral Billiards\\u00a7r\\n\\n\\u00a78\\u00a7oElaine Y L Tsiang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2002.00791\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 4 Jan 2021 19:57:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMinor typo corrections. Added citation requested by the author of the cited article\\u00a7r"}']}
{title:'Slizovskaia et al. (§72021§r)', author: 'Olga Slizovskaia; Gloria Haro; Emilia Gómez', display:{Lore:['[{"text": "arXiv:2004.03873", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConditioned Source Separation for Music Instrument Performances\\u00a7r\\n\\n\\u00a78\\u00a7oOlga Slizovskaia\\nGloria Haro\\nEmilia G\\u00f3mez\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2004.03873\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3082331\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 7 Jul 2021 21:36:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages, 5 figures, underreview\\u00a7r"}']}
{title:'Xu et al. (§72021§r)', author: 'Menglong Xu; Xiao-Lei Zhang', display:{Lore:['[{"text": "arXiv:2004.12200", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDepthwise Separable Convolutional ResNet with Squeeze-and-Excitation Blocks for Small-footprint Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oMenglong Xu\\nXiao-Lei Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2004.12200\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2020-1045\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 22 Jul 2021 15:28:32 GMT)\\u00a7r"}']}
{title:'Miranda (§72021§r)', author: 'Eduardo R. Miranda', display:{Lore:['[{"text": "arXiv:2005.05832", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.ET\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCreative Quantum Computing: Inverse FFT, Sound Synthesis, Adaptive Sequencing and Musical Composition\\u00a7r\\n\\n\\u00a78\\u00a7oEduardo R. Miranda\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2005.05832\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 6 Dec 2021 08:47:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPre-publication draft. Replacement of figures 5 and 8, 06 Dec 21\\u00a7r"}']}
{title:'Nardelli (§72021§r)', author: 'Marco Buongiorno Nardelli', display:{Lore:['[{"text": "arXiv:2006.01033", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.SI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTonal harmony and the topology of dynamical score networks\\u00a7r\\n\\n\\u00a78\\u00a7oMarco Buongiorno Nardelli\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2006.01033\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 27 Jan 2021 00:12:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o24 pages\\u00a7r"}']}
{title:'Eren et al. (§72021§r)', author: 'Ayşegül Özkaya Eren; Mustafa Sert', display:{Lore:['[{"text": "arXiv:2006.03391", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Captioning using Gated Recurrent Units\\u00a7r\\n\\n\\u00a78\\u00a7oAy\\u015feg\\u00fcl \\u00d6zkaya Eren\\nMustafa Sert\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2006.03391\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 3 Jan 2021 07:53:01 GMT)\\u00a7r"}']}
{title:'Donahue et al. (§72021§r)', author: 'Jeff Donahue; Sander Dieleman; Mikołaj Bińkowski; Erich Elsen; Karen Simonyan', display:{Lore:['[{"text": "arXiv:2006.03575", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Adversarial Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oJeff Donahue\\nSander Dieleman\\nMiko\\u0142aj Bi\\u0144kowski\\nErich Elsen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2006.03575\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 17 Mar 2021 11:42:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o23 pages. In proceedings of ICLR 2021\\u00a7r"}']}
{title:'Grover et al. (§72021§r)', author: 'Manraj Singh Grover; Pakhi Bamdev; Ratin Kumar Brala; Yaman Kumar; Mika Hama; Rajiv Ratn Shah', display:{Lore:['[{"text": "arXiv:2006.05236", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7laudino: A Modern Annotation Tool for Audio and Speech\\u00a7r\\n\\n\\u00a78\\u00a7oManraj Singh Grover\\nPakhi Bamdev\\nRatin Kumar Brala\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2006.05236\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 28 Nov 2021 09:02:34 GMT)\\u00a7r"}']}
{title:'Brown et al. (§72021§r)', author: 'Chloë Brown; Jagmohan Chauhan; Andreas Grammenos; Jing Han; Apinan Hasthanasombat; Dimitris Spathis; Tong Xia; Pietro Cicuta; Cecilia Mascolo', display:{Lore:['[{"text": "arXiv:2006.05919", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Automatic Diagnosis of COVID-19 from Crowdsourced Respiratory Sound Data\\u00a7r\\n\\n\\u00a78\\u00a7oChlo\\u00eb Brown\\nJagmohan Chauhan\\nAndreas Grammenos\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2006.05919\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3394486.3412865\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 18 Jan 2021 07:08:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 6 figures, 2 tables, Accepted for publication at KDD\'20 (Health Day)\\u00a7r"}']}
{title:'Hilmkil et al. (§72021§r)', author: 'Agrin Hilmkil; Carl Thomé; Anders Arpteg', display:{Lore:['[{"text": "arXiv:2006.06287", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerceiving Music Quality with GANs\\u00a7r\\n\\n\\u00a78\\u00a7oAgrin Hilmkil\\nCarl Thom\\u00e9\\nAnders Arpteg\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2006.06287\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 4 Apr 2021 14:01:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oExtended abstract (first version) accepted for the Northern Lights Deep Learning Workshop 2020\\u00a7r"}']}
{title:'Pachet et al. (§72021§r)', author: 'François Pachet; Pierre Roy; Benoit Carré', display:{Lore:['[{"text": "arXiv:2006.09232", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAssisted music creation with Flow Machines: towards new categories of new\\u00a7r\\n\\n\\u00a78\\u00a7oFran\\u00e7ois Pachet\\nPierre Roy\\nBenoit Carr\\u00e9\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2006.09232\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 4 Jan 2021 16:02:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper is a hyperlinked version of chapter 18 of the book \\"Handbook of Artificial Intelligencefor Music\\", Eduardo Miranda ed., Springer, 2020. Accompanying website: https://www.francoispachet.fr/flow-machines-synthesis-p"}','{"text": "aper/ This version corrects some typos\\u00a7r"}']}
{title:'Naranjo-Alcazar et al. (§72021§r)', author: 'Javier Naranjo-Alcazar; Sergi Perez-Castanos; Jose Ferrandis; Pedro Zuccarello; Maximo Cobos', display:{Lore:['[{"text": "arXiv:2006.14436", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Event Localization and Detection using Squeeze-Excitation Residual CNNs\\u00a7r\\n\\n\\u00a78\\u00a7oJavier Naranjo-Alcazar\\nSergi Perez-Castanos\\nJose Ferrandis\\nPedro Zuccarello\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2006.14436\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 30 Jul 2021 11:16:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in URSI 2021, Vigo, Spain\\u00a7r"}']}
{title:'Chung et al. (§72021§r)', author: 'Joon Son Chung; Jaesung Huh; Arsha Nagrani; Triantafyllos Afouras; Andrew Zisserman', display:{Lore:['[{"text": "arXiv:2007.01216", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpot the conversation: speaker diarisation in the wild\\u00a7r\\n\\n\\u00a78\\u00a7oJoon Son Chung\\nJaesung Huh\\nArsha Nagrani\\nTriantafyllos Afouras\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.01216\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2020-2337\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 15 Aug 2021 04:01:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe dataset will be availablefor download from http://www.robots.ox.ac.uk/vgg/data/voxceleb/voxconverse.html . The development set will be released in July 2020, and the test set will be released in October 2020\\u00a7r"}']}
{title:'Mimilakis et al. (§72021§r)', author: 'Stylianos Ioannis Mimilakis; Konstantinos Drossos; Gerald Schuller', display:{Lore:['[{"text": "arXiv:2007.02780", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRevisiting Representation Learning for Singing Voice Separation with Sinkhorn Distances\\u00a7r\\n\\n\\u00a78\\u00a7oStylianos Ioannis Mimilakis\\nKonstantinos Drossos\\nGerald Schuller\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.02780\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 8 Jan 2021 17:32:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUpdate including additional results justifying hyper-parameter choices, clarifications for the supervision debate, notes on interpretability\\u00a7r"}']}
{title:'Noufi et al. (§72021§r)', author: 'Camille Noufi; Prateek Verma', display:{Lore:['[{"text": "arXiv:2007.09060", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Learning of Context-Aware Pitch Prosody Representations\\u00a7r\\n\\n\\u00a78\\u00a7oCamille Noufi\\nPrateek Verma\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.09060\\u00a7r\\n\\nVersion:\\u00a77v4 (Sun, 1 Aug 2021 05:16:48 GMT)\\u00a7r"}']}
{title:'Bitton et al. (§72021§r)', author: 'Adrien Bitton; Philippe Esling; Tatsuya Harada', display:{Lore:['[{"text": "arXiv:2008.01393", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Granular Sound Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oAdrien Bitton\\nPhilippe Esling\\nTatsuya Harada\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.01393\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 3 Jul 2021 17:26:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7opresented for ICMC 2021 (2020 postponed)\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Rui Liu; Berrak Sisman; Guanglai Gao; Haizhou Li', display:{Lore:['[{"text": "arXiv:2008.01490", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExpressive TTS Training with Frame and Style Reconstruction Loss\\u00a7r\\n\\n\\u00a78\\u00a7oRui Liu\\nBerrak Sisman\\nGuanglai Gao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.01490\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 12 Apr 2021 07:30:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEE/ACM Transactionson Audio, Speech and Language Processing\\u00a7r"}']}
{title:'Yin et al. (§72021§r)', author: 'Guanghao Yin; Shouqian Sun; Dian Yu; Dejian Li; Kejun Zhang', display:{Lore:['[{"text": "arXiv:2008.09743", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Efficient Multimodal Framework for Large Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals\\u00a7r\\n\\n\\u00a78\\u00a7oGuanghao Yin\\nShouqian Sun\\nDian Yu\\nDejian Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.09743\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3490686\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 2 Dec 2021 03:04:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oACM Transactions on Multimedia Computing, Communications, and Applications (Acceptance 07-Oct-2021)\\u00a7r"}']}
{title:'Shibata et al. (§72021§r)', author: 'Kentaro Shibata; Eita Nakamura; Kazuyoshi Yoshii', display:{Lore:['[{"text": "arXiv:2008.12710", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-Local Musical Statistics as Guides for Audio-to-Score Piano Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oKentaro Shibata\\nEita Nakamura\\nKazuyoshi Yoshii\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.12710\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.ins.2021.03.014\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInformation Sciences, vol. 566, p. 262, 2021\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 3 Apr 2021 12:15:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages, 7 figures, typos corrected\\u00a7r"}']}
{title:'Koo et al. (§72021§r)', author: 'Junghyun Koo; Jie Hwan Lee; Jaewoo Pyo; Yujin Jo; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2009.04070", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting Multi-Modal Features From Pre-trained Networks for Alzheimer\'s Dementia Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJunghyun Koo\\nJie Hwan Lee\\nJaewoo Pyo\\nYujin Jo\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2009.04070\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 3 Mar 2021 03:15:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn the Proceedings of INTERSPEECH 2020\\u00a7r"}']}
{title:'Vial et al. (§72021§r)', author: 'Pierre-Hugo Vial; Paul Magron; Thomas Oberlin; Cédric Févotte', display:{Lore:['[{"text": "arXiv:2010.00392", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhase retrieval with Bregman divergences and application to audio signal recovery\\u00a7r\\n\\n\\u00a78\\u00a7oPierre-Hugo Vial\\nPaul Magron\\nThomas Oberlin\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.00392\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JSTSP.2021.3051870\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 13 Jan 2021 16:30:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o23 pages, 3 figures, accepted for publication in the IEEE Journal of Selected Topics in Signal Processing\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Zhong-Qiu Wang; Peidong Wang; DeLiang Wang', display:{Lore:['[{"text": "arXiv:2010.01703", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-microphone Complex Spectral Mapping for Utterance-wise and Continuous Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oZhong-Qiu Wang\\nPeidong Wang\\nDeLiang Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.01703\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 24 May 2021 15:00:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages, 6 figures. To appear in IEEE/ACM Transactions on Audio, Speech, and Language Processing. Sound demo https://zqwang7.github.io/demos/SMSWSJ_demo/taslp20_SMSWSJ_demo.html\\u00a7r"}']}
{title:'Kong et al. (§72021§r)', author: 'Qiuqiang Kong; Bochen Li; Xuchen Song; Yuan Wan; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2010.01815", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHigh-resolution Piano Transcription with Pedals by Regressing Onset and Offset Times\\u00a7r\\n\\n\\u00a78\\u00a7oQiuqiang Kong\\nBochen Li\\nXuchen Song\\nYuan Wan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.01815\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 31 Jul 2021 13:30:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages\\u00a7r"}']}
{title:'Takahashi et al. (§72021§r)', author: 'Naoya Takahashi; Shota Inoue; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2010.03164", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial attacks on audio source separation\\u00a7r\\n\\n\\u00a78\\u00a7oNaoya Takahashi\\nShota Inoue\\nYuki Mitsufuji\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.03164\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 15 Feb 2021 04:12:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2021\\u00a7r"}']}
{title:'Shen et al. (§72021§r)', author: 'Jonathan Shen; Ye Jia; Mike Chrzanowski; Yu Zhang; Isaac Elias; Heiga Zen; Yonghui Wu', display:{Lore:['[{"text": "arXiv:2010.04301", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-Attentive Tacotron: Robust and Controllable Neural TTS Synthesis Including Unsupervised Duration Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oJonathan Shen\\nYe Jia\\nMike Chrzanowski\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.04301\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 11 May 2021 04:12:14 GMT)\\u00a7r"}']}
{title:'Ding et al. (§72021§r)', author: 'Mingshuo Ding; Yinghao Ma', display:{Lore:['[{"text": "arXiv:2010.07758", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Transformer Based Pitch Sequence Autoencoder with MIDI Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oMingshuo Ding\\nYinghao Ma\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.07758\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 1 Feb 2021 05:06:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to CSMT 2020 challenge track\\u00a7r"}']}
{title:'Phan et al. (§72021§r)', author: 'Huy Phan; Huy Le Nguyen; Oliver Y. Chén; Philipp Koch; Ngoc Q. K. Duong; Ian McLoughlin; Alfred Mertins', display:{Lore:['[{"text": "arXiv:2010.09132", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Attention Generative Adversarial Network for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oHuy Phan\\nHuy Le Nguyen\\nOliver Y. Ch\\u00e9n\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.09132\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 6 Feb 2021 19:51:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o46th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2021). Source code is available at http://github.com/pquochuy/sasegan\\u00a7r"}']}
{title:'Mocanu et al. (§72021§r)', author: 'Alexandru Mocanu; Benjamin Ricaud; Milos Cernak', display:{Lore:['[{"text": "arXiv:2010.09453", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast accuracy estimation of deep learning based multi-class musical source separation\\u00a7r\\n\\n\\u00a78\\u00a7oAlexandru Mocanu\\nBenjamin Ricaud\\nMilos Cernak\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.09453\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 1 Dec 2021 07:55:09 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Yuzhuo Liu; Hangting Chen; YunWang; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2010.09985", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPower pooling: An adaptive pooling function for weakly labelled sound event detection\\u00a7r\\n\\n\\u00a78\\u00a7oYuzhuo Liu\\nHangting Chen\\nYunWang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.09985\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Jan 2021 07:50:42 GMT)\\u00a7r"}']}
{title:'Magron et al. (§72021§r)', author: 'Paul Magron; Pierre-Hugo Vial; Thomas Oberlin; Cédric Févotte', display:{Lore:['[{"text": "arXiv:2010.10255", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhase recovery with Bregman divergences for audio source separation\\u00a7r\\n\\n\\u00a78\\u00a7oPaul Magron\\nPierre-Hugo Vial\\nThomas Oberlin\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.10255\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 Feb 2021 15:45:28 GMT)\\u00a7r"}']}
{title:'Abdulatif et al. (§72021§r)', author: 'Sherif Abdulatif; Karim Armanious; Jayasankar T. Sajeev; Karim Guirguis; Bin Yang', display:{Lore:['[{"text": "arXiv:2010.10468", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating Cross-Domain Losses for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oSherif Abdulatif\\nKarim Armanious\\nJayasankar T. Sajeev\\nKarim Guirguis\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.10468\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 30 May 2021 01:56:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures and 1 table\\u00a7r"}']}
{title:'Fan et al. (§72021§r)', author: 'Ziqi Fan; Vibhav Vineet; Chenshen Lu; T. W. Wu; Kyla McMullen', display:{Lore:['[{"text": "arXiv:2010.10691", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrediction of Object Geometry from Acoustic Scattering Using Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oZiqi Fan\\nVibhav Vineet\\nChenshen Lu\\nT. W. Wu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.10691\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 10 Feb 2021 23:24:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2021\\u00a7r"}']}
{title:'Thienpondt et al. (§72021§r)', author: 'Jenthe Thienpondt; Brecht Desplanques; Kris Demuynck', display:{Lore:['[{"text": "arXiv:2010.11255", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe IDLAB VoxSRC-20 Submission: Large Margin Fine-Tuning and Quality-Aware Score Calibration in DNN Based Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oJenthe Thienpondt\\nBrecht Desplanques\\nKris Demuynck\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.11255\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9414600\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 6 Apr 2021 09:37:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oproceedings of ICASSP 2021\\u00a7r"}']}
{title:'Esmaeilpour et al. (§72021§r)', author: 'Mohammad Esmaeilpour; Patrick Cardinal; Alessandro Lameiras Koerich', display:{Lore:['[{"text": "arXiv:2010.11352", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lClass-Conditional Defense GAN Against End-to-End Speech Attacks\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Esmaeilpour\\nPatrick Cardinal\\nAlessandro Lameiras Koerich\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.11352\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n46th IEEE International Conference on Acoustics, Speech, & Signal\\n  Processing (ICASSP), 2021\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 20 Feb 2021 02:51:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Verma et al. (§72021§r)', author: 'Prateek Verma; Julius Smith', display:{Lore:['[{"text": "arXiv:2010.11459", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Framework for Generative and Contrastive Learning of Audio Representations\\u00a7r\\n\\n\\u00a78\\u00a7oPrateek Verma\\nJulius Smith\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.11459\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 Mar 2021 21:41:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 2 figures, 5 pageversion\\u00a7r"}']}
{title:'Shi et al. (§72021§r)', author: 'Yao Shi; Hui Bu; Xin Xu; Shaoji Zhang; Ming Li', display:{Lore:['[{"text": "arXiv:2010.11567", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAISHELL-3: A Multi-speaker Mandarin TTS Corpus and the Baselines\\u00a7r\\n\\n\\u00a78\\u00a7oYao Shi\\nHui Bu\\nXin Xu\\nShaoji Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.11567\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 22 Apr 2021 07:51:51 GMT)\\u00a7r"}']}
{title:'Choi et al. (§72021§r)', author: 'Woosung Choi; Minseok Kim; Jaehwa Chung; Soonyoung Jung', display:{Lore:['[{"text": "arXiv:2010.11631", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLaSAFT: Latent Source Attentive Frequency Transformation for Conditioned Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oWoosung Choi\\nMinseok Kim\\nJaehwa Chung\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.11631\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 14 Apr 2021 05:31:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 2 tables. accepted to ICASSP 2021\\u00a7r"}']}
{title:'Chen et al. (§72021§r)', author: 'Mingjie Chen; Yanpei Shi; Thomas Hain', display:{Lore:['[{"text": "arXiv:2010.11646", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Low-Resource StarGAN Voice Conversion using Weight Adaptive Instance Normalization\\u00a7r\\n\\n\\u00a78\\u00a7oMingjie Chen\\nYanpei Shi\\nThomas Hain\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.11646\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 10 Apr 2021 08:33:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2021\\u00a7r"}']}
{title:'Djukanović et al. (§72021§r)', author: 'Slobodan Djukanović; Yash Patel; Jiři Matas; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2010.11659", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Network-based Acoustic Vehicle Counting\\u00a7r\\n\\n\\u00a78\\u00a7oSlobodan Djukanovi\\u0107\\nYash Patel\\nJi\\u0159i Matas\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.11659\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 27 Mar 2021 08:53:19 GMT)\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Zeqian Li; Jacob Whitehill', display:{Lore:['[{"text": "arXiv:2010.11803", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCompositional embedding models for speaker identification and diarization with simultaneous speech from 2+ speakers\\u00a7r\\n\\n\\u00a78\\u00a7oZeqian Li\\nJacob Whitehill\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.11803\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 10 Feb 2021 15:47:18 GMT)\\u00a7r"}']}
{title:'Tachibana (§72021§r)', author: 'Hideyuki Tachibana', display:{Lore:['[{"text": "arXiv:2010.11871", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Listening to 10 People Simultaneously: An Efficient Permutation Invariant Training of Audio Source Separation Using Sinkhorn\'s Algorithm\\u00a7r\\n\\n\\u00a78\\u00a7oHideyuki Tachibana\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.11871\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9414508\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. ICASSP (2021)\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 16 May 2021 13:40:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 8 figures, IEEE ICASSP 2021\\u00a7r"}']}
{title:'Chang et al. (§72021§r)', author: 'Sungkyun Chang; Donmoon Lee; Jeongsoo Park; Hyungui Lim; Kyogu Lee; Karam Ko; Yoonchang Han', display:{Lore:['[{"text": "arXiv:2010.11910", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Audio Fingerprint for High-specific Audio Retrieval based on Contrastive Learning\\u00a7r\\n\\n\\u00a78\\u00a7oSungkyun Chang\\nDonmoon Lee\\nJeongsoo Park\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.11910\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 10 Feb 2021 08:23:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2021 (accepted)\\u00a7r"}']}
{title:'Sun et al. (§72021§r)', author: 'Guangzhi Sun; Chao Zhang; Phil Woodland', display:{Lore:['[{"text": "arXiv:2010.12025", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCombination of Deep Speaker Embeddings for Diarisation\\u00a7r\\n\\n\\u00a78\\u00a7oGuangzhi Sun\\nChao Zhang\\nPhil Woodland\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.12025\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.neunet.2021.04.020\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 7 May 2021 08:59:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oManualscript accepted by Neural Networks\\u00a7r"}']}
{title:'Doutre et al. (§72021§r)', author: 'Thibault Doutre; Wei Han; Min Ma; Zhiyun Lu; Chung-Cheng Chiu; Ruoming Pang; Arun Narayanan; Ananya Misra; Yu Zhang; Liangliang Cao', display:{Lore:['[{"text": "arXiv:2010.12096", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Streaming Automatic Speech Recognition With Non-Streaming Model Distillation On Unsupervised Data\\u00a7r\\n\\n\\u00a78\\u00a7oThibault Doutre\\nWei Han\\nMin Ma\\n+ 6 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.12096\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 21 Feb 2021 21:56:06 GMT)\\u00a7r"}']}
{title:'Park et al. (§72021§r)', author: 'Soochul Park; Ben Sangbae Chon', display:{Lore:['[{"text": "arXiv:2010.12139", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGSEP: A robust vocal and accompaniment separation system using gated CBHG module and loudness normalization\\u00a7r\\n\\n\\u00a78\\u00a7oSoochul Park\\nBen Sangbae Chon\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.12139\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 22 Feb 2021 06:46:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures\\u00a7r"}']}
{title:'Xu et al. (§72021§r)', author: 'Menglong Xu; Shengqiang Li; Xiao-Lei Zhang', display:{Lore:['[{"text": "arXiv:2010.12155", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransformer-based End-to-End Speech Recognition with Local Dense Synthesizer Attention\\u00a7r\\n\\n\\u00a78\\u00a7oMenglong Xu\\nShengqiang Li\\nXiao-Lei Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.12155\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9414353\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 24 Jul 2021 03:52:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Sarfjoo et al. (§72021§r)', author: 'Seyyed Saeed Sarfjoo; Srikanth Madikeri; Petr Motlicek', display:{Lore:['[{"text": "arXiv:2010.12277", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Activity Detection Based on Multilingual Speech Recognition System\\u00a7r\\n\\n\\u00a78\\u00a7oSeyyed Saeed Sarfjoo\\nSrikanth Madikeri\\nPetr Motlicek\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.12277\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 11 Apr 2021 10:40:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2021\\u00a7r"}']}
{title:'Pandey et al. (§72021§r)', author: 'Ashutosh Pandey; DeLiang Wang', display:{Lore:['[{"text": "arXiv:2010.12713", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDual-path Self-Attention RNN for Real-Time Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oAshutosh Pandey\\nDeLiang Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.12713\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 29 Apr 2021 16:36:10 GMT)\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Hang Li; Wenbiao Ding; Zhongqin Wu; Zitao Liu', display:{Lore:['[{"text": "arXiv:2010.12733", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Fine-Grained Cross Modality Excitement for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHang Li\\nWenbiao Ding\\nZhongqin Wu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.12733\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 15 Jul 2021 04:30:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe Interspeech Conference, 2021 (INTERSPEECH 2021)\\u00a7r"}']}
{title:'Mathov et al. (§72021§r)', author: 'Yael Mathov; Tal Ben Senior; Asaf Shabtai; Yuval Elovici', display:{Lore:['[{"text": "arXiv:2010.12809", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStop Bugging Me! Evading Modern-Day Wiretapping Using Adversarial Perturbations\\u00a7r\\n\\n\\u00a78\\u00a7oYael Mathov\\nTal Ben Senior\\nAsaf Shabtai\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.12809\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 2 Sep 2021 07:54:52 GMT)\\u00a7r"}']}
{title:'Cao et al. (§72021§r)', author: 'Yin Cao; Turab Iqbal; Qiuqiang Kong; Fengyan An; Wenwu Wang; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:2010.13092", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Improved Event-Independent Network for Polyphonic Sound Event Localization and Detection\\u00a7r\\n\\n\\u00a78\\u00a7oYin Cao\\nTurab Iqbal\\nQiuqiang Kong\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.13092\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 11 Feb 2021 04:44:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2021 IEEE International Conference on Acoustics, Speech and Signal Processing\\u00a7r"}']}
{title:'Kawahara et al. (§72021§r)', author: 'Hideki Kawahara; Kohei Yatabe', display:{Lore:['[{"text": "arXiv:2010.13185", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCascaded all-pass filters with randomized center frequencies and phase polarity for acoustic and speech measurement and data augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oHideki Kawahara\\nKohei Yatabe\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.13185\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 13 Feb 2021 03:58:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, Accepted ICASSP2021(Review comment by all reviewers: Very original)\\u00a7r"}']}
{title:'Ratnarajah et al. (§72021§r)', author: 'Anton Ratnarajah; Zhenyu Tang; Dinesh Manocha', display:{Lore:['[{"text": "arXiv:2010.13219", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIR-GAN: Room Impulse Response Generator for Far-field Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAnton Ratnarajah\\nZhenyu Tang\\nDinesh Manocha\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.13219\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 6 Apr 2021 18:18:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oconference revision\\u00a7r"}']}
{title:'Yang et al. (§72021§r)', author: 'Chao-Han Huck Yang; Jun Qi; Samuel Yen-Chi Chen; Pin-Yu Chen; Sabato Marco Siniscalchi; Xiaoli Ma; Chin-Hui Lee', display:{Lore:['[{"text": "arXiv:2010.13309", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75quant-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDecentralizing Feature Extraction with Quantum Convolutional Neural Network for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oChao-Han Huck Yang\\nJun Qi\\nSamuel Yen-Chi Chen\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.13309\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9413453\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2021 IEEE International Conference on Acoustics, Speech and Signal\\n  Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 12 Feb 2021 05:53:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEICASSP 2021. Code is available: https://github.com/huckiyang/QuantumSpeech-QCNN\\u00a7r"}']}
{title:'Turner et al. (§72021§r)', author: 'Henry Turner; Giulio Lovisotto; Ivan Martinovic', display:{Lore:['[{"text": "arXiv:2010.13457", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Anonymization with Distribution-Preserving X-Vector Generation for the VoicePrivacy Challenge 2020\\u00a7r\\n\\n\\u00a78\\u00a7oHenry Turner\\nGiulio Lovisotto\\nIvan Martinovic\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.13457\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 5 Jan 2021 16:11:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages Replacement: A small processing bug led to slightly incorrectresults. Conclusions remain the same\\u00a7r"}']}
{title:'Sun et al. (§72021§r)', author: 'Chung-En Sun; Yi-Wei Chen; Hung-Shin Lee; Yen-Hsing Chen; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:2010.13468", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMelody Harmonization Using Orderless NADE, Chord Balancing, and Blocked Gibbs Sampling\\u00a7r\\n\\n\\u00a78\\u00a7oChung-En Sun\\nYi-Wei Chen\\nHung-Shin Lee\\nYen-Hsing Chen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.13468\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 1 Feb 2021 08:25:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2021, and Demo is available at: https://chord-generation.herokuapp.com/demo\\u00a7r"}']}
{title:'Du et al. (§72021§r)', author: 'Xingjian Du; Zhesong Yu; Bilei Zhu; Xiaoou Chen; Zejun Ma', display:{Lore:['[{"text": "arXiv:2010.14022", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lByteCover: Cover Song Identification via Multi-Loss Training\\u00a7r\\n\\n\\u00a78\\u00a7oXingjian Du\\nZhesong Yu\\nBilei Zhu\\nXiaoou Chen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.14022\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 23 Apr 2021 06:30:03 GMT)\\u00a7r"}']}
{title:'Luu et al. (§72021§r)', author: 'Chau Luu; Peter Bell; Steve Renals', display:{Lore:['[{"text": "arXiv:2010.14269", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging speaker attribute information using multi task learning for speaker verification and diarization\\u00a7r\\n\\n\\u00a78\\u00a7oChau Luu\\nPeter Bell\\nSteve Renals\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.14269\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 23 Apr 2021 16:45:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2021\\u00a7r"}']}
{title:'Pons et al. (§72021§r)', author: 'Jordi Pons; Santiago Pascual; Giulio Cengarle; Joan Serrà', display:{Lore:['[{"text": "arXiv:2010.14356", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUpsampling artifacts in neural audio synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oJordi Pons\\nSantiago Pascual\\nGiulio Cengarle\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.14356\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 Feb 2021 17:21:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn proceedings of ICASSP2021. Code: https://github.com/DolbyLaboratories/neural-upsampling-artifacts-audio\\u00a7r"}']}
{title:'Pappagari et al. (§72021§r)', author: 'Raghavendra Pappagari; Jesús Villalba; Piotr Żelasko; Laureano Moro-Velazquez; Najim Dehak', display:{Lore:['[{"text": "arXiv:2010.14602", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCopyPaste: An Augmentation Method for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oRaghavendra Pappagari\\nJes\\u00fas Villalba\\nPiotr \\u017belasko\\nLaureano Moro-Velazquez\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.14602\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 11 Feb 2021 16:04:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP2021\\u00a7r"}']}
{title:'Zhou et al. (§72021§r)', author: 'Kun Zhou; Berrak Sisman; Rui Liu; Haizhou Li', display:{Lore:['[{"text": "arXiv:2010.14794", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSeen and Unseen emotional style transfer for voice conversion with a new emotional speech dataset\\u00a7r\\n\\n\\u00a78\\u00a7oKun Zhou\\nBerrak Sisman\\nRui Liu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.14794\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 11 Feb 2021 02:30:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2021\\u00a7r"}']}
{title:'Song et al. (§72021§r)', author: 'Xingchen Song; Zhiyong Wu; Yiheng Huang; Chao Weng; Dan Su; Helen Meng', display:{Lore:['[{"text": "arXiv:2010.15025", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-Autoregressive Transformer ASR with CTC-Enhanced Decoder Input\\u00a7r\\n\\n\\u00a78\\u00a7oXingchen Song\\nZhiyong Wu\\nYiheng Huang\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.15025\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 16 Apr 2021 03:42:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021, final version\\u00a7r"}']}
{title:'Bailey et al. (§72021§r)', author: 'Andrew Bailey; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:2010.15120", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGender Bias in Depression Detection Using Audio Features\\u00a7r\\n\\n\\u00a78\\u00a7oAndrew Bailey\\nMark D. Plumbley\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.15120\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 18 Aug 2021 10:00:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, to be published at EUSIPCO 2021\\u00a7r"}']}
{title:'Hsieh et al. (§72021§r)', author: 'Tsun-An Hsieh; Cheng Yu; Szu-Wei Fu; Xugang Lu; Yu Tsao', display:{Lore:['[{"text": "arXiv:2010.15174", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Perceptual Quality by Phone-Fortified Perceptual Loss using Wasserstein Distance for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oTsun-An Hsieh\\nCheng Yu\\nSzu-Wei Fu\\nXugang Lu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.15174\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 27 Apr 2021 08:14:56 GMT)\\u00a7r"}']}
{title:'Reddy et al. (§72021§r)', author: 'Chandan K A Reddy; Vishak Gopal; Ross Cutler', display:{Lore:['[{"text": "arXiv:2010.15258", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDNSMOS: A Non-Intrusive Perceptual Objective Speech Quality metric to evaluate Noise Suppressors\\u00a7r\\n\\n\\u00a78\\u00a7oChandan K A Reddy\\nVishak Gopal\\nRoss Cutler\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.15258\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 10 Feb 2021 22:37:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Huang et al. (§72021§r)', author: 'Sung-Feng Huang; Shun-Po Chuang; Da-Rong Liu; Yi-Chen Chen; Gene-Ping Yang; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2010.15366", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStabilizing Label Assignment for Speech Separation by Self-supervised Pre-training\\u00a7r\\n\\n\\u00a78\\u00a7oSung-Feng Huang\\nShun-Po Chuang\\nDa-Rong Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.15366\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 22 Aug 2021 06:26:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2021\\u00a7r"}']}
{title:'Brown et al. (§72021§r)', author: 'Andrew Brown; Jaesung Huh; Arsha Nagrani; Joon Son Chung; Andrew Zisserman', display:{Lore:['[{"text": "arXiv:2010.15716", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPlaying a Part: Speaker Verification at the Movies\\u00a7r\\n\\n\\u00a78\\u00a7oAndrew Brown\\nJaesung Huh\\nArsha Nagrani\\nJoon Son Chung\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.15716\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 11 Feb 2021 09:23:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe first three authors contributed equally to this work\\u00a7r"}']}
{title:'Gairola et al. (§72021§r)', author: 'Siddhartha Gairola; Francis Tom; Nipun Kwatra; Mohit Jain', display:{Lore:['[{"text": "arXiv:2011.00196", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRespireNet: A Deep Neural Network for Accurately Detecting Abnormal Lung Sounds in Limited Data Setting\\u00a7r\\n\\n\\u00a78\\u00a7oSiddhartha Gairola\\nFrancis Tom\\nNipun Kwatra\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.00196\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 7 May 2021 14:31:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCode visible at https://github.com/microsoft/RespireNet\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Tingle Li; Yichen Liu; Chenxu Hu; Hang Zhao', display:{Lore:['[{"text": "arXiv:2011.00782", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCVC: Contrastive Learning for Non-parallel Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oTingle Li\\nYichen Liu\\nChenxu Hu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.00782\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 2 Apr 2021 16:28:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted Interspeech 2021, Project Page: https://tinglok.netlify.app/files/cvc/\\u00a7r"}']}
{title:'Tzinis et al. (§72021§r)', author: 'Efthymios Tzinis; Scott Wisdom; Aren Jansen; Shawn Hershey; Tal Remez; Daniel P. W. Ellis; John R. Hershey', display:{Lore:['[{"text": "arXiv:2011.01143", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInto the Wild with AudioScope: Unsupervised Audio-Visual Separation of On-Screen Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oEfthymios Tzinis\\nScott Wisdom\\nAren Jansen\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.01143\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 30 May 2021 03:47:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICLR 2021, 27 pages\\u00a7r"}']}
{title:'Shrivastava et al. (§72021§r)', author: 'Ashish Shrivastava; Arnav Kundu; Chandra Dhir; Devang Naik; Oncel Tuzel', display:{Lore:['[{"text": "arXiv:2011.01151", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimize what matters: Training DNN-HMM Keyword Spotting Model Using End Metric\\u00a7r\\n\\n\\u00a78\\u00a7oAshish Shrivastava\\nArnav Kundu\\nChandra Dhir\\nDevang Naik\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.01151\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 26 Feb 2021 00:06:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2021\\u00a7r"}']}
{title:'Balian et al. (§72021§r)', author: 'Julien Balian; Raffaele Tavarone; Mathieu Poumeyrol; Alice Coucke', display:{Lore:['[{"text": "arXiv:2011.01709", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSmall footprint Text-Independent Speaker Verification for Embedded Systems\\u00a7r\\n\\n\\u00a78\\u00a7oJulien Balian\\nRaffaele Tavarone\\nMathieu Poumeyrol\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.01709\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nAcoustics, Speech and Signal Processing (ICASSP), 2021 IEEE\\n  International Conference\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 21 Apr 2021 16:18:53 GMT)\\u00a7r"}']}
{title:'Han et al. (§72021§r)', author: 'Eunjung Han; Chul Lee; Andreas Stolcke', display:{Lore:['[{"text": "arXiv:2011.02678", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBW-EDA-EEND: Streaming End-to-End Neural Speaker Diarization for a Variable Number of Speakers\\u00a7r\\n\\n\\u00a78\\u00a7oEunjung Han\\nChul Lee\\nAndreas Stolcke\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.02678\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9414371\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. IEEE ICASSP, June 2021, pp. 7193-7197\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 12 Feb 2021 18:21:17 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72021§r)', author: 'Haici Yang; Hongda Mao; Ruirui Li; Chelsea J. T. Ju; Oguz Elibol', display:{Lore:['[{"text": "arXiv:2011.03682", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-local convolutional neural networks (nlcnn) for speaker recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHaici Yang\\nHongda Mao\\nRuirui Li\\nChelsea J. T. Ju\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.03682\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 20 May 2021 00:13:28 GMT)\\u00a7r"}']}
{title:'Paul et al. (§72021§r)', author: 'Soumava Paul; Gurunath Reddy M; K Sreenivasa Rao; Partha Pratim Das', display:{Lore:['[{"text": "arXiv:2011.04297", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKnowledge Distillation for Singing Voice Detection\\u00a7r\\n\\n\\u00a78\\u00a7oSoumava Paul\\nGurunath Reddy M\\nK Sreenivasa Rao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.04297\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 19 Aug 2021 18:27:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2021. 5 pages, 3 figures\\u00a7r"}']}
{title:'Lian et al. (§72021§r)', author: 'Jiachen Lian; Aiswarya Vinod Kumar; Hira Dhamyal; Bhiksha Raj; Rita Singh', display:{Lore:['[{"text": "arXiv:2011.04491", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMasked Proxy Loss For Text-Independent Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oJiachen Lian\\nAiswarya Vinod Kumar\\nHira Dhamyal\\nBhiksha Raj\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.04491\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-2190\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 25 Jun 2021 03:10:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2021\\u00a7r"}']}
{title:'Peplinski et al. (§72021§r)', author: 'Jacob Peplinski; Joel Shor; Sachin Joglekar; Jake Garrison; Shwetak Patel', display:{Lore:['[{"text": "arXiv:2011.04609", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFRILL: A Non-Semantic Speech Embedding for Mobile Devices\\u00a7r\\n\\n\\u00a78\\u00a7oJacob Peplinski\\nJoel Shor\\nSachin Joglekar\\nJake Garrison\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.04609\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-2070\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2021\\u00a7r\\n\\nVersion:\\u00a77v5 (Thu, 10 Jun 2021 16:18:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Shiguang Liu; Dinesh Manocha', display:{Lore:['[{"text": "arXiv:2011.05538", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.GR\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Synthesis, Propagation, and Rendering: A Survey\\u00a7r\\n\\n\\u00a78\\u00a7oShiguang Liu\\nDinesh Manocha\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.05538\\u00a7r\\n\\nVersion:\\u00a77v5 (Tue, 4 May 2021 03:11:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o27 pages\\u00a7r"}']}
{title:'Casebeer et al. (§72021§r)', author: 'Jonah Casebeer; Jamshed Kaikaus; Paris Smaragdis', display:{Lore:['[{"text": "arXiv:2011.07348", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCommunication-Cost Aware Microphone Selection For Neural Speech Enhancement with Ad-hoc Microphone Arrays\\u00a7r\\n\\n\\u00a78\\u00a7oJonah Casebeer\\nJamshed Kaikaus\\nParis Smaragdis\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.07348\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 21 Apr 2021 15:16:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, ICASSP 2021\\u00a7r"}']}
{title:'Kodrasi et al. (§72021§r)', author: 'I. Kodrasi; M. Pernon; M. Laganaro; H. Bourlard', display:{Lore:['[{"text": "arXiv:2011.07542", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic and perceptual discrimination between dysarthria, apraxia of speech, and neurotypical speech\\u00a7r\\n\\n\\u00a78\\u00a7oI. Kodrasi\\nM. Pernon\\nM. Laganaro\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.07542\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 2 Jun 2021 08:36:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2021\\u00a7r"}']}
{title:'Shamsabadi et al. (§72021§r)', author: 'Ali Shahin Shamsabadi; Francisco Sepúlveda Teixeira; Alberto Abad; Bhiksha Raj; Andrea Cavallaro; Isabel Trancoso', display:{Lore:['[{"text": "arXiv:2011.08483", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFoolHD: Fooling speaker identification by Highly imperceptible adversarial Disturbances\\u00a7r\\n\\n\\u00a78\\u00a7oAli Shahin Shamsabadi\\nFrancisco Sep\\u00falveda Teixeira\\nAlberto Abad\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.08483\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 20 Feb 2021 12:15:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ohttps://fsepteixeira.github.io/FoolHD/\\u00a7r"}']}
{title:'Wu et al. (§72021§r)', author: 'Yuan-Kuei Wu; Kuan-Po Huang; Yu Tsao; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2011.10233", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOne Shot Learning for Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oYuan-Kuei Wu\\nKuan-Po Huang\\nYu Tsao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.10233\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 3 May 2021 11:23:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Soltanian et al. (§72021§r)', author: 'Mohammad Soltanian; Junaid Malik; Jenni Raitoharju; Alexandros Iosifidis; Serkan Kiranyaz; Moncef Gabbouj', display:{Lore:['[{"text": "arXiv:2011.11436", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Command Recognition in Computationally Constrained Environments with a Quadratic Self-organized Operational Layer\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Soltanian\\nJunaid Malik\\nJenni Raitoharju\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.11436\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 10 Feb 2021 18:28:13 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Wei Wang; Chao Zhang; Xiaopei Wu', display:{Lore:['[{"text": "arXiv:2011.12461", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Discriminative Feature Learning for Accent Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oWei Wang\\nChao Zhang\\nXiaopei Wu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.12461\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 25 Aug 2021 09:18:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 4 figures, conference\\u00a7r"}']}
{title:'Sarkar et al. (§72021§r)', author: 'Achintya kr. Sarkar; Zheng-Hua Tan', display:{Lore:['[{"text": "arXiv:2011.12536", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVocal Tract Length Perturbation for Text-Dependent Speaker Verification with Autoregressive Prediction Coding\\u00a7r\\n\\n\\u00a78\\u00a7oAchintya kr. Sarkar\\nZheng-Hua Tan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.12536\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2021.3055180\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Signal Processing Letters, vol. 28, pp. 364-368, 2021\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 Mar 2021 18:57:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCopyright (c) 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for "}','{"text": "advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Yadav et al. (§72021§r)', author: 'Hemant Yadav; Atul Anshuman Singh; Rachit Mittal; Sunayana Sitaram; Yi Yu; Rajiv Ratn Shah', display:{Lore:['[{"text": "arXiv:2011.12979", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lmask-Net: Learning Context Aware Invariant Features using Adversarial Forgetting (Student Abstract)\\u00a7r\\n\\n\\u00a78\\u00a7oHemant Yadav\\nAtul Anshuman Singh\\nRachit Mittal\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.12979\\u00a7r\\n\\nVersion:\\u00a77v5 (Mon, 18 Oct 2021 12:56:38 GMT)\\u00a7r"}']}
{title:'Lu et al. (§72021§r)', author: 'Liang Lu; Naoyuki Kanda; Jinyu Li; Yifan Gong', display:{Lore:['[{"text": "arXiv:2011.13148", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreaming end-to-end multi-talker speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oLiang Lu\\nNaoyuki Kanda\\nJinyu Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.13148\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2021.3070817\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 12 Mar 2021 19:56:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures. Accepted to IEEE SignalProcessing Letters 2021\\u00a7r"}']}
{title:'Chaudhari et al. (§72021§r)', author: 'Gunvant Chaudhari; Xinyi Jiang; Ahmed Fakhry; Asriel Han; Jaclyn Xiao; Sabrina Shen; Amil Khanzada', display:{Lore:['[{"text": "arXiv:2011.13320", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVirufy: Global Applicability of Crowdsourced and Clinical Datasets for AI Detection of COVID-19 from Cough\\u00a7r\\n\\n\\u00a78\\u00a7oGunvant Chaudhari\\nXinyi Jiang\\nAhmed Fakhry\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.13320\\u00a7r\\n\\nVersion:\\u00a77v4 (Sat, 9 Jan 2021 05:23:19 GMT)\\u00a7r"}']}
{title:'Shi et al. (§72021§r)', author: 'Jiatong Shi; Chunlei Zhang; Chao Weng; Shinji Watanabe; Meng Yu; Dong Yu', display:{Lore:['[{"text": "arXiv:2011.13393", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving RNN Transducer With Target Speaker Extraction and Neural Uncertainty Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oJiatong Shi\\nChunlei Zhang\\nChao Weng\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.13393\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 26 Feb 2021 16:16:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2021\\u00a7r"}']}
{title:'Zhu et al. (§72021§r)', author: 'Wenbo Zhu; Mou Wang; Xiao-Lei Zhang; Susanto Rahardja', display:{Lore:['[{"text": "arXiv:2011.14295", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA comparison of handcrafted, parameterized, and learnable features for speech separation\\u00a7r\\n\\n\\u00a78\\u00a7oWenbo Zhu\\nMou Wang\\nXiao-Lei Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.14295\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Jan 2021 10:40:00 GMT)\\u00a7r"}']}
{title:'Boeddeker et al. (§72021§r)', author: 'Christoph Boeddeker; Wangyou Zhang; Tomohiro Nakatani; Keisuke Kinoshita; Tsubasa Ochiai; Marc Delcroix; Naoyuki Kamo; Yanmin Qian; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2011.15003", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConvolutive Transfer Function Invariant SDR training criteria for Multi-Channel Reverberant Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oChristoph Boeddeker\\nWangyou Zhang\\nTomohiro Nakatani\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.15003\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 8 Jun 2021 07:40:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2021\\u00a7r"}']}
{title:'Zeng et al. (§72021§r)', author: 'Donghuo Zeng; Yi Yu; Keizo Oyama', display:{Lore:['[{"text": "arXiv:2012.00290", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.DB\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusicTM-Dataset for Joint Representation Learning among Sheet Music, Lyrics, and Musical Audio\\u00a7r\\n\\n\\u00a78\\u00a7oDonghuo Zeng\\nYi Yu\\nKeizo Oyama\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.00290\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nCSMT2020\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 7 May 2021 14:20:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 5 figures, 2 tables\\u00a7r"}']}
{title:'Sharma et al. (§72021§r)', author: 'Bidisha Sharma; Xiaoxue Gao; Karthika Vijayan; Xiaohai Tian; Haizhou Li', display:{Lore:['[{"text": "arXiv:2012.00337", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNHSS: A Speech and Singing Parallel Database\\u00a7r\\n\\n\\u00a78\\u00a7oBidisha Sharma\\nXiaoxue Gao\\nKarthika Vijayan\\nXiaohai Tian\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.00337\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 5 Aug 2021 11:32:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Speech Communication\\u00a7r"}']}
{title:'Pahar et al. (§72021§r)', author: 'Madhurananda Pahar; Marisa Klopper; Robin Warren; Thomas Niesler', display:{Lore:['[{"text": "arXiv:2012.01926", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCOVID-19 Cough Classification using Machine Learning and Global Smartphone Recordings\\u00a7r\\n\\n\\u00a78\\u00a7oMadhurananda Pahar\\nMarisa Klopper\\nRobin Warren\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.01926\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.compbiomed.2021.104572\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nComputers in Biology and Medicine, 2021\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 14 Jun 2021 10:24:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper has been accepted in \\"Computersin Medicine and Biology\\" and currently under production\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Shengchen Li; Yinji Jing; György Fazekas', display:{Lore:['[{"text": "arXiv:2012.03646", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA novel dataset for the identification of computer generated melodies in the CSMT challenge\\u00a7r\\n\\n\\u00a78\\u00a7oShengchen Li\\nYinji Jing\\nGy\\u00f6rgy Fazekas\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.03646\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-981-16-1649-5_15\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIn Proceedings of the 8th Conference on Sound and Music\\n  Technology. CSMT 2020. Lecture Notes in Electrical Engineering, vol 761.\\n  Springer\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 1 Dec 2021 01:26:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished by Conference on Sound and Music Technology\\u00a7r"}']}
{title:'Chowdhury et al. (§72021§r)', author: 'Anurag Chowdhury; Arun Ross; Prabu David', display:{Lore:['[{"text": "arXiv:2012.05084", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeepTalk: Vocal Style Encoding for Speaker Recognition and Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oAnurag Chowdhury\\nArun Ross\\nPrabu David\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.05084\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 14 Feb 2021 14:24:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in IEEE ICASSP 2021, 5 pages, 3 figures\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Binbin Zhang; Di Wu; Zhuoyuan Yao; Xiong Wang; Fan Yu; Chao Yang; Liyong Guo; Yaguang Hu; Lei Xie; Xin Lei', display:{Lore:['[{"text": "arXiv:2012.05481", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oBinbin Zhang\\nDi Wu\\nZhuoyuan Yao\\n+ 6 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.05481\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 29 Dec 2021 10:38:00 GMT)\\u00a7r"}']}
{title:'Fan et al. (§72021§r)', author: 'Zhiyun Fan; Meng Li; Shiyu Zhou; Bo Xu', display:{Lore:['[{"text": "arXiv:2012.06185", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring wav2vec 2.0 on speaker verification and language identification\\u00a7r\\n\\n\\u00a78\\u00a7oZhiyun Fan\\nMeng Li\\nShiyu Zhou\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.06185\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Jan 2021 14:17:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSelf-supervised, speaker verification, language identification, multi-tasklearning, wav2vec 2.0\\u00a7r"}']}
{title:'Bortolozzo et al. (§72021§r)', author: 'Marcelo Bortolozzo; Rodrigo Schramm; Claudio R. Jung', display:{Lore:['[{"text": "arXiv:2012.07055", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving the Classification of Rare Chords with Unlabeled Data\\u00a7r\\n\\n\\u00a78\\u00a7oMarcelo Bortolozzo\\nRodrigo Schramm\\nClaudio R. Jung\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.07055\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 10 Feb 2021 11:57:24 GMT)\\u00a7r"}']}
{title:'Vashkevich et al. (§72021§r)', author: 'Maxim Vashkevich; Yulia Rushkevich', display:{Lore:['[{"text": "arXiv:2012.07347", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lClassification of ALS patients based on acoustic analysis of sustained vowel phonations\\u00a7r\\n\\n\\u00a78\\u00a7oMaxim Vashkevich\\nYulia Rushkevich\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.07347\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.bspc.2020.102350\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nBiomedical Signal Processing and Control, Volume 65, March 2021,\\n  102350\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 11 Jan 2021 08:41:07 GMT)\\u00a7r"}']}
{title:'Xie et al. (§72021§r)', author: 'Xurong Xie; Xunying Liu; Tan Lee; Lan Wang', display:{Lore:['[{"text": "arXiv:2012.07460", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBayesian Learning for Deep Neural Network Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oXurong Xie\\nXunying Liu\\nTan Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.07460\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3084072\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  Volume: 29, 2021\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 15 Aug 2021 08:45:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7opublished in TASLP, and with extra appendices of released codes and updated results\\u00a7r"}']}
{title:'Goudarzi et al. (§72021§r)', author: 'Armin Goudarzi; Carsten Spehr; Steffen Herbold', display:{Lore:['[{"text": "arXiv:2012.09643", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic source localization and spectra generation from sparse beamforming maps\\u00a7r\\n\\n\\u00a78\\u00a7oArmin Goudarzi\\nCarsten Spehr\\nSteffen Herbold\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.09643\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0005885\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 22 Jul 2021 09:30:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint for JASA special issue on machine learning in acoustics, Revision 2\\u00a7r"}']}
{title:'Yao et al. (§72021§r)', author: 'Wei Yao; Shen Chen; Jiamin Cui; Yaolin Lou', display:{Lore:['[{"text": "arXiv:2012.11159", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-stream Convolutional Neural Network with Frequency Selection for Robust Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oWei Yao\\nShen Chen\\nJiamin Cui\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.11159\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 12 Jan 2021 11:29:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 11 figures, 8 tables\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Lantian Li; Ruiqi Liu; Jiawen Kang; Yue Fan; Hao Cui; Yunqi Cai; Ravichander Vipperla; Thomas Fang Zheng; Dong Wang', display:{Lore:['[{"text": "arXiv:2012.12468", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCN-Celeb: multi-genre speaker recognition\\u00a7r\\n\\n\\u00a78\\u00a7oLantian Li\\nRuiqi Liu\\nJiawen Kang\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.12468\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 24 Nov 2021 08:55:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to Speech Communication\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Lantian Li; Dong Wang; Jiawen Kang; Renyu Wang; Jing Wu; Zhendong Gao; Xiao Chen', display:{Lore:['[{"text": "arXiv:2012.12471", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Principle Solution for Enroll-Test Mismatch in Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oLantian Li\\nDong Wang\\nJiawen Kang\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.12471\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 24 Nov 2021 08:39:13 GMT)\\u00a7r"}']}
{title:'Saeki et al. (§72021§r)', author: 'Takaaki Saeki; Shinnosuke Takamichi; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2012.12612", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIncremental Text-to-Speech Synthesis Using Pseudo Lookahead with Large Pretrained Language Model\\u00a7r\\n\\n\\u00a78\\u00a7oTakaaki Saeki\\nShinnosuke Takamichi\\nHiroshi Saruwatari\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.12612\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2021.3073869\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 14 Apr 2021 18:42:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for IEEE Signal Processing Letters\\u00a7r"}']}
{title:'Szelogowski (§72021§r)', author: 'Daniel Szelogowski', display:{Lore:['[{"text": "arXiv:2101.00169", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerative Deep Learning for Virtuosic Classical Music: Generative Adversarial Networks as Renowned Composers\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Szelogowski\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.00169\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 13 Nov 2021 01:26:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 6 figures Update: Revised format to align closer to IEEE standards\\u00a7r"}']}
{title:'Latif et al. (§72021§r)', author: 'Siddique Latif; Heriberto Cuayáhuitl; Farrukh Pervez; Fahad Shamshad; Hafiz Shehbaz Ali; Erik Cambria', display:{Lore:['[{"text": "arXiv:2101.00240", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Survey on Deep Reinforcement Learning for Audio-Based Applications\\u00a7r\\n\\n\\u00a78\\u00a7oSiddique Latif\\nHeriberto Cuay\\u00e1huitl\\nFarrukh Pervez\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.00240\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Jan 2021 14:15:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder Review\\u00a7r"}']}
{title:'Lordelo et al. (§72021§r)', author: 'Carlos Lordelo; Emmanouil Benetos; Simon Dixon; Sven Ahlbäck; Patrik Ohlsson', display:{Lore:['[{"text": "arXiv:2101.00701", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Unsupervised Domain Adaptation for Harmonic-Percussive Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oCarlos Lordelo\\nEmmanouil Benetos\\nSimon Dixon\\nSven Ahlb\\u00e4ck\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.00701\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2020.3045915 10.5281/zenodo.4308731\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 3 Jan 2021 20:46:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures and 1 table. Accepted for publication in IEEE Signal Processing Letters\\u00a7r"}']}
{title:'Rajapakshe et al. (§72021§r)', author: 'Thejan Rajapakshe; Rajib Rana; Sara Khalifa; Björn W. Schuller; Jiajun Liu', display:{Lore:['[{"text": "arXiv:2101.00738", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA novel policy for pre-trained Deep Reinforcement Learning for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oThejan Rajapakshe\\nRajib Rana\\nSara Khalifa\\nBj\\u00f6rn W. Schuller\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.00738\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3511616.3513104\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 31 Jan 2021 10:06:52 GMT)\\u00a7r"}']}
{title:'Xu et al. (§72021§r)', author: 'Yong Xu; Zhuohuang Zhang; Meng Yu; Shi-Xiong Zhang; Dong Yu', display:{Lore:['[{"text": "arXiv:2101.01280", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeneralized Spatio-Temporal RNN Beamformer for Target Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oYong Xu\\nZhuohuang Zhang\\nMeng Yu\\nShi-Xiong Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.01280\\u00a7r\\n\\nVersion:\\u00a77v5 (Sat, 3 Apr 2021 08:16:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech2021, Demo: https://yongxuustc.github.io/grnnbf/\\u00a7r"}']}
{title:'Hsu et al. (§72021§r)', author: 'Fu-Shun Hsu; Chao-Jung Huang; Chen-Yi Kuo; Shang-Ran Huang; Yuan-Ren Cheng; Jia-Horng Wang; Yi-Lin Wu; Tzu-Ling Tzeng; Feipei Lai', display:{Lore:['[{"text": "arXiv:2101.01352", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDevelopment of a Respiratory Sound Labeling Software for Training a Deep Learning-Based Respiratory Sound Analysis Model\\u00a7r\\n\\n\\u00a78\\u00a7oFu-Shun Hsu\\nChao-Jung Huang\\nChen-Yi Kuo\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.01352\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Jan 2021 04:59:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 2 figures, Accepted by International Forum On Medical Imaging In Asia (IFMIA 2021)\\u00a7r"}']}
{title:'Chang et al. (§72021§r)', author: 'Xuankai Chang; Naoyuki Kanda; Yashesh Gaur; Xiaofei Wang; Zhong Meng; Takuya Yoshioka', display:{Lore:['[{"text": "arXiv:2101.01853", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHypothesis Stitcher for End-to-End Speaker-attributed ASR on Long-form Multi-talker Recordings\\u00a7r\\n\\n\\u00a78\\u00a7oXuankai Chang\\nNaoyuki Kanda\\nYashesh Gaur\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.01853\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Jan 2021 03:36:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2021\\u00a7r"}']}
{title:'Jiang et al. (§72021§r)', author: 'Chunheng Jiang; Jae-wook Ahn; Nirmit Desai', display:{Lore:['[{"text": "arXiv:2101.01863", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnvironment Transfer for Distributed Systems\\u00a7r\\n\\n\\u00a78\\u00a7oChunheng Jiang\\nJae-wook Ahn\\nNirmit Desai\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.01863\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Jan 2021 04:27:24 GMT)\\u00a7r"}']}
{title:'Reddy et al. (§72021§r)', author: 'Chandan K A Reddy; Harishchandra Dubey; Kazuhito Koishida; Arun Nair; Vishak Gopal; Ross Cutler; Sebastian Braun; Hannes Gamper; Robert Aichner; Sriram Srinivasan', display:{Lore:['[{"text": "arXiv:2101.01902", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInterspeech 2021 Deep Noise Suppression Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oChandan K A Reddy\\nHarishchandra Dubey\\nKazuhito Koishida\\n+ 6 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.01902\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 5 Apr 2021 01:19:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2009.06122\\u00a7r"}']}
{title:'Grumiaux et al. (§72021§r)', author: 'Pierre-Amaury Grumiaux; Srdan Kitic; Laurent Girin; Alexandre Guérin', display:{Lore:['[{"text": "arXiv:2101.01977", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultichannel CRNN for Speaker Counting: an Analysis of Performance\\u00a7r\\n\\n\\u00a78\\u00a7oPierre-Amaury Grumiaux\\nSrdan Kitic\\nLaurent Girin\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.01977\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Jan 2021 11:29:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at Forum Acusticum 2020\\u00a7r"}']}
{title:'Yesiler et al. (§72021§r)', author: 'Furkan Yesiler; Emilio Molina; Joan Serrà; Emilia Gómez', display:{Lore:['[{"text": "arXiv:2101.02098", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating the efficacy of music version retrieval systems for setlist identification\\u00a7r\\n\\n\\u00a78\\u00a7oFurkan Yesiler\\nEmilio Molina\\nJoan Serr\\u00e0\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.02098\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Jan 2021 15:41:12 GMT)\\u00a7r"}']}
{title:'Hsiao et al. (§72021§r)', author: 'Wen-Yi Hsiao; Jen-Yu Liu; Yin-Cheng Yeh; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2101.02402", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCompound Word Transformer: Learning to Compose Full-Song Music over Dynamic Directed Hypergraphs\\u00a7r\\n\\n\\u00a78\\u00a7oWen-Yi Hsiao\\nJen-Yu Liu\\nYin-Cheng Yeh\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.02402\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Jan 2021 06:57:34 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Yangyong Zhang; Maliheh Shirvanian; Sunpreet S. Arora; Jianwei Huang; Guofei Gu', display:{Lore:['[{"text": "arXiv:2101.04773", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPractical Speech Re-use Prevention in Voice-driven Services\\u00a7r\\n\\n\\u00a78\\u00a7oYangyong Zhang\\nMaliheh Shirvanian\\nSunpreet S. Arora\\nJianwei Huang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.04773\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Jan 2021 22:00:59 GMT)\\u00a7r"}']}
{title:'Broek (§72021§r)', author: 'Korneel van den Broek', display:{Lore:['[{"text": "arXiv:2101.04785", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\\u00a7r\\n\\n\\u00a78\\u00a7oKorneel van den Broek\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.04785\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Jan 2021 22:37:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 8 figures, samples and source code available on https://korneelvdbroek.github.io/mp3net\\u00a7r"}']}
{title:'Ren et al. (§72021§r)', author: 'Zhao Ren; Kun Qian; Fengquan Dong; Zhenyu Dai; Yoshiharu Yamamoto; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2101.04979", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Attention-based Representation Learning for Heart Sound Classification\\u00a7r\\n\\n\\u00a78\\u00a7oZhao Ren\\nKun Qian\\nFengquan Dong\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.04979\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Jan 2021 10:20:59 GMT)\\u00a7r"}']}
{title:'Kaushik et al. (§72021§r)', author: 'Manav Kaushik; Van Tung Pham; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2101.05056", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Speaker Height and age estimation using Attention Mechanism with LSTM-RNN\\u00a7r\\n\\n\\u00a78\\u00a7oManav Kaushik\\nVan Tung Pham\\nEng Siong Chng\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.05056\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Jan 2021 13:41:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 Pages\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Shengchen Li; Ke Tian; Rui Wang', display:{Lore:['[{"text": "arXiv:2101.05443", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised heart abnormality detection based on phonocardiogram analysis with Beta Variational Auto-Encoders\\u00a7r\\n\\n\\u00a78\\u00a7oShengchen Li\\nKe Tian\\nRui Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.05443\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Jan 2021 03:52:47 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Jianyu Wang; Shanzheng Guan; Shupei Liu; Xiao-Lei Zhang', display:{Lore:['[{"text": "arXiv:2101.06398", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMinimum-volume Multichannel Nonnegative matrix factorization for blind source separation\\u00a7r\\n\\n\\u00a78\\u00a7oJianyu Wang\\nShanzheng Guan\\nShupei Liu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.06398\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 Mar 2021 03:11:58 GMT)\\u00a7r"}']}
{title:'Takahashi et al. (§72021§r)', author: 'Naoya Takahashi; Mayank Kumar Singh; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2101.06842", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierarchical disentangled representation learning for singing voice conversion\\u00a7r\\n\\n\\u00a78\\u00a7oNaoya Takahashi\\nMayank Kumar Singh\\nYuki Mitsufuji\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.06842\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 26 Apr 2021 00:55:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at IJCNN 2021\\u00a7r"}']}
{title:'Garcia-Valencia et al. (§72021§r)', author: 'Sebastian Garcia-Valencia; Alejandro Betancourt; Juan G. Lalinde-Pulido', display:{Lore:['[{"text": "arXiv:2101.07669", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA framework to compare music generative models using automatic evaluation metrics extended to rhythm\\u00a7r\\n\\n\\u00a78\\u00a7oSebastian Garcia-Valencia\\nAlejandro Betancourt\\nJuan G. Lalinde-Pulido\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.07669\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Jan 2021 15:04:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2012.01231\\u00a7r"}']}
{title:'Dinkel et al. (§72021§r)', author: 'Heinrich Dinkel; Mengyue Wu; Kai Yu', display:{Lore:['[{"text": "arXiv:2101.07687", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards duration robust weakly supervised sound event detection\\u00a7r\\n\\n\\u00a78\\u00a7oHeinrich Dinkel\\nMengyue Wu\\nKai Yu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.07687\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3054313\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 5 Feb 2021 02:16:04 GMT)\\u00a7r"}']}
{title:'Altan et al. (§72021§r)', author: 'Gökhan Altan; Yakup Kutlu; Adnan Özhan Pekmezci; Serkan Nural', display:{Lore:['[{"text": "arXiv:2101.08288", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Diagnosis of Asthma using Hilbert-Huang Transform and Deep Learning on Lung Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oG\\u00f6khan Altan\\nYakup Kutlu\\nAdnan \\u00d6zhan Pekmezci\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.08288\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Jan 2021 19:04:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, in Turkish language, journal of intelligent systems with applications\\u00a7r"}']}
{title:'Balli et al. (§72021§r)', author: 'Osman Balli; Yakup Kutlu', display:{Lore:['[{"text": "arXiv:2101.08438", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEffect of Deep Learning Feature Inference Techniques on Respiratory Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oOsman Balli\\nYakup Kutlu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.08438\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7njournal of intelligent systems with applications, 2020\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Jan 2021 04:52:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, journal of intelligent systems with applications\\u00a7r"}']}
{title:'Karaca et al. (§72021§r)', author: 'Gizem Karaca; Yakup Kutlu', display:{Lore:['[{"text": "arXiv:2101.08441", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTurkish Voice Commands based Chess Game using Gammatone Cepstral Coefficients\\u00a7r\\n\\n\\u00a78\\u00a7oGizem Karaca\\nYakup Kutlu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.08441\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJournal of Artificial Intelligence with Application, 2020\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Jan 2021 04:58:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, Journal of Artificial Intelligence with Application\\u00a7r"}']}
{title:'Xue et al. (§72021§r)', author: 'Yawen Xue; Shota Horiguchi; Yusuke Fujita; Yuki Takashima; Shinji Watanabe; Paola Garcia; Kenji Nagamatsu', display:{Lore:['[{"text": "arXiv:2101.08473", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOnline Streaming End-to-End Neural Diarization Handling Overlapping Speech and Flexible Numbers of Speakers\\u00a7r\\n\\n\\u00a78\\u00a7oYawen Xue\\nShota Horiguchi\\nYusuke Fujita\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.08473\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 7 Apr 2021 02:43:32 GMT)\\u00a7r"}']}
{title:'Balli et al. (§72021§r)', author: 'Osman Balli; Yakup Kutlu', display:{Lore:['[{"text": "arXiv:2101.08495", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEffect of Window Size for Detection of Abnormalities in Respiratory Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oOsman Balli\\nYakup Kutlu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.08495\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nNatural and Engineering Sciences, 2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Jan 2021 08:28:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 3 figures, Natural and Engineering Sciences\\u00a7r"}']}
{title:'Ito et al. (§72021§r)', author: 'Nobutaka Ito; Rintaro Ikeshita; Hiroshi Sawada; Tomohiro Nakatani', display:{Lore:['[{"text": "arXiv:2101.08563", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Joint Diagonalization Based Efficient Approach to Underdetermined Blind Audio Source Separation Using the Multichannel Wiener Filter\\u00a7r\\n\\n\\u00a78\\u00a7oNobutaka Ito\\nRintaro Ikeshita\\nHiroshi Sawada\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.08563\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Jan 2021 11:42:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to IEEE/ACM Transactionson Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Zeghidour et al. (§72021§r)', author: 'Neil Zeghidour; Olivier Teboul; Félix de Chaumont Quitry; Marco Tagliasacchi', display:{Lore:['[{"text": "arXiv:2101.08596", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLEAF: A Learnable Frontend for Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oNeil Zeghidour\\nOlivier Teboul\\nF\\u00e9lix de Chaumont Quitry\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.08596\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Jan 2021 13:25:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICLR2021\\u00a7r"}']}
{title:'Pinto et al. (§72021§r)', author: 'Dennis Pinto; Jose-María Arnau; Antonio González', display:{Lore:['[{"text": "arXiv:2101.09083", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting Beam Search Confidence for Energy-Efficient Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDennis Pinto\\nJose-Mar\\u00eda Arnau\\nAntonio Gonz\\u00e1lez\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.09083\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Jan 2021 12:35:35 GMT)\\u00a7r"}']}
{title:'Kumar et al. (§72021§r)', author: 'A Kishore Kumar; Shefali Waldekar; Goutam Saha; Md Sahidullah', display:{Lore:['[{"text": "arXiv:2101.09884", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDomain-Dependent Speaker Diarization for the Third DIHARD Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oA Kishore Kumar\\nShefali Waldekar\\nGoutam Saha\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.09884\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Jan 2021 04:01:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work was presented in The Third DIHARD Speech Diarization Challenge Workshop\\u00a7r"}']}
{title:'Koppula et al. (§72021§r)', author: 'Sai Koppula; Shivang Singh', display:{Lore:['[{"text": "arXiv:2101.09904", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.NI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing Angle of Arrival for Improving Indoor Localization\\u00a7r\\n\\n\\u00a78\\u00a7oSai Koppula\\nShivang Singh\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.09904\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Jan 2021 05:52:19 GMT)\\u00a7r"}']}
{title:'Ziemer et al. (§72021§r)', author: 'Tim Ziemer; Pattararat Kiattipadungkul; Tanyarin Karuchit', display:{Lore:['[{"text": "arXiv:2101.10201", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNovel Recording Studio Features for Music Information Retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oTim Ziemer\\nPattararat Kiattipadungkul\\nTanyarin Karuchit\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.10201\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/2.0001363\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of Meetings on Acoustics 42(1), 2020, paper number\\n  035004\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Jan 2021 16:09:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 9 figures, Meeting of the Acoustical Society of America, Dec. 2020\\u00a7r"}']}
{title:'Al-Radhi (§72021§r)', author: 'Mohammed Salah Al-Radhi', display:{Lore:['[{"text": "arXiv:2101.10278", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHigh-Quality Vocoding Design with Signal Processing for Speech Synthesis and Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oMohammed Salah Al-Radhi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.10278\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Jan 2021 17:58:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPh.D. Dissertation https://repozitorium.omikk.bme.hu/handle/10890/13411\\u00a7r"}']}
{title:'Neekhara et al. (§72021§r)', author: 'Paarth Neekhara; Shehzeen Hussain; Shlomo Dubnov; Farinaz Koushanfar; Julian McAuley', display:{Lore:['[{"text": "arXiv:2102.00151", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExpressive Neural Voice Cloning\\u00a7r\\n\\n\\u00a78\\u00a7oPaarth Neekhara\\nShehzeen Hussain\\nShlomo Dubnov\\nFarinaz Koushanfar\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.00151\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 30 Jan 2021 05:09:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 2 figures, 2 tables\\u00a7r"}']}
{title:'Ferraro et al. (§72021§r)', author: 'Andres Ferraro; Yuntae Kim; Soohyeon Lee; Biho Kim; Namjun Jo; Semi Lim; Suyon Lim; Jungtaek Jang; Sehwan Kim; Xavier Serra; Dmitry Bogdanov', display:{Lore:['[{"text": "arXiv:2102.00201", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMelon Playlist Dataset: a public dataset for audio-based playlist generation and music tagging\\u00a7r\\n\\n\\u00a78\\u00a7oAndres Ferraro\\nYuntae Kim\\nSoohyeon Lee\\n+ 7 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.00201\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 30 Jan 2021 10:13:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2021 IEEE International Conference on Acoustics, Speech and Signal Processing\\u00a7r"}']}
{title:'Huang et al. (§72021§r)', author: 'Wen-Chin Huang; Chia-Hua Wu; Shang-Bao Luo; Kuan-Yu Chen; Hsin-Min Wang; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2102.00291", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Recognition by Simply Fine-tuning BERT\\u00a7r\\n\\n\\u00a78\\u00a7oWen-Chin Huang\\nChia-Hua Wu\\nShang-Bao Luo\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.00291\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 30 Jan 2021 19:06:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Kavalerov et al. (§72021§r)', author: 'Ilya Kavalerov; Ruijie Zheng; Wojciech Czaja; Rama Chellappa', display:{Lore:['[{"text": "arXiv:2102.00313", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCortical Features for Defense Against Adversarial Audio Attacks\\u00a7r\\n\\n\\u00a78\\u00a7oIlya Kavalerov\\nRuijie Zheng\\nWojciech Czaja\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.00313\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 17 Nov 2021 17:53:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCo-author legal name changed\\u00a7r"}']}
{title:'Agrawal et al. (§72021§r)', author: 'Ruchit Agrawal; Daniel Wolff; Simon Dixon', display:{Lore:['[{"text": "arXiv:2102.00382", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStructure-Aware Audio-to-Score Alignment using Progressively Dilated Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oRuchit Agrawal\\nDaniel Wolff\\nSimon Dixon\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.00382\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 14 Feb 2021 04:52:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2021 camera-ready version.Copyrights belong to IEEE\\u00a7r"}']}
{title:'Polyak et al. (§72021§r)', author: 'Adam Polyak; Lior Wolf; Yossi Adi; Ori Kabeli; Yaniv Taigman', display:{Lore:['[{"text": "arXiv:2102.00429", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHigh Fidelity Speech Regeneration with Application to Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oAdam Polyak\\nLior Wolf\\nYossi Adi\\nOri Kabeli\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.00429\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 31 Jan 2021 10:54:27 GMT)\\u00a7r"}']}
{title:'Noyum et al. (§72021§r)', author: 'Victoire Djimna Noyum; Younous Perieukeu Mofenjou; Cyrille Feudjio; Alkan Göktug; Ernest Fokoué', display:{Lore:['[{"text": "arXiv:2102.00550", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBoosting the Predictive Accurary of Singer Identification Using Discrete Wavelet Transform For Feature Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oVictoire Djimna Noyum\\nYounous Perieukeu Mofenjou\\nCyrille Feudjio\\nAlkan G\\u00f6ktug\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.00550\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 31 Jan 2021 21:58:55 GMT)\\u00a7r"}']}
{title:'Sarkar et al. (§72021§r)', author: 'Uddalok Sarkar; Sayan Nag; Medha Basu; Archi Banerjee; Shankha Sanyal; Ranjan Sengupta; Dipak Ghosh', display:{Lore:['[{"text": "arXiv:2102.00616", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Network architectures to classify emotions in Indian Classical Music\\u00a7r\\n\\n\\u00a78\\u00a7oUddalok Sarkar\\nSayan Nag\\nMedha Basu\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.00616\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Feb 2021 03:41:25 GMT)\\u00a7r"}']}
{title:'Dubnov (§72021§r)', author: 'Shlomo Dubnov', display:{Lore:['[{"text": "arXiv:2102.01133", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Music Information Dynamics\\u00a7r\\n\\n\\u00a78\\u00a7oShlomo Dubnov\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.01133\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nThe 2020 Joint Conference on AI Music Creativity, October 19-23,\\n  2020, Royal Institute of Technology (KTH), Stockholm, Sweden\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Feb 2021 19:59:59 GMT)\\u00a7r"}']}
{title:'Gong et al. (§72021§r)', author: 'Yuan Gong; Yu-An Chung; James Glass', display:{Lore:['[{"text": "arXiv:2102.01243", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation\\u00a7r\\n\\n\\u00a78\\u00a7oYuan Gong\\nYu-An Chung\\nJames Glass\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.01243\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3120633\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nin IEEE/ACM Transactions on Audio, Speech, and Language\\n  Processing, vol. 29, pp. 3292-3306, 2021\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 17 Nov 2021 17:41:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in IEEE/ACM Transactionson Audio Speech and Language Processing. Code at https://github.com/YuanGongND/psla\\u00a7r"}']}
{title:'Yao et al. (§72021§r)', author: 'Zhuoyuan Yao; Di Wu; Xiong Wang; Binbin Zhang; Fan Yu; Chao Yang; Zhendong Peng; Xiaoyu Chen; Lei Xie; Xin Lei', display:{Lore:['[{"text": "arXiv:2102.01547", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeNet: Production oriented Streaming and Non-streaming End-to-End Speech Recognition Toolkit\\u00a7r\\n\\n\\u00a78\\u00a7oZhuoyuan Yao\\nDi Wu\\nXiong Wang\\n+ 6 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.01547\\u00a7r\\n\\nVersion:\\u00a77v5 (Wed, 29 Dec 2021 10:10:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 4 tables\\u00a7r"}']}
{title:'Saha et al. (§72021§r)', author: 'Pramit Saha; Debasish Ray Mohapatra; Sidney Fels', display:{Lore:['[{"text": "arXiv:2102.01640", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSPEAK WITH YOUR HANDS Using Continuous Hand Gestures to control Articulatory Speech Synthesizer\\u00a7r\\n\\n\\u00a78\\u00a7oPramit Saha\\nDebasish Ray Mohapatra\\nSidney Fels\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.01640\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Feb 2021 17:49:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2 pages, 1 figure\\u00a7r"}']}
{title:'Alvarez-Blanco et al. (§72021§r)', author: 'Ana Lilia Alvarez-Blanco; Eugenia Cordoba-Warner; Marvin Coto-Jimenez; Vivian Fallas-Lopez; Maribel Morales Rodriguez', display:{Lore:['[{"text": "arXiv:2102.01692", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeneracion de voces artificiales infantiles en castellano con acento costarricense\\u00a7r\\n\\n\\u00a78\\u00a7oAna Lilia Alvarez-Blanco\\nEugenia Cordoba-Warner\\nMarvin Coto-Jimenez\\nVivian Fallas-Lopez\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.01692\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Feb 2021 02:12:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, in Spanish\\u00a7r"}']}
{title:'Fan et al. (§72021§r)', author: 'Weiquan Fan; Xiangmin Xu; Xiaofen Xing; Weidong Chen; Dongyan Huang', display:{Lore:['[{"text": "arXiv:2102.01754", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLSSED: a large-scale dataset and benchmark for speech emotion recognition\\u00a7r\\n\\n\\u00a78\\u00a7oWeiquan Fan\\nXiangmin Xu\\nXiaofen Xing\\nWeidong Chen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.01754\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 30 Jan 2021 11:15:32 GMT)\\u00a7r"}']}
{title:'Ferrer et al. (§72021§r)', author: 'Luciana Ferrer; Mitchell McLaren; Niko Brummer', display:{Lore:['[{"text": "arXiv:2102.01760", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Speaker Verification Backend with Robust Performance across Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oLuciana Ferrer\\nMitchell McLaren\\nNiko Brummer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.01760\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.csl.2021.101258\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nComputer Speech and Language, Volume 71, 2021\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 17 Aug 2021 17:30:49 GMT)\\u00a7r"}']}
{title:'Xu et al. (§72021§r)', author: 'Mingke Xu; Fan Zhang; Xiaodong Cui; Wei Zhang', display:{Lore:['[{"text": "arXiv:2102.01813", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Emotion Recognition with Multiscale Area Attention and Data Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oMingke Xu\\nFan Zhang\\nXiaodong Cui\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.01813\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Feb 2021 00:39:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2021\\u00a7r"}']}
{title:'Imoto et al. (§72021§r)', author: 'Keisuke Imoto; Sakiko Mishima; Yumi Arai; Reishi Kondo', display:{Lore:['[{"text": "arXiv:2102.01927", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImpact of Sound Duration and Inactive Frames on Sound Event Detection Performance\\u00a7r\\n\\n\\u00a78\\u00a7oKeisuke Imoto\\nSakiko Mishima\\nYumi Arai\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.01927\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Feb 2021 08:00:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021. arXiv admin note: text overlapwith arXiv:2006.15253\\u00a7r"}']}
{title:'Zhao et al. (§72021§r)', author: 'Yucheng Zhao; Dacheng Yin; Chong Luo; Zhiyuan Zhao; Chuanxin Tang; Wenjun Zeng; Zheng-Jun Zha', display:{Lore:['[{"text": "arXiv:2102.01930", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeneral-Purpose Speech Representation Learning through a Self-Supervised Multi-Granularity Framework\\u00a7r\\n\\n\\u00a78\\u00a7oYucheng Zhao\\nDacheng Yin\\nChong Luo\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.01930\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Feb 2021 08:13:21 GMT)\\u00a7r"}']}
{title:'Zhao et al. (§72021§r)', author: 'Shengkui Zhao; Hao Wang; Trung Hieu Nguyen; Bin Ma', display:{Lore:['[{"text": "arXiv:2102.01991", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Natural and Controllable Cross-Lingual Voice Conversion Based on Neural TTS Model and Phonetic Posteriorgram\\u00a7r\\n\\n\\u00a78\\u00a7oShengkui Zhao\\nHao Wang\\nTrung Hieu Nguyen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.01991\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Feb 2021 10:28:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 4 tables, accepted by ICASSP 2021\\u00a7r"}']}
{title:'Zhao et al. (§72021§r)', author: 'Shengkui Zhao; Trung Hieu Nguyen; Bin Ma', display:{Lore:['[{"text": "arXiv:2102.01993", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMonaural Speech Enhancement with Complex Convolutional Block Attention Module and Joint Time Frequency Losses\\u00a7r\\n\\n\\u00a78\\u00a7oShengkui Zhao\\nTrung Hieu Nguyen\\nBin Ma\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.01993\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Feb 2021 10:30:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, 2 tables, accepted by ICASSP 2021\\u00a7r"}']}
{title:'Lluís et al. (§72021§r)', author: 'Francesc Lluís; Vasileios Chatziioannou; Alex Hofmann', display:{Lore:['[{"text": "arXiv:2102.02028", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic source separation conditioned on 3D point clouds\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesc Llu\\u00eds\\nVasileios Chatziioannou\\nAlex Hofmann\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.02028\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Feb 2021 12:18:35 GMT)\\u00a7r"}']}
{title:'Sun et al. (§72021§r)', author: 'Xuecong Sun; Han Jia; Yuzhen Yang; Han Zhao; Yafeng Bi; Zhaoyong Sun; Jun Yang', display:{Lore:['[{"text": "arXiv:2102.02063", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a75physics.app-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic Structure Inverse Design and Optimization Using Deep Learning\\u00a7r\\n\\n\\u00a78\\u00a7oXuecong Sun\\nHan Jia\\nYuzhen Yang\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.02063\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 15 Apr 2021 10:29:26 GMT)\\u00a7r"}']}
{title:'Sarkar et al. (§72021§r)', author: 'Achintya Kumar Sarkar; Md Sahidullah; Zheng-Hua Tan', display:{Lore:['[{"text": "arXiv:2102.02074", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData Generation Using Pass-phrase-dependent Deep Auto-encoders for Text-Dependent Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oAchintya Kumar Sarkar\\nMd Sahidullah\\nZheng-Hua Tan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.02074\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Feb 2021 14:06:29 GMT)\\u00a7r"}']}
{title:'Di Giorgi et al. (§72021§r)', author: 'Bruno Di Giorgi; Matthias Mauch; Mark Levy', display:{Lore:['[{"text": "arXiv:2102.02282", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDownbeat Tracking with Tempo-Invariant Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oBruno Di Giorgi\\nMatthias Mauch\\nMark Levy\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.02282\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 21st International Society for Music\\n  Information Retrieval Conference (2020) 216-222\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Feb 2021 20:25:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 5 figures, Proceedings of the 21st International Society for Music Information Retrieval Conference, ISMIR 2020\\u00a7r"}']}
{title:'Tay et al. (§72021§r)', author: 'Kai Yuan Tay; Lynnette Ng; Wei Han Chua; Lucerne Loke; Danqi Ye; Melissa Chua', display:{Lore:['[{"text": "arXiv:2102.02417", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Adversarial Examples: Attacks Using Vocal Masks\\u00a7r\\n\\n\\u00a78\\u00a7oKai Yuan Tay\\nLynnette Ng\\nWei Han Chua\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.02417\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 6 Feb 2021 03:31:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 1 figure, 2 tables. Submitted to COLING2020\\u00a7r"}']}
{title:'Min et al. (§72021§r)', author: 'Gang Min; Xiongwei Zhang; Xia Zou; Xiangyang Liu', display:{Lore:['[{"text": "arXiv:2102.02640", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow Bit-Rate Wideband Speech Coding: A Deep Generative Model based Approach\\u00a7r\\n\\n\\u00a78\\u00a7oGang Min\\nXiongwei Zhang\\nXia Zou\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.02640\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Feb 2021 14:37:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages\\u00a7r"}']}
{title:'Lahnala et al. (§72021§r)', author: 'Allison Lahnala; Gauri Kambhatla; Jiajun Peng; Matthew Whitehead; Gillian Minnehan; Eric Guldan; Jonathan K. Kummerfeld; Anıl Çamcı; Rada Mihalcea', display:{Lore:['[{"text": "arXiv:2102.02917", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChord Embeddings: Analyzing What They Capture and Their Role for Next Chord Prediction and Artist Attribute Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oAllison Lahnala\\nGauri Kambhatla\\nJiajun Peng\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.02917\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nComputational Intelligence in Music, Sound, Art and Design, 10th\\n  International Conference, EvoMUSART 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Feb 2021 22:17:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages, accepted to EvoMUSART\\u00a7r"}']}
{title:'Sunouchi et al. (§72021§r)', author: 'Motohiro Sunouchi; Masaharu Yoshioka', display:{Lore:['[{"text": "arXiv:2102.02964", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiversity-Robust Acoustic Feature Signatures Based on Multiscale Fractal Dimension for Similarity Search of Environmental Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oMotohiro Sunouchi\\nMasaharu Yoshioka\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.02964\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1587/transinf.2021EDP7016\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 23 Jun 2021 05:51:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 14 figures\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Ruizhi Li; Gregory Sell; Hynek Hermansky', display:{Lore:['[{"text": "arXiv:2102.03055", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTwo-Stage Augmentation and Adaptive CTC Fusion for Improved Robustness of Multi-Stream End-to-End ASR\\u00a7r\\n\\n\\u00a78\\u00a7oRuizhi Li\\nGregory Sell\\nHynek Hermansky\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.03055\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Feb 2021 08:36:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at IEEESLT 2021\\u00a7r"}']}
{title:'Mitcheltree et al. (§72021§r)', author: 'Christopher Mitcheltree; Hideki Koike', display:{Lore:['[{"text": "arXiv:2102.03170", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhite-box Audio VST Effect Programming\\u00a7r\\n\\n\\u00a78\\u00a7oChristopher Mitcheltree\\nHideki Koike\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.03170\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n4th Workshop on Machine Learning for Creativity and Design at\\n  NeurIPS 2020, Vancouver, Canada\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Feb 2021 13:45:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe latest version of the system is to appear at EvoMUSART 2021 as a full paper. Audio samples of the latest system can be listened to at https://bit.ly/serum_rnn\\u00a7r"}']}
{title:'Choi et al. (§72021§r)', author: 'Hyeong-Seok Choi; Sungjin Park; Jie Hwan Lee; Hoon Heo; Dongsuk Jeon; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2102.03207", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-time Denoising and Dereverberation with Tiny Recurrent U-Net\\u00a7r\\n\\n\\u00a78\\u00a7oHyeong-Seok Choi\\nSungjin Park\\nJie Hwan Lee\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.03207\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 23 Jun 2021 03:08:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). arXiv admin note: text overlap with arXiv:2006.00687\\u00a7r"}']}
{title:'Wu et al. (§72021§r)', author: 'Ho-Hsiang Wu; Chieh-Chi Kao; Qingming Tang; Ming Sun; Brian McFee; Juan Pablo Bello; Chao Wang', display:{Lore:['[{"text": "arXiv:2102.03229", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Task Self-Supervised Pre-Training for Music Classification\\u00a7r\\n\\n\\u00a78\\u00a7oHo-Hsiang Wu\\nChieh-Chi Kao\\nQingming Tang\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.03229\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Feb 2021 15:19:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCopyright 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising "}','{"text": "or promotional purposes, creating newcollective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Mridha et al. (§72021§r)', author: 'M. F. Mridha; Abu Quwsar Ohi; Muhammad Mostafa Monowar; Md. Abdul Hamid; Md. Rashedul Islam; Yutaka Watanobe', display:{Lore:['[{"text": "arXiv:2102.03868", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lU-vectors: Generating clusterable speaker embedding from unlabeled data\\u00a7r\\n\\n\\u00a78\\u00a7oM. F. Mridha\\nAbu Quwsar Ohi\\nMuhammad Mostafa Monowar\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.03868\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 22 Oct 2021 13:05:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages, 7 figures\\u00a7r"}']}
{title:'Kuruvila et al. (§72021§r)', author: 'Ivine Kuruvila; Jan Muncke; Eghart Fischer; Ulrich Hoppe', display:{Lore:['[{"text": "arXiv:2102.03957", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExtracting the Auditory Attention in a Dual-Speaker Scenario from EEG using a Joint CNN-LSTM Model\\u00a7r\\n\\n\\u00a78\\u00a7oIvine Kuruvila\\nJan Muncke\\nEghart Fischer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.03957\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 9 Jul 2021 02:07:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages, 6 figures\\u00a7r"}']}
{title:'Luo et al. (§72021§r)', author: 'Renqian Luo; Xu Tan; Rui Wang; Tao Qin; Jinzhu Li; Sheng Zhao; Enhong Chen; Tie-Yan Liu', display:{Lore:['[{"text": "arXiv:2102.04040", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLightSpeech: Lightweight and Fast Text to Speech with Neural Architecture Search\\u00a7r\\n\\n\\u00a78\\u00a7oRenqian Luo\\nXu Tan\\nRui Wang\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.04040\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Feb 2021 07:45:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 21\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Chenxing Li; Jiaming Xu; Nima Mesgarani; Bo Xu', display:{Lore:['[{"text": "arXiv:2102.04056", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker and Direction Inferred Dual-channel Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oChenxing Li\\nJiaming Xu\\nNima Mesgarani\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.04056\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Feb 2021 08:28:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2021\\u00a7r"}']}
{title:'Hsu et al. (§72021§r)', author: 'Fu-Shun Hsu; Shang-Ran Huang; Chien-Wen Huang; Yuan-Ren Cheng; Chun-Chieh Chen; Jack Hsiao; Chung-Wei Chen; Feipei Lai', display:{Lore:['[{"text": "arXiv:2102.04062", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Update on a Progressively Expanded Database for Automated Lung Sound Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oFu-Shun Hsu\\nShang-Ran Huang\\nChien-Wen Huang\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.04062\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 29 Sep 2021 06:11:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review, 14 pages, 5 figures, 3 tables\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Andong Li; Wenzhe Liu; Xiaoxue Luo; Chengshi Zheng; Xiaodong Li', display:{Lore:['[{"text": "arXiv:2102.04198", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lICASSP 2021 Deep Noise Suppression Challenge: Decoupling Magnitude and Phase Optimization with a Two-Stage Deep Network\\u00a7r\\n\\n\\u00a78\\u00a7oAndong Li\\nWenzhe Liu\\nXiaoxue Luo\\nChengshi Zheng\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.04198\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 1 Mar 2021 06:22:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted by ICASSP 2021\\u00a7r"}']}
{title:'Cui et al. (§72021§r)', author: 'Xiaodong Cui; Songtao Lu; Brian Kingsbury', display:{Lore:['[{"text": "arXiv:2102.04429", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.DC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFederated Acoustic Modeling For Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXiaodong Cui\\nSongtao Lu\\nBrian Kingsbury\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.04429\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Feb 2021 18:39:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2021\\u00a7r"}']}
{title:'Mohapatra et al. (§72021§r)', author: 'Debasish Ray Mohapatra; Victor Zappi; Sidney Fels', display:{Lore:['[{"text": "arXiv:2102.04588", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA comparative study of two-dimensional vocal tract acoustic modeling based on Finite-Difference Time-Domain methods\\u00a7r\\n\\n\\u00a78\\u00a7oDebasish Ray Mohapatra\\nVictor Zappi\\nSidney Fels\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.04588\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Feb 2021 00:40:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 3 figures\\u00a7r"}']}
{title:'Jeong et al. (§72021§r)', author: 'Dasaem Jeong; Seungheon Doh; Taegyun Kwon', display:{Lore:['[{"text": "arXiv:2102.04680", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTr\\u00e4umerAI: Dreaming Music with StyleGAN\\u00a7r\\n\\n\\u00a78\\u00a7oDasaem Jeong\\nSeungheon Doh\\nTaegyun Kwon\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.04680\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Feb 2021 07:04:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7opresented in NeurIPS Workshop 2020: Machine Learning for Creativity and Design\\u00a7r"}']}
{title:'Maleki (§72021§r)', author: 'Masoud Maleki', display:{Lore:['[{"text": "arXiv:2102.04880", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a72math.OC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiagnosis of COVID-19 and Non-COVID-19 Patients by Classifying Only a Single Cough Sound\\u00a7r\\n\\n\\u00a78\\u00a7oMasoud Maleki\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.04880\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Feb 2021 12:38:37 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Xiaoyu Liu; Jordi Pons', display:{Lore:['[{"text": "arXiv:2102.04945", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn permutation invariant training for speech source separation\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoyu Liu\\nJordi Pons\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.04945\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 5 Apr 2021 07:04:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn proceedings of ICASSP2021\\u00a7r"}']}
{title:'Iqbal et al. (§72021§r)', author: 'Turab Iqbal; Karim Helwani; Arvindh Krishnaswamy; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2102.05151", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Audio Augmentation Methods with Consistency Learning\\u00a7r\\n\\n\\u00a78\\u00a7oTurab Iqbal\\nKarim Helwani\\nArvindh Krishnaswamy\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.05151\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 19 Apr 2021 15:04:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to 46thInternational Conference on Acoustics, Speech, and Signal Processing (ICASSP 2021)\\u00a7r"}']}
{title:'Han et al. (§72021§r)', author: 'Jing Han; Chloë Brown; Jagmohan Chauhan; Andreas Grammenos; Apinan Hasthanasombat; Dimitris Spathis; Tong Xia; Pietro Cicuta; Cecilia Mascolo', display:{Lore:['[{"text": "arXiv:2102.05225", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Automatic COVID-19 Diagnosis via voice and symptoms from Crowdsourced Data\\u00a7r\\n\\n\\u00a78\\u00a7oJing Han\\nChlo\\u00eb Brown\\nJagmohan Chauhan\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.05225\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9414576\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Feb 2021 02:28:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 2 tables, Accepted for publication at ICASSP 2021\\u00a7r"}']}
{title:'Tonami et al. (§72021§r)', author: 'Noriyuki Tonami; Keisuke Imoto; Yuki Okamoto; Takahiro Fukumori; Yoichi Yamashita', display:{Lore:['[{"text": "arXiv:2102.05288", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Event Detection Based on Curriculum Learning Considering Learning Difficulty of Events\\u00a7r\\n\\n\\u00a78\\u00a7oNoriyuki Tonami\\nKeisuke Imoto\\nYuki Okamoto\\nTakahiro Fukumori\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.05288\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Feb 2021 07:04:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Ruggiero et al. (§72021§r)', author: 'Giuseppe Ruggiero; Enrico Zovato; Luigi Di Caro; Vincent Pollet', display:{Lore:['[{"text": "arXiv:2102.05630", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Cloning: a Multi-Speaker Text-to-Speech Synthesis Approach based on Transfer Learning\\u00a7r\\n\\n\\u00a78\\u00a7oGiuseppe Ruggiero\\nEnrico Zovato\\nLuigi Di Caro\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.05630\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Feb 2021 18:43:56 GMT)\\u00a7r"}']}
{title:'Cífka et al. (§72021§r)', author: 'Ondřej Cífka; Alexey Ozerov; Umut Şimşekli; Gaël Richard', display:{Lore:['[{"text": "arXiv:2102.05749", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised VQ-VAE for One-Shot Music Style Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oOnd\\u0159ej C\\u00edfka\\nAlexey Ozerov\\nUmut \\u015eim\\u015fekli\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.05749\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9414235\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2021 - 2021 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (2021) 96-100\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 10 Jun 2021 15:15:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2021. Website: https://adasp.telecom-paris.fr/s/ss-vq-vae\\u00a7r"}']}
{title:'Nassif et al. (§72021§r)', author: 'Ali Bou Nassif; Ismail Shahin; Shibani Hamsa; Nawel Nemmour; Keikichi Hirose', display:{Lore:['[{"text": "arXiv:2102.05894", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCASA-Based Speaker Identification Using Cascaded GMM-CNN Classifier in Noisy and Emotional Talking Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oAli Bou Nassif\\nIsmail Shahin\\nShibani Hamsa\\nNawel Nemmour\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.05894\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.asoc.2021.107141\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nApplied Soft Computing, Elsevier, 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Feb 2021 08:56:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in Applied Soft Computing journal\\u00a7r"}']}
{title:'Sarkar et al. (§72021§r)', author: 'Uddalok Sarkar; Sayan Nag; Chirayata Bhattacharya; Shankha Sanyal; Archi Banerjee; Ranjan Sengupta; Dipak Ghosh', display:{Lore:['[{"text": "arXiv:2102.06003", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLanguage Independent Emotion Quantification using Non linear Modelling of Speech\\u00a7r\\n\\n\\u00a78\\u00a7oUddalok Sarkar\\nSayan Nag\\nChirayata Bhattacharya\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.06003\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Feb 2021 13:48:25 GMT)\\u00a7r"}']}
{title:'Chazan et al. (§72021§r)', author: 'Shlomo E. Chazan; Jacob Goldberger; Sharon Gannot', display:{Lore:['[{"text": "arXiv:2102.06034", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech enhancement with mixture-of-deep-experts with clean clustering pre-training\\u00a7r\\n\\n\\u00a78\\u00a7oShlomo E. Chazan\\nJacob Goldberger\\nSharon Gannot\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.06034\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Feb 2021 14:18:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:1703.09302\\u00a7r"}']}
{title:'Nag et al. (§72021§r)', author: 'Sayan Nag; Uddalok Sarkar; Shankha Sanyal; Archi Banerjee; Souparno Roy; Samir Karmakar; Ranjan Sengupta; Dipak Ghosh', display:{Lore:['[{"text": "arXiv:2102.06038", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Fractal Approach to Characterize Emotions in Audio and Visual Domain: A Study on Cross-Modal Interaction\\u00a7r\\n\\n\\u00a78\\u00a7oSayan Nag\\nUddalok Sarkar\\nShankha Sanyal\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.06038\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Feb 2021 14:30:22 GMT)\\u00a7r"}']}
{title:'Arteaga et al. (§72021§r)', author: 'Daniel Arteaga; Jordi Pons', display:{Lore:['[{"text": "arXiv:2102.06142", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultichannel-based learning for audio object extraction\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Arteaga\\nJordi Pons\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.06142\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9414585\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2021 - 2021 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP), 2021, pp. 206-210\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 20 Dec 2021 21:48:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn proceedings of ICASSP2021. Appendix added\\u00a7r"}']}
{title:'Sarı et al. (§72021§r)', author: 'Leda Sarı; Kritika Singh; Jiatong Zhou; Lorenzo Torresani; Nayan Singhal; Yatharth Saraf', display:{Lore:['[{"text": "arXiv:2102.06291", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Multi-View Approach To Audio-Visual Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oLeda Sar\\u0131\\nKritika Singh\\nJiatong Zhou\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.06291\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Feb 2021 22:29:25 GMT)\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Mao Li; Bo Yang; Joshua Levy; Andreas Stolcke; Viktor Rozgic; Spyros Matsoukas; Constantinos Papayiannis; Daniel Bone; Chao Wang', display:{Lore:['[{"text": "arXiv:2102.06357", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContrastive Unsupervised Learning for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMao Li\\nBo Yang\\nJoshua Levy\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.06357\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Feb 2021 06:06:02 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Peng Liu; Yuewen Cao; Songxiang Liu; Na Hu; Guangzhi Li; Chao Weng; Dan Su', display:{Lore:['[{"text": "arXiv:2102.06431", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVARA-TTS: Non-Autoregressive Text-to-Speech Synthesis based on Very Deep VAE with Residual Attention\\u00a7r\\n\\n\\u00a78\\u00a7oPeng Liu\\nYuewen Cao\\nSongxiang Liu\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.06431\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Feb 2021 10:26:57 GMT)\\u00a7r"}']}
{title:'Kristoffersen et al. (§72021§r)', author: 'Miklas Strøm Kristoffersen; Martin Bo Møller; Pablo Martínez-Nuevo; Jan Østergaard', display:{Lore:['[{"text": "arXiv:2102.06455", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Sound Field Reconstruction in Real Rooms: Introducing the ISOBEL Sound Field Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oMiklas Str\\u00f8m Kristoffersen\\nMartin Bo M\\u00f8ller\\nPablo Mart\\u00ednez-Nuevo\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.06455\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Feb 2021 11:34:18 GMT)\\u00a7r"}']}
{title:'Sun et al. (§72021§r)', author: 'G. Sun; D. Liu; C. Zhang; P. C. Woodland', display:{Lore:['[{"text": "arXiv:2102.06467", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContent-Aware Speaker Embeddings for Speaker Diarisation\\u00a7r\\n\\n\\u00a78\\u00a7oG. Sun\\nD. Liu\\nC. Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.06467\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Feb 2021 12:02:03 GMT)\\u00a7r"}']}
{title:'Avramidis et al. (§72021§r)', author: 'Kleanthis Avramidis; Agelos Kratimenos; Christos Garoufis; Athanasia Zlatintsi; Petros Maragos', display:{Lore:['[{"text": "arXiv:2102.06930", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Convolutional and Recurrent Networks for Polyphonic Instrument Classification from Monophonic Raw Audio Waveforms\\u00a7r\\n\\n\\u00a78\\u00a7oKleanthis Avramidis\\nAgelos Kratimenos\\nChristos Garoufis\\nAthanasia Zlatintsi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.06930\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 13 Feb 2021 13:44:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, 6 tables, to be published in the Proc. of the 46th International Conference on Acoustics, Speech and Signal Processing (ICASSP 2021) @ Toronto, Ontario, Canada\\u00a7r"}']}
{title:'Tzirakis et al. (§72021§r)', author: 'Panagiotis Tzirakis; Anurag Kumar; Jacob Donley', display:{Lore:['[{"text": "arXiv:2102.06934", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Channel Speech Enhancement using Graph Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oPanagiotis Tzirakis\\nAnurag Kumar\\nJacob Donley\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.06934\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. ICASSP 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 13 Feb 2021 14:20:40 GMT)\\u00a7r"}']}
{title:'Salvi et al. (§72021§r)', author: 'Davide Salvi; Sebastian Gonzalez; Fabio Antonacci; Augusto Sarti', display:{Lore:['[{"text": "arXiv:2102.07133", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParametric Optimization of Violin Top Plates using Machine Learning\\u00a7r\\n\\n\\u00a78\\u00a7oDavide Salvi\\nSebastian Gonzalez\\nFabio Antonacci\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.07133\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 18 Feb 2021 15:07:04 GMT)\\u00a7r"}']}
{title:'Karmakar et al. (§72021§r)', author: 'Priyabrata Karmakar; Shyh Wei Teng; Guojun Lu', display:{Lore:['[{"text": "arXiv:2102.07259", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThank you for Attention: A survey on Attention-based Artificial Neural Networks for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oPriyabrata Karmakar\\nShyh Wei Teng\\nGuojun Lu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.07259\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 14 Feb 2021 22:28:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEE/ACM Trans. on Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Feng et al. (§72021§r)', author: 'Chuyao Feng; Eva van Leer; Mackenzie Lee Curtis; David V. Anderson', display:{Lore:['[{"text": "arXiv:2102.07307", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lI-vector Based Within Speaker Voice Quality Identification on connected speech\\u00a7r\\n\\n\\u00a78\\u00a7oChuyao Feng\\nEva van Leer\\nMackenzie Lee Curtis\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.07307\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Feb 2021 02:26:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7os\\u00a7r"}']}
{title:'Nunes (§72021§r)', author: 'Eduardo C. Nunes', display:{Lore:['[{"text": "arXiv:2102.07820", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnomalous Sound Detection with Machine Learning: A Systematic Review\\u00a7r\\n\\n\\u00a78\\u00a7oEduardo C. Nunes\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.07820\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Feb 2021 19:57:03 GMT)\\u00a7r"}']}
{title:'Lin et al. (§72021§r)', author: 'Yi Lin; Qin Li; Bo Yang; Zhen Yan; Huachun Tan; Zhengmao Chen', display:{Lore:['[{"text": "arXiv:2102.08015", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving speech recognition models with small samples for air traffic control systems\\u00a7r\\n\\n\\u00a78\\u00a7oYi Lin\\nQin Li\\nBo Yang\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.08015\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Feb 2021 08:28:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been accepted by Neurocomputing for publication\\u00a7r"}']}
{title:'Bhosale et al. (§72021§r)', author: 'Swapnil Bhosale; Rupayan Chakraborty; Sunil Kumar Kopparapu', display:{Lore:['[{"text": "arXiv:2102.08074", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi Supervised Learning For Few-shot Audio Classification By Episodic Triplet Mining\\u00a7r\\n\\n\\u00a78\\u00a7oSwapnil Bhosale\\nRupayan Chakraborty\\nSunil Kumar Kopparapu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.08074\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Feb 2021 10:55:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Coppock et al. (§72021§r)', author: 'Harry Coppock; Alexander Gaskell; Panagiotis Tzirakis; Alice Baird; Lyn Jones; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2102.08359", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-2-End COVID-19 Detection from Breath     Cough Audio\\u00a7r\\n\\n\\u00a78\\u00a7oHarry Coppock\\nAlexander Gaskell\\nPanagiotis Tzirakis\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.08359\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Jan 2021 01:13:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Ziteng Wang; Yueyue Na; Zhang Liu; Biao Tian; Qiang Fu', display:{Lore:['[{"text": "arXiv:2102.08551", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeighted Recursive Least Square Filter and Neural Network based Residual Echo Suppression for the AEC-Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oZiteng Wang\\nYueyue Na\\nZhang Liu\\nBiao Tian\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.08551\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 18 Feb 2021 06:01:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, accepted by ICASSP 2021\\u00a7r"}']}
{title:'Basak et al. (§72021§r)', author: 'Sakya Basak; Shrutina Agarwal; Sriram Ganapathy; Naoya Takahashi', display:{Lore:['[{"text": "arXiv:2102.08575", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end lyrics Recognition with Voice to Singing Style Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oSakya Basak\\nShrutina Agarwal\\nSriram Ganapathy\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.08575\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Feb 2021 04:52:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at ICASSP 2021\\u00a7r"}']}
{title:'Johnson et al. (§72021§r)', author: 'David S. Johnson; Wolfgang Lorenz; Michael Taenzer; Stylianos Mimilakis; Sascha Grollmisch; Jakob Abeßer; Hanna Lukashevich', display:{Lore:['[{"text": "arXiv:2102.08833", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.DC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDESED-FL and URBAN-FL: Federated Learning Datasets for Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oDavid S. Johnson\\nWolfgang Lorenz\\nMichael Taenzer\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.08833\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 31 May 2021 10:32:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in EUSIPCO 2021\\u00a7r"}']}
{title:'Demirel et al. (§72021§r)', author: 'Emir Demirel; Sven Ahlbäck; Simon Dixon', display:{Lore:['[{"text": "arXiv:2102.09202", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow Resource Audio-to-Lyrics Alignment From Polyphonic Music Recordings\\u00a7r\\n\\n\\u00a78\\u00a7oEmir Demirel\\nSven Ahlb\\u00e4ck\\nSimon Dixon\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.09202\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Feb 2021 07:54:56 GMT)\\u00a7r"}']}
{title:'Yu et al. (§72021§r)', author: 'Shuai Yu; Xiaoheng Sun; Yi Yu; Wei Li', display:{Lore:['[{"text": "arXiv:2102.09763", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrequency-Temporal Attention Network for Singing Melody Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oShuai Yu\\nXiaoheng Sun\\nYi Yu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.09763\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2021.3080625\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Feb 2021 06:43:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper has been accepted by ICASSP 2021\\u00a7r"}']}
{title:'Guo et al. (§72021§r)', author: 'Zixun Guo; Makris Dimos; Herremans Dorien', display:{Lore:['[{"text": "arXiv:2102.09794", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierarchical Recurrent Neural Networks for Conditional Melody Generation with Long-term Structure\\u00a7r\\n\\n\\u00a78\\u00a7oZixun Guo\\nMakris Dimos\\nHerremans Dorien\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.09794\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. of the International Joint Conference on Neural Networks\\n  (IJCNN), Shenzhen, China, 18-22 July 2021(virtual)\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 25 Apr 2021 08:58:49 GMT)\\u00a7r"}']}
{title:'Huang et al. (§72021§r)', author: 'Houjun Huang; Xu Xiang; Fei Zhao; Shuai Wang; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2102.09817", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnit selection synthesis based data augmentation for fixed phrase speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oHoujun Huang\\nXu Xiang\\nFei Zhao\\nShuai Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.09817\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Feb 2021 09:14:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Huang et al. (§72021§r)', author: 'Houjun Huang; Xu Xiang; Yexin Yang; Rao Ma; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2102.09828", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAISPEECH-SJTU accent identification system for the Accented English Speech Recognition Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oHoujun Huang\\nXu Xiang\\nYexin Yang\\nRao Ma\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.09828\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Feb 2021 09:40:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Song et al. (§72021§r)', author: 'Xuchen Song; Qiuqiang Kong; Xingjian Du; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2102.09966", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCatNet: music source separation system with mix-audio augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oXuchen Song\\nQiuqiang Kong\\nXingjian Du\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.09966\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Feb 2021 15:01:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Kong et al. (§72021§r)', author: 'Qiuqiang Kong; Haohe Liu; Xingjian Du; Li Chen; Rui Xia; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2102.09971", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech enhancement with weakly labelled data from AudioSet\\u00a7r\\n\\n\\u00a78\\u00a7oQiuqiang Kong\\nHaohe Liu\\nXingjian Du\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.09971\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Feb 2021 15:08:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Zining Zhang; Bingsheng He; Zhenjie Zhang', display:{Lore:['[{"text": "arXiv:2102.09978", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransMask: A Compact and Fast Speech Separation Model Based on Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oZining Zhang\\nBingsheng He\\nZhenjie Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.09978\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Feb 2021 15:19:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ICASSP2021\\u00a7r"}']}
{title:'Shi et al. (§72021§r)', author: 'Xian Shi; Fan Yu; Yizhou Lu; Yuhao Liang; Qiangze Feng; Daliang Wang; Yanmin Qian; Lei Xie', display:{Lore:['[{"text": "arXiv:2102.10233", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Accented English Speech Recognition Challenge 2020: Open Datasets, Tracks, Baselines, Results and Methods\\u00a7r\\n\\n\\u00a78\\u00a7oXian Shi\\nFan Yu\\nYizhou Lu\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.10233\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 20 Feb 2021 02:50:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2021\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Xulong Zhang; Jiale Qian; Yi Yu; Yifu Sun; Wei Li', display:{Lore:['[{"text": "arXiv:2102.10236", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSinger Identification Using Deep Timbre Feature Learning with KNN-Net\\u00a7r\\n\\n\\u00a78\\u00a7oXulong Zhang\\nJiale Qian\\nYi Yu\\nYifu Sun\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.10236\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 20 Feb 2021 02:55:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished as a conference paper at ICASSP 2021\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Xuechen Liu; Md Sahidullah; Tomi Kinnunen', display:{Lore:['[{"text": "arXiv:2102.10322", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearnable MFCCs for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oXuechen Liu\\nMd Sahidullah\\nTomi Kinnunen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.10322\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 20 Feb 2021 12:16:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ISCAS 2021\\u00a7r"}']}
{title:'Kumari et al. (§72021§r)', author: 'Pratibha Kumari; Mukesh Saini', display:{Lore:['[{"text": "arXiv:2102.10515", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnomaly Detection in Audio with Concept Drift using Adaptive Huffman Coding\\u00a7r\\n\\n\\u00a78\\u00a7oPratibha Kumari\\nMukesh Saini\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.10515\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 7 Aug 2021 03:52:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o22 pages, 8 figures\\u00a7r"}']}
{title:'Sankaran et al. (§72021§r)', author: 'Shreeviknesh Sankaran; Sukavanan Nanjundan; G. Paavai Anand', display:{Lore:['[{"text": "arXiv:2102.11058", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnyone GAN Sing\\u00a7r\\n\\n\\u00a78\\u00a7oShreeviknesh Sankaran\\nSukavanan Nanjundan\\nG. Paavai Anand\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.11058\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInternational Journal of Emerging Technologies and Innovative\\n  Research (www.jetir.org), ISSN: 2349-5162, Vol.7, Issue 5, page no. 25-29,\\n  May-2020\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Feb 2021 14:30:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 8 figures\\u00a7r"}']}
{title:'Broughton et al. (§72021§r)', author: 'Samuel J. Broughton; Md Asif Jalal; Roger K. Moore', display:{Lore:['[{"text": "arXiv:2102.11420", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating Deep Neural Structures and their Interpretability in the Domain of Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oSamuel J. Broughton\\nMd Asif Jalal\\nRoger K. Moore\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.11420\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Feb 2021 23:54:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oFor demo, see https://samuelbroughton.github.io/interpretability-demo-2020/\\u00a7r"}']}
{title:'Xu et al. (§72021§r)', author: 'Xuenan Xu; Heinrich Dinkel; Mengyue Wu; Zeyu Xie; Kai Yu', display:{Lore:['[{"text": "arXiv:2102.11457", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating Local and Global Information for Automated Audio Captioning with Transfer Learning\\u00a7r\\n\\n\\u00a78\\u00a7oXuenan Xu\\nHeinrich Dinkel\\nMengyue Wu\\nZeyu Xie\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.11457\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Feb 2021 02:09:49 GMT)\\u00a7r"}']}
{title:'Xu et al. (§72021§r)', author: 'Xuenan Xu; Heinrich Dinkel; Mengyue Wu; Kai Yu', display:{Lore:['[{"text": "arXiv:2102.11474", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lText-to-Audio Grounding: Building Correspondence Between Captions and Sound Events\\u00a7r\\n\\n\\u00a78\\u00a7oXuenan Xu\\nHeinrich Dinkel\\nMengyue Wu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.11474\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Feb 2021 03:44:14 GMT)\\u00a7r"}']}
{title:'Duan et al. (§72021§r)', author: 'Richeng Duan; Nancy F. Chen', display:{Lore:['[{"text": "arXiv:2102.11488", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSenone-aware Adversarial Multi-task Training for Unsupervised Child to Adult Speech Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oRicheng Duan\\nNancy F. Chen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.11488\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Feb 2021 04:49:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted for presentation at ICASSP-2021\\u00a7r"}']}
{title:'Venkatesh et al. (§72021§r)', author: 'Ganesh Venkatesh; Alagappan Valliappan; Jay Mahadeokar; Yuan Shangguan; Christian Fuegen; Michael L. Seltzer; Vikas Chandra', display:{Lore:['[{"text": "arXiv:2102.11531", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMemory-efficient Speech Recognition on Smart Devices\\u00a7r\\n\\n\\u00a78\\u00a7oGanesh Venkatesh\\nAlagappan Valliappan\\nJay Mahadeokar\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.11531\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Feb 2021 07:43:45 GMT)\\u00a7r"}']}
{title:'Wissing et al. (§72021§r)', author: 'Julio Wissing; Benedikt Boenninghoff; Dorothea Kolossa; Tsubasa Ochiai; Marc Delcroix; Keisuke Kinoshita; Tomohiro Nakatani; Shoko Araki; Christopher Schymura', display:{Lore:['[{"text": "arXiv:2102.11588", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData Fusion for Audiovisual Speaker Localization: Extending Dynamic Stream Weights to the Spatial Domain\\u00a7r\\n\\n\\u00a78\\u00a7oJulio Wissing\\nBenedikt Boenninghoff\\nDorothea Kolossa\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.11588\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 24 Feb 2021 07:57:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 6 figures, ICASSP 2021\\u00a7r"}']}
{title:'Neto et al. (§72021§r)', author: 'Antonio Joia Neto; Andre G C Pacheco; Diogo C Luvizon', display:{Lore:['[{"text": "arXiv:2102.11771", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Deep Learning Sound Events Classifiers using Gram Matrix Feature-wise Correlations\\u00a7r\\n\\n\\u00a78\\u00a7oAntonio Joia Neto\\nAndre G C Pacheco\\nDiogo C Luvizon\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.11771\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Feb 2021 16:08:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear on ICASSP 2021\\u00a7r"}']}
{title:'Van et al. (§72021§r)', author: 'Toan Pham Van; Ngoc N. Tran; Ta Minh Thanh', display:{Lore:['[{"text": "arXiv:2102.12111", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning Approach for Singer Voice Classification of Vietnamese Popular Music\\u00a7r\\n\\n\\u00a78\\u00a7oToan Pham Van\\nNgoc N. Tran\\nTa Minh Thanh\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.12111\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3368926.3369700\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nSoICT 2019: Proceedings of the Tenth International Symposium on\\n  Information and Communication Technology\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Feb 2021 08:03:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in SoICT 2019: Proceedings of the Tenth International Symposium on Information and Communication Technology\\u00a7r"}']}
{title:'Colt et al. (§72021§r)', author: 'Robert-George Colt; Csongor-Huba Várady; Riccardo Volpi; Luigi Malagò', display:{Lore:['[{"text": "arXiv:2102.12289", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Feature Extraction for Heartbeat Anomaly Detection\\u00a7r\\n\\n\\u00a78\\u00a7oRobert-George Colt\\nCsongor-Huba V\\u00e1rady\\nRiccardo Volpi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.12289\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Feb 2021 13:55:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 2 figures, Presented at PharML 2020 Workshop - European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases(ECML-PKDD), see https://sites.google.com/view/pharml2020/accept"}','{"text": "ed-submissions, source-code: https://github.com/rist-ro/argo\\u00a7r"}']}
{title:'Maqueda et al. (§72021§r)', author: 'Emmanuel Maqueda; Javier Alvarez-Jimenez; Carlos Mena; Ivan Meza', display:{Lore:['[{"text": "arXiv:2102.12564", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTriplet loss based embeddings for forensic speaker identification in Spanish\\u00a7r\\n\\n\\u00a78\\u00a7oEmmanuel Maqueda\\nJavier Alvarez-Jimenez\\nCarlos Mena\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.12564\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s00521-021-06408-6\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 14 Sep 2021 00:01:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oLong Paper: Neural Computing and Applications, Special Issue on LatinX in AI Research (2021). 11 pages, 5 figures\\u00a7r"}']}
{title:'Kaneko et al. (§72021§r)', author: 'Takuhiro Kaneko; Hirokazu Kameoka; Kou Tanaka; Nobukatsu Hojo', display:{Lore:['[{"text": "arXiv:2102.12841", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMaskCycleGAN-VC: Learning Non-parallel Voice Conversion with Filling in Frames\\u00a7r\\n\\n\\u00a78\\u00a7oTakuhiro Kaneko\\nHirokazu Kameoka\\nKou Tanaka\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.12841\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 Feb 2021 13:26:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021. Project page: http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/maskcyclegan-vc/index.html\\u00a7r"}']}
{title:'Chowdhury et al. (§72021§r)', author: 'Shreyan Chowdhury; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2102.13479", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Explaining Expressive Qualities in Piano Recordings: Transfer of Explanatory Features via Acoustic Domain Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oShreyan Chowdhury\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.13479\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Feb 2021 13:49:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures; accepted for IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2021)\\u00a7r"}']}
{title:'Hou et al. (§72021§r)', author: 'Jingyong Hou; Li Zhang; Yihui Fu; Qing Wang; Zhanheng Yang; Qijie Shao; Lei Xie', display:{Lore:['[{"text": "arXiv:2102.13552", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe NPU System for the 2020 Personalized Voice Trigger Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oJingyong Hou\\nLi Zhang\\nYihui Fu\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.13552\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Feb 2021 15:44:38 GMT)\\u00a7r"}']}
{title:'Leng et al. (§72021§r)', author: 'Yichong Leng; Xu Tan; Sheng Zhao; Frank Soong; Xiang-Yang Li; Tao Qin', display:{Lore:['[{"text": "arXiv:2103.00110", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMBNet: MOS Prediction for Synthesized Speech with Mean-Bias Network\\u00a7r\\n\\n\\u00a78\\u00a7oYichong Leng\\nXu Tan\\nSheng Zhao\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.00110\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 27 Feb 2021 02:48:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2021\\u00a7r"}']}
{title:'Goudarzi et al. (§72021§r)', author: 'Armin Goudarzi; Carsten Spehr; Steffen Herbold', display:{Lore:['[{"text": "arXiv:2103.00255", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExpert Decision Support System for aeroacoustic source type identification using clustering\\u00a7r\\n\\n\\u00a78\\u00a7oArmin Goudarzi\\nCarsten Spehr\\nSteffen Herbold\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.00255\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0009322\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 18 Nov 2021 17:55:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint for JASA Journal\\u00a7r"}']}
{title:'Krishna et al. (§72021§r)', author: 'Gautam Krishna; Mason Carnahan; Shilpa Shamapant; Yashitha Surendranath; Saumya Jain; Arundhati Ghosh; Co Tran; Jose del R Millan; Ahmed H Tewfik', display:{Lore:['[{"text": "arXiv:2103.00383", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBrain Signals to Rescue Aphasia, Apraxia and Dysarthria Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGautam Krishna\\nMason Carnahan\\nShilpa Shamapant\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.00383\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 18 Jul 2021 00:02:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEEMBC 2021\\u00a7r"}']}
{title:'Schymura et al. (§72021§r)', author: 'Christopher Schymura; Tsubasa Ochiai; Marc Delcroix; Keisuke Kinoshita; Tomohiro Nakatani; Shoko Araki; Dorothea Kolossa', display:{Lore:['[{"text": "arXiv:2103.00417", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting Attention-based Sequence-to-Sequence Architectures for Sound Event Localization\\u00a7r\\n\\n\\u00a78\\u00a7oChristopher Schymura\\nTsubasa Ochiai\\nMarc Delcroix\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.00417\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 28 Feb 2021 07:52:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in Proceedings of the 28th European Signal Processing Conference (EUSIPCO), 2020\\u00a7r"}']}
{title:'Boenninghoff et al. (§72021§r)', author: 'Benedikt Boenninghoff; Robert M. Nickel; Steffen Zeiler; Dorothea Kolossa', display:{Lore:['[{"text": "arXiv:2103.01173", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Classification of Voiced Speech and Pitch Tracking Using Forward-Backward Kalman Filtering\\u00a7r\\n\\n\\u00a78\\u00a7oBenedikt Boenninghoff\\nRobert M. Nickel\\nSteffen Zeiler\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.01173\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Mar 2021 18:13:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSpeech Communication; 12. ITG Symposium, 5-7 Oct. 2016\\u00a7r"}']}
{title:'Makishima et al. (§72021§r)', author: 'Naoki Makishima; Mana Ihori; Akihiko Takashima; Tomohiro Tanaka; Shota Orihashi; Ryo Masumura', display:{Lore:['[{"text": "arXiv:2103.01463", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-Visual Speech Separation Using Cross-Modal Correspondence Loss\\u00a7r\\n\\n\\u00a78\\u00a7oNaoki Makishima\\nMana Ihori\\nAkihiko Takashima\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.01463\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Mar 2021 04:29:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Schmalenstroeer et al. (§72021§r)', author: 'Joerg Schmalenstroeer; Jens Heitkaemper; Joerg Ullmann; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2103.01599", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOpen Range Pitch Tracking for Carrier Frequency Difference Estimation from HF Transmitted Speech\\u00a7r\\n\\n\\u00a78\\u00a7oJoerg Schmalenstroeer\\nJens Heitkaemper\\nJoerg Ullmann\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.01599\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 3 Mar 2021 07:46:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to EUSIPCO 2021\\u00a7r"}']}
{title:'Fakhry et al. (§72021§r)', author: 'Ahmed Fakhry; Xinyi Jiang; Jaclyn Xiao; Gunvant Chaudhari; Asriel Han; Amil Khanzada', display:{Lore:['[{"text": "arXiv:2103.01806", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVirufy: A Multi-Branch Deep Learning Network for Automated Detection of COVID-19\\u00a7r\\n\\n\\u00a78\\u00a7oAhmed Fakhry\\nXinyi Jiang\\nJaclyn Xiao\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.01806\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 Mar 2021 21:31:01 GMT)\\u00a7r"}']}
{title:'Gerstoft et al. (§72021§r)', author: 'Peter Gerstoft; Yihan Hu; Michael J. Bianco; Chaitanya Patil; Ardel Alegre; Yoav Freund; Francois Grondin', display:{Lore:['[{"text": "arXiv:2103.01830", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio scene monitoring using redundant ad-hoc microphone array networks\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Gerstoft\\nYihan Hu\\nMichael J. Bianco\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.01830\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JIOT.2021.3103523\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 23 Aug 2021 20:21:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIN press, IEEE Internet ofThings Journal\\u00a7r"}']}
{title:'Choi et al. (§72021§r)', author: 'Keunwoo Choi; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2103.01893", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lListen, Read, and Identify: Multimodal Singing Language Identification of Music\\u00a7r\\n\\n\\u00a78\\u00a7oKeunwoo Choi\\nYuxuan Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.01893\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 27 Jul 2021 19:23:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oISMIR 2021 camera-ready\\u00a7r"}']}
{title:'Neumann et al. (§72021§r)', author: 'Michael Neumann; Ngoc Thang Vu', display:{Lore:['[{"text": "arXiv:2103.01894", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigations on Audiovisual Emotion Recognition in Noisy Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oMichael Neumann\\nNgoc Thang Vu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.01894\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Mar 2021 17:45:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at the IEEE workshop on Spoken Language Technology (SLT) 2021\\u00a7r"}']}
{title:'Nasiri et al. (§72021§r)', author: 'Alireza Nasiri; Jianjun Hu', display:{Lore:['[{"text": "arXiv:2103.01929", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSoundCLR: Contrastive Learning of Representations For Improved Environmental Sound Classification\\u00a7r\\n\\n\\u00a78\\u00a7oAlireza Nasiri\\nJianjun Hu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.01929\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Mar 2021 18:42:45 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Dongmei Wang; Takuya Yoshioka; Zhuo Chen; Xiaofei Wang; Tianyan Zhou; Zhong Meng', display:{Lore:['[{"text": "arXiv:2103.02378", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContinuous Speech Separation with Ad Hoc Microphone Arrays\\u00a7r\\n\\n\\u00a78\\u00a7oDongmei Wang\\nTakuya Yoshioka\\nZhuo Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.02378\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Mar 2021 13:01:08 GMT)\\u00a7r"}']}
{title:'Phan et al. (§72021§r)', author: 'Huy Phan; Huy Le Nguyen; Oliver Y. Chén; Lam Pham; Philipp Koch; Ian McLoughlin; Alfred Mertins', display:{Lore:['[{"text": "arXiv:2103.02420", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-view Audio and Music Classification\\u00a7r\\n\\n\\u00a78\\u00a7oHuy Phan\\nHuy Le Nguyen\\nOliver Y. Ch\\u00e9n\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.02420\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Mar 2021 14:18:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Tzinis et al. (§72021§r)', author: 'Efthymios Tzinis; Zhepei Wang; Xilin Jiang; Paris Smaragdis', display:{Lore:['[{"text": "arXiv:2103.02644", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCompute and memory efficient universal sound source separation\\u00a7r\\n\\n\\u00a78\\u00a7oEfthymios Tzinis\\nZhepei Wang\\nXilin Jiang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.02644\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s11265-021-01683-x\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 14 Jul 2021 23:14:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Journal of Signal Processing Systems https://www.springer.com/journal/11265. arXiv admin note: substantial text overlap with arXiv:2007.06833\\u00a7r"}']}
{title:'Tzirakis et al. (§72021§r)', author: 'Panagiotis Tzirakis; Anh Nguyen; Stefanos Zafeiriou; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2103.02993", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Emotion Recognition using Semantic Information\\u00a7r\\n\\n\\u00a78\\u00a7oPanagiotis Tzirakis\\nAnh Nguyen\\nStefanos Zafeiriou\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.02993\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Mar 2021 12:34:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2021\\u00a7r"}']}
{title:'Awasthi et al. (§72021§r)', author: 'Abhijeet Awasthi; Aman Kansal; Sunita Sarawagi; Preethi Jyothi', display:{Lore:['[{"text": "arXiv:2103.03142", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lError-driven Fixed-Budget ASR Personalization for Accented Speakers\\u00a7r\\n\\n\\u00a78\\u00a7oAbhijeet Awasthi\\nAman Kansal\\nSunita Sarawagi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.03142\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 2 Jun 2021 17:13:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn ICASSP 2021\\u00a7r"}']}
{title:'Kazakos et al. (§72021§r)', author: 'Evangelos Kazakos; Arsha Nagrani; Andrew Zisserman; Dima Damen', display:{Lore:['[{"text": "arXiv:2103.03516", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSlow-Fast Auditory Streams For Audio Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oEvangelos Kazakos\\nArsha Nagrani\\nAndrew Zisserman\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.03516\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Mar 2021 07:51:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for presentation at ICASSP 2021\\u00a7r"}']}
{title:'Georgiou et al. (§72021§r)', author: 'Efthymios Georgiou; Athanasios Katsamanis', display:{Lore:['[{"text": "arXiv:2103.03927", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudioVisual Speech Synthesis: A brief literature review\\u00a7r\\n\\n\\u00a78\\u00a7oEfthymios Georgiou\\nAthanasios Katsamanis\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.03927\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Feb 2021 19:13:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oreviewis written in Greek\\u00a7r"}']}
{title:'Tits et al. (§72021§r)', author: 'Noé Tits; Kevin El Haddad; Thierry Dutoit', display:{Lore:['[{"text": "arXiv:2103.04097", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalysis and Assessment of Controllability of an Expressive Deep Learning-based TTS system\\u00a7r\\n\\n\\u00a78\\u00a7oNo\\u00e9 Tits\\nKevin El Haddad\\nThierry Dutoit\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.04097\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 6 Mar 2021 11:06:13 GMT)\\u00a7r"}']}
{title:'You et al. (§72021§r)', author: 'Jaeseong You; Dalhyun Kim; Gyuhyeon Nam; Geumbyeol Hwang; Gyeongsu Chae', display:{Lore:['[{"text": "arXiv:2103.05236", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGAN Vocoder: Multi-Resolution Discriminator Is All You Need\\u00a7r\\n\\n\\u00a78\\u00a7oJaeseong You\\nDalhyun Kim\\nGyuhyeon Nam\\nGeumbyeol Hwang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.05236\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 23 Aug 2021 11:33:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021\\u00a7r"}']}
{title:'Kaneko (§72021§r)', author: 'Shoken Kaneko', display:{Lore:['[{"text": "arXiv:2103.05719", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpheroidal Ambisonics: a Spatial Audio Framework Using Spheroidal Bases\\u00a7r\\n\\n\\u00a78\\u00a7oShoken Kaneko\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.05719\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0005942\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJASA Express Letters 1.8 (2021): 084803\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Mar 2021 21:03:42 GMT)\\u00a7r"}']}
{title:'Khanal et al. (§72021§r)', author: 'Abhish Khanal; Deepak Chand; Prakash Chaudhary; Subash Timilsina; Sanjeeb Prasad Panday; Aman Shakya; Rom Kant Pandey', display:{Lore:['[{"text": "arXiv:2103.06049", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSearch Disaster Victims using Sound Source Localization\\u00a7r\\n\\n\\u00a78\\u00a7oAbhish Khanal\\nDeepak Chand\\nPrakash Chaudhary\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.06049\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIscram 2020 1022-1030\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Mar 2021 13:44:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 17 figures, 17th ISCRAM Conference Blacksburg, VA, USA\\u00a7r"}']}
{title:'Tripathi et al. (§72021§r)', author: 'Ayush Tripathi; Swapnil Bhosale; Sunil Kumar Kopparapu', display:{Lore:['[{"text": "arXiv:2103.06157", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Speaker Independent Dysarthric Speech Intelligibility Assessment System\\u00a7r\\n\\n\\u00a78\\u00a7oAyush Tripathi\\nSwapnil Bhosale\\nSunil Kumar Kopparapu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.06157\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.csl.2021.101213\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Mar 2021 16:15:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o29 pages, 2 figures, Computer Speech Language 2021\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Luyu Wang; Aaron van den Oord', display:{Lore:['[{"text": "arXiv:2103.06508", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Format Contrastive Learning of Audio Representations\\u00a7r\\n\\n\\u00a78\\u00a7oLuyu Wang\\nAaron van den Oord\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.06508\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 24 Mar 2021 02:44:05 GMT)\\u00a7r"}']}
{title:'Tran et al. (§72021§r)', author: 'Mai Lan Tran; Changbom Park; Jae-Hun Jung', display:{Lore:['[{"text": "arXiv:2103.06620", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTopological Data Analysis of Korean Music in Jeongganbo: A Cycle Structure\\u00a7r\\n\\n\\u00a78\\u00a7oMai Lan Tran\\nChangbom Park\\nJae-Hun Jung\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.06620\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 30 Jun 2021 18:19:03 GMT)\\u00a7r"}']}
{title:'Riad et al. (§72021§r)', author: 'Rachid Riad; Julien Karadayi; Anne-Catherine Bachoud-Lévi; Emmanuel Dupoux', display:{Lore:['[{"text": "arXiv:2103.07125", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning spectro-temporal representations of complex sounds with parameterized neural networks\\u00a7r\\n\\n\\u00a78\\u00a7oRachid Riad\\nJulien Karadayi\\nAnne-Catherine Bachoud-L\\u00e9vi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.07125\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0005482\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Mar 2021 07:53:47 GMT)\\u00a7r"}']}
{title:'Alonso et al. (§72021§r)', author: 'Juan Alonso; Cumhur Erkut', display:{Lore:['[{"text": "arXiv:2103.07197", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLatent Space Explorations of Singing Voice Synthesis using DDSP\\u00a7r\\n\\n\\u00a78\\u00a7oJuan Alonso\\nCumhur Erkut\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.07197\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Mar 2021 10:38:29 GMT)\\u00a7r"}']}
{title:'Ganis et al. (§72021§r)', author: 'Francesco Ganis; Erik Frej Knudesn; Søren V. K. Lyster; Robin Otterbein; David Südholt; Cumhur Erkut', display:{Lore:['[{"text": "arXiv:2103.07220", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-time Timbre Transfer and Sound Synthesis using DDSP\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesco Ganis\\nErik Frej Knudesn\\nS\\u00f8ren V. K. Lyster\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.07220\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Mar 2021 11:49:51 GMT)\\u00a7r"}']}
{title:'Chalmers et al. (§72021§r)', author: 'C. Chalmers; P. Fergus; S. Wich; S. N. Longmore', display:{Lore:['[{"text": "arXiv:2103.07276", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModelling Animal Biodiversity Using Acoustic Monitoring and Deep Learning\\u00a7r\\n\\n\\u00a78\\u00a7oC. Chalmers\\nP. Fergus\\nS. Wich\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.07276\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Mar 2021 13:50:31 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Xinran Zhang; Maosong Sun; Jiafeng Liu; Xiaobing Li', display:{Lore:['[{"text": "arXiv:2103.07656", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimal Embedding Calibration for Symbolic Music Similarity\\u00a7r\\n\\n\\u00a78\\u00a7oXinran Zhang\\nMaosong Sun\\nJiafeng Liu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.07656\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 9 Sep 2021 02:13:26 GMT)\\u00a7r"}']}
{title:'Duangpummet et al. (§72021§r)', author: 'Suradej Duangpummet; Jessada Karnjana; Waree Kongprawechnon; Masashi Unoki', display:{Lore:['[{"text": "arXiv:2103.07904", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBlind Estimation of Room Acoustic Parameters and Speech Transmission Index using MTF-based CNNs\\u00a7r\\n\\n\\u00a78\\u00a7oSuradej Duangpummet\\nJessada Karnjana\\nWaree Kongprawechnon\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.07904\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 14 Mar 2021 12:11:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 10 figures, IEEEtran class\\u00a7r"}']}
{title:'Esmaeilpour et al. (§72021§r)', author: 'Mohammad Esmaeilpour; Patrick Cardinal; Alessandro Lameiras Koerich', display:{Lore:['[{"text": "arXiv:2103.08086", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Discriminator Sobolev Defense-GAN Against Adversarial Attacks for End-to-End Speech Systems\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Esmaeilpour\\nPatrick Cardinal\\nAlessandro Lameiras Koerich\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.08086\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Mar 2021 01:11:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages\\u00a7r"}']}
{title:'Esmaeilpour et al. (§72021§r)', author: 'Mohammad Esmaeilpour; Patrick Cardinal; Alessandro Lameiras Koerich', display:{Lore:['[{"text": "arXiv:2103.08095", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Robust Speech-to-Text Adversarial Attack\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Esmaeilpour\\nPatrick Cardinal\\nAlessandro Lameiras Koerich\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.08095\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Mar 2021 01:51:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Bader et al. (§72021§r)', author: 'Rolf Bader; Michael Blaß; Jonas Franke', display:{Lore:['[{"text": "arXiv:2103.08203", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComputational timbre and tonal system similarity analysis of the music of Northern Myanmar-based Kachin compared to Xinjiang-based Uyghur ethnic groups\\u00a7r\\n\\n\\u00a78\\u00a7oRolf Bader\\nMichael Bla\\u00df\\nJonas Franke\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.08203\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Mar 2021 08:23:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 9 figures\\u00a7r"}']}
{title:'Gerczuk et al. (§72021§r)', author: 'Maurice Gerczuk; Shahin Amiriparian; Sandra Ottl; Björn Schuller', display:{Lore:['[{"text": "arXiv:2103.08310", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmoNet: A Transfer Learning Framework for Multi-Corpus Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMaurice Gerczuk\\nShahin Amiriparian\\nSandra Ottl\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.08310\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Mar 2021 19:12:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages, 7 figures\\u00a7r"}']}
{title:'Tu et al. (§72021§r)', author: 'Zehai Tu; Ning Ma; Jon Barker', display:{Lore:['[{"text": "arXiv:2103.08569", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDHASP: Differentiable Hearing Aid Speech Processing\\u00a7r\\n\\n\\u00a78\\u00a7oZehai Tu\\nNing Ma\\nJon Barker\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.08569\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Mar 2021 17:34:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at ICASSP 2021\\u00a7r"}']}
{title:'Mohamud et al. (§72021§r)', author: 'Jama Hussein Mohamud; Lloyd Acquaye Thompson; Aissatou Ndoye; Laurent Besacier', display:{Lore:['[{"text": "arXiv:2103.08993", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast Development of ASR in African Languages using Self Supervised Speech Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oJama Hussein Mohamud\\nLloyd Acquaye Thompson\\nAissatou Ndoye\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.08993\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Mar 2021 11:37:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at AfricaNLP2021 workshop at EACL 2021\\u00a7r"}']}
{title:'Lv et al. (§72021§r)', author: 'Hang Lv; Zhehuai Chen; Hainan Xu; Daniel Povey; Lei Xie; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:2103.09063", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Asynchronous WFST-Based Decoder For Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHang Lv\\nZhehuai Chen\\nHainan Xu\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.09063\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Mar 2021 13:35:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, icassp\\u00a7r"}']}
{title:'Spijkervet et al. (§72021§r)', author: 'Janne Spijkervet; John Ashley Burgoyne', display:{Lore:['[{"text": "arXiv:2103.09410", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContrastive Learning of Musical Representations\\u00a7r\\n\\n\\u00a78\\u00a7oJanne Spijkervet\\nJohn Ashley Burgoyne\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.09410\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 24 Sep 2021 23:19:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 8 figures. In Proceedings of the 22nd International Society for Music Information Retrieval Conference, ISMIR, 2021\\u00a7r"}']}
{title:'Carr et al. (§72021§r)', author: 'Andrew N Carr; Quentin Berthet; Mathieu Blondel; Olivier Teboul; Neil Zeghidour', display:{Lore:['[{"text": "arXiv:2103.09879", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Learning of Audio Representations from Permutations with Differentiable Ranking\\u00a7r\\n\\n\\u00a78\\u00a7oAndrew N Carr\\nQuentin Berthet\\nMathieu Blondel\\nOlivier Teboul\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.09879\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2021.3067635\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Mar 2021 19:36:04 GMT)\\u00a7r"}']}
{title:'Ning et al. (§72021§r)', author: 'Hailong Ning; Xiangtao Zheng; Yuan Yuan; Xiaoqiang Lu', display:{Lore:['[{"text": "arXiv:2103.10018", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Description from Image by Modal Translation Network\\u00a7r\\n\\n\\u00a78\\u00a7oHailong Ning\\nXiangtao Zheng\\nYuan Yuan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.10018\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.neucom.2020.10.053\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Mar 2021 04:48:29 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Yuxuan Wang; Maokui He; Shutong Niu; Lei Sun; Tian Gao; Xin Fang; Jia Pan; Jun Du; Chin-Hui Lee', display:{Lore:['[{"text": "arXiv:2103.10661", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUSTC-NELSLIP System Description for DIHARD-III Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oYuxuan Wang\\nMaokui He\\nShutong Niu\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.10661\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Mar 2021 07:00:51 GMT)\\u00a7r"}']}
{title:'Ristea et al. (§72021§r)', author: 'Nicolae-Catalin Ristea; Radu Tudor Ionescu', display:{Lore:['[{"text": "arXiv:2103.11988", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-paced ensemble learning for speech and audio classification\\u00a7r\\n\\n\\u00a78\\u00a7oNicolae-Catalin Ristea\\nRadu Tudor Ionescu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.11988\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 8 Jun 2021 17:22:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2021\\u00a7r"}']}
{title:'Parker et al. (§72021§r)', author: 'Andrew Parker; Steven Fenton', display:{Lore:['[{"text": "arXiv:2103.12152", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusical Mix Clarity Predication using Decomposition and Perceptual Masking Thresholds\\u00a7r\\n\\n\\u00a78\\u00a7oAndrew Parker\\nSteven Fenton\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.12152\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3390/app11209578\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 30 Sep 2021 16:43:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages, 7 figures, 2 tables\\u00a7r"}']}
{title:'Elliott et al. (§72021§r)', author: 'David Elliott; Carlos E. Otero; Steven Wyatt; Evan Martino', display:{Lore:['[{"text": "arXiv:2103.12157", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTiny Transformers for Environmental Sound Classification at the Edge\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Elliott\\nCarlos E. Otero\\nSteven Wyatt\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.12157\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Mar 2021 20:12:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, submitted to IEEE Journal of Internet of Things\\u00a7r"}']}
{title:'Yadav et al. (§72021§r)', author: 'Sarthak Yadav; Mary Ellen Foster', display:{Lore:['[{"text": "arXiv:2103.12306", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGISE-51: A scalable isolated sound events dataset\\u00a7r\\n\\n\\u00a78\\u00a7oSarthak Yadav\\nMary Ellen Foster\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.12306\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 7 Oct 2021 08:36:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTechnical Report\\u00a7r"}']}
{title:'Jansson et al. (§72021§r)', author: 'Andreas Jansson; Rachel M. Bittner; Nicola Montecchio; Tillman Weyde', display:{Lore:['[{"text": "arXiv:2103.12864", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearned complex masks for multi-instrument source separation\\u00a7r\\n\\n\\u00a78\\u00a7oAndreas Jansson\\nRachel M. Bittner\\nNicola Montecchio\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.12864\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Mar 2021 21:56:28 GMT)\\u00a7r"}']}
{title:'Liang et al. (§72021§r)', author: 'Beici Liang; György Fazekas; Mark Sandler', display:{Lore:['[{"text": "arXiv:2103.13219", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransfer Learning for Piano Sustain-Pedal Detection\\u00a7r\\n\\n\\u00a78\\u00a7oBeici Liang\\nGy\\u00f6rgy Fazekas\\nMark Sandler\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.13219\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/IJCNN.2019.8851724\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Mar 2021 14:28:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in 2019International Joint Conference on Neural Networks (IJCNN)\\u00a7r"}']}
{title:'Pahar et al. (§72021§r)', author: 'Madhurananda Pahar; Marisa Klopper; Byron Reeve; Grant Theron; Rob Warren; Thomas Niesler', display:{Lore:['[{"text": "arXiv:2103.13300", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Cough Classification for Tuberculosis Screening in a Real-World Environment\\u00a7r\\n\\n\\u00a78\\u00a7oMadhurananda Pahar\\nMarisa Klopper\\nByron Reeve\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.13300\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1088/1361-6579/ac2fb8\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPhysiological Measurement, 2021\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 17 Oct 2021 17:56:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper has been accepted in Physiological Measurement (2021)\\u00a7r"}']}
{title:'Pfeifenberger et al. (§72021§r)', author: 'Lukas Pfeifenberger; Franz Pernkopf', display:{Lore:['[{"text": "arXiv:2103.13443", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBlind Speech Separation and Dereverberation using Neural Beamforming\\u00a7r\\n\\n\\u00a78\\u00a7oLukas Pfeifenberger\\nFranz Pernkopf\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.13443\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 5 Nov 2021 00:51:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 9 figures\\u00a7r"}']}
{title:'Chang et al. (§72021§r)', author: 'Simyung Chang; Hyoungwoo Park; Janghoon Cho; Hyunsin Park; Sungrack Yun; Kyuwoong Hwang', display:{Lore:['[{"text": "arXiv:2103.13620", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSubSpectral Normalization for Neural Audio Data Processing\\u00a7r\\n\\n\\u00a78\\u00a7oSimyung Chang\\nHyoungwoo Park\\nJanghoon Cho\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.13620\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 Mar 2021 05:55:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, ICASSP \'21 accepted\\u00a7r"}']}
{title:'Singh et al. (§72021§r)', author: 'Nikhil Singh; Jeff Mentch; Jerry Ng; Matthew Beveridge; Iddo Drori', display:{Lore:['[{"text": "arXiv:2103.14201", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImage2Reverb: Cross-Modal Reverb Impulse Response Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oNikhil Singh\\nJeff Mentch\\nJerry Ng\\nMatthew Beveridge\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.14201\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 13 Aug 2021 18:48:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICCV 2021. Project page: https://web.media.mit.edu/ nsingh1/image2reverb/\\u00a7r"}']}
{title:'Longyu et al. (§72021§r)', author: 'Jiang Longyu; Zhang Zhe; Roux Philippe', display:{Lore:['[{"text": "arXiv:2103.14206", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThree dimensional higher-order raypath separation in a shallow-water waveguide\\u00a7r\\n\\n\\u00a78\\u00a7oJiang Longyu\\nZhang Zhe\\nRoux Philippe\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.14206\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Mar 2021 01:49:08 GMT)\\u00a7r"}']}
{title:'Huang et al. (§72021§r)', author: 'Jiawen Huang; Ju-Chiang Wang; Jordan B. L. Smith; Xuchen Song; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2103.14208", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModeling the Compatibility of Stem Tracks to Generate Music Mashups\\u00a7r\\n\\n\\u00a78\\u00a7oJiawen Huang\\nJu-Chiang Wang\\nJordan B. L. Smith\\nXuchen Song\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.14208\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Mar 2021 01:51:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis is a preprint of the paperaccepted by AAAI-21. Please cite the version included in the Proceedings of the 35th AAAI Conference on Artificial Intelligence\\u00a7r"}']}
{title:'Jiang et al. (§72021§r)', author: 'Longyu Jiang; Zhe Zhang; Rui Jin; Xiao Zhou; Philippe Roux', display:{Lore:['[{"text": "arXiv:2103.14236", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSubspace-based compressive sensing algorithm for raypath separation in a shallow-water waveguide\\u00a7r\\n\\n\\u00a78\\u00a7oLongyu Jiang\\nZhe Zhang\\nRui Jin\\nXiao Zhou\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.14236\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Mar 2021 02:55:28 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Congyi Wang; Yu Chen; Bin Wang; Yi Shi', display:{Lore:['[{"text": "arXiv:2103.14245", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImprove GAN-based Neural Vocoder using Pointwise Relativistic LeastSquare GAN\\u00a7r\\n\\n\\u00a78\\u00a7oCongyi Wang\\nYu Chen\\nBin Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.14245\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 29 Mar 2021 03:00:21 GMT)\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Hao Li; Xueliang Zhang; Guanglai Gao', display:{Lore:['[{"text": "arXiv:2103.14330", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGuided Training: A Simple Method for Single-channel Speaker Separation\\u00a7r\\n\\n\\u00a78\\u00a7oHao Li\\nXueliang Zhang\\nGuanglai Gao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.14330\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Mar 2021 08:46:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Elias et al. (§72021§r)', author: 'Isaac Elias; Heiga Zen; Jonathan Shen; Yu Zhang; Ye Jia; RJ Skerry-Ryan; Yonghui Wu', display:{Lore:['[{"text": "arXiv:2103.14574", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParallel Tacotron 2: A Non-Autoregressive Neural TTS Model with Differentiable Duration Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oIsaac Elias\\nHeiga Zen\\nJonathan Shen\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.14574\\u00a7r\\n\\nVersion:\\u00a77v7 (Sun, 29 Aug 2021 06:35:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Esmaeilpour et al. (§72021§r)', author: 'Mohammad Esmaeilpour; Patrick Cardinal; Alessandro Lameiras Koerich', display:{Lore:['[{"text": "arXiv:2103.14717", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCyclic Defense GAN Against Speech Adversarial Attacks\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Esmaeilpour\\nPatrick Cardinal\\nAlessandro Lameiras Koerich\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.14717\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2021.3106239\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Signal Processing Letters (2021) 1-5\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 22 Aug 2021 13:30:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5\\u00a7r"}']}
{title:'Ando et al. (§72021§r)', author: 'Shintaro Ando; Hiromasa Fujihara', display:{Lore:['[{"text": "arXiv:2103.14736", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConstruction of a Large-scale Japanese ASR Corpus on TV Recordings\\u00a7r\\n\\n\\u00a78\\u00a7oShintaro Ando\\nHiromasa Fujihara\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.14736\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Mar 2021 21:14:12 GMT)\\u00a7r"}']}
{title:'Kolbæk et al. (§72021§r)', author: 'Morten Kolbæk; Zheng-Hua Tan; Søren Holdt Jensen; Jesper Jensen', display:{Lore:['[{"text": "arXiv:2103.14882", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn TasNet for Low-Latency Single-Speaker Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oMorten Kolb\\u00e6k\\nZheng-Hua Tan\\nS\\u00f8ren Holdt Jensen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.14882\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 27 Mar 2021 11:29:59 GMT)\\u00a7r"}']}
{title:'Malvermi et al. (§72021§r)', author: 'R. Malvermi; S. Gonzalez; M. Quintavalla; F. Antonacci; A. Sarti; J. A. Torres; R. Corradi', display:{Lore:['[{"text": "arXiv:2103.14895", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFeature-based Representation for Violin Bridge Admittances\\u00a7r\\n\\n\\u00a78\\u00a7oR. Malvermi\\nS. Gonzalez\\nM. Quintavalla\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.14895\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 27 Mar 2021 12:53:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 6 figures, submitted to \\"The 27th International Congresson Sound and Vibration\\"(ICSV)\\u00a7r"}']}
{title:'Liang et al. (§72021§r)', author: 'Chengdong Liang; Menglong Xu; Xiao-Lei Zhang', display:{Lore:['[{"text": "arXiv:2103.15722", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransformer-based end-to-end speech recognition with residual Gaussian-based self-attention\\u00a7r\\n\\n\\u00a78\\u00a7oChengdong Liang\\nMenglong Xu\\nXiao-Lei Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.15722\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 8 Oct 2021 05:19:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThere is an error in the description of section 3.2.1\\u00a7r"}']}
{title:'Donaher et al. (§72021§r)', author: 'Santiago Donaher; Alessio Xompero; Andrea Cavallaro', display:{Lore:['[{"text": "arXiv:2103.15999", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio classification of the content of food containers and drinking glasses\\u00a7r\\n\\n\\u00a78\\u00a7oSantiago Donaher\\nAlessio Xompero\\nAndrea Cavallaro\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.15999\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 9 Jun 2021 16:14:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCamera-ready version. Paper accepted to EUSIPCO21.5 pages, 4 figures, 3 tables. Minor improvements to the paper presentation\\u00a7r"}']}
{title:'Zheng et al. (§72021§r)', author: 'Weiping Zheng; Dacan Jiang; Gansen Zhao', display:{Lore:['[{"text": "arXiv:2103.16079", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnvironmental sound analysis with mixup based multitask learning and cross-task fusion\\u00a7r\\n\\n\\u00a78\\u00a7oWeiping Zheng\\nDacan Jiang\\nGansen Zhao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.16079\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Mar 2021 05:11:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figue\\u00a7r"}']}
{title:'Mittal et al. (§72021§r)', author: 'Gautam Mittal; Jesse Engel; Curtis Hawthorne; Ian Simon', display:{Lore:['[{"text": "arXiv:2103.16091", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSymbolic Music Generation with Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oGautam Mittal\\nJesse Engel\\nCurtis Hawthorne\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.16091\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 Nov 2021 19:58:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oISMIR 2021\\u00a7r"}']}
{title:'Xiao et al. (§72021§r)', author: 'Feiyang Xiao; Jian Guan; Qiuqiang Kong; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2103.16149", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTime-domain Speech Enhancement with Generative Adversarial Learning\\u00a7r\\n\\n\\u00a78\\u00a7oFeiyang Xiao\\nJian Guan\\nQiuqiang Kong\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.16149\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 19 Sep 2021 09:06:20 GMT)\\u00a7r"}']}
{title:'Ratnarajah et al. (§72021§r)', author: 'Anton Ratnarajah; Zhenyu Tang; Dinesh Manocha', display:{Lore:['[{"text": "arXiv:2103.16804", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTS-RIR: Translated synthetic room impulse responses for speech augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oAnton Ratnarajah\\nZhenyu Tang\\nDinesh Manocha\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.16804\\u00a7r\\n\\nVersion:\\u00a77v5 (Fri, 12 Nov 2021 02:24:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEASRU 2021. Source code is available at https://github.com/GAMMA-UMD/TS-RIR\\u00a7r"}']}
{title:'Olivieri et al. (§72021§r)', author: 'Marco Olivieri; Mirco Pezzoli; Fabio Antonacci; Augusto Sarti', display:{Lore:['[{"text": "arXiv:2103.16935", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNear field Acoustic Holography on arbitrary shapes using Convolutional Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oMarco Olivieri\\nMirco Pezzoli\\nFabio Antonacci\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.16935\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 29 Jun 2021 10:06:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted for publication in EUSIPCO21\\u00a7r"}']}
{title:'Rovithis et al. (§72021§r)', author: 'Emmanuel Rovithis; Nikolaos Moustakas; Konstantinos Vogklis; Konstantinos Drossos; Andreas Floros', display:{Lore:['[{"text": "arXiv:2103.16988", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Citizen Science for Smart Cities: A Framework for a Collaborative Game of Bird Call Recognition Based on Internet of Sound Practices\\u00a7r\\n\\n\\u00a78\\u00a7oEmmanuel Rovithis\\nNikolaos Moustakas\\nKonstantinos Vogklis\\nKonstantinos Drossos\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.16988\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 Mar 2021 11:07:21 GMT)\\u00a7r"}']}
{title:'Ali et al. (§72021§r)', author: 'Hafiz Shehbaz Ali; Fakhar ul Hassan; Siddique Latif; Habib Ullah Manzoor; Junaid Qadir', display:{Lore:['[{"text": "arXiv:2103.17139", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrivacy Enhanced Speech Emotion Communication using Deep Learning Aided Edge Computing\\u00a7r\\n\\n\\u00a78\\u00a7oHafiz Shehbaz Ali\\nFakhar ul Hassan\\nSiddique Latif\\nHabib Ullah Manzoor\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.17139\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 Mar 2021 15:01:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted in ICC 2021 AffectiveSense workshop\\u00a7r"}']}
{title:'Polyak et al. (§72021§r)', author: 'Adam Polyak; Yossi Adi; Jade Copet; Eugene Kharitonov; Kushal Lakhotia; Wei-Ning Hsu; Abdelrahman Mohamed; Emmanuel Dupoux', display:{Lore:['[{"text": "arXiv:2104.00355", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Resynthesis from Discrete Disentangled Self-Supervised Representations\\u00a7r\\n\\n\\u00a78\\u00a7oAdam Polyak\\nYossi Adi\\nJade Copet\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.00355\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 27 Jul 2021 14:27:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of Interspeech 2021\\u00a7r"}']}
{title:'Ferraro et al. (§72021§r)', author: 'Andres Ferraro; Xavier Favory; Konstantinos Drossos; Yuntae Kim; Dmitry Bogdanov', display:{Lore:['[{"text": "arXiv:2104.00437", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnriched Music Representations with Multiple Cross-modal Contrastive Learning\\u00a7r\\n\\n\\u00a78\\u00a7oAndres Ferraro\\nXavier Favory\\nKonstantinos Drossos\\nYuntae Kim\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.00437\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2021.3071082\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Apr 2021 12:41:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication to IEEE Signal Processing Letters\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Jingsong Wang; Yuxuan He; Chunyu Zhao; Qijie Shao; Wei-Wei Tu; Tom Ko; Hung-yi Lee; Lei Xie', display:{Lore:['[{"text": "arXiv:2104.00513", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAuto-KWS 2021 Challenge: Task, Datasets, and Baselines\\u00a7r\\n\\n\\u00a78\\u00a7oJingsong Wang\\nYuxuan He\\nChunyu Zhao\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.00513\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 Mar 2021 14:56:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures\\u00a7r"}']}
{title:'Abbasi et al. (§72021§r)', author: 'Saad Abbasi; Mahmoud Famouri; Mohammad Javad Shafiee; Alexander Wong', display:{Lore:['[{"text": "arXiv:2104.00528", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOutlierNets: Highly Compact Deep Autoencoder Network Architectures for On-Device Acoustic Anomaly Detection\\u00a7r\\n\\n\\u00a78\\u00a7oSaad Abbasi\\nMahmoud Famouri\\nMohammad Javad Shafiee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.00528\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 19 Apr 2021 03:25:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages\\u00a7r"}']}
{title:'He et al. (§72021§r)', author: 'Qing He; Zhiping Xiu; Thilo Koehler; Jilong Wu', display:{Lore:['[{"text": "arXiv:2104.00705", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-rate attention architecture for fast streamable Text-to-speech spectrum modeling\\u00a7r\\n\\n\\u00a78\\u00a7oQing He\\nZhiping Xiu\\nThilo Koehler\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.00705\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Apr 2021 18:15:30 GMT)\\u00a7r"}']}
{title:'Brümmer et al. (§72021§r)', author: 'Niko Brümmer; Luciana Ferrer; Albert Swart', display:{Lore:['[{"text": "arXiv:2104.00732", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOut of a hundred trials, how many errors does your speaker verifier make?\\u00a7r\\n\\n\\u00a78\\u00a7oNiko Br\\u00fcmmer\\nLuciana Ferrer\\nAlbert Swart\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.00732\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Apr 2021 19:10:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2021\\u00a7r"}']}
{title:'Hsu et al. (§72021§r)', author: 'Wei-Ning Hsu; Anuroop Sriram; Alexei Baevski; Tatiana Likhomanenko; Qiantong Xu; Vineel Pratap; Jacob Kahn; Ann Lee; Ronan Collobert; Gabriel Synnaeve; Michael Auli', display:{Lore:['[{"text": "arXiv:2104.01027", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training\\u00a7r\\n\\n\\u00a78\\u00a7oWei-Ning Hsu\\nAnuroop Sriram\\nAlexei Baevski\\n+ 7 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01027\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 8 Sep 2021 04:12:36 GMT)\\u00a7r"}']}
{title:'Luo et al. (§72021§r)', author: 'Wenjie Luo; Zhenyu Yan; Qun Song; Rui Tan', display:{Lore:['[{"text": "arXiv:2104.01160", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhyAug: Physics-Directed Data Augmentation for Deep Sensing Model Transfer in Cyber-Physical Systems\\u00a7r\\n\\n\\u00a78\\u00a7oWenjie Luo\\nZhenyu Yan\\nQun Song\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01160\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3412382.3458255\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 19 Apr 2021 12:42:02 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72021§r)', author: 'Chao-Han Huck Yang; Sabato Marco Siniscalchi; Chin-Hui Lee', display:{Lore:['[{"text": "arXiv:2104.01271", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPATE-AAE: Incorporating Adversarial Autoencoder into Private Aggregation of Teacher Ensembles for Spoken Command Classification\\u00a7r\\n\\n\\u00a78\\u00a7oChao-Han Huck Yang\\nSabato Marco Siniscalchi\\nChin-Hui Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01271\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-640\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2021\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 15 Jun 2021 06:09:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021\\u00a7r"}']}
{title:'Tumminia et al. (§72021§r)', author: 'Jeffrey Tumminia; Amanda Kuznecov; Sophia Tsilerides; Ilana Weinstein; Brian McFee; Michael Picheny; Aaron R. Kaufman', display:{Lore:['[{"text": "arXiv:2104.01304", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiarization of Legal Proceedings. Identifying and Transcribing Judicial Speech from Recorded Court Audio\\u00a7r\\n\\n\\u00a78\\u00a7oJeffrey Tumminia\\nAmanda Kuznecov\\nSophia Tsilerides\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01304\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 3 Apr 2021 03:31:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review for InterSpeech2021\\u00a7r"}']}
{title:'Kawahara et al. (§72021§r)', author: 'Hideki Kawahara; Toshie Matsui; Kohei Yatabe; Ken-Ichi Sakakibara; Minoru Tsuzaki; Masanori Morise; Toshio Irino', display:{Lore:['[{"text": "arXiv:2104.01444", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMixture of orthogonal sequences made from extended time-stretched pulses enables measurement of involuntary voice fundamental frequency response to pitch perturbation\\u00a7r\\n\\n\\u00a78\\u00a7oHideki Kawahara\\nToshie Matsui\\nKohei Yatabe\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01444\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-2073\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 3 Apr 2021 16:35:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 9 figures, submitted to Interspeech2021\\u00a7r"}']}
{title:'Gong et al. (§72021§r)', author: 'Yuan Gong; Yu-An Chung; James Glass', display:{Lore:['[{"text": "arXiv:2104.01778", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAST: Audio Spectrogram Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oYuan Gong\\nYu-An Chung\\nJames Glass\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01778\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 8 Jul 2021 20:16:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2021. Code at https://github.com/YuanGongND/ast\\u00a7r"}']}
{title:'Moritani et al. (§72021§r)', author: 'Asuka Moritani; Ryo Ozaki; Shoki Sakamoto; Hirokazu Kameoka; Tadahiro Taniguchi', display:{Lore:['[{"text": "arXiv:2104.01807", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStarGAN-based Emotional Voice Conversion for Japanese Phrases\\u00a7r\\n\\n\\u00a78\\u00a7oAsuka Moritani\\nRyo Ozaki\\nShoki Sakamoto\\nHirokazu Kameoka\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01807\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Apr 2021 08:08:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2021\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Haoqi Li; Yelin Kim; Cheng-Hao Kuo; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2104.01978", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lActed vs. Improvised: Domain Adaptation for Elicitation Approaches in Audio-Visual Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHaoqi Li\\nYelin Kim\\nCheng-Hao Kuo\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01978\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 7 Jun 2021 14:42:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7opaper accepted by INTERSPEECH2021\\u00a7r"}']}
{title:'Xia et al. (§72021§r)', author: 'Tong Xia; Jing Han; Lorena Qendro; Ting Dang; Cecilia Mascolo', display:{Lore:['[{"text": "arXiv:2104.02005", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUncertainty-Aware COVID-19 Detection from Imbalanced Sound Data\\u00a7r\\n\\n\\u00a78\\u00a7oTong Xia\\nJing Han\\nLorena Qendro\\nTing Dang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02005\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 18 Jun 2021 21:10:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2021\\u00a7r"}']}
{title:'Lu et al. (§72021§r)', author: 'Liang Lu; Naoyuki Kanda; Jinyu Li; Yifan Gong', display:{Lore:['[{"text": "arXiv:2104.02109", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreaming Multi-talker Speech Recognition with Joint Speaker Identification\\u00a7r\\n\\n\\u00a78\\u00a7oLiang Lu\\nNaoyuki Kanda\\nJinyu Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02109\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Apr 2021 18:37:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, submitted to Interspeech 2021\\u00a7r"}']}
{title:'Shangguan et al. (§72021§r)', author: 'Yuan Shangguan; Rohit Prabhavalkar; Hang Su; Jay Mahadeokar; Yangyang Shi; Jiatong Zhou; Chunyang Wu; Duc Le; Ozlem Kalinli; Christian Fuegen; Michael L. Seltzer', display:{Lore:['[{"text": "arXiv:2104.02207", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDissecting User-Perceived Latency of On-Device E2E Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYuan Shangguan\\nRohit Prabhavalkar\\nHang Su\\n+ 7 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02207\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 11 Aug 2021 19:31:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProc. of Interspeech 2021\\u00a7r"}']}
{title:'Mahadeokar et al. (§72021§r)', author: 'Jay Mahadeokar; Yangyang Shi; Yuan Shangguan; Chunyang Wu; Alex Xiao; Hang Su; Duc Le; Ozlem Kalinli; Christian Fuegen; Michael L. Seltzer', display:{Lore:['[{"text": "arXiv:2104.02232", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFlexi-Transducer: Optimizing Latency, Accuracy and Compute forMulti-Domain On-Device Scenarios\\u00a7r\\n\\n\\u00a78\\u00a7oJay Mahadeokar\\nYangyang Shi\\nYuan Shangguan\\n+ 6 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02232\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Apr 2021 01:50:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2021 (under review)\\u00a7r"}']}
{title:'Zhu et al. (§72021§r)', author: 'Tinglong Zhu; Xiaoyi Qin; Ming Li', display:{Lore:['[{"text": "arXiv:2104.02306", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBinary Neural Network for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oTinglong Zhu\\nXiaoyi Qin\\nMing Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02306\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Apr 2021 06:04:57 GMT)\\u00a7r"}']}
{title:'Middlebrook et al. (§72021§r)', author: 'Kai Middlebrook; Shyam Sudhakaran; David Guy Brizan', display:{Lore:['[{"text": "arXiv:2104.02309", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMuSLCAT: Multi-Scale Multi-Level Convolutional Attention Transformer for Discriminative Music Modeling on Raw Waveforms\\u00a7r\\n\\n\\u00a78\\u00a7oKai Middlebrook\\nShyam Sudhakaran\\nDavid Guy Brizan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02309\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Apr 2021 06:17:22 GMT)\\u00a7r"}']}
{title:'Raissi et al. (§72021§r)', author: 'Tina Raissi; Eugen Beck; Ralf Schlüter; Hermann Ney', display:{Lore:['[{"text": "arXiv:2104.02387", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Consistent Hybrid HMM Acoustic Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oTina Raissi\\nEugen Beck\\nRalf Schl\\u00fcter\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02387\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 12 Oct 2021 11:52:41 GMT)\\u00a7r"}']}
{title:'Pahar et al. (§72021§r)', author: 'Madhurananda Pahar; Marisa Klopper; Robin Warren; Thomas Niesler', display:{Lore:['[{"text": "arXiv:2104.02477", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCOVID-19 Detection in Cough, Breath and Speech using Deep Transfer Learning and Bottleneck Features\\u00a7r\\n\\n\\u00a78\\u00a7oMadhurananda Pahar\\nMarisa Klopper\\nRobin Warren\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02477\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.compbiomed.2021.105153\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nComputers in Biology and Medicine, 2022\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 18 Aug 2021 00:16:14 GMT)\\u00a7r"}']}
{title:'Vyas et al. (§72021§r)', author: 'Apoorv Vyas; Srikanth Madikeri; Hervé Bourlard', display:{Lore:['[{"text": "arXiv:2104.02558", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparing CTC and LFMMI for out-of-domain adaptation of wav2vec 2.0 acoustic model\\u00a7r\\n\\n\\u00a78\\u00a7oApoorv Vyas\\nSrikanth Madikeri\\nHerv\\u00e9 Bourlard\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02558\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Apr 2021 14:56:04 GMT)\\u00a7r"}']}
{title:'Shi et al. (§72021§r)', author: 'Xian Shi; Pan Zhou; Wei Chen; Lei Xie', display:{Lore:['[{"text": "arXiv:2104.02868", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDarts-Conformer: Towards Efficient Gradient-Based Neural Architecture Search For End-to-End ASR\\u00a7r\\n\\n\\u00a78\\u00a7oXian Shi\\nPan Zhou\\nWei Chen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02868\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 10 Aug 2021 04:02:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ASRU 2021\\u00a7r"}']}
{title:'Georges et al. (§72021§r)', author: 'Marc-Antoine Georges; Laurent Girin; Jean-Luc Schwartz; Thomas Hueber', display:{Lore:['[{"text": "arXiv:2104.03204", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning robust speech representation with an articulatory-regularized variational autoencoder\\u00a7r\\n\\n\\u00a78\\u00a7oMarc-Antoine Georges\\nLaurent Girin\\nJean-Luc Schwartz\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.03204\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Apr 2021 15:47:04 GMT)\\u00a7r"}']}
{title:'Pepino et al. (§72021§r)', author: 'Leonardo Pepino; Pablo Riera; Luciana Ferrer', display:{Lore:['[{"text": "arXiv:2104.03502", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotion Recognition from Speech Using Wav2vec 2.0 Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oLeonardo Pepino\\nPablo Riera\\nLuciana Ferrer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.03502\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Apr 2021 04:31:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures. Submitted to Interspeech 2021\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Xiang Li; Changhe Song; Jingbei Li; Zhiyong Wu; Jia Jia; Helen Meng', display:{Lore:['[{"text": "arXiv:2104.03521", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Multi-Scale Style Control for Expressive Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oXiang Li\\nChanghe Song\\nJingbei Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.03521\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Apr 2021 05:50:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, submitted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Fu et al. (§72021§r)', author: 'Szu-Wei Fu; Cheng Yu; Tsun-An Hsieh; Peter Plantinga; Mirco Ravanelli; Xugang Lu; Yu Tsao', display:{Lore:['[{"text": "arXiv:2104.03538", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMetricGAN+: An Improved Version of MetricGAN for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oSzu-Wei Fu\\nCheng Yu\\nTsun-An Hsieh\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.03538\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 4 Jun 2021 09:15:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2021\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Zhichao Wang; Wenwen Yang; Pan Zhou; Wei Chen', display:{Lore:['[{"text": "arXiv:2104.03587", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWNARS: WFST based Non-autoregressive Streaming End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhichao Wang\\nWenwen Yang\\nPan Zhou\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.03587\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 21 Apr 2021 03:57:44 GMT)\\u00a7r"}']}
{title:'Fu et al. (§72021§r)', author: 'Yihui Fu; Luyao Cheng; Shubo Lv; Yukai Jv; Yuxiang Kong; Zhuo Chen; Yanxin Hu; Lei Xie; Jian Wu; Hui Bu; Xin Xu; Jun Du; Jingdong Chen', display:{Lore:['[{"text": "arXiv:2104.03603", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAISHELL-4: An Open Source Dataset for Speech Enhancement, Separation, Recognition and Speaker Diarization in Conference Scenario\\u00a7r\\n\\n\\u00a78\\u00a7oYihui Fu\\nLuyao Cheng\\nShubo Lv\\n+ 9 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.03603\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 10 Aug 2021 09:15:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2021\\u00a7r"}']}
{title:'Kashyap et al. (§72021§r)', author: 'Madhav Mahesh Kashyap; Anuj Tambwekar; Krishnamoorthy Manohara; S Natarajan', display:{Lore:['[{"text": "arXiv:2104.03838", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Denoising Without Clean Training Data: A Noise2Noise Approach\\u00a7r\\n\\n\\u00a78\\u00a7oMadhav Mahesh Kashyap\\nAnuj Tambwekar\\nKrishnamoorthy Manohara\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.03838\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-1130\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2021, 2716-2720\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 19 Sep 2021 08:11:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in Interspeech 2021 ( Seehttps://www.isca-speech.org/archive/interspeech_2021/kashyap21_interspeech.html ). 5 pages, 2 figures, 1 table\\u00a7r"}']}
{title:'Mitcheltree et al. (§72021§r)', author: 'Christopher Mitcheltree; Hideki Koike', display:{Lore:['[{"text": "arXiv:2104.03876", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSerumRNN: Step by Step Audio VST Effect Programming\\u00a7r\\n\\n\\u00a78\\u00a7oChristopher Mitcheltree\\nHideki Koike\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.03876\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-030-72914-1_15\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n10th International Conference on Artificial Intelligence in Music,\\n  Sound, Art, and Design (EvoMUSART 2021), Seville, Spain\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Apr 2021 16:32:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAudio samples of the system can be listened to at bit.ly/serum_rnn\\u00a7r"}']}
{title:'Elyasi et al. (§72021§r)', author: 'Mahsa Elyasi; Gaurav Bharaj', display:{Lore:['[{"text": "arXiv:2104.04050", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFlavored Tacotron: Conditional Learning for Prosodic-linguistic Features\\u00a7r\\n\\n\\u00a78\\u00a7oMahsa Elyasi\\nGaurav Bharaj\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.04050\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Apr 2021 20:50:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5\\u00a7r"}']}
{title:'Gao et al. (§72021§r)', author: 'Yang Gao; Tyler Vuong; Mahsa Elyasi; Gaurav Bharaj; Rita Singh', display:{Lore:['[{"text": "arXiv:2104.04111", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeneralized Spoofing Detection Inspired from Audio Generation Artifacts\\u00a7r\\n\\n\\u00a78\\u00a7oYang Gao\\nTyler Vuong\\nMahsa Elyasi\\nGaurav Bharaj\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.04111\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 26 Jun 2021 00:14:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCamera ready version. Accepted by INTERSPEECH 2021\\u00a7r"}']}
{title:'Serra-Peralta et al. (§72021§r)', author: 'Marc Serra-Peralta; Joan Serrà; Álvaro Corral', display:{Lore:['[{"text": "arXiv:2104.04143", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.soc-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHeaps\' Law and Vocabulary Richness in the History of Classical Music Harmony\\u00a7r\\n\\n\\u00a78\\u00a7oMarc Serra-Peralta\\nJoan Serr\\u00e0\\n\\u00c1lvaro Corral\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.04143\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1140/epjds/s13688-021-00293-8\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Apr 2021 11:13:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages\\u00a7r"}']}
{title:'Na et al. (§72021§r)', author: 'Yueyue Na; Ziteng Wang; Zhang Liu; Biao Tian; Qiang Fu', display:{Lore:['[{"text": "arXiv:2104.04325", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Online Multichannel Acoustic Echo Cancellation, Speech Dereverberation and Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oYueyue Na\\nZiteng Wang\\nZhang Liu\\nBiao Tian\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.04325\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Apr 2021 12:13:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Lamba et al. (§72021§r)', author: 'Jatin Lamba; Abhishek; Jayaprakash Akula; Rishabh Dabral; Preethi Jyothi; Ganesh Ramakrishnan', display:{Lore:['[{"text": "arXiv:2104.04598", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-Modal learning for Audio-Visual Video Parsing\\u00a7r\\n\\n\\u00a78\\u00a7oJatin Lamba\\nAbhishek\\nJayaprakash Akula\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.04598\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 21 Jun 2021 10:56:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWork accepted at Interspeech 2021\\u00a7r"}']}
{title:'Yoneyama et al. (§72021§r)', author: 'Reo Yoneyama; Yi-Chiao Wu; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2104.04668", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnified Source-Filter GAN: Unified Source-filter Network Based On Factorization of Quasi-Periodic Parallel WaveGAN\\u00a7r\\n\\n\\u00a78\\u00a7oReo Yoneyama\\nYi-Chiao Wu\\nTomoki Toda\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.04668\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 27 Jun 2021 11:30:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Yu et al. (§72021§r)', author: 'Fan Yu; Haoneng Luo; Pengcheng Guo; Yuhao Liang; Zhuoyuan Yao; Lei Xie; Yingying Gao; Leijing Hou; Shilei Zhang', display:{Lore:['[{"text": "arXiv:2104.04702", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBoundary and Context Aware Training for CIF-based Non-Autoregressive End-to-end ASR\\u00a7r\\n\\n\\u00a78\\u00a7oFan Yu\\nHaoneng Luo\\nPengcheng Guo\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.04702\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 26 Sep 2021 14:30:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages,4 figures\\u00a7r"}']}
{title:'Tang et al. (§72021§r)', author: 'Jiyang Tang; Ming Li', display:{Lore:['[{"text": "arXiv:2104.05657", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Mandarin Tone Classification with Short Term Context Information\\u00a7r\\n\\n\\u00a78\\u00a7oJiyang Tang\\nMing Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.05657\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 17 Dec 2021 14:08:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by APSIPA ASC 2021\\u00a7r"}']}
{title:'Gao et al. (§72021§r)', author: 'Zhifu Gao; Yiwu Yao; Shiliang Zhang; Jun Yang; Ming Lei; Ian McLoughlin', display:{Lore:['[{"text": "arXiv:2104.05784", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExtremely Low Footprint End-to-End ASR System for Smart Device\\u00a7r\\n\\n\\u00a78\\u00a7oZhifu Gao\\nYiwu Yao\\nShiliang Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.05784\\u00a7r\\n\\nVersion:\\u00a77v5 (Wed, 7 Jul 2021 03:26:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, accepted by INTERSPEECH 2021\\u00a7r"}']}
{title:'Zhou et al. (§72021§r)', author: 'Ziang Zhou; Yanze Xu; Ming Li', display:{Lore:['[{"text": "arXiv:2104.06004", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetecting Escalation Level from Speech with Transfer Learning and Acoustic-Lexical Information Fusion\\u00a7r\\n\\n\\u00a78\\u00a7oZiang Zhou\\nYanze Xu\\nMing Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.06004\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 4 Nov 2021 03:01:46 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Shijun Wang; Damian Borth', display:{Lore:['[{"text": "arXiv:2104.06074", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoiseVC: Towards High Quality Zero-Shot Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oShijun Wang\\nDamian Borth\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.06074\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Apr 2021 10:12:38 GMT)\\u00a7r"}']}
{title:'Xu et al. (§72021§r)', author: 'Xudong Xu; Hang Zhou; Ziwei Liu; Bo Dai; Xiaogang Wang; Dahua Lin', display:{Lore:['[{"text": "arXiv:2104.06162", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisually Informed Binaural Audio Generation without Binaural Audios\\u00a7r\\n\\n\\u00a78\\u00a7oXudong Xu\\nHang Zhou\\nZiwei Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.06162\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Apr 2021 13:07:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by CVPR 2021. Code, models, and demo video are available on our webpage: https://sheldontsui.github.io/projects/PseudoBinaural>\\u00a7r"}']}
{title:'Koh et al. (§72021§r)', author: 'Eunjeong Koh; Shlomo Dubnov', display:{Lore:['[{"text": "arXiv:2104.06517", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparison and Analysis of Deep Audio Embeddings for Music Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oEunjeong Koh\\nShlomo Dubnov\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.06517\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nAAAI 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Apr 2021 21:09:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAAAI Workshop on Affective Content Analysis 2021 Camera Ready Version\\u00a7r"}']}
{title:'Cheuk et al. (§72021§r)', author: 'Kin Wai Cheuk; Yin-Jyun Luo; Emmanouil Benetos; Dorien Herremans', display:{Lore:['[{"text": "arXiv:2104.06607", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRevisiting the Onsets and Frames Model with Additive Attention\\u00a7r\\n\\n\\u00a78\\u00a7oKin Wai Cheuk\\nYin-Jyun Luo\\nEmmanouil Benetos\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.06607\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Apr 2021 03:14:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in IJCNN 2021 Special Session S04. https://dr-costas.github.io/rlasmp2021-website/\\u00a7r"}']}
{title:'Peter et al. (§72021§r)', author: 'David Peter; Wolfgang Roth; Franz Pernkopf', display:{Lore:['[{"text": "arXiv:2104.06666", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end Keyword Spotting using Neural Architecture Search and Quantization\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Peter\\nWolfgang Roth\\nFranz Pernkopf\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.06666\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Apr 2021 07:22:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2012.10138\\u00a7r"}']}
{title:'Hayashi et al. (§72021§r)', author: 'Tomoki Hayashi; Wen-Chin Huang; Kazuhiro Kobayashi; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2104.06793", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-autoregressive sequence-to-sequence voice conversion\\u00a7r\\n\\n\\u00a78\\u00a7oTomoki Hayashi\\nWen-Chin Huang\\nKazuhiro Kobayashi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.06793\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Apr 2021 11:53:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP2021. Demo HP: https://kan-bayashi.github.io/NonARSeq2SeqVC/\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Shengqiang Li; Menglong Xu; Xiao-Lei Zhang', display:{Lore:['[{"text": "arXiv:2104.06865", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient conformer-based speech recognition with linear attention\\u00a7r\\n\\n\\u00a78\\u00a7oShengqiang Li\\nMenglong Xu\\nXiao-Lei Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.06865\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 23 Jul 2021 13:02:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to APSIPA ASC 2021\\u00a7r"}']}
{title:'Kameoka et al. (§72021§r)', author: 'Hirokazu Kameoka; Kou Tanaka; Takuhiro Kaneko', display:{Lore:['[{"text": "arXiv:2104.06900", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFastS2S-VC: Streaming Non-Autoregressive Sequence-to-Sequence Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oHirokazu Kameoka\\nKou Tanaka\\nTakuhiro Kaneko\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.06900\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Apr 2021 14:48:22 GMT)\\u00a7r"}']}
{title:'Narayanaswamy et al. (§72021§r)', author: 'Vivek Sivaraman Narayanaswamy; Jayaraman J. Thiagarajan; Andreas Spanias', display:{Lore:['[{"text": "arXiv:2104.07161", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Design of Deep Priors for Unsupervised Audio Restoration\\u00a7r\\n\\n\\u00a78\\u00a7oVivek Sivaraman Narayanaswamy\\nJayaraman J. Thiagarajan\\nAndreas Spanias\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.07161\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Apr 2021 23:16:25 GMT)\\u00a7r"}']}
{title:'Ma et al. (§72021§r)', author: 'Haoxin Ma; Jiangyan Yi; Jianhua Tao; Ye Bai; Zhengkun Tian; Chenglong Wang', display:{Lore:['[{"text": "arXiv:2104.07286", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContinual Learning for Fake Audio Detection\\u00a7r\\n\\n\\u00a78\\u00a7oHaoxin Ma\\nJiangyan Yi\\nJianhua Tao\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.07286\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-794\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2021, 886-890\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Apr 2021 07:57:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, conference\\u00a7r"}']}
{title:'Hou et al. (§72021§r)', author: 'Wenxin Hou; Jindong Wang; Xu Tan; Tao Qin; Takahiro Shinozaki', display:{Lore:['[{"text": "arXiv:2104.07491", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-domain Speech Recognition with Unsupervised Character-level Distribution Matching\\u00a7r\\n\\n\\u00a78\\u00a7oWenxin Hou\\nJindong Wang\\nXu Tan\\nTao Qin\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.07491\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 9 Jun 2021 01:43:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2021; code available at https://github.com/jindongwang/transferlearning/tree/master/code/ASR/CMatch\\u00a7r"}']}
{title:'Bazin et al. (§72021§r)', author: 'Théis Bazin; Gaëtan Hadjeres; Philippe Esling; Mikhail Malt', display:{Lore:['[{"text": "arXiv:2104.07519", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpectrogram Inpainting for Interactive Generation of Instrument Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oTh\\u00e9is Bazin\\nGa\\u00ebtan Hadjeres\\nPhilippe Esling\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.07519\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.30746/978-91-519-5560-5\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 1st Joint Conference on AI Music Creativity,\\n  2020 (p. 10). Stockholm, Sweden: AIMC\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Apr 2021 15:17:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages + references + appendices. 4 figures. Published as a conference paper at the The 2020 Joint Conference on AI MusicCreativity, October 19-23, 2020, organized and hosted virtually by the Royal Institute of "}','{"text": "Technology (KTH), Stockholm, Sweden\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Xiyun Li; Yong Xu; Meng Yu; Shi-Xiong Zhang; Jiaming Xu; Bo Xu; Dong Yu', display:{Lore:['[{"text": "arXiv:2104.08450", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMIMO Self-attentive RNN Beamformer for Multi-speaker Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oXiyun Li\\nYong Xu\\nMeng Yu\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.08450\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 26 Apr 2021 08:18:36 GMT)\\u00a7r"}']}
{title:'Marmoret et al. (§72021§r)', author: 'Axel Marmoret; Jérémy E. Cohen; Nancy Bertin; Frédéric Bimbot', display:{Lore:['[{"text": "arXiv:2104.08580", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUncovering audio patterns in music with Nonnegative Tucker Decomposition for structural segmentation\\u00a7r\\n\\n\\u00a78\\u00a7oAxel Marmoret\\nJ\\u00e9r\\u00e9my E. Cohen\\nNancy Bertin\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.08580\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n21st International Society for Music Information Retrieval\\n  Conference (ISMIR), Montr\\\\\'eal, Canada, 2020, 788-794\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 17 Apr 2021 15:48:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 6 figures; Code and experiments detailsavailable at https://gitlab.inria.fr/amarmore/musicntd/-/tree/0.1.0; Experiments details available at https://ax-le.github.io/resources/ISMIR2020/Notebooks_mainpage.html\\u00a7r"}']}
{title:'Andreas et al. (§72021§r)', author: 'Jacob Andreas; Gašper Beguš; Michael M. Bronstein; Roee Diamant; Denley Delaney; Shane Gero; Shafi Goldwasser; David F. Gruber; Sarah de Haas; Peter Malkin; Roger Payne; Giovanni Petri; Daniela Rus; Pratyusha Sharma; Dan Tchernov; Pernille Tønnesen; Antonio Torralba; Daniel Vogt; Robert J. Wood', display:{Lore:['[{"text": "arXiv:2104.08614", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCetacean Translation Initiative: a roadmap to deciphering the communication of sperm whales\\u00a7r\\n\\n\\u00a78\\u00a7oJacob Andreas\\nGa\\u0161per Begu\\u0161\\nMichael M. Bronstein\\n+ 15 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.08614\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 17 Apr 2021 18:39:22 GMT)\\u00a7r"}']}
{title:'Morikawa (§72021§r)', author: 'Masahiro Morikawa', display:{Lore:['[{"text": "arXiv:2104.08872", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.gen-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Frequency Characterization of Music Sounds \\u2013 Ultra-Bass Richness from the Sound Wave Beats\\u00a7r\\n\\n\\u00a78\\u00a7oMasahiro Morikawa\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.08872\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 18 Apr 2021 14:54:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o23 pages, 7 figures\\u00a7r"}']}
{title:'Dovrat et al. (§72021§r)', author: 'Shaked Dovrat; Eliya Nachmani; Lior Wolf', display:{Lore:['[{"text": "arXiv:2104.08955", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMany-Speakers Single Channel Speech Separation with Optimal Permutation Training\\u00a7r\\n\\n\\u00a78\\u00a7oShaked Dovrat\\nEliya Nachmani\\nLior Wolf\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.08955\\u00a7r\\n\\nVersion:\\u00a77v4 (Sun, 7 Nov 2021 07:06:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021, Datacreation link added\\u00a7r"}']}
{title:'Lerch et al. (§72021§r)', author: 'Alexander Lerch; Claire Arthur; Ashis Pati; Siddharth Gururani', display:{Lore:['[{"text": "arXiv:2104.09018", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.DL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Interdisciplinary Review of Music Performance Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oAlexander Lerch\\nClaire Arthur\\nAshis Pati\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.09018\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5334/tismir.53\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nTransactions of the International Society for Music Information\\n  Retrieval, 3(1), pp.221-245, 2020\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Apr 2021 02:21:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:1907.00178\\u00a7r"}']}
{title:'Yan et al. (§72021§r)', author: 'Yuzi Yan; Xu Tan; Bohan Li; Tao Qin; Sheng Zhao; Yuan Shen; Tie-Yan Liu', display:{Lore:['[{"text": "arXiv:2104.09715", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaSpeech 2: Adaptive Text to Speech with Untranscribed Data\\u00a7r\\n\\n\\u00a78\\u00a7oYuzi Yan\\nXu Tan\\nBohan Li\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.09715\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Apr 2021 01:53:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2021\\u00a7r"}']}
{title:'Rao et al. (§72021§r)', author: 'Adrit Rao; Kevin Battenfield; Oliver Aalami', display:{Lore:['[{"text": "arXiv:2104.09748", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWaveform Phasicity Prediction from Arterial Sounds through Spectrogram Analysis using Convolutional Neural Networks for Limb Perfusion Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oAdrit Rao\\nKevin Battenfield\\nOliver Aalami\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.09748\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 15 Jun 2021 22:34:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 8 figures\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Tianyun Liu; Diqun Yan', display:{Lore:['[{"text": "arXiv:2104.09832", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIdentification of fake stereo audio\\u00a7r\\n\\n\\u00a78\\u00a7oTianyun Liu\\nDiqun Yan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.09832\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Apr 2021 08:35:38 GMT)\\u00a7r"}']}
{title:'Montesinos et al. (§72021§r)', author: 'Juan F. Montesinos; Venkatesh S. Kadandale; Gloria Haro', display:{Lore:['[{"text": "arXiv:2104.09946", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA cappella: Audio-visual Singing Voice Separation\\u00a7r\\n\\n\\u00a78\\u00a7oJuan F. Montesinos\\nVenkatesh S. Kadandale\\nGloria Haro\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.09946\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 18 Oct 2021 18:54:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper accepted at The 32nd British Machine Vision Conference, BMVC 2021\\u00a7r"}']}
{title:'Mu et al. (§72021§r)', author: 'Zhaoxi Mu; Xinyu Yang; Yizhuo Dong', display:{Lore:['[{"text": "arXiv:2104.09995", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReview of end-to-end speech synthesis technology based on deep learning\\u00a7r\\n\\n\\u00a78\\u00a7oZhaoxi Mu\\nXinyu Yang\\nYizhuo Dong\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.09995\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Apr 2021 14:24:05 GMT)\\u00a7r"}']}
{title:'Amiriparian et al. (§72021§r)', author: 'Shahin Amiriparian; Artem Sokolov; Ilhan Aslan; Lukas Christ; Maurice Gerczuk; Tobias Hübner; Dmitry Lamanov; Manuel Milling; Sandra Ottl; Ilya Poduremennykh; Evgeniy Shuranov; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2104.10121", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Impact of Word Error Rate on Acoustic-Linguistic Speech Emotion Recognition: An Update for the Deep Learning Era\\u00a7r\\n\\n\\u00a78\\u00a7oShahin Amiriparian\\nArtem Sokolov\\nIlhan Aslan\\n+ 8 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.10121\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Apr 2021 17:10:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure\\u00a7r"}']}
{title:'Lee et al. (§72021§r)', author: 'Jaejun Lee; Donmoon Lee; Hyeong-Seok Choi; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2104.10431", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRoom adaptive conditioning method for sound event classification in reverberant environments\\u00a7r\\n\\n\\u00a78\\u00a7oJaejun Lee\\nDonmoon Lee\\nHyeong-Seok Choi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.10431\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Apr 2021 09:42:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, In Proceedings of the 2021 IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP)\\u00a7r"}']}
{title:'Stoidis et al. (§72021§r)', author: 'Dimitrios Stoidis; Andrea Cavallaro', display:{Lore:['[{"text": "arXiv:2104.11051", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProtecting gender and identity with disentangled speech representations\\u00a7r\\n\\n\\u00a78\\u00a7oDimitrios Stoidis\\nAndrea Cavallaro\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.11051\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 16 Jun 2021 19:11:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Jianwei Zhang; Suren Jayasuriya; Visar Berisha', display:{Lore:['[{"text": "arXiv:2104.11347", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRestoring degraded speech via a modified diffusion model\\u00a7r\\n\\n\\u00a78\\u00a7oJianwei Zhang\\nSuren Jayasuriya\\nVisar Berisha\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.11347\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-1889\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2021, 221-225, 2021)\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 2 Sep 2021 16:56:20 GMT)\\u00a7r"}']}
{title:'Ji et al. (§72021§r)', author: 'Chunyan Ji; Yi Pan', display:{Lore:['[{"text": "arXiv:2104.11395", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInfant Vocal Tract Development Analysis and Diagnosis by Cry Signals with CNN Age Classification\\u00a7r\\n\\n\\u00a78\\u00a7oChunyan Ji\\nYi Pan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.11395\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Apr 2021 03:09:16 GMT)\\u00a7r"}']}
{title:'Tóth et al. (§72021§r)', author: 'László Tóth; Amin Honarmandi Shandiz', display:{Lore:['[{"text": "arXiv:2104.11532", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l3D Convolutional Neural Networks for Ultrasound-Based Silent Speech Interfaces\\u00a7r\\n\\n\\u00a78\\u00a7oL\\u00e1szl\\u00f3 T\\u00f3th\\nAmin Honarmandi Shandiz\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.11532\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-030-61401-0_16\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Apr 2021 10:56:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 2 tables , 3 figures\\u00a7r"}']}
{title:'Guzhov et al. (§72021§r)', author: 'Andrey Guzhov; Federico Raue; Jörn Hees; Andreas Dengel', display:{Lore:['[{"text": "arXiv:2104.11587", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lESResNe(X)t-fbsp: Learning Robust Time-Frequency Transformation of Audio\\u00a7r\\n\\n\\u00a78\\u00a7oAndrey Guzhov\\nFederico Raue\\nJ\\u00f6rn Hees\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.11587\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Apr 2021 13:39:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted IJCNN 2021\\u00a7r"}']}
{title:'Yu et al. (§72021§r)', author: 'Yide Yu; Amin Honarmandi Shandiz; László Tóth', display:{Lore:['[{"text": "arXiv:2104.11598", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReconstructing Speech from Real-Time Articulatory MRI Using Neural Vocoders\\u00a7r\\n\\n\\u00a78\\u00a7oYide Yu\\nAmin Honarmandi Shandiz\\nL\\u00e1szl\\u00f3 T\\u00f3th\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.11598\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Apr 2021 13:46:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages. 4 tables, 3 figures\\u00a7r"}']}
{title:'Shandiz et al. (§72021§r)', author: 'Amin Honarmandi Shandiz; László Tóth; Gábor Gosztolya; Alexandra Markó; Tamás Gábor Csapó', display:{Lore:['[{"text": "arXiv:2104.11601", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Neural Silent Speech Interface Models by Adversarial Training\\u00a7r\\n\\n\\u00a78\\u00a7oAmin Honarmandi Shandiz\\nL\\u00e1szl\\u00f3 T\\u00f3th\\nG\\u00e1bor Gosztolya\\nAlexandra Mark\\u00f3\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.11601\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-030-76346-6_39\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Apr 2021 13:48:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 3 tables, 2 figures\\u00a7r"}']}
{title:'Amiriparian et al. (§72021§r)', author: 'Shahin Amiriparian; Tobias Hübner; Maurice Gerczuk; Sandra Ottl; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2104.11629", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeepSpectrumLite: A Power-Efficient Transfer Learning Framework for Embedded Speech and Audio Processing from Decentralised Data\\u00a7r\\n\\n\\u00a78\\u00a7oShahin Amiriparian\\nTobias H\\u00fcbner\\nMaurice Gerczuk\\nSandra Ottl\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.11629\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Apr 2021 14:32:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure\\u00a7r"}']}
{title:'Mittag et al. (§72021§r)', author: 'Gabriel Mittag; Sebastian Möller', display:{Lore:['[{"text": "arXiv:2104.11673", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning Based Assessment of Synthetic Speech Naturalness\\u00a7r\\n\\n\\u00a78\\u00a7oGabriel Mittag\\nSebastian M\\u00f6ller\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.11673\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2020-2382\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Apr 2021 16:05:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oLate upload, presented at Interspeech 2020\\u00a7r"}']}
{title:'Gaido et al. (§72021§r)', author: 'Marco Gaido; Matteo Negri; Mauro Cettolo; Marco Turchi', display:{Lore:['[{"text": "arXiv:2104.11710", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBeyond Voice Activity Detection: Hybrid Audio Segmentation for Direct Speech Translation\\u00a7r\\n\\n\\u00a78\\u00a7oMarco Gaido\\nMatteo Negri\\nMauro Cettolo\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.11710\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Oct 2021 12:18:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICNLSP 2021\\u00a7r"}']}
{title:'HekmatiAthar et al. (§72021§r)', author: 'SeyyedPooya HekmatiAthar; Mohd Anwar', display:{Lore:['[{"text": "arXiv:2104.11880", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Embedding: A Tool for Incorporating Music Theory into Computational Music Applications\\u00a7r\\n\\n\\u00a78\\u00a7oSeyyedPooya HekmatiAthar\\nMohd Anwar\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.11880\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 24 Apr 2021 04:32:45 GMT)\\u00a7r"}']}
{title:'Manco et al. (§72021§r)', author: 'Ilaria Manco; Emmanouil Benetos; Elio Quinton; Gyorgy Fazekas', display:{Lore:['[{"text": "arXiv:2104.11984", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusCaps: Generating Captions for Music Audio\\u00a7r\\n\\n\\u00a78\\u00a7oIlaria Manco\\nEmmanouil Benetos\\nElio Quinton\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.11984\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/IJCNN52387.2021.9533461\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 24 Apr 2021 16:34:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IJCNN 2021 for the Special Session on Representation Learning for Audio, Speech, and Music Processing\\u00a7r"}']}
{title:'Dhar et al. (§72021§r)', author: 'Sandipan Dhar; Nanda Dulal Jana; Swagatam Das', display:{Lore:['[{"text": "arXiv:2104.12159", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Adaptive Learning based Generative Adversarial Network for One-To-One Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oSandipan Dhar\\nNanda Dulal Jana\\nSwagatam Das\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.12159\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 25 Apr 2021 13:44:32 GMT)\\u00a7r"}']}
{title:'Gu et al. (§72021§r)', author: 'Rongzhi Gu; Shi-Xiong Zhang; Yuexian Zou; Dong Yu', display:{Lore:['[{"text": "arXiv:2104.12359", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComplex Neural Spatial Filter: Enhancing Multi-channel Target Speech Separation in Complex Domain\\u00a7r\\n\\n\\u00a78\\u00a7oRongzhi Gu\\nShi-Xiong Zhang\\nYuexian Zou\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.12359\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2021.3076374\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Apr 2021 06:04:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Giraudo (§72021§r)', author: 'Samuele Giraudo', display:{Lore:['[{"text": "arXiv:2104.12432", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a72math.CO\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeneration of musical patterns through operads\\u00a7r\\n\\n\\u00a78\\u00a7oSamuele Giraudo\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.12432\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJourn\\\\\'ees d\'informatique musicale, 2020\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Apr 2021 09:45:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages\\u00a7r"}']}
{title:'Elizalde et al. (§72021§r)', author: 'Benjamin Elizalde; Radu Revutchi; Samarjit Das; Bhiksha Raj; Ian Lane; Laurie M. Heller', display:{Lore:['[{"text": "arXiv:2104.12693", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIdentifying Actions for Sound Event Classification\\u00a7r\\n\\n\\u00a78\\u00a7oBenjamin Elizalde\\nRadu Revutchi\\nSamarjit Das\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.12693\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 5 Aug 2021 20:45:56 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Luyu Wang; Pauline Luc; Adria Recasens; Jean-Baptiste Alayrac; Aaron van den Oord', display:{Lore:['[{"text": "arXiv:2104.12807", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultimodal Self-Supervised Learning of General Audio Representations\\u00a7r\\n\\n\\u00a78\\u00a7oLuyu Wang\\nPauline Luc\\nAdria Recasens\\nJean-Baptiste Alayrac\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.12807\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 28 Apr 2021 15:59:38 GMT)\\u00a7r"}']}
{title:'Turian et al. (§72021§r)', author: 'Joseph Turian; Jordie Shier; George Tzanetakis; Kirk McNally; Max Henry', display:{Lore:['[{"text": "arXiv:2104.12922", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOne Billion Audio Sounds from GPU-enabled Modular Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oJoseph Turian\\nJordie Shier\\nGeorge Tzanetakis\\nKirk McNally\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.12922\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 20 Jul 2021 17:46:41 GMT)\\u00a7r"}']}
{title:'Makris et al. (§72021§r)', author: 'Dimos Makris; Kat R. Agres; Dorien Herremans', display:{Lore:['[{"text": "arXiv:2104.13056", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerating Lead Sheets with Affect: A Novel Conditional seq2seq Framework\\u00a7r\\n\\n\\u00a78\\u00a7oDimos Makris\\nKat R. Agres\\nDorien Herremans\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.13056\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Apr 2021 09:04:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for theInternational Joint Conference on Neural Networks (IJCNN), Shenzhen, China, 18-22 July 2021 (virtual)\\u00a7r"}']}
{title:'Calegario et al. (§72021§r)', author: 'Filipe Calegario; João Tragtenberg; Giordano Cabral; Geber Ramalho', display:{Lore:['[{"text": "arXiv:2104.13266", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBatebit Controller: Popularizing Digital Musical Instruments Development Process\\u00a7r\\n\\n\\u00a78\\u00a7oFilipe Calegario\\nJo\\u00e3o Tragtenberg\\nGiordano Cabral\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.13266\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5753/sbcm.2019.10453\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Apr 2021 15:34:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2 pages, 2 figures, 17th Brazilian Symposium on Computer Music\\u00a7r"}']}
{title:'Meseguer-Brocal (§72021§r)', author: 'Gabriel Meseguer-Brocal', display:{Lore:['[{"text": "arXiv:2104.13276", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.DB\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMULTIMODAL ANALYSIS: Informed content estimation and audio source separation\\u00a7r\\n\\n\\u00a78\\u00a7oGabriel Meseguer-Brocal\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.13276\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 29 Oct 2021 16:31:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPh.D. dissertation. Thesis supervisor: Geoffroy Peeters. Jury:Laurent Girin, Ga\\u00ebl Richard, Rachel Bittner, Elena Cabrio, Bruno Gas, Perfecto Herrera Boyer, Antoine Liutkus\\u00a7r"}']}
{title:'Fenu et al. (§72021§r)', author: 'Gianni Fenu; Giacomo Medda; Mirko Marras; Giacomo Meloni', display:{Lore:['[{"text": "arXiv:2104.14067", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Fairness in Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGianni Fenu\\nGiacomo Medda\\nMirko Marras\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.14067\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3393822.3432325\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 30 Apr 2021 20:36:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the 2020 European Symposium on Software Engineering (ESSE 2020)\\u00a7r"}']}
{title:'Gao et al. (§72021§r)', author: 'Yan Gao; Titouan Parcollet; Salah Zaiem; Javier Fernandez-Marques; Pedro P. B. de Gusmao; Daniel J. Beutel; Nicholas D. Lane', display:{Lore:['[{"text": "arXiv:2104.14297", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Speech Recognition from Federated Acoustic Models\\u00a7r\\n\\n\\u00a78\\u00a7oYan Gao\\nTitouan Parcollet\\nSalah Zaiem\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.14297\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 9 Jul 2021 14:41:12 GMT)\\u00a7r"}']}
{title:'Kouni et al. (§72021§r)', author: 'Vasiliki Kouni; Holger Rauhut; Theoharis Theoharis', display:{Lore:['[{"text": "arXiv:2104.14468", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IT\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a72math.IT\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStar DGT: a Robust Gabor Transform for Speech Denoising\\u00a7r\\n\\n\\u00a78\\u00a7oVasiliki Kouni\\nHolger Rauhut\\nTheoharis Theoharis\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.14468\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s43670-023-00053-x\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 27 Dec 2021 20:46:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2103.11233\\u00a7r"}']}
{title:'Szelogowski (§72021§r)', author: 'Daniel Szelogowski', display:{Lore:['[{"text": "arXiv:2105.00173", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CY\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotion Recognition of the Singing Voice: Toward a Real-Time Analysis Tool for Singers\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Szelogowski\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.00173\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 4 Jul 2021 07:34:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o26 pages, 10 figures, 6 tables\\u00a7r"}']}
{title:'Verma et al. (§72021§r)', author: 'Prateek Verma; Jonathan Berger', display:{Lore:['[{"text": "arXiv:2105.00335", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Transformers:Transformer Architectures For Large Scale Audio Understanding. Adieu Convolutions\\u00a7r\\n\\n\\u00a78\\u00a7oPrateek Verma\\nJonathan Berger\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.00335\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 1 May 2021 19:38:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures; Underreview WASPAA 2021\\u00a7r"}']}
{title:'Hu et al. (§72021§r)', author: 'Shell Xu Hu; Md Rifat Arefin; Viet-Nhat Nguyen; Alish Dipani; Xaq Pitkow; Andreas Savas Tolias', display:{Lore:['[{"text": "arXiv:2105.00609", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAvaTr: One-Shot Speaker Extraction with Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oShell Xu Hu\\nMd Rifat Arefin\\nViet-Nhat Nguyen\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.00609\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 3 May 2021 02:43:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 4 main figures, 2 supplemental figures\\u00a7r"}']}
{title:'Lin et al. (§72021§r)', author: 'Yan-Bo Lin; Yu-Chiang Frank Wang', display:{Lore:['[{"text": "arXiv:2105.00708", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting Audio-Visual Consistency with Partial Supervision for Spatial Audio Generation\\u00a7r\\n\\n\\u00a78\\u00a7oYan-Bo Lin\\nYu-Chiang Frank Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.00708\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 3 May 2021 09:34:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAAAI\'21\\u00a7r"}']}
{title:'Mahanta et al. (§72021§r)', author: 'Saranga Kingkor Mahanta; Abdullah Faiz Ur Rahman Khilji; Partha Pakray', display:{Lore:['[{"text": "arXiv:2105.00933", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Neural Network for Musical Instrument Recognition using MFCCs\\u00a7r\\n\\n\\u00a78\\u00a7oSaranga Kingkor Mahanta\\nAbdullah Faiz Ur Rahman Khilji\\nPartha Pakray\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.00933\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nComputacion y Sistemas, Vol 25, No 2 (2021): 25(2) 2021\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 5 May 2021 13:32:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWas suggested to upload ona later date\\u00a7r"}']}
{title:'Kim et al. (§72021§r)', author: 'Chanwoo Kim; Abhinav Garg; Dhananjaya Gowda; Seongkyu Mun; Changwoo Han', display:{Lore:['[{"text": "arXiv:2105.01254", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreaming end-to-end speech recognition with jointly trained neural feature enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oChanwoo Kim\\nAbhinav Garg\\nDhananjaya Gowda\\nSeongkyu Mun\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.01254\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 4 May 2021 02:25:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Nistal et al. (§72021§r)', author: 'Javier Nistal; Cyran Aouameur; Stefan Lattner; Gaël Richard', display:{Lore:['[{"text": "arXiv:2105.01531", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVQCPC-GAN: Variable-Length Adversarial Audio Synthesis Using Vector-Quantized Contrastive Predictive Coding\\u00a7r\\n\\n\\u00a78\\u00a7oJavier Nistal\\nCyran Aouameur\\nStefan Lattner\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.01531\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Workshop on Applications of Signal Processing to Audio and\\n  Acoustics (WASPAA), 2021\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 30 Jul 2021 10:06:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, 1 table; accepted to IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)\\u00a7r"}']}
{title:'Imoto (§72021§r)', author: 'Keisuke Imoto', display:{Lore:['[{"text": "arXiv:2105.01836", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic Scene Classification Using Multichannel Observation with Partially Missing Channels\\u00a7r\\n\\n\\u00a78\\u00a7oKeisuke Imoto\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.01836\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 5 May 2021 02:21:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to EUSIPCO2021\\u00a7r"}']}
{title:'Grumiaux et al. (§72021§r)', author: 'Pierre-Amaury Grumiaux; Srdan Kitic; Laurent Girin; Alexandre Guérin', display:{Lore:['[{"text": "arXiv:2105.01897", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved feature extraction for CRNN-based multiple sound source localization\\u00a7r\\n\\n\\u00a78\\u00a7oPierre-Amaury Grumiaux\\nSrdan Kitic\\nLaurent Girin\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.01897\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 5 May 2021 07:12:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures. Accepted to EUSIPCO 2021\\u00a7r"}']}
{title:'Maiti et al. (§72021§r)', author: 'Soumi Maiti; Hakan Erdogan; Kevin Wilson; Scott Wisdom; Shinji Watanabe; John R. Hershey', display:{Lore:['[{"text": "arXiv:2105.02096", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Diarization for Variable Number of Speakers with Local-Global Networks and Discriminative Speaker Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oSoumi Maiti\\nHakan Erdogan\\nKevin Wilson\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.02096\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2021, SPE-54.1\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 5 May 2021 14:55:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, ICASSP 2021\\u00a7r"}']}
{title:'Fonseca et al. (§72021§r)', author: 'Eduardo Fonseca; Aren Jansen; Daniel P. W. Ellis; Scott Wisdom; Marco Tagliasacchi; John R. Hershey; Manoj Plakal; Shawn Hershey; R. Channing Moore; Xavier Serra', display:{Lore:['[{"text": "arXiv:2105.02132", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Learning from Automatically Separated Sound Scenes\\u00a7r\\n\\n\\u00a78\\u00a7oEduardo Fonseca\\nAren Jansen\\nDaniel P. W. Ellis\\n+ 6 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.02132\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 15 Sep 2021 01:17:15 GMT)\\u00a7r"}']}
{title:'Cooper et al. (§72021§r)', author: 'Erica Cooper; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2105.02373", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHow do Voices from Past Speech Synthesis Challenges Compare Today?\\u00a7r\\n\\n\\u00a78\\u00a7oErica Cooper\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.02373\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 30 Jun 2021 05:45:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at ISCA Speech Synthesis Workshop 2021\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Kanghao Zhang; Shulin He; Hao Li; Xueliang Zhang', display:{Lore:['[{"text": "arXiv:2105.02436", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDBNet: A Dual-branch Network Architecture Processing on Spectrum and Waveform for Single-channel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oKanghao Zhang\\nShulin He\\nHao Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.02436\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 6 May 2021 04:31:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Kondo et al. (§72021§r)', author: 'Yuto Kondo; Yuki Kubo; Norihiro Takamune; Daichi Kitamura; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2105.02491", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeficient Basis Estimation of Noise Spatial Covariance Matrix for Rank-Constrained Spatial Covariance Matrix Estimation Method in Blind Speech Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oYuto Kondo\\nYuki Kubo\\nNorihiro Takamune\\nDaichi Kitamura\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.02491\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 6 May 2021 07:44:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, ICASSP2021\\u00a7r"}']}
{title:'Ke et al. (§72021§r)', author: 'Dengfeng Ke; Jinsong Zhang; Yanlu Xie; Yanyan Xu; Binghuai Lin', display:{Lore:['[{"text": "arXiv:2105.02509", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Enhancement using Separable Polling Attention and Global Layer Normalization followed with PReLU\\u00a7r\\n\\n\\u00a78\\u00a7oDengfeng Ke\\nJinsong Zhang\\nYanlu Xie\\nYanyan Xu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.02509\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 6 May 2021 08:18:02 GMT)\\u00a7r"}']}
{title:'Tanabe et al. (§72021§r)', author: 'Ryo Tanabe; Harsh Purohit; Kota Dohi; Takashi Endo; Yuki Nikaido; Toshiki Nakamura; Yohei Kawaguchi', display:{Lore:['[{"text": "arXiv:2105.02702", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMIMII DUE: Sound Dataset for Malfunctioning Industrial Machine Investigation and Inspection with Domain Shifts due to Changes in Operational and Environmental Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oRyo Tanabe\\nHarsh Purohit\\nKota Dohi\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.02702\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 27 Sep 2021 15:36:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEWASPAA 2021\\u00a7r"}']}
{title:'You et al. (§72021§r)', author: 'Zhao You; Shulin Feng; Dan Su; Dong Yu', display:{Lore:['[{"text": "arXiv:2105.03036", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeechMoE: Scaling to Large Acoustic Models with Dynamic Routing Mixture of Experts\\u00a7r\\n\\n\\u00a78\\u00a7oZhao You\\nShulin Feng\\nDan Su\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.03036\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 7 May 2021 02:38:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures. Submitted to Interspeech 2021\\u00a7r"}']}
{title:'Dinkel et al. (§72021§r)', author: 'Heinrich Dinkel; Shuai Wang; Xuenan Xu; Mengyue Wu; Kai Yu', display:{Lore:['[{"text": "arXiv:2105.04065", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice activity detection in the wild: A data-driven approach using teacher-student training\\u00a7r\\n\\n\\u00a78\\u00a7oHeinrich Dinkel\\nShuai Wang\\nXuenan Xu\\nMengyue Wu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.04065\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3073596\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol. 29, pp. 1542-1555, 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 May 2021 01:26:03 GMT)\\u00a7r"}']}
{title:'Saito et al. (§72021§r)', author: 'Koichi Saito; Tomohiko Nakamura; Kohei Yatabe; Yuma Koizumi; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2105.04079", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSampling-Frequency-Independent Audio Source Separation Using Convolution Layer Based on Impulse Invariant Method\\u00a7r\\n\\n\\u00a78\\u00a7oKoichi Saito\\nTomohiko Nakamura\\nKohei Yatabe\\nYuma Koizumi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.04079\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 May 2021 02:33:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted for EuropeanSignal Processing Conference 2021 (EUSIPCO 2021)\\u00a7r"}']}
{title:'Chen et al. (§72021§r)', author: 'Jinyin Chen; Linhui Ye; Zhaoyan Ming', display:{Lore:['[{"text": "arXiv:2105.04124", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMASS: Multi-task Anthropomorphic Speech Synthesis Framework\\u00a7r\\n\\n\\u00a78\\u00a7oJinyin Chen\\nLinhui Ye\\nZhaoyan Ming\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.04124\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 May 2021 05:54:08 GMT)\\u00a7r"}']}
{title:'Henkel et al. (§72021§r)', author: 'Florian Henkel; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2105.04309", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-modal Conditional Bounding Box Regression for Music Score Following\\u00a7r\\n\\n\\u00a78\\u00a7oFlorian Henkel\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.04309\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 May 2021 12:43:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in the Proceedings of the 29th European Signal Processing Conference (EUSIPCO), Dublin, Ireland, 2021\\u00a7r"}']}
{title:'Kumar et al. (§72021§r)', author: 'Shakti Kumar; Jithin Pradeep; Hussain Zaidi', display:{Lore:['[{"text": "arXiv:2105.04458", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Robust Latent Representations for Controllable Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oShakti Kumar\\nJithin Pradeep\\nHussain Zaidi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.04458\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 May 2021 15:49:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ACL2021 Findings\\u00a7r"}']}
{title:'Giannakopoulos et al. (§72021§r)', author: 'Petros Giannakopoulos; Aggelos Pikrakis; Yannis Cotronis', display:{Lore:['[{"text": "arXiv:2105.04488", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Deep Reinforcement Learning Approach to Audio-Based Navigation in a Multi-Speaker Environment\\u00a7r\\n\\n\\u00a78\\u00a7oPetros Giannakopoulos\\nAggelos Pikrakis\\nYannis Cotronis\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.04488\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9415013\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 May 2021 16:26:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in ICASSP 2021\\u00a7r"}']}
{title:'Dai et al. (§72021§r)', author: 'Shuqi Dai; Xichu Ma; Ye Wang; Roger B. Dannenberg', display:{Lore:['[{"text": "arXiv:2105.04709", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPersonalized Popular Music Generation Using Imitation and Structure\\u00a7r\\n\\n\\u00a78\\u00a7oShuqi Dai\\nXichu Ma\\nYe Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.04709\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 May 2021 23:43:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o26 pages, 12 figures\\u00a7r"}']}
{title:'Tzinis et al. (§72021§r)', author: 'Efthymios Tzinis; Jonah Casebeer; Zhepei Wang; Paris Smaragdis', display:{Lore:['[{"text": "arXiv:2105.04727", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSeparate but Together: Unsupervised Federated Learning for Speech Enhancement from Non-IID Data\\u00a7r\\n\\n\\u00a78\\u00a7oEfthymios Tzinis\\nJonah Casebeer\\nZhepei Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.04727\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/WASPAA52581.2021.9632783\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2021 IEEE Workshop on Applications of Signal Processing to Audio\\n  and Acoustics (WASPAA)\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 26 Sep 2021 23:04:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to WASPAA 21\\u00a7r"}']}
{title:'Hu et al. (§72021§r)', author: 'Tianxue Hu; Claire Arthur', display:{Lore:['[{"text": "arXiv:2105.05385", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.AP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Statistical Model for Melody Reduction\\u00a7r\\n\\n\\u00a78\\u00a7oTianxue Hu\\nClaire Arthur\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.05385\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 12 May 2021 01:10:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure. Proceeding and presentation available at Future Directions of Music Cognition but the conference has not yetofficially published until summer 2021. http://org.osu.edu/mascats/march-6-talks/\\u00a7r"}']}
{title:'Ishizuka et al. (§72021§r)', author: 'Ryoto Ishizuka; Ryo Nishikimi; Kazuyoshi Yoshii', display:{Lore:['[{"text": "arXiv:2105.05791", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGlobal Structure-Aware Drum Transcription Based on Self-Attention Mechanisms\\u00a7r\\n\\n\\u00a78\\u00a7oRyoto Ishizuka\\nRyo Nishikimi\\nKazuyoshi Yoshii\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.05791\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 12 May 2021 17:04:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Signals (ISSN 2624-6120)\\u00a7r"}']}
{title:'Chivukula et al. (§72021§r)', author: 'V. N. Aditya Datta Chivukula; Rupaj Kumar Nayak', display:{Lore:['[{"text": "arXiv:2105.05938", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe impact of the additional features on the performance of regression analysis: a case study on regression analysis of music signal\\u00a7r\\n\\n\\u00a78\\u00a7oV. N. Aditya Datta Chivukula\\nRupaj Kumar Nayak\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.05938\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 May 2021 06:40:15 GMT)\\u00a7r"}']}
{title:'Qian et al. (§72021§r)', author: 'Xinyuan Qian; Maulik Madhavi; Zexu Pan; Jiadong Wang; Haizhou Li', display:{Lore:['[{"text": "arXiv:2105.06107", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-target DoA Estimation with an Audio-visual Fusion Mechanism\\u00a7r\\n\\n\\u00a78\\u00a7oXinyuan Qian\\nMaulik Madhavi\\nZexu Pan\\nJiadong Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.06107\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 13 May 2021 06:55:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2021 accepted\\u00a7r"}']}
{title:'Eren et al. (§72021§r)', author: 'Ayşegül Özkaya Eren; Mustafa Sert', display:{Lore:['[{"text": "arXiv:2105.06355", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Captioning with Composition of Acoustic and Semantic Information\\u00a7r\\n\\n\\u00a78\\u00a7oAy\\u015feg\\u00fcl \\u00d6zkaya Eren\\nMustafa Sert\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.06355\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 13 May 2021 15:30:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in International Journal of Semantic Computing. arXiv admin note: substantial text overlap with arXiv:2006.03391\\u00a7r"}']}
{title:'Chiragkumar (§72021§r)', author: 'Shah Riya Chiragkumar', display:{Lore:['[{"text": "arXiv:2105.07019", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChord Recognition- Music and Audio Information Retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oShah Riya Chiragkumar\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.07019\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 8 Sep 2021 05:44:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7owork inprogress\\u00a7r"}']}
{title:'Hershey et al. (§72021§r)', author: 'Shawn Hershey; Daniel P W Ellis; Eduardo Fonseca; Aren Jansen; Caroline Liu; R Channing Moore; Manoj Plakal', display:{Lore:['[{"text": "arXiv:2105.07031", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Benefit Of Temporally-Strong Labels In Audio Event Classification\\u00a7r\\n\\n\\u00a78\\u00a7oShawn Hershey\\nDaniel P W Ellis\\nEduardo Fonseca\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.07031\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 May 2021 18:48:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at ICASSP 2021\\u00a7r"}']}
{title:'Allamy et al. (§72021§r)', author: 'Safaa Allamy; Alessandro Lameiras Koerich', display:{Lore:['[{"text": "arXiv:2105.07302", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l1D CNN Architectures for Music Genre Classification\\u00a7r\\n\\n\\u00a78\\u00a7oSafaa Allamy\\nAlessandro Lameiras Koerich\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.07302\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 15 May 2021 22:33:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages\\u00a7r"}']}
{title:'Xue et al. (§72021§r)', author: 'Hao Xue; Flora D. Salim', display:{Lore:['[{"text": "arXiv:2105.07566", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Self-Supervised Representation Ensembles for COVID-19 Cough Classification\\u00a7r\\n\\n\\u00a78\\u00a7oHao Xue\\nFlora D. Salim\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.07566\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3447548.3467263\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 3 Jun 2021 06:25:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ACM KDD 2021\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Zhepei Wang; Jonah Casebeer; Adam Clemmitt; Efthymios Tzinis; Paris Smaragdis', display:{Lore:['[{"text": "arXiv:2105.07596", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Event Detection with Adaptive Frequency Selection\\u00a7r\\n\\n\\u00a78\\u00a7oZhepei Wang\\nJonah Casebeer\\nAdam Clemmitt\\nEfthymios Tzinis\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.07596\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 29 Jul 2021 05:02:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE Workshop on Applications of Signal Processing to Audio and Acoustics 2021\\u00a7r"}']}
{title:'Meng et al. (§72021§r)', author: 'Hsien-Yu Meng; Zhenyu Tang; Dinesh Manocha', display:{Lore:['[{"text": "arXiv:2105.08177", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPoint-based Acoustic Scattering for Interactive Sound Propagation via Surface Encoding\\u00a7r\\n\\n\\u00a78\\u00a7oHsien-Yu Meng\\nZhenyu Tang\\nDinesh Manocha\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.08177\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 May 2021 21:49:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIJCAI 2021 main track paper\\u00a7r"}']}
{title:'Green et al. (§72021§r)', author: 'Marc C. Green; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:2105.08550", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFederated Learning With Highly Imbalanced Audio Data\\u00a7r\\n\\n\\u00a78\\u00a7oMarc C. Green\\nMark D. Plumbley\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.08550\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 May 2021 14:35:55 GMT)\\u00a7r"}']}
{title:'Zaheer et al. (§72021§r)', author: 'Nimra Zaheer; Obaid Ullah Ahmad; Ammar Ahmed; Muhammad Shehryar Khan; Mudassir Shabbir', display:{Lore:['[{"text": "arXiv:2105.08957", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSEMOUR: A Scripted Emotional Speech Repository for Urdu\\u00a7r\\n\\n\\u00a78\\u00a7oNimra Zaheer\\nObaid Ullah Ahmad\\nAmmar Ahmed\\nMuhammad Shehryar Khan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.08957\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3411764.3445171\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 19 May 2021 07:15:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted in CHI 2021\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Weiyi Zhang; Shuning Zhao; Le Liu; Jianmin Li; Xingliang Cheng; Thomas Fang Zheng; Xiaolin Hu', display:{Lore:['[{"text": "arXiv:2105.09022", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttack on practical speaker verification system using universal adversarial perturbations\\u00a7r\\n\\n\\u00a78\\u00a7oWeiyi Zhang\\nShuning Zhao\\nLe Liu\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.09022\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9413467\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 19 May 2021 09:43:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 2 figures\\u00a7r"}']}
{title:'Ingale et al. (§72021§r)', author: 'Vaishali Ingale; Anush Mohan; Divit Adlakha; Krishan Kumar; Mohit Gupta', display:{Lore:['[{"text": "arXiv:2105.09046", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Generation using Three-layered LSTM\\u00a7r\\n\\n\\u00a78\\u00a7oVaishali Ingale\\nAnush Mohan\\nDivit Adlakha\\nKrishan Kumar\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.09046\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 9 Jun 2021 08:15:19 GMT)\\u00a7r"}']}
{title:'Hornauer et al. (§72021§r)', author: 'Sascha Hornauer; Ke Li; Stella X. Yu; Shabnam Ghaffarzadegan; Liu Ren', display:{Lore:['[{"text": "arXiv:2105.09279", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Discriminative Learning of Sounds for Audio Event Classification\\u00a7r\\n\\n\\u00a78\\u00a7oSascha Hornauer\\nKe Li\\nStella X. Yu\\nShabnam Ghaffarzadegan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.09279\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9413482\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 20 May 2021 10:51:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2021 - 2021 IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP) | 978-1-7281-7605-5/20/$31.00 (c) 2021 IEEE | DOI: 10.1109/ICASSP39728.2021.9413482\\u00a7r"}']}
{title:'Javaheri (§72021§r)', author: 'Behzad Javaheri', display:{Lore:['[{"text": "arXiv:2105.09406", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech     Song Emotion Recognition Using Multilayer Perceptron and Standard Vector Machine\\u00a7r\\n\\n\\u00a78\\u00a7oBehzad Javaheri\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.09406\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.20944/preprints202105.0441.v1\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 19 May 2021 21:28:05 GMT)\\u00a7r"}']}
{title:'Tobing et al. (§72021§r)', author: 'Patrick Lumban Tobing; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2105.09856", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHigh-Fidelity and Low-Latency Universal Neural Vocoder based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oPatrick Lumban Tobing\\nTomoki Toda\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.09856\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 5 Jul 2021 00:43:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for INTERSPEECH 2021\\u00a7r"}']}
{title:'Tobing et al. (§72021§r)', author: 'Patrick Lumban Tobing; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2105.09858", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Latency Real-Time Non-Parallel Voice Conversion based on Cyclic Variational Autoencoder and Multiband WaveRNN with Data-Driven Linear Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oPatrick Lumban Tobing\\nTomoki Toda\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.09858\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 5 Jul 2021 00:59:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for SSW11\\u00a7r"}']}
{title:'Sodhi et al. (§72021§r)', author: 'Sukhdeep S. Sodhi; Ellie Ka-In Chio; Ambarish Jash; Santiago Ontañón; Ajit Apte; Ankit Kumar; Ayooluwakunmi Jeje; Dima Kuzmin; Harry Fung; Heng-Tze Cheng; Jon Effrat; Tarush Bali; Nitin Jindal; Pei Cao; Sarvjeet Singh; Senqiang Zhou; Tameen Khan; Amol Wankhede; Moustafa Alzantot; Allen Wu; Tushar Chandra', display:{Lore:['[{"text": "arXiv:2105.09930", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMondegreen: A Post-Processing Solution to Speech Recognition Error Correction for Voice Search Queries\\u00a7r\\n\\n\\u00a78\\u00a7oSukhdeep S. Sodhi\\nEllie Ka-In Chio\\nAmbarish Jash\\n+ 17 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.09930\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 20 May 2021 17:45:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in KDD 2021\\u00a7r"}']}
{title:'Yang et al. (§72021§r)', author: 'Dongchao Yang; Helin Wang; Yuexian Zou', display:{Lore:['[{"text": "arXiv:2105.10340", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Multi-Target Domain Adaptation for Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oDongchao Yang\\nHelin Wang\\nYuexian Zou\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.10340\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 May 2021 13:30:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5pages,4figures,submit to interspeech2021\\u00a7r"}']}
{title:'Chandna et al. (§72021§r)', author: 'Pritish Chandna; António Ramires; Xavier Serra; Emilia Gómez', display:{Lore:['[{"text": "arXiv:2105.10371", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLoopNet: Musical Loop Synthesis Conditioned On Intuitive Musical Parameters\\u00a7r\\n\\n\\u00a78\\u00a7oPritish Chandna\\nAnt\\u00f3nio Ramires\\nXavier Serra\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.10371\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 May 2021 14:24:34 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Tony Zhang; Szymon Zmyslony; Sergei Nozdrenkov; Matthew Smith; Brandon Hopkins', display:{Lore:['[{"text": "arXiv:2105.10536", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-Supervised Audio Representation Learning for Modeling Beehive Strengths\\u00a7r\\n\\n\\u00a78\\u00a7oTony Zhang\\nSzymon Zmyslony\\nSergei Nozdrenkov\\nMatthew Smith\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.10536\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 May 2021 18:59:29 GMT)\\u00a7r"}']}
{title:'Elizalde et al. (§72021§r)', author: 'Benjamin Elizalde; Daniel Tompkins', display:{Lore:['[{"text": "arXiv:2105.10619", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCOVID-19 Detection Using Recorded Coughs in the 2021 DiCOVA Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oBenjamin Elizalde\\nDaniel Tompkins\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.10619\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 22 May 2021 02:04:42 GMT)\\u00a7r"}']}
{title:'Doumanidis et al. (§72021§r)', author: 'Constantine C. Doumanidis; Christina Anagnostou; Evangelia-Sofia Arvaniti; Anthi Papadopoulou', display:{Lore:['[{"text": "arXiv:2105.11813", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRNNoise-Ex: Hybrid Speech Enhancement System based on RNN and Spectral Features\\u00a7r\\n\\n\\u00a78\\u00a7oConstantine C. Doumanidis\\nChristina Anagnostou\\nEvangelia-Sofia Arvaniti\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.11813\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 May 2021 10:32:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 5 figures, presented at ECESCON 12, for code see https://github.com/CedArctic/rnnoise-ex\\u00a7r"}']}
{title:'Vahidi et al. (§72021§r)', author: 'Cyrus Vahidi; Charalampos Saitis; György Fazekas', display:{Lore:['[{"text": "arXiv:2105.11836", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Modulation Front-End for Music Audio Tagging\\u00a7r\\n\\n\\u00a78\\u00a7oCyrus Vahidi\\nCharalampos Saitis\\nGy\\u00f6rgy Fazekas\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.11836\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 May 2021 11:05:24 GMT)\\u00a7r"}']}
{title:'Kośmider (§72021§r)', author: 'Michał Kośmider', display:{Lore:['[{"text": "arXiv:2105.11856", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpectrum Correction: Acoustic Scene Classification with Mismatched Recording Devices\\u00a7r\\n\\n\\u00a78\\u00a7oMicha\\u0142 Ko\\u015bmider\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.11856\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2020-3088\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInterspeech (2020) 4641-4645\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 May 2021 11:53:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, published at Interspeech 2020, see https://isca-speech.org/archive/Interspeech_2020/abstracts/3088.html\\u00a7r"}']}
{title:'Koutini et al. (§72021§r)', author: 'Khaled Koutini; Hamid Eghbal-zadeh; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2105.12395", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReceptive Field Regularization Techniques for Audio Classification and Tagging with Deep Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oKhaled Koutini\\nHamid Eghbal-zadeh\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.12395\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3082307\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 May 2021 08:36:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in IEEE/ACM Transactionson Audio, Speech, and Language Processing. Code available: https://github.com/kkoutini/cpjku_dcase20\\u00a7r"}']}
{title:'Anders et al. (§72021§r)', author: 'Franz Anders; Ammie K. Kalan; Hjalmar S. Kühl; Mirco Fuchs', display:{Lore:['[{"text": "arXiv:2105.12502", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCompensating class imbalance for acoustic chimpanzee detection with convolutional recurrent neural networks\\u00a7r\\n\\n\\u00a78\\u00a7oFranz Anders\\nAmmie K. Kalan\\nHjalmar S. K\\u00fchl\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.12502\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 May 2021 12:03:26 GMT)\\u00a7r"}']}
{title:'Du et al. (§72021§r)', author: 'Chenpeng Du; Kai Yu', display:{Lore:['[{"text": "arXiv:2105.13086", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhone-Level Prosody Modelling with GMM-Based MDN for Diverse and Controllable Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oChenpeng Du\\nKai Yu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.13086\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 Nov 2021 14:57:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to TASLP 2021. arXiv adminnote: substantial text overlap with arXiv: 2102.00851\\u00a7r"}']}
{title:'Id et al. (§72021§r)', author: 'Ibnu Daqiqil Id; Masanobu Abe; Sunao Hara', display:{Lore:['[{"text": "arXiv:2105.13220", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluation of concept drift adaptation for acoustic scene classifier based on Kernel Density Drift Detection and Combine Merge Gaussian Mixture Model\\u00a7r\\n\\n\\u00a78\\u00a7oIbnu Daqiqil Id\\nMasanobu Abe\\nSunao Hara\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.13220\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5281/zenodo.4595866\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 May 2021 15:09:24 GMT)\\u00a7r"}']}
{title:'Park et al. (§72021§r)', author: 'Sangwook Park; David K. Han; Mounya Elhilali', display:{Lore:['[{"text": "arXiv:2105.13392", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-Referencing Self-Training Network for Sound Event Detection in Audio Mixtures\\u00a7r\\n\\n\\u00a78\\u00a7oSangwook Park\\nDavid K. Han\\nMounya Elhilali\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.13392\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 May 2021 18:46:59 GMT)\\u00a7r"}']}
{title:'Shandiz et al. (§72021§r)', author: 'Amin Honarmandi Shandiz; László Tóth', display:{Lore:['[{"text": "arXiv:2105.13718", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Activity Detection for Ultrasound-based Silent Speech Interfaces using Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oAmin Honarmandi Shandiz\\nL\\u00e1szl\\u00f3 T\\u00f3th\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.13718\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-030-83527-9_43\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 18 Sep 2021 20:47:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 7 tables, 4 figures\\u00a7r"}']}
{title:'Zeghidour et al. (§72021§r)', author: 'Neil Zeghidour; Olivier Teboul; David Grangier', display:{Lore:['[{"text": "arXiv:2105.13802", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDIVE: End-to-end Speech Diarization via Iterative Speaker Embedding\\u00a7r\\n\\n\\u00a78\\u00a7oNeil Zeghidour\\nOlivier Teboul\\nDavid Grangier\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.13802\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 May 2021 13:15:52 GMT)\\u00a7r"}']}
{title:'Ma et al. (§72021§r)', author: 'Lu Ma; Song Yang; Yaguang Gong; Xintian Wang; Zhongqin Wu', display:{Lore:['[{"text": "arXiv:2105.14666", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEchoFilter: End-to-End Neural Network for Acoustic Echo Cancellation\\u00a7r\\n\\n\\u00a78\\u00a7oLu Ma\\nSong Yang\\nYaguang Gong\\nXintian Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.14666\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 May 2021 01:39:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 6 tabels\\u00a7r"}']}
{title:'Ma et al. (§72021§r)', author: 'Lu Ma; Xintian Wang; Song Yang; Yaguang Gong; Zhongqin Wu', display:{Lore:['[{"text": "arXiv:2105.14717", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Scale Temporal Convolution Network for Classroom Voice Detection\\u00a7r\\n\\n\\u00a78\\u00a7oLu Ma\\nXintian Wang\\nSong Yang\\nYaguang Gong\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.14717\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 May 2021 06:26:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 1 table\\u00a7r"}']}
{title:'Ma et al. (§72021§r)', author: 'Lu Ma; Song Yang; Yaguang Gong; Zhongqin Wu', display:{Lore:['[{"text": "arXiv:2105.14719", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoise Classification Aided Attention-Based Neural Network for Monaural Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oLu Ma\\nSong Yang\\nYaguang Gong\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.14719\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 May 2021 06:30:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2 figures, 3 tables\\u00a7r"}']}
{title:'Renault et al. (§72021§r)', author: 'Lenny Renault; Andrea Vaglio; Romain Hennequin', display:{Lore:['[{"text": "arXiv:2105.15014", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSinging Language Identification using a Deep Phonotactic Approach\\u00a7r\\n\\n\\u00a78\\u00a7oLenny Renault\\nAndrea Vaglio\\nRomain Hennequin\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.15014\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9414203\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2021 - 2021 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP), pp. 271-275\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 May 2021 14:53:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, ICASSP 2021\\u00a7r"}']}
{title:'Ma et al. (§72021§r)', author: 'Lu Ma; Song Yang; Yaguang Gong; Zhongqin Wu', display:{Lore:['[{"text": "arXiv:2106.00010", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Scale Attention Neural Network for Acoustic Echo Cancellation\\u00a7r\\n\\n\\u00a78\\u00a7oLu Ma\\nSong Yang\\nYaguang Gong\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.00010\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 May 2021 06:30:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 4 tables. arXiv admin note:substantial text overlap with arXiv:2105.14666\\u00a7r"}']}
{title:'Yanchenko et al. (§72021§r)', author: 'Anna K. Yanchenko; Mohammadreza Soltani; Robert J. Ravier; Sayan Mukherjee; Vahid Tarokh', display:{Lore:['[{"text": "arXiv:2106.00110", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Methodology for Exploring Deep Convolutional Features in Relation to Hand-Crafted Features with an Application to Music Audio Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oAnna K. Yanchenko\\nMohammadreza Soltani\\nRobert J. Ravier\\nSayan Mukherjee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.00110\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 9 Oct 2021 15:56:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCode available at https://github.com/aky4wn/convolutions-for-music-audio\\u00a7r"}']}
{title:'Wu et al. (§72021§r)', author: 'Haibin Wu; Xu Li; Andy T. Liu; Zhiyong Wu; Helen Meng; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2106.00273", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving the Adversarial Robustness for Speaker Verification by Self-Supervised Learning\\u00a7r\\n\\n\\u00a78\\u00a7oHaibin Wu\\nXu Li\\nAndy T. Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.00273\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 6 Dec 2021 10:32:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by TASLP\\u00a7r"}']}
{title:'Wu et al. (§72021§r)', author: 'Yu-Te Wu; Yin-Jyun Luo; Tsung-Ping Chen; I-Chieh Wei; Jui-Yang Hsu; Yi-Chin Chuang; Li Su', display:{Lore:['[{"text": "arXiv:2106.00497", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOmnizart: A General Toolbox for Automatic Music Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oYu-Te Wu\\nYin-Jyun Luo\\nTsung-Ping Chen\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.00497\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Jun 2021 14:00:14 GMT)\\u00a7r"}']}
{title:'Agustín-Aquino et al. (§72021§r)', author: 'Octavio A. Agustín-Aquino; Jeffery Liu; Guerino Mazzola', display:{Lore:['[{"text": "arXiv:2106.00806", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Exotic Counterpoint Compositions\\u00a7r\\n\\n\\u00a78\\u00a7oOctavio A. Agust\\u00edn-Aquino\\nJeffery Liu\\nGuerino Mazzola\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.00806\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Jun 2021 21:17:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages, 7 figures\\u00a7r"}']}
{title:'Nguyen et al. (§72021§r)', author: 'Bac Nguyen; Fabien Cardinaux', display:{Lore:['[{"text": "arXiv:2106.00992", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNVC-Net: End-to-End Adversarial Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oBac Nguyen\\nFabien Cardinaux\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.00992\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Jun 2021 07:19:58 GMT)\\u00a7r"}']}
{title:'Wu et al. (§72021§r)', author: 'Ho-Hsiang Wu; Magdalena Fuentes; Juan P. Bello', display:{Lore:['[{"text": "arXiv:2106.01149", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring modality-agnostic representations for music classification\\u00a7r\\n\\n\\u00a78\\u00a7oHo-Hsiang Wu\\nMagdalena Fuentes\\nJuan P. Bello\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.01149\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Jun 2021 13:39:42 GMT)\\u00a7r"}']}
{title:'Huang et al. (§72021§r)', author: 'Wen-Chin Huang; Kazuhiro Kobayashi; Yu-Huai Peng; Ching-Feng Liu; Yu Tsao; Hsin-Min Wang; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2106.01415", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Preliminary Study of a Two-Stage Paradigm for Preserving Speaker Identity in Dysarthric Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oWen-Chin Huang\\nKazuhiro Kobayashi\\nYu-Huai Peng\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.01415\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Jun 2021 18:41:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021. 5 pages, 3 figures, 1 table\\u00a7r"}']}
{title:'Bhaduri et al. (§72021§r)', author: 'Susmita Bhaduri; Anirban Bhaduri; Rajib Sarkar', display:{Lore:['[{"text": "arXiv:2106.01684", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLanguage Independent Speech Emotion and Non-invasive Early Detection of Neurocognitive Disorder\\u00a7r\\n\\n\\u00a78\\u00a7oSusmita Bhaduri\\nAnirban Bhaduri\\nRajib Sarkar\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.01684\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 3 Jun 2021 08:38:22 GMT)\\u00a7r"}']}
{title:'Azam et al. (§72021§r)', author: 'Farhat Binte Azam; Md. Istiaq Ansari; Ian Mclane; Taufiq Hasan', display:{Lore:['[{"text": "arXiv:2106.01865", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHeart Sound Classification Considering Additive Noise and Convolutional Distortion\\u00a7r\\n\\n\\u00a78\\u00a7oFarhat Binte Azam\\nMd. Istiaq Ansari\\nIan Mclane\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.01865\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 3 Jun 2021 14:09:04 GMT)\\u00a7r"}']}
{title:'Vechtomova et al. (§72021§r)', author: 'Olga Vechtomova; Gaurav Sahu; Dhruv Kumar', display:{Lore:['[{"text": "arXiv:2106.01960", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLyricJam: A system for generating lyrics for live instrumental music\\u00a7r\\n\\n\\u00a78\\u00a7oOlga Vechtomova\\nGaurav Sahu\\nDhruv Kumar\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.01960\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 3 Jun 2021 16:06:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to International Conference on Computational Creativity (ICCC) 2021 [Oral]\\u00a7r"}']}
{title:'Heitkaemper et al. (§72021§r)', author: 'Jens Heitkaemper; Joerg Schmalenstroeer; Joerg Ullmann; Valentin Ion; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2106.02472", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Database for Research on Detection and Enhancement of Speech Transmitted over HF links\\u00a7r\\n\\n\\u00a78\\u00a7oJens Heitkaemper\\nJoerg Schmalenstroeer\\nJoerg Ullmann\\nValentin Ion\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.02472\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 21 Jul 2021 08:25:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ITG 2021\\u00a7r"}']}
{title:'Farris et al. (§72021§r)', author: 'Nicholas Farris; Brian Model; Richard Savery; Gil Weinberg', display:{Lore:['[{"text": "arXiv:2106.02556", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusical Prosody-Driven Emotion Classification: Interpreting Vocalists Portrayal of Emotions Through Machine Learning\\u00a7r\\n\\n\\u00a78\\u00a7oNicholas Farris\\nBrian Model\\nRichard Savery\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.02556\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 13 Jun 2021 19:56:51 GMT)\\u00a7r"}']}
{title:'Bao et al. (§72021§r)', author: 'Yuanyuan Bao; Yanze Xu; Na Xu; Wenjing Yang; Hongfeng Li; Shicong Li; Yongtao Jia; Fei Xiang; Jincheng He; Ming Li', display:{Lore:['[{"text": "arXiv:2106.02934", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLightweight Dual-channel Target Speaker Separation for Mobile Voice Communication\\u00a7r\\n\\n\\u00a78\\u00a7oYuanyuan Bao\\nYanze Xu\\nNa Xu\\n+ 6 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.02934\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 5 Jun 2021 17:19:34 GMT)\\u00a7r"}']}
{title:'Hasumi et al. (§72021§r)', author: 'Takuya Hasumi; Tomohiko Nakamura; Norihiro Takamune; Hiroshi Saruwatari; Daichi Kitamura; Yu Takahashi; Kazunobu Kondo', display:{Lore:['[{"text": "arXiv:2106.03492", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmpirical Bayesian Independent Deeply Learned Matrix Analysis For Multichannel Audio Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oTakuya Hasumi\\nTomohiko Nakamura\\nNorihiro Takamune\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.03492\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Jun 2021 10:26:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, accepted for EuropeanSignal Processing Conference 2021 (EUSIPCO 2021)\\u00a7r"}']}
{title:'Pouthier et al. (§72021§r)', author: 'Baptiste Pouthier; Laurent Pilati; Leela K. Gudupudi; Charles Bouveyron; Frederic Precioso', display:{Lore:['[{"text": "arXiv:2106.03821", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lActive Speaker Detection as a Multi-Objective Optimization with Uncertainty-based Multimodal Fusion\\u00a7r\\n\\n\\u00a78\\u00a7oBaptiste Pouthier\\nLaurent Pilati\\nLeela K. Gudupudi\\nCharles Bouveyron\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.03821\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-80\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2021, 2381-2385\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 15 Sep 2021 12:32:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn INTERSPEECH 2021\\u00a7r"}']}
{title:'Sharma et al. (§72021§r)', author: 'Makkunda Sharma; Nikhil Shenoy; Jigar Doshi; Piyush Bagad; Aman Dalmia; Parag Bhamare; Amrita Mahale; Saurabh Rane; Neeraj Agrawal; Rahul Panicker', display:{Lore:['[{"text": "arXiv:2106.03851", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImpact of data-splits on generalization: Identifying COVID-19 from cough and context\\u00a7r\\n\\n\\u00a78\\u00a7oMakkunda Sharma\\nNikhil Shenoy\\nJigar Doshi\\n+ 6 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.03851\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 5 Jun 2021 07:48:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished as a workshop paper at ICLR2021 AI for Public Health Workshop and ICLR 20201 Machine Learning for Preventing and Combating Pandemics Workshop\\u00a7r"}']}
{title:'Schymura et al. (§72021§r)', author: 'Christopher Schymura; Benedikt Bönninghoff; Tsubasa Ochiai; Marc Delcroix; Keisuke Kinoshita; Tomohiro Nakatani; Shoko Araki; Dorothea Kolossa', display:{Lore:['[{"text": "arXiv:2106.03903", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPILOT: Introducing Transformers for Probabilistic Sound Event Localization\\u00a7r\\n\\n\\u00a78\\u00a7oChristopher Schymura\\nBenedikt B\\u00f6nninghoff\\nTsubasa Ochiai\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.03903\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Jun 2021 18:29:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2021\\u00a7r"}']}
{title:'Nazi et al. (§72021§r)', author: 'Zabir Al Nazi; Sayed Mohammed Tasmimul Huda', display:{Lore:['[{"text": "arXiv:2106.03937", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lByakto Speech: Real-time long speech synthesis with convolutional neural network: Transfer learning from English to Bangla\\u00a7r\\n\\n\\u00a78\\u00a7oZabir Al Nazi\\nSayed Mohammed Tasmimul Huda\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.03937\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 May 2021 20:39:35 GMT)\\u00a7r"}']}
{title:'Peng et al. (§72021§r)', author: 'Zixuan Peng; Yu Lu; Shengfeng Pan; Yunfeng Liu', display:{Lore:['[{"text": "arXiv:2106.04133", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Speech Emotion Recognition Using Multi-Scale CNN and Attention\\u00a7r\\n\\n\\u00a78\\u00a7oZixuan Peng\\nYu Lu\\nShengfeng Pan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.04133\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9414286\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP,2021 pp. 3020-3024\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Jun 2021 06:45:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oFirst two authors contributed equally.Accepted by ICASSP 2021\\u00a7r"}']}
{title:'Lam et al. (§72021§r)', author: 'Max W. Y. Lam; Jun Wang; Chao Weng; Dan Su; Dong Yu', display:{Lore:['[{"text": "arXiv:2106.04275", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRaw Waveform Encoder with Multi-Scale Globally Attentive Locally Recurrent Networks for End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMax W. Y. Lam\\nJun Wang\\nChao Weng\\nDan Su\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.04275\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Jun 2021 12:12:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in Interspeech 2021\\u00a7r"}']}
{title:'Mama et al. (§72021§r)', author: 'Rayhane Mama; Marc S. Tyndel; Hashiam Kadhim; Cole Clifford; Ragavan Thurairatnam', display:{Lore:['[{"text": "arXiv:2106.04283", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNWT: Towards natural audio-to-video generation with representation learning\\u00a7r\\n\\n\\u00a78\\u00a7oRayhane Mama\\nMarc S. Tyndel\\nHashiam Kadhim\\nCole Clifford\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.04283\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Jun 2021 12:22:29 GMT)\\u00a7r"}']}
{title:'Kamble et al. (§72021§r)', author: 'Madhu R. Kamble; Jose A. Gonzalez-Lopez; Teresa Grau; Juan M. Espin; Lorenzo Cascioli; Yiqing Huang; Alejandro Gomez-Alanis; Jose Patino; Roberto Font; Antonio M. Peinado; Angel M. Gomez; Nicholas Evans; Maria A. Zuluaga; Massimiliano Todisco', display:{Lore:['[{"text": "arXiv:2106.04423", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPANACEA cough sound-based diagnosis of COVID-19 for the DiCOVA 2021 Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oMadhu R. Kamble\\nJose A. Gonzalez-Lopez\\nTeresa Grau\\n+ 10 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.04423\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Jun 2021 09:09:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in INTERSPEECH 2021\\u00a7r"}']}
{title:'Shandiz et al. (§72021§r)', author: 'Amin Honarmandi Shandiz; László Tóth; Gábor Gosztolya; Alexandra Markó; Tamás Gábor Csapó', display:{Lore:['[{"text": "arXiv:2106.04552", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Speaker Embeddings for Ultrasound-based Silent Speech Interfaces\\u00a7r\\n\\n\\u00a78\\u00a7oAmin Honarmandi Shandiz\\nL\\u00e1szl\\u00f3 T\\u00f3th\\nG\\u00e1bor Gosztolya\\nAlexandra Mark\\u00f3\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.04552\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 11 Jun 2021 12:06:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 3 tables\\u00a7r"}']}
{title:'Tu et al. (§72021§r)', author: 'Zehai Tu; Ning Ma; Jon Barker', display:{Lore:['[{"text": "arXiv:2106.04639", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimising Hearing Aid Fittings for Speech in Noise with a Differentiable Hearing Loss Model\\u00a7r\\n\\n\\u00a78\\u00a7oZehai Tu\\nNing Ma\\nJon Barker\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.04639\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Jun 2021 18:58:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021\\u00a7r"}']}
{title:'Narisawa et al. (§72021§r)', author: 'Naoki Narisawa; Rintaro Ikeshita; Norihiro Takamune; Daichi Kitamura; Tomohiko Nakamura; Hiroshi Saruwatari; Tomohiro Nakatani', display:{Lore:['[{"text": "arXiv:2106.05529", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIndependent Deeply Learned Tensor Analysis for Determined Audio Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oNaoki Narisawa\\nRintaro Ikeshita\\nNorihiro Takamune\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.05529\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Jun 2021 06:33:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, accepted for EuropeanSignal Processing Conference 2021 (EUSIPCO 2021)\\u00a7r"}']}
{title:'Boeddeker et al. (§72021§r)', author: 'Christoph Boeddeker; Frederik Rautenberg; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2106.05627", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparison and Combination of Unsupervised Blind Source Separation Techniques\\u00a7r\\n\\n\\u00a78\\u00a7oChristoph Boeddeker\\nFrederik Rautenberg\\nReinhold Haeb-Umbach\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.05627\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Jun 2021 10:11:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ITG 2021\\u00a7r"}']}
{title:'Zeng et al. (§72021§r)', author: 'Mingliang Zeng; Xu Tan; Rui Wang; Zeqian Ju; Tao Qin; Tie-Yan Liu', display:{Lore:['[{"text": "arXiv:2106.05630", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training\\u00a7r\\n\\n\\u00a78\\u00a7oMingliang Zeng\\nXu Tan\\nRui Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.05630\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Jun 2021 10:13:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ACL 2021 Findings\\u00a7r"}']}
{title:'Wu et al. (§72021§r)', author: 'Di Wu; Binbin Zhang; Chao Yang; Zhendong Peng; Wenjing Xia; Xiaoyu Chen; Xin Lei', display:{Lore:['[{"text": "arXiv:2106.05642", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lU2++: Unified Two-pass Bidirectional End-to-end Model for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDi Wu\\nBinbin Zhang\\nChao Yang\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.05642\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 30 Dec 2021 00:30:30 GMT)\\u00a7r"}']}
{title:'Vallés-Pérez et al. (§72021§r)', author: 'Iván Vallés-Pérez; Julian Roth; Grzegorz Beringer; Roberto Barra-Chicote; Jasha Droppo', display:{Lore:['[{"text": "arXiv:2106.05762", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving multi-speaker TTS prosody variance with a residual encoder and normalizing flows\\u00a7r\\n\\n\\u00a78\\u00a7oIv\\u00e1n Vall\\u00e9s-P\\u00e9rez\\nJulian Roth\\nGrzegorz Beringer\\nRoberto Barra-Chicote\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.05762\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Jun 2021 14:08:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oin Proceedings of Interspeech 2021 conference\\u00a7r"}']}
{title:'Kim et al. (§72021§r)', author: 'Jaehyeon Kim; Jungil Kong; Juhee Son', display:{Lore:['[{"text": "arXiv:2106.06103", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oJaehyeon Kim\\nJungil Kong\\nJuhee Son\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.06103\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Jun 2021 01:07:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICML 2021\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Jing Liu; Rupak Vignesh Swaminathan; Sree Hari Krishnan Parthasarathi; Chunchuan Lyu; Athanasios Mouchtaris; Siegfried Kunzmann', display:{Lore:['[{"text": "arXiv:2106.06126", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting Large-scale Teacher-Student Training for On-device Acoustic Models\\u00a7r\\n\\n\\u00a78\\u00a7oJing Liu\\nRupak Vignesh Swaminathan\\nSree Hari Krishnan Parthasarathi\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.06126\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Jun 2021 02:23:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTSD2021\\u00a7r"}']}
{title:'Kuroyanagi et al. (§72021§r)', author: 'Ibuki Kuroyanagi; Tomoki Hayashi; Kazuya Takeda; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2106.06151", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnomalous Sound Detection Using a Binary Classification Model and Class Centroids\\u00a7r\\n\\n\\u00a78\\u00a7oIbuki Kuroyanagi\\nTomoki Hayashi\\nKazuya Takeda\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.06151\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Jun 2021 03:35:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 2 figures, 2 tables, EUSIPCO2021\\u00a7r"}']}
{title:'Puchtler et al. (§72021§r)', author: 'Pascal Puchtler; Johannes Wirth; René Peinl', display:{Lore:['[{"text": "arXiv:2106.06309", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHUI-Audio-Corpus-German: A high quality TTS dataset\\u00a7r\\n\\n\\u00a78\\u00a7oPascal Puchtler\\nJohannes Wirth\\nRen\\u00e9 Peinl\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.06309\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Jun 2021 10:59:09 GMT)\\u00a7r"}']}
{title:'Kinnunen et al. (§72021§r)', author: 'Tomi Kinnunen; Andreas Nautsch; Md Sahidullah; Nicholas Evans; Xin Wang; Massimiliano Todisco; Héctor Delgado; Junichi Yamagishi; Kong Aik Lee', display:{Lore:['[{"text": "arXiv:2106.06362", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.AP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisualizing Classifier Adjacency Relations: A Case Study in Speaker Verification and Voice Anti-Spoofing\\u00a7r\\n\\n\\u00a78\\u00a7oTomi Kinnunen\\nAndreas Nautsch\\nMd Sahidullah\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.06362\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Jun 2021 13:03:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021. Example code available at https://github.com/asvspoof-challenge/classifier-adjacency\\u00a7r"}']}
{title:'Greshler et al. (§72021§r)', author: 'Gal Greshler; Tamar Rott Shaham; Tomer Michaeli', display:{Lore:['[{"text": "arXiv:2106.06426", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCatch-A-Waveform: Learning to Generate Audio from a Single Short Example\\u00a7r\\n\\n\\u00a78\\u00a7oGal Greshler\\nTamar Rott Shaham\\nTomer Michaeli\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.06426\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 26 Oct 2021 13:34:03 GMT)\\u00a7r"}']}
{title:'Bie et al. (§72021§r)', author: 'Xiaoyu Bie; Laurent Girin; Simon Leglaive; Thomas Hueber; Xavier Alameda-Pineda', display:{Lore:['[{"text": "arXiv:2106.06500", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Benchmark of Dynamical Variational Autoencoders applied to Speech Spectrogram Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoyu Bie\\nLaurent Girin\\nSimon Leglaive\\nThomas Hueber\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.06500\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 14 Jun 2021 11:03:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021. arXiv admin note: text overlap with arXiv:2008.12595\\u00a7r"}']}
{title:'Pham et al. (§72021§r)', author: 'Lam Pham; Hieu Tang; Anahid Jalali; Alexander Schindler; Ross King', display:{Lore:['[{"text": "arXiv:2106.06838", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Low-Compexity Deep Learning Framework For Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oLam Pham\\nHieu Tang\\nAnahid Jalali\\nAlexander Schindler\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.06838\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 12 Jun 2021 19:20:39 GMT)\\u00a7r"}']}
{title:'Pham et al. (§72021§r)', author: 'Lam Pham; Alexander Schindler; Mina Schütz; Jasmin Lampert; Sven Schlarb; Ross King', display:{Lore:['[{"text": "arXiv:2106.06840", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning Frameworks Applied For Audio-Visual Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oLam Pham\\nAlexander Schindler\\nMina Sch\\u00fctz\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.06840\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 12 Jun 2021 19:37:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages\\u00a7r"}']}
{title:'Al-Radhi et al. (§72021§r)', author: 'Mohammed Salah Al-Radhi; Tamás Gábor Csapó; Csaba Zainkó; Géza Németh', display:{Lore:['[{"text": "arXiv:2106.06863", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContinuous Wavelet Vocoder-based Decomposition of Parametric Speech Waveform Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oMohammed Salah Al-Radhi\\nTam\\u00e1s G\\u00e1bor Csap\\u00f3\\nCsaba Zaink\\u00f3\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.06863\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 12 Jun 2021 20:55:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, accepted to the conference of Interspeech 2021\\u00a7r"}']}
{title:'Chen et al. (§72021§r)', author: 'Guoguo Chen; Shuzhou Chai; Guanbo Wang; Jiayu Du; Wei-Qiang Zhang; Chao Weng; Dan Su; Daniel Povey; Jan Trmal; Junbo Zhang; Mingjie Jin; Sanjeev Khudanpur; Shinji Watanabe; Shuaijiang Zhao; Wei Zou; Xiangang Li; Xuchen Yao; Yongqing Wang; Yujun Wang; Zhao You; Zhiyong Yan', display:{Lore:['[{"text": "arXiv:2106.06909", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio\\u00a7r\\n\\n\\u00a78\\u00a7oGuoguo Chen\\nShuzhou Chai\\nGuanbo Wang\\n+ 17 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.06909\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 13 Jun 2021 04:09:16 GMT)\\u00a7r"}']}
{title:'He et al. (§72021§r)', author: 'Yuhang He; Niki Trigoni; Andrew Markham', display:{Lore:['[{"text": "arXiv:2106.06969", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSoundDet: Polyphonic Moving Sound Event Detection and Localization from Raw Waveform\\u00a7r\\n\\n\\u00a78\\u00a7oYuhang He\\nNiki Trigoni\\nAndrew Markham\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.06969\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 21 Aug 2021 15:44:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICML21\\u00a7r"}']}
{title:'Kaneko et al. (§72021§r)', author: 'Shoken Kaneko; Ramani Duraiswami', display:{Lore:['[{"text": "arXiv:2106.07157", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultiple scattering ambisonics: three-dimensional sound field estimation using interacting spheres\\u00a7r\\n\\n\\u00a78\\u00a7oShoken Kaneko\\nRamani Duraiswami\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.07157\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0005832\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJASA Express Lett. 1 (8), 084801 (2021)\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 15 Aug 2021 05:16:19 GMT)\\u00a7r"}']}
{title:'Kwon et al. (§72021§r)', author: 'Young D. Kwon; Jagmohan Chauhan; Cecilia Mascolo', display:{Lore:['[{"text": "arXiv:2106.07268", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFastICARL: Fast Incremental Classifier and Representation Learning with Efficient Budget Allocation in Audio Sensing Applications\\u00a7r\\n\\n\\u00a78\\u00a7oYoung D. Kwon\\nJagmohan Chauhan\\nCecilia Mascolo\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.07268\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 24 Jun 2021 19:32:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at INTERSPEECH 2021\\u00a7r"}']}
{title:'Santos et al. (§72021§r)', author: 'Rodrigo dos Santos; Shirin Nilizadeh', display:{Lore:['[{"text": "arXiv:2106.07428", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Attacks and Defenses against AED Systems \\u2013 A Practical Study\\u00a7r\\n\\n\\u00a78\\u00a7oRodrigo dos Santos\\nShirin Nilizadeh\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.07428\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 10 Nov 2021 17:30:29 GMT)\\u00a7r"}']}
{title:'Rouard et al. (§72021§r)', author: 'Simon Rouard; Gaëtan Hadjeres', display:{Lore:['[{"text": "arXiv:2106.07431", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCRASH: Raw Audio Score-based Generative Modeling for Controllable High-resolution Drum Sound Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oSimon Rouard\\nGa\\u00ebtan Hadjeres\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.07431\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Jun 2021 13:48:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 11 figures\\u00a7r"}']}
{title:'Mehrbani et al. (§72021§r)', author: 'Ezsan Mehrbani; Sezedeh Fatemeh Mirhoseini; Noushin Riahi', display:{Lore:['[{"text": "arXiv:2106.07448", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Novel mapping for visual to auditory sensory substitution\\u00a7r\\n\\n\\u00a78\\u00a7oEzsan Mehrbani\\nSezedeh Fatemeh Mirhoseini\\nNoushin Riahi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.07448\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Jun 2021 14:14:50 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Shimin Zhang; Yuxiang Kong; Shubo Lv; Yanxin Hu; Lei Xie', display:{Lore:['[{"text": "arXiv:2106.07577", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lF-T-LSTM based Complex Network for Joint Acoustic Echo Cancellation and Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oShimin Zhang\\nYuxiang Kong\\nShubo Lv\\nYanxin Hu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.07577\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-1359\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 16 Jun 2021 09:00:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2021\\u00a7r"}']}
{title:'Chowdhury et al. (§72021§r)', author: 'Shreyan Chowdhury; Verena Praher; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2106.07787", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTracing Back Music Emotion Predictions to Sound Sources and Intuitive Perceptual Qualities\\u00a7r\\n\\n\\u00a78\\u00a7oShreyan Chowdhury\\nVerena Praher\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.07787\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 16 Jun 2021 16:25:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of the 18th Sound and Music Computing Conference (SMC 2021)\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Jisi Zhang; Catalin Zorila; Rama Doddipatla; Jon Barker', display:{Lore:['[{"text": "arXiv:2106.07843", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTeacher-Student MixIT for Unsupervised and Semi-supervised Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oJisi Zhang\\nCatalin Zorila\\nRama Doddipatla\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.07843\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 9 Sep 2021 14:14:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021\\u00a7r"}']}
{title:'Ma et al. (§72021§r)', author: 'Zhizhong Ma; Chris Bullen; Joanna Ting Wai Chu; Ruili Wang; Yingchun Wang; Satwinder Singh', display:{Lore:['[{"text": "arXiv:2106.07874", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards the Objective Speech Assessment of Smoking Status based on Voice Features: A Review of the Literature\\u00a7r\\n\\n\\u00a78\\u00a7oZhizhong Ma\\nChris Bullen\\nJoanna Ting Wai Chu\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.07874\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Jun 2021 04:20:26 GMT)\\u00a7r"}']}
{title:'Tae et al. (§72021§r)', author: 'Jaesung Tae; Hyeongju Kim; Younggun Lee', display:{Lore:['[{"text": "arXiv:2106.07886", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMLP Singer: Towards Rapid Parallel Korean Singing Voice Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oJaesung Tae\\nHyeongju Kim\\nYounggun Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.07886\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/MLSP52302.2021.9596184\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2021 IEEE 31st International Workshop on Machine Learning for\\n  Signal Processing (MLSP)\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 20 Nov 2021 21:22:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 5 figures, 2 tables, IEEE MLSP 2021\\u00a7r"}']}
{title:'Xiao (§72021§r)', author: 'Runqiu Xiao', display:{Lore:['[{"text": "arXiv:2106.08004", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptive Margin Circle Loss for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oRunqiu Xiao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.08004\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Jun 2021 09:35:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2021\\u00a7r"}']}
{title:'Chen et al. (§72021§r)', author: 'Long Chen; Venkatesh Ravichandran; Andreas Stolcke', display:{Lore:['[{"text": "arXiv:2106.08207", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGraph-based Label Propagation for Semi-Supervised Speaker Identification\\u00a7r\\n\\n\\u00a78\\u00a7oLong Chen\\nVenkatesh Ravichandran\\nAndreas Stolcke\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.08207\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-1209\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech, Sept. 2021, pp. 4588-4592\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Jun 2021 15:10:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Interspeech 2021\\u00a7r"}']}
{title:'Illa et al. (§72021§r)', author: 'Marc Illa; Bence Mark Halpern; Rob van Son; Laureano Moro-Velazquez; Odette Scharenborg', display:{Lore:['[{"text": "arXiv:2106.08427", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPathological voice adaptation with autoencoder-based voice conversion\\u00a7r\\n\\n\\u00a78\\u00a7oMarc Illa\\nBence Mark Halpern\\nRob van Son\\nLaureano Moro-Velazquez\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.08427\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Jun 2021 20:38:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 3 figures. Accepted to the 11th ISCA Speech Synthesis Workshop (2021)\\u00a7r"}']}
{title:'Mathew (§72021§r)', author: 'Steve Mathew', display:{Lore:['[{"text": "arXiv:2106.08479", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTonal Frequencies, Consonance, Dissonance: A Math-Bio Intersection\\u00a7r\\n\\n\\u00a78\\u00a7oSteve Mathew\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.08479\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Jun 2021 09:36:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 1 figure, 1 table\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Kexun Zhang; Yi Ren; Changliang Xu; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2106.08507", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWSRGlow: A Glow-based Waveform Generative Model for Audio Super-Resolution\\u00a7r\\n\\n\\u00a78\\u00a7oKexun Zhang\\nYi Ren\\nChangliang Xu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.08507\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Jun 2021 01:37:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2021\\u00a7r"}']}
{title:'Chiu et al. (§72021§r)', author: 'Ching-Yu Chiu; Alvin Wen-Yu Su; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2106.08685", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDrum-Aware Ensemble Architecture for Improved Joint Musical Beat and Downbeat Tracking\\u00a7r\\n\\n\\u00a78\\u00a7oChing-Yu Chiu\\nAlvin Wen-Yu Su\\nYi-Hsuan Yang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.08685\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2021.3084504\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Jun 2021 10:47:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEESignal Processing Letters (May 2021)\\u00a7r"}']}
{title:'Chiu et al. (§72021§r)', author: 'Ching-Yu Chiu; Joann Ching; Wen-Yi Hsiao; Yu-Hua Chen; Alvin Wen-Yu Su; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2106.08703", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSource Separation-based Data Augmentation for Improved Joint Beat and Downbeat Tracking\\u00a7r\\n\\n\\u00a78\\u00a7oChing-Yu Chiu\\nJoann Ching\\nWen-Yi Hsiao\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.08703\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Jun 2021 11:09:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to European Signal Processing Conference (EUSIPCO 2021)\\u00a7r"}']}
{title:'Mottini et al. (§72021§r)', author: 'Alejandro Mottini; Jaime Lorenzo-Trueba; Sri Vishnu Kumar Karlapati; Thomas Drugman', display:{Lore:['[{"text": "arXiv:2106.08873", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoicy: Zero-Shot Non-Parallel Voice Conversion in Noisy Reverberant Environments\\u00a7r\\n\\n\\u00a78\\u00a7oAlejandro Mottini\\nJaime Lorenzo-Trueba\\nSri Vishnu Kumar Karlapati\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.08873\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Jun 2021 15:47:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at theSpeech Synthesis Workshops 2021 (SSW11)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Xiong Wang; Sining Sun; Lei Xie; Long Ma', display:{Lore:['[{"text": "arXiv:2106.09236", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Conformer with Prob-Sparse Attention Mechanism for End-to-EndSpeech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXiong Wang\\nSining Sun\\nLei Xie\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.09236\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Jun 2021 04:04:04 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Li Zhang; Qing Wang; Kong Aik Lee; Lei Xie; Haizhou Li', display:{Lore:['[{"text": "arXiv:2106.09320", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Level Transfer Learning from Near-Field to Far-Field Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oLi Zhang\\nQing Wang\\nKong Aik Lee\\nLei Xie\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.09320\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Jun 2021 08:37:29 GMT)\\u00a7r"}']}
{title:'Tzinis et al. (§72021§r)', author: 'Efthymios Tzinis; Scott Wisdom; Tal Remez; John R. Hershey', display:{Lore:['[{"text": "arXiv:2106.09669", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving On-Screen Sound Separation for Open-Domain Videos with Audio-Visual Self-Attention\\u00a7r\\n\\n\\u00a78\\u00a7oEfthymios Tzinis\\nScott Wisdom\\nTal Remez\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.09669\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Oct 2021 04:05:54 GMT)\\u00a7r"}']}
{title:'An et al. (§72021§r)', author: 'Xiaochun An; Frank K. Soong; Lei Xie', display:{Lore:['[{"text": "arXiv:2106.10003", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Performance of Seen and Unseen Speech Style Transfer in End-to-end Neural TTS\\u00a7r\\n\\n\\u00a78\\u00a7oXiaochun An\\nFrank K. Soong\\nLei Xie\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.10003\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Jun 2021 08:48:30 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Cong Zhang; Jian Zhu', display:{Lore:['[{"text": "arXiv:2106.10045", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynchronising speech segments with musical beats in Mandarin and English singing\\u00a7r\\n\\n\\u00a78\\u00a7oCong Zhang\\nJian Zhu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.10045\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-1841\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2021, 1199-1203 (2001)\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Jun 2021 10:32:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in the Proceeding of Interspeech 2021\\u00a7r"}']}
{title:'Du et al. (§72021§r)', author: 'Hongqiang Du; Lei Xie', display:{Lore:['[{"text": "arXiv:2106.10406", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving robustness of one-shot voice conversion with deep discriminative speaker encoder\\u00a7r\\n\\n\\u00a78\\u00a7oHongqiang Du\\nLei Xie\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.10406\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 19 Jun 2021 03:09:11 GMT)\\u00a7r"}']}
{title:'Al-Radhi et al. (§72021§r)', author: 'Mohammed Salah Al-Radhi; Tamás Gábor Csapó; Géza Németh', display:{Lore:['[{"text": "arXiv:2106.10481", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdvances in Speech Vocoding for Text-to-Speech with Continuous Parameters\\u00a7r\\n\\n\\u00a78\\u00a7oMohammed Salah Al-Radhi\\nTam\\u00e1s G\\u00e1bor Csap\\u00f3\\nG\\u00e9za N\\u00e9meth\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.10481\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 19 Jun 2021 12:05:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 3 figures, International Conference on Artificial Intelligence and Speech Technology(AIST2020)\\u00a7r"}']}
{title:'Ghahabi et al. (§72021§r)', author: 'Omid Ghahabi; Volker Fischer', display:{Lore:['[{"text": "arXiv:2106.11075", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEML Online Speech Activity Detection for the Fearless Steps Challenge Phase-III\\u00a7r\\n\\n\\u00a78\\u00a7oOmid Ghahabi\\nVolker Fischer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.11075\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Jun 2021 12:55:51 GMT)\\u00a7r"}']}
{title:'Izadi et al. (§72021§r)', author: 'Mohammad Rasool Izadi; Robert Stevenson; Laura N. Kloepper', display:{Lore:['[{"text": "arXiv:2106.11233", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAffinity Mixup for Weakly Supervised Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Rasool Izadi\\nRobert Stevenson\\nLaura N. Kloepper\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.11233\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Jun 2021 16:25:09 GMT)\\u00a7r"}']}
{title:'Kumar et al. (§72021§r)', author: 'Anurag Kumar; Yun Wang; Vamsi Krishna Ithapu; Christian Fuegen', display:{Lore:['[{"text": "arXiv:2106.11335", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDo sound event representations generalize to other audio tasks? A case study in audio transfer learning\\u00a7r\\n\\n\\u00a78\\u00a7oAnurag Kumar\\nYun Wang\\nVamsi Krishna Ithapu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.11335\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Jun 2021 18:04:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted Interspeech 2021\\u00a7r"}']}
{title:'Hou et al. (§72021§r)', author: 'Yuanbo Hou; Zhesong Yu; Xia Liang; Xingjian Du; Bilei Zhu; Zejun Ma; Dick Botteldooren', display:{Lore:['[{"text": "arXiv:2106.11411", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention-based cross-modal fusion for audio-visual voice activity detection in musical video streams\\u00a7r\\n\\n\\u00a78\\u00a7oYuanbo Hou\\nZhesong Yu\\nXia Liang\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.11411\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Jun 2021 20:59:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2021\\u00a7r"}']}
{title:'Huang et al. (§72021§r)', author: 'Kuan-Po Huang; Yuan-Kuei Wu; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2106.11713", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-accent Speech Separation with One Shot Learning\\u00a7r\\n\\n\\u00a78\\u00a7oKuan-Po Huang\\nYuan-Kuei Wu\\nHung-yi Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.11713\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 5 Aug 2021 06:00:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ACL 2021 Meta Learning forNLP\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Andong Li; Chengshi Zheng; Lu Zhang; Xiaodong Li', display:{Lore:['[{"text": "arXiv:2106.11730", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning to Inference with Early Exit in the Progressive Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oAndong Li\\nChengshi Zheng\\nLu Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.11730\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Jun 2021 13:07:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by EUSIPCO2021\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Andong Li; Chengshi Zheng; Lu Zhang; Xiaodong Li', display:{Lore:['[{"text": "arXiv:2106.11789", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGlance and Gaze: A Collaborative Learning Framework for Single-channel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oAndong Li\\nChengshi Zheng\\nLu Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.11789\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Jun 2021 13:58:08 GMT)\\u00a7r"}']}
{title:'Makishima et al. (§72021§r)', author: 'Naoki Makishima; Mana Ihori; Tomohiro Tanaka; Akihiko Takashima; Shota Orihashi; Ryo Masumura', display:{Lore:['[{"text": "arXiv:2106.12132", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnrollment-less training for personalized voice activity detection\\u00a7r\\n\\n\\u00a78\\u00a7oNaoki Makishima\\nMana Ihori\\nTomohiro Tanaka\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.12132\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 23 Jun 2021 02:53:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Andong Li; Wenzhe Liu; Xiaoxue Luo; Guochen Yu; Chengshi Zheng; Xiaodong Li', display:{Lore:['[{"text": "arXiv:2106.12743", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Simultaneous Denoising and Dereverberation Framework with Target Decoupling\\u00a7r\\n\\n\\u00a78\\u00a7oAndong Li\\nWenzhe Liu\\nXiaoxue Luo\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.12743\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Jun 2021 03:01:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2021\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Zheng Li; Yan Liu; Lin Li; Qingyang Hong', display:{Lore:['[{"text": "arXiv:2106.12851", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdditive Phoneme-aware Margin Softmax Loss for Language Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZheng Li\\nYan Liu\\nLin Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.12851\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Jun 2021 09:24:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2021\\u00a7r"}']}
{title:'Shah et al. (§72021§r)', author: 'Raahil Shah; Kamil Pokora; Abdelhamid Ezzerg; Viacheslav Klimkov; Goeric Huybrechts; Bartosz Putrycz; Daniel Korzekwa; Thomas Merritt', display:{Lore:['[{"text": "arXiv:2106.12896", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-Autoregressive TTS with Explicit Duration Modelling for Low-Resource Highly Expressive Speech\\u00a7r\\n\\n\\u00a78\\u00a7oRaahil Shah\\nKamil Pokora\\nAbdelhamid Ezzerg\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.12896\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 25 Jun 2021 18:25:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 5 figures. Accepted to Speech Synthesis Workshop (SSW) 2021\\u00a7r"}']}
{title:'Müller et al. (§72021§r)', author: 'Nicolas M. Müller; Franziska Dieckmann; Pavel Czempin; Roman Canals; Konstantin Böttinger; Jennifer Williams', display:{Lore:['[{"text": "arXiv:2106.12914", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech is Silver, Silence is Golden: What do ASVspoof-trained Models Really Learn?\\u00a7r\\n\\n\\u00a78\\u00a7oNicolas M. M\\u00fcller\\nFranziska Dieckmann\\nPavel Czempin\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.12914\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nASVspoof 2021 Workshop\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 28 Sep 2021 09:06:33 GMT)\\u00a7r"}']}
{title:'Barumerli et al. (§72021§r)', author: 'Roberto Barumerli; Daniele Bianchi; Michele Geronazzo; Federico Avanzini', display:{Lore:['[{"text": "arXiv:2106.12992", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSofaMyRoom: a fast and multiplatform \\"shoebox\\" room simulator for binaural room impulse response dataset generation\\u00a7r\\n\\n\\u00a78\\u00a7oRoberto Barumerli\\nDaniele Bianchi\\nMichele Geronazzo\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.12992\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Jun 2021 13:07:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages,4 figures, accompanying paper for an acoustic simulator description\\u00a7r"}']}
{title:'Guzhov et al. (§72021§r)', author: 'Andrey Guzhov; Federico Raue; Jörn Hees; Andreas Dengel', display:{Lore:['[{"text": "arXiv:2106.13043", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudioCLIP: Extending CLIP to Image, Text and Audio\\u00a7r\\n\\n\\u00a78\\u00a7oAndrey Guzhov\\nFederico Raue\\nJ\\u00f6rn Hees\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.13043\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Jun 2021 14:16:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to GCPR 2021\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Zhengxi Liu; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2106.13419", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBasis-MelGAN: Efficient Neural Vocoder Based on Audio Decomposition\\u00a7r\\n\\n\\u00a78\\u00a7oZhengxi Liu\\nYanmin Qian\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.13419\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Jun 2021 04:18:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Luong et al. (§72021§r)', author: 'Hieu-Thi Luong; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2106.13479", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPreliminary study on using vector quantization latent spaces for TTS/VC systems with consistent performance\\u00a7r\\n\\n\\u00a78\\u00a7oHieu-Thi Luong\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.13479\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Jun 2021 07:51:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oto be presented at SSW11\\u00a7r"}']}
{title:'Ivry et al. (§72021§r)', author: 'Amir Ivry; Israel Cohen; Baruch Berdugo', display:{Lore:['[{"text": "arXiv:2106.13511", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluation of Deep-Learning-Based Voice Activity Detectors and Room Impulse Response Models in Reverberant Environments\\u00a7r\\n\\n\\u00a78\\u00a7oAmir Ivry\\nIsrael Cohen\\nBaruch Berdugo\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.13511\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP40776.2020.9054610\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Jun 2021 09:05:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2020\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Yan Liu; Zheng Li; Lin Li; Qingyang Hong', display:{Lore:['[{"text": "arXiv:2106.13514", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhoneme-aware and Channel-wise Attentive Learning for Text DependentSpeaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oYan Liu\\nZheng Li\\nLin Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.13514\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Jun 2021 09:11:18 GMT)\\u00a7r"}']}
{title:'Ivry et al. (§72021§r)', author: 'Amir Ivry; Israel Cohen; Baruch Berdugo', display:{Lore:['[{"text": "arXiv:2106.13531", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Residual Echo Suppression with A Tunable Tradeoff Between Signal Distortion and Echo Suppression\\u00a7r\\n\\n\\u00a78\\u00a7oAmir Ivry\\nIsrael Cohen\\nBaruch Berdugo\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.13531\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9414958\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7npp. 126--130, year 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Jun 2021 09:49:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Ivry et al. (§72021§r)', author: 'Amir Ivry; Israel Cohen; Baruch Berdugo', display:{Lore:['[{"text": "arXiv:2106.13754", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNonlinear Acoustic Echo Cancellation with Deep Learning\\u00a7r\\n\\n\\u00a78\\u00a7oAmir Ivry\\nIsrael Cohen\\nBaruch Berdugo\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.13754\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Jun 2021 16:44:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021\\u00a7r"}']}
{title:'Ivry et al. (§72021§r)', author: 'Amir Ivry; Baruch Berdugo; Israel Cohen', display:{Lore:['[{"text": "arXiv:2106.13763", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Activity Detection for Transient Noisy Environment Based on Diffusion Nets\\u00a7r\\n\\n\\u00a78\\u00a7oAmir Ivry\\nBaruch Berdugo\\nIsrael Cohen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.13763\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JSTSP.2019.2909472\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nvolume 13, number 2, pp. 254--264, year 2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Jun 2021 17:05:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEjournal of selected topics in signal processing 2019\\u00a7r"}']}
{title:'Lin et al. (§72021§r)', author: 'Qingjian Lin; Lin Yang; Xuyang Wang; Luyuan Xie; Chen Jia; Junjie Wang', display:{Lore:['[{"text": "arXiv:2106.14371", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSparsely Overlapped Speech Training in the Time Domain: Joint Learning of Target Speech Separation and Personal VAD Benefits\\u00a7r\\n\\n\\u00a78\\u00a7oQingjian Lin\\nLin Yang\\nXuyang Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.14371\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Sep 2021 03:23:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by APSIPA 2021\\u00a7r"}']}
{title:'Han et al. (§72021§r)', author: 'Jing Han; Tong Xia; Dimitris Spathis; Erika Bondareva; Chloë Brown; Jagmohan Chauhan; Ting Dang; Andreas Grammenos; Apinan Hasthanasombat; Andres Floto; Pietro Cicuta; Cecilia Mascolo', display:{Lore:['[{"text": "arXiv:2106.15523", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSounds of COVID-19: exploring realistic performance of audio-based digital testing\\u00a7r\\n\\n\\u00a78\\u00a7oJing Han\\nTong Xia\\nDimitris Spathis\\n+ 8 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.15523\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Jun 2021 15:50:36 GMT)\\u00a7r"}']}
{title:'Fernando et al. (§72021§r)', author: 'Tharindu Fernando; Sridha Sridharan; Simon Denman; Houman Ghaemmaghami; Clinton Fookes', display:{Lore:['[{"text": "arXiv:2106.15835", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust and Interpretable Temporal Convolution Network for Event Detection in Lung Sound Recordings\\u00a7r\\n\\n\\u00a78\\u00a7oTharindu Fernando\\nSridha Sridharan\\nSimon Denman\\nHouman Ghaemmaghami\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.15835\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Jun 2021 06:36:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7opreprint submitted to JBHI\\u00a7r"}']}
{title:'Hládek et al. (§72021§r)', author: 'Ľuboš Hládek; Stephan D. Ewert; Bernhard U. Seeber', display:{Lore:['[{"text": "arXiv:2106.15916", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCommunication conditions in virtual acoustic scenes in an underground station\\u00a7r\\n\\n\\u00a78\\u00a7o\\u013dubo\\u0161 Hl\\u00e1dek\\nStephan D. Ewert\\nBernhard U. Seeber\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.15916\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 2 Nov 2021 15:30:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oI3DA conference paper, 8 figures, 9 pages\\u00a7r"}']}
{title:'Verma et al. (§72021§r)', author: 'Prateek Verma; Chris Chafe', display:{Lore:['[{"text": "arXiv:2106.16036", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Generative Model for Raw Audio Using Transformer Architectures\\u00a7r\\n\\n\\u00a78\\u00a7oPrateek Verma\\nChris Chafe\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.16036\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 8 Jul 2021 15:28:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDAFX 2021\\u00a7r"}']}
{title:'Liang et al. (§72021§r)', author: 'Chengdong Liang; Junqi Chen; Shanzheng Guan; Xiao-Lei Zhang', display:{Lore:['[{"text": "arXiv:2107.00178", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention-based multi-channel speaker verification with ad-hoc microphone arrays\\u00a7r\\n\\n\\u00a78\\u00a7oChengdong Liang\\nJunqi Chen\\nShanzheng Guan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.00178\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jul 2021 02:12:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to APSIPA ASC 2021\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Bochen Li; Yuxuan Wang; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2107.00231", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudiovisual Singing Voice Separation\\u00a7r\\n\\n\\u00a78\\u00a7oBochen Li\\nYuxuan Wang\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.00231\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jul 2021 06:04:53 GMT)\\u00a7r"}']}
{title:'Sharma et al. (§72021§r)', author: 'Bidisha Sharma; S. R. Mahadeva Prasanna', display:{Lore:['[{"text": "arXiv:2107.00297", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSonority Measurement Using System, Source, and Suprasegmental Information\\u00a7r\\n\\n\\u00a78\\u00a7oBidisha Sharma\\nS. R. Mahadeva Prasanna\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.00297\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2016.2641901\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing (\\n  Volume: 25, Issue: 3, March 2017)\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jul 2021 08:31:09 GMT)\\u00a7r"}']}
{title:'Halpern et al. (§72021§r)', author: 'Bence Mark Halpern; Julian Fritsch; Enno Hermann; Rob van Son; Odette Scharenborg; Mathew Magimai. -Doss', display:{Lore:['[{"text": "arXiv:2107.00308", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Objective Evaluation Framework for Pathological Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oBence Mark Halpern\\nJulian Fritsch\\nEnno Hermann\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.00308\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jul 2021 08:55:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 4 figures. Accepted to the ITG Conference on Speech Communication | 29.09.2021 - 01.10.2021| Kiel\\u00a7r"}']}
{title:'Fonseca et al. (§72021§r)', author: 'Eduardo Fonseca; Andres Ferraro; Xavier Serra', display:{Lore:['[{"text": "arXiv:2107.00623", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Sound Event Classification by Increasing Shift Invariance in Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oEduardo Fonseca\\nAndres Ferraro\\nXavier Serra\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.00623\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 22 Jul 2021 15:59:21 GMT)\\u00a7r"}']}
{title:'Han et al. (§72021§r)', author: 'Tao Han; Hantao Huang; Ziang Yang; Wei Han', display:{Lore:['[{"text": "arXiv:2107.00921", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSupervised Contrastive Learning for Accented Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oTao Han\\nHantao Huang\\nZiang Yang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.00921\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Jul 2021 09:23:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccented speech recognition, deep neural networks, model adaptation, supervised contrastive learning\\u00a7r"}']}
{title:'Pavlichenko et al. (§72021§r)', author: 'Nikita Pavlichenko; Ivan Stelmakh; Dmitry Ustalov', display:{Lore:['[{"text": "arXiv:2107.01091", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCrowdSpeech and VoxDIY: Benchmark Datasets for Crowdsourced Audio Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oNikita Pavlichenko\\nIvan Stelmakh\\nDmitry Ustalov\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.01091\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Oct 2021 08:09:06 GMT)\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Zhuo Li; Ce Fang; Runqiu Xiao; Zhigao Chen; Wenchao Wang; Yonghong Yan', display:{Lore:['[{"text": "arXiv:2107.01329", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe HCCL Speaker Verification System for Far-Field Speaker Verification Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oZhuo Li\\nCe Fang\\nRunqiu Xiao\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.01329\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 3 Jul 2021 03:04:18 GMT)\\u00a7r"}']}
{title:'Rittikar (§72021§r)', author: 'Sujay Uday Rittikar', display:{Lore:['[{"text": "arXiv:2107.01462", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDevelopment of a Conversation State Prediction System\\u00a7r\\n\\n\\u00a78\\u00a7oSujay Uday Rittikar\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.01462\\u00a7r\\n\\nVersion:\\u00a77v4 (Sat, 11 Dec 2021 15:50:29 GMT)\\u00a7r"}']}
{title:'Xue et al. (§72021§r)', author: 'Lanqing Xue; Kaitao Song; Duocai Wu; Xu Tan; Nevin L. Zhang; Tao Qin; Wei-Qiang Zhang; Tie-Yan Liu', display:{Lore:['[{"text": "arXiv:2107.01875", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oLanqing Xue\\nKaitao Song\\nDuocai Wu\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.01875\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Jul 2021 09:01:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ACL 2021 main conference\\u00a7r"}']}
{title:'Yan et al. (§72021§r)', author: 'Yuzi Yan; Xu Tan; Bohan Li; Guangyan Zhang; Tao Qin; Sheng Zhao; Yuan Shen; Wei-Qiang Zhang; Tie-Yan Liu', display:{Lore:['[{"text": "arXiv:2107.02530", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaSpeech 3: Adaptive Text to Speech for Spontaneous Style\\u00a7r\\n\\n\\u00a78\\u00a7oYuzi Yan\\nXu Tan\\nBohan Li\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.02530\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Jul 2021 10:40:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2021\\u00a7r"}']}
{title:'Kim et al. (§72021§r)', author: 'Nam Kyun Kim; Hong Kook Kim', display:{Lore:['[{"text": "arXiv:2107.02569", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-training with noisy student model and semi-supervised loss function for dcase 2021 challenge task 4\\u00a7r\\n\\n\\u00a78\\u00a7oNam Kyun Kim\\nHong Kook Kim\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.02569\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Jul 2021 12:11:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, DCASE 2021 challenge Task 4 technical report\\u00a7r"}']}
{title:'Pascual et al. (§72021§r)', author: 'Santiago Pascual; Joan Serrà; Jordi Pons', display:{Lore:['[{"text": "arXiv:2107.03100", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Auto-Encoding for Packet Loss Concealment\\u00a7r\\n\\n\\u00a78\\u00a7oSantiago Pascual\\nJoan Serr\\u00e0\\nJordi Pons\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.03100\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 8 Jul 2021 09:10:04 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Fangyuan Wang; Zhigang Song; Hongchen Jiang; Bo Xu', display:{Lore:['[{"text": "arXiv:2107.03104", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMACCIF-TDNN: Multi aspect aggregation of channel and context interdependence features in TDNN-based speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oFangyuan Wang\\nZhigang Song\\nHongchen Jiang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.03104\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Jul 2021 09:43:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages. arXiv admin note:text overlap with arXiv:2005.07143 by other authors\\u00a7r"}']}
{title:'Lu et al. (§72021§r)', author: 'Hui Lu; Zhiyong Wu; Xixin Wu; Xu Li; Shiyin Kang; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2107.03298", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVAENAR-TTS: Variational Auto-Encoder based Non-AutoRegressive Text-to-Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oHui Lu\\nZhiyong Wu\\nXixin Wu\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.03298\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Jul 2021 15:32:36 GMT)\\u00a7r"}']}
{title:'Zeghidour et al. (§72021§r)', author: 'Neil Zeghidour; Alejandro Luebs; Ahmed Omran; Jan Skoglund; Marco Tagliasacchi', display:{Lore:['[{"text": "arXiv:2107.03312", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSoundStream: An End-to-End Neural Audio Codec\\u00a7r\\n\\n\\u00a78\\u00a7oNeil Zeghidour\\nAlejandro Luebs\\nAhmed Omran\\nJan Skoglund\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.03312\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Jul 2021 15:45:42 GMT)\\u00a7r"}']}
{title:'Fenaux et al. (§72021§r)', author: 'Lucas Fenaux; Maria Juliana Quintero', display:{Lore:['[{"text": "arXiv:2107.03443", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBumbleBee: A Transformer for Music\\u00a7r\\n\\n\\u00a78\\u00a7oLucas Fenaux\\nMaria Juliana Quintero\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.03443\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Jul 2021 19:08:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 3 figures\\u00a7r"}']}
{title:'Donley et al. (§72021§r)', author: 'Jacob Donley; Vladimir Tourbabin; Jung-Suk Lee; Mark Broyles; Hao Jiang; Jie Shen; Maja Pantic; Vamsi Krishna Ithapu; Ravish Mehra', display:{Lore:['[{"text": "arXiv:2107.04174", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEasyCom: An Augmented Reality Dataset to Support Algorithms for Easy Communication in Noisy Environments\\u00a7r\\n\\n\\u00a78\\u00a7oJacob Donley\\nVladimir Tourbabin\\nJung-Suk Lee\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.04174\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 18 Oct 2021 22:37:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDataset is available at: https://github.com/facebookresearch/EasyComDataset\\u00a7r"}']}
{title:'Hsu et al. (§72021§r)', author: 'Fu-Shun Hsu; Shang-Ran Huang; Chien-Wen Huang; Chun-Chieh Chen; Yuan-Ren Cheng; Feipei Lai', display:{Lore:['[{"text": "arXiv:2107.04226", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-path Convolutional Neural Networks Efficiently Improve Feature Extraction in Continuous Adventitious Lung Sound Detection\\u00a7r\\n\\n\\u00a78\\u00a7oFu-Shun Hsu\\nShang-Ran Huang\\nChien-Wen Huang\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.04226\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Jul 2021 05:55:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be submitted, 32 pages, 8 figures, 2 tables\\u00a7r"}']}
{title:'Si et al. (§72021§r)', author: 'Shijing Si; Jianzong Wang; Huiming Sun; Jianhan Wu; Chuanyao Zhang; Xiaoyang Qu; Ning Cheng; Lei Chen; Jing Xiao', display:{Lore:['[{"text": "arXiv:2107.04803", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVariational Information Bottleneck for Effective Low-resource Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oShijing Si\\nJianzong Wang\\nHuiming Sun\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.04803\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 10 Jul 2021 09:44:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by InterSpeech 2021\\u00a7r"}']}
{title:'Si et al. (§72021§r)', author: 'Shijing Si; Jianzong Wang; Xiaoyang Qu; Ning Cheng; Wenqi Wei; Xinghua Zhu; Jing Xiao', display:{Lore:['[{"text": "arXiv:2107.04806", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech2Video: Cross-Modal Distillation for Speech to Video Generation\\u00a7r\\n\\n\\u00a78\\u00a7oShijing Si\\nJianzong Wang\\nXiaoyang Qu\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.04806\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 10 Jul 2021 10:27:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by InterSpeech2021\\u00a7r"}']}
{title:'Conde et al. (§72021§r)', author: 'Marcos V. Conde; Kumar Shubham; Prateek Agnihotri; Nitin D. Movva; Szilard Bessenyei', display:{Lore:['[{"text": "arXiv:2107.04878", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeakly-Supervised Classification and Detection of Bird Sounds in the Wild. A BirdCLEF 2021 Solution\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos V. Conde\\nKumar Shubham\\nPrateek Agnihotri\\nNitin D. Movva\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.04878\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 10 Jul 2021 17:11:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings Working Notes CEURWS @CLEF 2021 - BirdCLEF 2021\\u00a7r"}']}
{title:'Cheuk et al. (§72021§r)', author: 'Kin Wai Cheuk; Dorien Herremans; Li Su', display:{Lore:['[{"text": "arXiv:2107.04954", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data\\u00a7r\\n\\n\\u00a78\\u00a7oKin Wai Cheuk\\nDorien Herremans\\nLi Su\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.04954\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 29 Jul 2021 04:49:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ACMMM 21. Camera ready version\\u00a7r"}']}
{title:'Lee et al. (§72021§r)', author: 'Kyungyun Lee; Wonil Kim; Juhan Nam', display:{Lore:['[{"text": "arXiv:2107.05009", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPocketVAE: A Two-step Model for Groove Generation and Control\\u00a7r\\n\\n\\u00a78\\u00a7oKyungyun Lee\\nWonil Kim\\nJuhan Nam\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.05009\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 11 Jul 2021 10:26:00 GMT)\\u00a7r"}']}
{title:'Hayes et al. (§72021§r)', author: 'Ben Hayes; Charalampos Saitis; György Fazekas', display:{Lore:['[{"text": "arXiv:2107.05050", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Waveshaping Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oBen Hayes\\nCharalampos Saitis\\nGy\\u00f6rgy Fazekas\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.05050\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 27 Jul 2021 14:28:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ISMIR 2021; See online supplement at https://benhayes.net/projects/nws/\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Jing Li; Binling Wang; Yiming Zhi; Zheng Li; Lin Li; Qingyang Hong; Dong Wang', display:{Lore:['[{"text": "arXiv:2107.05365", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOriental Language Recognition (OLR) 2020: Summary and Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oJing Li\\nBinling Wang\\nYiming Zhi\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.05365\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Jul 2021 12:42:40 GMT)\\u00a7r"}']}
{title:'Le et al. (§72021§r)', author: 'Xiaohuai Le; Hongsheng Chen; Kai Chen; Jing Lu', display:{Lore:['[{"text": "arXiv:2107.05429", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDPCRN: Dual-Path Convolution Recurrent Network for Single Channel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oXiaohuai Le\\nHongsheng Chen\\nKai Chen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.05429\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 12 Jul 2021 13:50:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, accepted by Interspeech 2021\\u00a7r"}']}
{title:'Valenti et al. (§72021§r)', author: 'Andrea Valenti; Stefano Berti; Davide Bacciu', display:{Lore:['[{"text": "arXiv:2107.05546", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCalliope \\u2013 A Polyphonic Music Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oAndrea Valenti\\nStefano Berti\\nDavide Bacciu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.05546\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Jul 2021 08:18:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ESANN2021\\u00a7r"}']}
{title:'Castellon et al. (§72021§r)', author: 'Rodrigo Castellon; Chris Donahue; Percy Liang', display:{Lore:['[{"text": "arXiv:2107.05677", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCodified audio language modeling learns useful representations for music information retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oRodrigo Castellon\\nChris Donahue\\nPercy Liang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.05677\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 12 Jul 2021 18:28:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in the proceedings of ISMIR 2021\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Shengqiang Li; Menglong Xu; Xiao-Lei Zhang', display:{Lore:['[{"text": "arXiv:2107.05907", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConformer-based End-to-end Speech Recognition With Rotary Position Embedding\\u00a7r\\n\\n\\u00a78\\u00a7oShengqiang Li\\nMenglong Xu\\nXiao-Lei Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.05907\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Jul 2021 08:07:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Dong et al. (§72021§r)', author: 'Hao-Wen Dong; Chris Donahue; Taylor Berg-Kirkpatrick; Julian McAuley', display:{Lore:['[{"text": "arXiv:2107.05916", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Automatic Instrumentation by Learning to Separate Parts in Symbolic Multitrack Music\\u00a7r\\n\\n\\u00a78\\u00a7oHao-Wen Dong\\nChris Donahue\\nTaylor Berg-Kirkpatrick\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.05916\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 21 Oct 2021 08:04:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oISMIR 2021 camera ready\\u00a7r"}']}
{title:'Hadjeres et al. (§72021§r)', author: 'Gaëtan Hadjeres; Léopold Crestel', display:{Lore:['[{"text": "arXiv:2107.05944", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Piano Inpainting Application\\u00a7r\\n\\n\\u00a78\\u00a7oGa\\u00ebtan Hadjeres\\nL\\u00e9opold Crestel\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.05944\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Jul 2021 09:33:11 GMT)\\u00a7r"}']}
{title:'Hernandez-Olivan et al. (§72021§r)', author: 'Carlos Hernandez-Olivan; Jose R. Beltran', display:{Lore:['[{"text": "arXiv:2107.06231", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTimbre Classification of Musical Instruments with a Deep Learning Multi-Head Attention-Based Model\\u00a7r\\n\\n\\u00a78\\u00a7oCarlos Hernandez-Olivan\\nJose R. Beltran\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.06231\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Jul 2021 16:34:19 GMT)\\u00a7r"}']}
{title:'Aggarwal et al. (§72021§r)', author: 'Gunjan Aggarwal; Devi Parikh', display:{Lore:['[{"text": "arXiv:2107.06252", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDance2Music: Automatic Dance-driven Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oGunjan Aggarwal\\nDevi Parikh\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.06252\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 20 Jul 2021 09:20:29 GMT)\\u00a7r"}']}
{title:'Zhu et al. (§72021§r)', author: 'Hongning Zhu; Kong Aik Lee; Haizhou Li', display:{Lore:['[{"text": "arXiv:2107.06493", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSerialized Multi-Layer Multi-Head Attention for Neural Speaker Embedding\\u00a7r\\n\\n\\u00a78\\u00a7oHongning Zhu\\nKong Aik Lee\\nHaizhou Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.06493\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Jul 2021 05:38:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2021\\u00a7r"}']}
{title:'Hohmann (§72021§r)', author: 'Volker Hohmann', display:{Lore:['[{"text": "arXiv:2107.06645", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Period-Modulated Harmonic Locked Loop (PM-HLL): A low-effort algorithm for rapid time-domain multi-periodicity estimation\\u00a7r\\n\\n\\u00a78\\u00a7oVolker Hohmann\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.06645\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1051/aacus/2021050\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 Nov 2021 17:58:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint (2nd version), accepted for publication at acta acustica\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Zhong-Qiu Wang; DeLiang Wang', display:{Lore:['[{"text": "arXiv:2107.06853", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLocalization Based Sequential Grouping for Continuous Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oZhong-Qiu Wang\\nDeLiang Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.06853\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Jul 2021 17:18:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure\\u00a7r"}']}
{title:'Garcia et al. (§72021§r)', author: 'Hugo Flores Garcia; Aldo Aguilar; Ethan Manilow; Bryan Pardo', display:{Lore:['[{"text": "arXiv:2107.07029", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Hierarchical Structures for Few-Shot Musical Instrument Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHugo Flores Garcia\\nAldo Aguilar\\nEthan Manilow\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.07029\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 29 Jul 2021 21:58:30 GMT)\\u00a7r"}']}
{title:'Ivry et al. (§72021§r)', author: 'Amir Ivry; Israel Cohen; Baruch Berdugo', display:{Lore:['[{"text": "arXiv:2107.07471", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lObjective Metrics to Evaluate Residual-Echo Suppression During Double-Talk\\u00a7r\\n\\n\\u00a78\\u00a7oAmir Ivry\\nIsrael Cohen\\nBaruch Berdugo\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.07471\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Jul 2021 17:17:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to WASPAA\\u00a7r"}']}
{title:'Henkel et al. (§72021§r)', author: 'Christof Henkel; Pascal Pfeiffer; Philipp Singer', display:{Lore:['[{"text": "arXiv:2107.07728", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRecognizing bird species in diverse soundscapes under weak supervision\\u00a7r\\n\\n\\u00a78\\u00a7oChristof Henkel\\nPascal Pfeiffer\\nPhilipp Singer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.07728\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 16 Jul 2021 06:54:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAll authors contributed equally, 8 pages, 4 figures, submitted to CEUR-WS\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Hang Li; Yu Kang; Yang Hao; Wenbiao Ding; Zhongqin Wu; Zitao Liu', display:{Lore:['[{"text": "arXiv:2107.07956", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Multimodal Machine Learning Framework for Teacher Vocal Delivery Evaluation\\u00a7r\\n\\n\\u00a78\\u00a7oHang Li\\nYu Kang\\nYang Hao\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.07956\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Jul 2021 05:09:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAIED\'21: The 22nd International Conference on Artificial Intelligence in Education, 2021\\u00a7r"}']}
{title:'Berg et al. (§72021§r)', author: 'Jan Berg; Konstantinos Drossos', display:{Lore:['[{"text": "arXiv:2107.08028", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContinual Learning for Automated Audio Captioning Using The Learning Without Forgetting Approach\\u00a7r\\n\\n\\u00a78\\u00a7oJan Berg\\nKonstantinos Drossos\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.08028\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 16 Jul 2021 17:44:26 GMT)\\u00a7r"}']}
{title:'He et al. (§72021§r)', author: 'Xiangheng He; Junjie Chen; Georgios Rizos; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2107.08361", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Improved StarGAN for Emotional Voice Conversion: Enhancing Voice Quality and Data Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oXiangheng He\\nJunjie Chen\\nGeorgios Rizos\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.08361\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 18 Jul 2021 04:28:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2021\\u00a7r"}']}
{title:'Chin et al. (§72021§r)', author: 'Daniel Chin; Gus Xia', display:{Lore:['[{"text": "arXiv:2107.08727", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMeasuring a Six-hole Recorder Flute\'s Response to Breath Pressure Variations and Fitting a Model\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Chin\\nGus Xia\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.08727\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Jul 2021 09:52:57 GMT)\\u00a7r"}']}
{title:'Koutini et al. (§72021§r)', author: 'Khaled Koutini; Hamid Eghbal-zadeh; Florian Henkel; Jan Schlüter; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2107.08933", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOver-Parameterization and Generalization in Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oKhaled Koutini\\nHamid Eghbal-zadeh\\nFlorian Henkel\\nJan Schl\\u00fcter\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.08933\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Jul 2021 14:48:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at theICML 2021 Workshop on Overparameterization: Pitfalls Opportunities\\u00a7r"}']}
{title:'Hawthorne et al. (§72021§r)', author: 'Curtis Hawthorne; Ian Simon; Rigel Swavely; Ethan Manilow; Jesse Engel', display:{Lore:['[{"text": "arXiv:2107.09142", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSequence-to-Sequence Piano Transcription with Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oCurtis Hawthorne\\nIan Simon\\nRigel Swavely\\nEthan Manilow\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.09142\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Jul 2021 20:33:09 GMT)\\u00a7r"}']}
{title:'de Souza et al. (§72021§r)', author: 'Mila Soares de Oliveira de Souza; Pedro Nuno de Souza Moura; Jean-Pierre Briot', display:{Lore:['[{"text": "arXiv:2107.09208", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Tempo Estimation via Neural Networks \\u2013 A Comparative Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oMila Soares de Oliveira de Souza\\nPedro Nuno de Souza Moura\\nJean-Pierre Briot\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.09208\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Jul 2021 00:29:28 GMT)\\u00a7r"}']}
{title:'Pham (§72021§r)', author: 'Lam Pham', display:{Lore:['[{"text": "arXiv:2107.09268", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Deep Learning Frameworks for Acoustic Scene and Respiratory Sound Classification\\u00a7r\\n\\n\\u00a78\\u00a7oLam Pham\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.09268\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Jul 2021 05:41:00 GMT)\\u00a7r"}']}
{title:'Shu et al. (§72021§r)', author: 'Xiaofeng Shu; Yehang Zhu; Yanjie Chen; Li Chen; Haohe Liu; Chuanzeng Huang; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2107.09298", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Echo Cancellation and Noise Suppression based on Cascaded Magnitude and Complex Mask Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oXiaofeng Shu\\nYehang Zhu\\nYanjie Chen\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.09298\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Jul 2021 07:27:06 GMT)\\u00a7r"}']}
{title:'Vrysis et al. (§72021§r)', author: 'Lazaros Vrysis; Iordanis Thoidis; Charalampos Dimoulas; George Papanikolaou', display:{Lore:['[{"text": "arXiv:2107.09311", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPERSA+: A Deep Learning Front-End for Context-Agnostic Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oLazaros Vrysis\\nIordanis Thoidis\\nCharalampos Dimoulas\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.09311\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Jul 2021 08:03:04 GMT)\\u00a7r"}']}
{title:'Zheng et al. (§72021§r)', author: 'Siqi Zheng; Weilong Huang; Xianliang Wang; Hongbin Suo; Jinwei Feng; Zhijie Yan', display:{Lore:['[{"text": "arXiv:2107.09321", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Real-time Speaker Diarization System Based on Spatial Spectrum\\u00a7r\\n\\n\\u00a78\\u00a7oSiqi Zheng\\nWeilong Huang\\nXianliang Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.09321\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9413544\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Jul 2021 08:25:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP);\\u00a7r"}']}
{title:'Sudarsanam et al. (§72021§r)', author: 'Parthasaarathy Sudarsanam; Archontis Politis; Konstantinos Drossos', display:{Lore:['[{"text": "arXiv:2107.09388", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAssessment of Self-Attention on Learned Features For Sound Event Localization and Detection\\u00a7r\\n\\n\\u00a78\\u00a7oParthasaarathy Sudarsanam\\nArchontis Politis\\nKonstantinos Drossos\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.09388\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Sep 2021 09:32:34 GMT)\\u00a7r"}']}
{title:'Huang et al. (§72021§r)', author: 'Wen-Chin Huang; Tomoki Hayashi; Xinjian Li; Shinji Watanabe; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2107.09477", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Prosody Modeling for ASR+TTS based Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oWen-Chin Huang\\nTomoki Hayashi\\nXinjian Li\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.09477\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Jul 2021 13:30:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ASRU2021. Under review\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Ning Zhang; Junchi Yan', display:{Lore:['[{"text": "arXiv:2107.09877", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMelody Structure Transfer Network: Generating Music with Separable Self-Attention\\u00a7r\\n\\n\\u00a78\\u00a7oNing Zhang\\nJunchi Yan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.09877\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Jul 2021 04:38:15 GMT)\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Yinghao Aaron Li; Ali Zare; Nima Mesgarani', display:{Lore:['[{"text": "arXiv:2107.10394", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStarGANv2-VC: A Diverse, Unsupervised, Non-parallel Framework for Natural-Sounding Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oYinghao Aaron Li\\nAli Zare\\nNima Mesgarani\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.10394\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 23 Jul 2021 01:08:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2021\\u00a7r"}']}
{title:'Fernandez et al. (§72021§r)', author: 'Andres Fernandez; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:2107.10880", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.CO\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing UMAP to Inspect Audio Data for Unsupervised Anomaly Detection under Domain-Shift Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oAndres Fernandez\\nMark D. Plumbley\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.10880\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 15 Oct 2021 19:00:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the DCASE2021 Workshop\\u00a7r"}']}
{title:'Grumiaux et al. (§72021§r)', author: 'Pierre-Amaury Grumiaux; Srdan Kitic; Prerak Srivastava; Laurent Girin; Alexandre Guérin', display:{Lore:['[{"text": "arXiv:2107.11066", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSALADnet: Self-Attentive multisource Localization in the Ambisonics Domain\\u00a7r\\n\\n\\u00a78\\u00a7oPierre-Amaury Grumiaux\\nSrdan Kitic\\nPrerak Srivastava\\nLaurent Girin\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.11066\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Jul 2021 08:14:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Workshop on Applications of Signal Processing to Audio and Acoustics\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Quandong Wang; Junnan Wu; Zhao Yan; Sichong Qian; Liyong Guo; Lichun Fan; Weiji Zhuang; Peng Gao; Yujun Wang', display:{Lore:['[{"text": "arXiv:2107.11222", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-channel Speech Enhancement with 2-D Convolutional Time-frequency Domain Features and a Pre-trained Acoustic Model\\u00a7r\\n\\n\\u00a78\\u00a7oQuandong Wang\\nJunnan Wu\\nZhao Yan\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.11222\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 24 Sep 2021 08:24:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 3 figures, accepted to APSIPA 2021, revised\\u00a7r"}']}
{title:'Marmoret et al. (§72021§r)', author: 'Axel Marmoret; Nancy Bertin; Jeremy Cohen', display:{Lore:['[{"text": "arXiv:2107.11250", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Channel Automatic Music Transcription Using Tensor Algebra\\u00a7r\\n\\n\\u00a78\\u00a7oAxel Marmoret\\nNancy Bertin\\nJeremy Cohen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.11250\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Jul 2021 14:07:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o40 pages, 14 figues, 5 tables, code can be found at: https://gitlab.inria.fr/amarmore/nonnegative-factorization\\u00a7r"}']}
{title:'Nordby et al. (§72021§r)', author: 'Jon Nordby; Fabian Nemazi; Dag Rieber', display:{Lore:['[{"text": "arXiv:2107.11453", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Detection Of Noise Events at Shooting Range Using Machine Learning\\u00a7r\\n\\n\\u00a78\\u00a7oJon Nordby\\nFabian Nemazi\\nDag Rieber\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.11453\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Jul 2021 20:36:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at 27thInternational Congressof Sound and Vibration (ICSV27)\\u00a7r"}']}
{title:'Raghuvanshi (§72021§r)', author: 'Nikunj Raghuvanshi', display:{Lore:['[{"text": "arXiv:2107.11548", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDynamic Portal Occlusion for Precomputed Interactive Sound Propagation\\u00a7r\\n\\n\\u00a78\\u00a7oNikunj Raghuvanshi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.11548\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 27 Jul 2021 03:28:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 5 figures, planning to submit to IEEE TVCG Short papers at a future date\\u00a7r"}']}
{title:'Kulkarni et al. (§72021§r)', author: 'Vinay Kulkarni; Radhakrishnan Vadakkethil', display:{Lore:['[{"text": "arXiv:2107.11835", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCough Detection from Acoustic signals for patient monitoring system\\u00a7r\\n\\n\\u00a78\\u00a7oVinay Kulkarni\\nRadhakrishnan Vadakkethil\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.11835\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 25 Jul 2021 16:24:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 10 figures\\u00a7r"}']}
{title:'Krause et al. (§72021§r)', author: 'Daniel Aleksander Krause; Archontis Politis; Annamaria Mesaros', display:{Lore:['[{"text": "arXiv:2107.12033", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Direction and Proximity Classification of Overlapping Sound Events from Binaural Audio\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Aleksander Krause\\nArchontis Politis\\nAnnamaria Mesaros\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.12033\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Jul 2021 08:48:46 GMT)\\u00a7r"}']}
{title:'Benaroya et al. (§72021§r)', author: 'Laurent Benaroya; Nicolas Obin; Axel Roebel', display:{Lore:['[{"text": "arXiv:2107.12346", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBeyond Voice Identity Conversion: Manipulating Voice Attributes by Adversarial Learning of Structured Disentangled Representations\\u00a7r\\n\\n\\u00a78\\u00a7oLaurent Benaroya\\nNicolas Obin\\nAxel Roebel\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.12346\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 27 Jul 2021 16:49:15 GMT)\\u00a7r"}']}
{title:'Pan et al. (§72021§r)', author: 'Shifeng Pan; Lei He', display:{Lore:['[{"text": "arXiv:2107.12562", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-speaker Style Transfer with Prosody Bottleneck in Neural Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oShifeng Pan\\nLei He\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.12562\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Jul 2021 02:43:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oin Proceedings of INTERSPEECH 2021\\u00a7r"}']}
{title:'Simonetta et al. (§72021§r)', author: 'Federico Simonetta; Stavros Ntalampiras; Federico Avanzini', display:{Lore:['[{"text": "arXiv:2107.12854", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-to-Score Alignment Using Deep Automatic Music Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oFederico Simonetta\\nStavros Ntalampiras\\nFederico Avanzini\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.12854\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 30 Dec 2021 13:58:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE MMSP 2021 - ERRATUM\\u00a7r"}']}
{title:'Yu et al. (§72021§r)', author: 'Guochen Yu; Yutian Wang; Chengshi Zheng; Hui Wang; Qin Zhang', display:{Lore:['[{"text": "arXiv:2107.13143", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCycleGAN-based Non-parallel Speech Enhancement with an Adaptive Attention-in-attention Mechanism\\u00a7r\\n\\n\\u00a78\\u00a7oGuochen Yu\\nYutian Wang\\nChengshi Zheng\\nHui Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.13143\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 14 Sep 2021 02:44:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by APSIPA-ASC 2021 (7 pages)\\u00a7r"}']}
{title:'Chowdhury et al. (§72021§r)', author: 'Shreyan Chowdhury; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2107.13231", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Perceived Emotion in Expressive Piano Performance: Further Experimental Evidence for the Relevance of Mid-level Perceptual Features\\u00a7r\\n\\n\\u00a78\\u00a7oShreyan Chowdhury\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.13231\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Jul 2021 09:22:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of the 22nd International Society for Music Information Retrieval (ISMIR) Conference, Online, 2021\\u00a7r"}']}
{title:'Lordelo et al. (§72021§r)', author: 'Carlos Lordelo; Emmanouil Benetos; Simon Dixon; Sven Ahlbäck', display:{Lore:['[{"text": "arXiv:2107.13617", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPitch-Informed Instrument Assignment Using a Deep Convolutional Network with Multiple Kernel Shapes\\u00a7r\\n\\n\\u00a78\\u00a7oCarlos Lordelo\\nEmmanouil Benetos\\nSimon Dixon\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.13617\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Jul 2021 19:48:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 figures, 4 tables and 7 pages. Accepted for publication at ISMIR Conference 2021\\u00a7r"}']}
{title:'Srivastava et al. (§72021§r)', author: 'Prerak Srivastava; Antoine Deleforge; Emmanuel Vincent', display:{Lore:['[{"text": "arXiv:2107.13832", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBlind Room Parameter Estimation Using Multiple-Multichannel Speech Recordings\\u00a7r\\n\\n\\u00a78\\u00a7oPrerak Srivastava\\nAntoine Deleforge\\nEmmanuel Vincent\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.13832\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 29 Jul 2021 08:51:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted In WASPAA 2021 ( IEEE Workshop on Applications of Signal Processing to Audio and Acoustics )\\u00a7r"}']}
{title:'Foscarin et al. (§72021§r)', author: "Francesco Foscarin; Nicolas Audebert; Raphaël Fournier-S'Niehotta", display:{Lore:['[{"text": "arXiv:2107.14009", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPKSpell: Data-Driven Pitch Spelling and Key Signature Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesco Foscarin\\nNicolas Audebert\\nRapha\\u00ebl Fournier-S\'Niehotta\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.14009\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Jul 2021 13:34:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInternational Society for Music Information Retrieval Conference (ISMIR), Nov 2021, Online, India\\u00a7r"}']}
{title:'Kumar et al. (§72021§r)', author: 'Agni Kumar; Vikramjit Mitra; Carolyn Oliver; Adeeti Ullal; Matt Biddulph; Irida Mance', display:{Lore:['[{"text": "arXiv:2107.14028", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEstimating Respiratory Rate From Breath Audio Obtained Through Wearable Microphones\\u00a7r\\n\\n\\u00a78\\u00a7oAgni Kumar\\nVikramjit Mitra\\nCarolyn Oliver\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.14028\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Jul 2021 17:24:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInternational Conference of the IEEEEngineering in Medicine and Biology Society (EMBC) 2021\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Lin Zhang; Xin Wang; Erica Cooper; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2107.14132", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Task Learning in Utterance-Level and Segmental-Level Spoof Detection\\u00a7r\\n\\n\\u00a78\\u00a7oLin Zhang\\nXin Wang\\nErica Cooper\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.14132\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 31 Aug 2021 16:02:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ASVspoof 2021 Workshop\\u00a7r"}']}
{title:'Akman et al. (§72021§r)', author: 'Alican Akman; Harry Coppock; Alexander Gaskell; Panagiotis Tzirakis; Lyn Jones; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2107.14549", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluating the COVID-19 Identification ResNet (CIdeR) on the INTERSPEECH COVID-19 from Audio Challenges\\u00a7r\\n\\n\\u00a78\\u00a7oAlican Akman\\nHarry Coppock\\nAlexander Gaskell\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.14549\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Jul 2021 10:59:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure\\u00a7r"}']}
{title:'Naranjo-Alcazar et al. (§72021§r)', author: 'Javier Naranjo-Alcazar; Sergi Perez-Castanos; Pedro Zuccarello; Francesc J. Ferri; Maximo Cobos', display:{Lore:['[{"text": "arXiv:2107.14561", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTASK3 DCASE2021 Challenge: Sound event localization and detection using squeeze-excitation residual CNNs\\u00a7r\\n\\n\\u00a78\\u00a7oJavier Naranjo-Alcazar\\nSergi Perez-Castanos\\nPedro Zuccarello\\nFrancesc J. Ferri\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.14561\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Jul 2021 11:34:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Task3 DCASE Challenge 2021\\u00a7r"}']}
{title:'Sarmento et al. (§72021§r)', author: 'Pedro Sarmento; Adarsh Kumar; CJ Carr; Zack Zukowski; Mathieu Barthet; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2107.14653", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDadaGP: A Dataset of Tokenized GuitarPro Songs for Sequence Models\\u00a7r\\n\\n\\u00a78\\u00a7oPedro Sarmento\\nAdarsh Kumar\\nCJ Carr\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.14653\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Jul 2021 14:21:36 GMT)\\u00a7r"}']}
{title:'Naranjo-Alcazar et al. (§72021§r)', author: 'Javier Naranjo-Alcazar; Sergi Perez-Castanos; Maximo Cobos; Francesc J. Ferri; Pedro Zuccarello', display:{Lore:['[{"text": "arXiv:2107.14658", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTask 1A DCASE 2021: Acoustic Scene Classification with mismatch-devices using squeeze-excitation technique and low-complexity constraint\\u00a7r\\n\\n\\u00a78\\u00a7oJavier Naranjo-Alcazar\\nSergi Perez-Castanos\\nMaximo Cobos\\nFrancesc J. Ferri\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.14658\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Jul 2021 14:24:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Task1a DCASE 2021 Challenge\\u00a7r"}']}
{title:'Chen et al. (§72021§r)', author: 'Yi-Wei Chen; Hung-Shin Lee; Yen-Hsing Chen; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:2108.00378", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSurpriseNet: Melody Harmonization Conditioning on User-controlled Surprise Contours\\u00a7r\\n\\n\\u00a78\\u00a7oYi-Wei Chen\\nHung-Shin Lee\\nYen-Hsing Chen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.00378\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 24 Aug 2021 04:29:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of the 22nd International Society for Music Information Retrieval Conference, ISMIR 2021\\u00a7r"}']}
{title:'Bhattacharjee et al. (§72021§r)', author: 'Prithwiraj Bhattacharjee; Rajan Saha Raju; Arif Ahmad; M. Shahidur Rahman', display:{Lore:['[{"text": "arXiv:2108.00500", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd to End Bangla Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oPrithwiraj Bhattacharjee\\nRajan Saha Raju\\nArif Ahmad\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.00500\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 1 Aug 2021 17:16:03 GMT)\\u00a7r"}']}
{title:"d'Eon et al. (§72021§r)", author: "Jason d'Eon; Sri Harsha Dumpala; Chandramouli Shama Sastry; Dani Oore; Sageev Oore", display:{Lore:['[{"text": "arXiv:2108.01043", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusical Speech: A Transformer-based Composition Tool\\u00a7r\\n\\n\\u00a78\\u00a7oJason d\'Eon\\nSri Harsha Dumpala\\nChandramouli Shama Sastry\\nDani Oore\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.01043\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 2 Aug 2021 17:03:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oNeurIPS 2020 Demonstration Track; extended for PMLR\\u00a7r"}']}
{title:'Mandeel et al. (§72021§r)', author: 'Ali Raheem Mandeel; Mohammed Salah Al-Radhi; Tamás Gábor Csapó', display:{Lore:['[{"text": "arXiv:2108.01154", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Adaptation with Continuous Vocoder-based DNN-TTS\\u00a7r\\n\\n\\u00a78\\u00a7oAli Raheem Mandeel\\nMohammed Salah Al-Radhi\\nTam\\u00e1s G\\u00e1bor Csap\\u00f3\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.01154\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 2 Aug 2021 20:08:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 3 figures, 23RD INTERNATIONAL CONFERENCE ON SPEECH AND COMPUTER SPECOM 2021\\u00a7r"}']}
{title:'Nistal et al. (§72021§r)', author: 'Javier Nistal; Stefan Lattner; Gaël Richard', display:{Lore:['[{"text": "arXiv:2108.01216", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDarkGAN: Exploiting Knowledge Distillation for Comprehensible Audio Synthesis with GANs\\u00a7r\\n\\n\\u00a78\\u00a7oJavier Nistal\\nStefan Lattner\\nGa\\u00ebl Richard\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.01216\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n22nd International Society for Music Information Retrieval (ISMIR\\n  2021)\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 Aug 2021 00:26:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 3 figures, 2 tables, accepted to ISMIR2021\\u00a7r"}']}
{title:'He et al. (§72021§r)', author: 'Bradley He; Martin Radfar', display:{Lore:['[{"text": "arXiv:2108.01245", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Performance Evaluation of Attention-Based Neural ASR under Mixed Speech Input\\u00a7r\\n\\n\\u00a78\\u00a7oBradley He\\nMartin Radfar\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.01245\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 Aug 2021 02:08:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Shafiei (§72021§r)', author: 'Sepideh Shafiei', display:{Lore:['[{"text": "arXiv:2108.01283", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn analysis of Iranian Music Intervals based on Pitch Histogram\\u00a7r\\n\\n\\u00a78\\u00a7oSepideh Shafiei\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.01283\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 Aug 2021 04:09:40 GMT)\\u00a7r"}']}
{title:'Hung et al. (§72021§r)', author: 'Hsiao-Tzu Hung; Joann Ching; Seungheon Doh; Nabin Kim; Juhan Nam; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2108.01374", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEMOPIA: A Multi-Modal Pop Piano Dataset For Emotion Recognition and Emotion-based Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oHsiao-Tzu Hung\\nJoann Ching\\nSeungheon Doh\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.01374\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 Aug 2021 08:59:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe paper has been accepted for publication at ISMIR 2021\\u00a7r"}']}
{title:'Pati et al. (§72021§r)', author: 'Ashis Pati; Alexander Lerch', display:{Lore:['[{"text": "arXiv:2108.01450", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIs Disentanglement enough? On Latent Representations for Controllable Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oAshis Pati\\nAlexander Lerch\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.01450\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 1 Aug 2021 18:37:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in: Proceedings of 22nd International Society for Music Information Retrieval Conference (ISMIR), Online, 2021\\u00a7r"}']}
{title:'Seshadri et al. (§72021§r)', author: 'Pavan Seshadri; Alexander Lerch', display:{Lore:['[{"text": "arXiv:2108.01711", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Music Performance Assessment with Contrastive Learning\\u00a7r\\n\\n\\u00a78\\u00a7oPavan Seshadri\\nAlexander Lerch\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.01711\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 Aug 2021 19:24:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at 22nd International Society for Music Information Retrieval Conference, Online, 2021\\u00a7r"}']}
{title:'Dai et al. (§72021§r)', author: 'Xudong Dai; Cheng Gong; Longbiao Wang; Kaili Zhang', display:{Lore:['[{"text": "arXiv:2108.01831", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInformation Sieve: Content Leakage Reduction in End-to-End Prosody For Expressive Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oXudong Dai\\nCheng Gong\\nLongbiao Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.01831\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 4 Aug 2021 03:45:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted By Interspeech 2021\\u00a7r"}']}
{title:'Travers et al. (§72021§r)', author: 'Adelin Travers; Lorna Licollari; Guanghan Wang; Varun Chandrasekaran; Adam Dziedzic; David Lie; Nicolas Papernot', display:{Lore:['[{"text": "arXiv:2108.02010", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Exploitability of Audio Machine Learning Pipelines to Surreptitious Adversarial Examples\\u00a7r\\n\\n\\u00a78\\u00a7oAdelin Travers\\nLorna Licollari\\nGuanghan Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.02010\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 Aug 2021 16:21:08 GMT)\\u00a7r"}']}
{title:'Ibrahim et al. (§72021§r)', author: 'Ahmed Ibrahim; Ayman El-Refai; Sara Ahmed; Mariam Aboul-Ela; Hesham M. Eraqi; Mohamed Moustafa', display:{Lore:['[{"text": "arXiv:2108.02148", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPervasive Hand Gesture Recognition for Smartphones using Non-audible Sound and Deep Learning\\u00a7r\\n\\n\\u00a78\\u00a7oAhmed Ibrahim\\nAyman El-Refai\\nSara Ahmed\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.02148\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 4 Aug 2021 16:23:26 GMT)\\u00a7r"}']}
{title:'Padi et al. (§72021§r)', author: 'Sarala Padi; Seyed Omid Sadjadi; Dinesh Manocha; Ram D. Sriram', display:{Lore:['[{"text": "arXiv:2108.02510", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Speech Emotion Recognition using Transfer Learning and Spectrogram Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oSarala Padi\\nSeyed Omid Sadjadi\\nDinesh Manocha\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.02510\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 16 Aug 2021 14:47:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ACM/SIGCHI ICMI\'21\\u00a7r"}']}
{title:'Qian et al. (§72021§r)', author: 'Xinyuan Qian; Bidisha Sharma; Amine El Abridi; Haizhou Li', display:{Lore:['[{"text": "arXiv:2108.02539", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.DB\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSLoClas: A Database for Joint Sound Localization and Classification\\u00a7r\\n\\n\\u00a78\\u00a7oXinyuan Qian\\nBidisha Sharma\\nAmine El Abridi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.02539\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 5 Aug 2021 11:49:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to O-COCOSDA 2021\\u00a7r"}']}
{title:'Rafee et al. (§72021§r)', author: 'Syed Rifat Mahmud Rafee; Gyorgy Fazekas; Geraint A. ~Wiggins', display:{Lore:['[{"text": "arXiv:2108.02576", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerformer Identification From Symbolic Representation of Music Using Statistical Models\\u00a7r\\n\\n\\u00a78\\u00a7oSyed Rifat Mahmud Rafee\\nGyorgy Fazekas\\nGeraint A. ~Wiggins\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.02576\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 5 Aug 2021 12:32:54 GMT)\\u00a7r"}']}
{title:'Demirel et al. (§72021§r)', author: 'Emir Demirel; Sven Ahlbäck; Simon Dixon', display:{Lore:['[{"text": "arXiv:2108.02625", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMSTRE-Net: Multistreaming Acoustic Modeling for Automatic Lyrics Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oEmir Demirel\\nSven Ahlb\\u00e4ck\\nSimon Dixon\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.02625\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 5 Aug 2021 13:59:11 GMT)\\u00a7r"}']}
{title:'Ke et al. (§72021§r)', author: 'Dengfeng Ke; Yuxing Lu; Xudong Liu; Yanyan Xu; Jing Sun; Cheng-Hao Cai', display:{Lore:['[{"text": "arXiv:2108.03008", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Empirical Study on End-to-End Singing Voice Synthesis with Encoder-Decoder Architectures\\u00a7r\\n\\n\\u00a78\\u00a7oDengfeng Ke\\nYuxing Lu\\nXudong Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.03008\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 Aug 2021 08:51:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o27 pages, 4 figures, 5 tables\\u00a7r"}']}
{title:'Kim et al. (§72021§r)', author: 'Gwantae Kim; David K. Han; Hanseok Ko', display:{Lore:['[{"text": "arXiv:2108.03020", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpecMix : A Mixed Sample Data Augmentation method for Training withTime-Frequency Domain Features\\u00a7r\\n\\n\\u00a78\\u00a7oGwantae Kim\\nDavid K. Han\\nHanseok Ko\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.03020\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 Aug 2021 09:37:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to interspeech 2021\\u00a7r"}']}
{title:'Ren et al. (§72021§r)', author: 'Zhao Ren; Yi Chang; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2108.03041", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe EIHW-GLAM Deep Attentive Multi-model Fusion System for Cough-based COVID-19 Recognition in the DiCOVA 2021 Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oZhao Ren\\nYi Chang\\nBj\\u00f6rn W. Schuller\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.03041\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 Aug 2021 10:43:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o3 pages, Technical Report\\u00a7r"}']}
{title:'Lin et al. (§72021§r)', author: 'Liwei Lin; Qiuqiang Kong; Junyan Jiang; Gus Xia', display:{Lore:['[{"text": "arXiv:2108.03456", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Unified Model for Zero-shot Music Source Separation, Transcription and Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oLiwei Lin\\nQiuqiang Kong\\nJunyan Jiang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.03456\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 7 Aug 2021 14:28:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ISMIR2021\\u00a7r"}']}
{title:'Chen et al. (§72021§r)', author: 'Xinru Chen; Menghan Hu; Guangtao Zhai', display:{Lore:['[{"text": "arXiv:2108.03538", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCough Detection Using Selected Informative Features from Audio Signals\\u00a7r\\n\\n\\u00a78\\u00a7oXinru Chen\\nMenghan Hu\\nGuangtao Zhai\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.03538\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 7 Aug 2021 23:05:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 3 figures\\u00a7r"}']}
{title:'Chatterjee et al. (§72021§r)', author: 'Debdutta Chatterjee; Arindam Dutta; Dibakar Sil; Aniruddha Chandra', display:{Lore:['[{"text": "arXiv:2108.03569", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Single Shot Musical Instrument Identification using Scalograms\\u00a7r\\n\\n\\u00a78\\u00a7oDebdutta Chatterjee\\nArindam Dutta\\nDibakar Sil\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.03569\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 8 Aug 2021 05:11:07 GMT)\\u00a7r"}']}
{title:'Deshpande et al. (§72021§r)', author: 'Darshan Deshpande; Harshavardhan Abichandani', display:{Lore:['[{"text": "arXiv:2108.03703", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Spectral Enhancement: Leveraging Autoencoders for Low Latency Reconstruction of Long, Lossy Audio Sequences\\u00a7r\\n\\n\\u00a78\\u00a7oDarshan Deshpande\\nHarshavardhan Abichandani\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.03703\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 8 Aug 2021 18:06:21 GMT)\\u00a7r"}']}
{title:'Bondareva et al. (§72021§r)', author: 'Erika Bondareva; Jing Han; William Bradlow; Cecilia Mascolo', display:{Lore:['[{"text": "arXiv:2108.04139", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSegmentation-free Heart Pathology Detection Using Deep Learning\\u00a7r\\n\\n\\u00a78\\u00a7oErika Bondareva\\nJing Han\\nWilliam Bradlow\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.04139\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 9 Aug 2021 16:09:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 2 tables, Accepted at EMBC\'21 - Annual International Conference of the IEEEEngineering in Medicine and Biology Society\\u00a7r"}']}
{title:'Sakamoto et al. (§72021§r)', author: 'Shoki Sakamoto; Akira Taniguchi; Tadahiro Taniguchi; Hirokazu Kameoka', display:{Lore:['[{"text": "arXiv:2108.04395", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStarGAN-VC+ASR: StarGAN-based Non-Parallel Voice Conversion Regularized by Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oShoki Sakamoto\\nAkira Taniguchi\\nTadahiro Taniguchi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.04395\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-492\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nINTERSPEECH 2021, 1359--1363\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Aug 2021 01:18:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 6 figures, Accepted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Behr et al. (§72021§r)', author: 'David Behr; Ciira wa Maina; Vukosi Marivate', display:{Lore:['[{"text": "arXiv:2108.04449", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CY\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn empirical investigation into audio pipeline approaches for classifying bird species\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Behr\\nCiira wa Maina\\nVukosi Marivate\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.04449\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Aug 2021 05:02:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, Accepted and to be published in AFRICON2021\\u00a7r"}']}
{title:'Parida et al. (§72021§r)', author: 'Kranti Kumar Parida; Siddharth Srivastava; Neeraj Matiyali; Gaurav Sharma', display:{Lore:['[{"text": "arXiv:2108.04906", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDepth Infused Binaural Audio Generation using Hierarchical Cross-Modal Attention\\u00a7r\\n\\n\\u00a78\\u00a7oKranti Kumar Parida\\nSiddharth Srivastava\\nNeeraj Matiyali\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.04906\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Aug 2021 20:26:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at Sight and Sound Workshop, CVPR 2021\\u00a7r"}']}
{title:'Wu et al. (§72021§r)', author: 'Yuzhong Wu; Tan Lee', display:{Lore:['[{"text": "arXiv:2108.05008", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Feature Learning on Long-Duration Sounds for Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oYuzhong Wu\\nTan Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.05008\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 11 Aug 2021 03:33:05 GMT)\\u00a7r"}']}
{title:'Chang et al. (§72021§r)', author: 'Chin-Jui Chang; Chun-Yi Lee; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2108.05064", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVariable-Length Music Score Infilling via XLNet and Musically Specialized Positional Encoding\\u00a7r\\n\\n\\u00a78\\u00a7oChin-Jui Chang\\nChun-Yi Lee\\nYi-Hsuan Yang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.05064\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 11 Aug 2021 07:07:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe paper has been accepted for publication at ISMIR 2021\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Zhong-Qiu Wang; Gordon Wichern; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2108.05470", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn The Compensation Between Magnitude and Phase in Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oZhong-Qiu Wang\\nGordon Wichern\\nJonathan Le Roux\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.05470\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2021.3116502\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Sep 2021 16:01:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oin IEEESignal Processing Letters\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Li Wang; Rongzhi Gu; Nuo Chen; Yuexian Zou', display:{Lore:['[{"text": "arXiv:2108.05516", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lText Anchor Based Metric Learning for Small-footprint Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oLi Wang\\nRongzhi Gu\\nNuo Chen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.05516\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 12 Aug 2021 03:43:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for Interspeech2021\\u00a7r"}']}
{title:'Ovaska et al. (§72021§r)', author: 'Mikael Ovaska; Joni Kultanen; Teemu Autto; Joonas Uusnäkki; Antti Kariluoto; Joonas Himmanen; Mikko Virtaneva; Pasi Kaitila; Pekka Abrahamsson', display:{Lore:['[{"text": "arXiv:2108.05553", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Neural Network Voice Activity Detector for Downsampled Audio Data: An Experiment Report\\u00a7r\\n\\n\\u00a78\\u00a7oMikael Ovaska\\nJoni Kultanen\\nTeemu Autto\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.05553\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 12 Aug 2021 06:16:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPre-print. 7 Pages, 2 figures and 5 tables\\u00a7r"}']}
{title:'Ma et al. (§72021§r)', author: 'Youxuan Ma; Zongze Ren; Shugong Xu', display:{Lore:['[{"text": "arXiv:2108.05684", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRW-Resnet: A Novel Speech Anti-Spoofing Model Using Raw Waveform\\u00a7r\\n\\n\\u00a78\\u00a7oYouxuan Ma\\nZongze Ren\\nShugong Xu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.05684\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 13 Aug 2021 01:56:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for Interspeech2021\\u00a7r"}']}
{title:'Shahidi et al. (§72021§r)', author: 'Lidea K. Shahidi; Leslie M. Collins; Boyla O. Mainsah', display:{Lore:['[{"text": "arXiv:2108.05929", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParameter Tuning of Time-Frequency Masking Algorithms for Reverberant Artifact Removal within the Cochlear Implant Stimulus\\u00a7r\\n\\n\\u00a78\\u00a7oLidea K. Shahidi\\nLeslie M. Collins\\nBoyla O. Mainsah\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.05929\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 12 Aug 2021 19:08:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Yang Liu; Alexandros Neophytou; Sunando Sengupta; Eric Sommerlade', display:{Lore:['[{"text": "arXiv:2108.06401", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-modal Spectrum Transformation Network For Acoustic Scene classification\\u00a7r\\n\\n\\u00a78\\u00a7oYang Liu\\nAlexandros Neophytou\\nSunando Sengupta\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.06401\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 13 Aug 2021 21:05:14 GMT)\\u00a7r"}']}
{title:'Sadjadi (§72021§r)', author: 'Seyed Omid Sadjadi', display:{Lore:['[{"text": "arXiv:2108.07118", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNIST SRE CTS Superset: A large-scale dataset for telephony speaker recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSeyed Omid Sadjadi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.07118\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 16 Aug 2021 14:39:23 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Zhong-Qiu Wang; Gordon Wichern; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2108.07194", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConvolutive Prediction for Reverberant Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oZhong-Qiu Wang\\nGordon Wichern\\nJonathan Le Roux\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.07194\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 16 Aug 2021 16:05:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oin IEEEWorkshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2021\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Zhong-Qiu Wang; Gordon Wichern; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2108.07376", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConvolutive Prediction for Monaural Speech Dereverberation and Noisy-Reverberant Speaker Separation\\u00a7r\\n\\n\\u00a78\\u00a7oZhong-Qiu Wang\\nGordon Wichern\\nJonathan Le Roux\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.07376\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 10 Nov 2021 18:40:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oin IEEE/ACM Transactionson Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Zhao et al. (§72021§r)', author: 'Haoyue Zhao; Xin Guan; Qiang Li', display:{Lore:['[{"text": "arXiv:2108.09058", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEstimation of Playable Piano Fingering by Pitch-difference Fingering Matching Model\\u00a7r\\n\\n\\u00a78\\u00a7oHaoyue Zhao\\nXin Guan\\nQiang Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.09058\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 20 Aug 2021 08:39:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o31 pages,12 figures\\u00a7r"}']}
{title:'Chatterjee et al. (§72021§r)', author: 'Oindrila Chatterjee; Shantanu Chakrabartty', display:{Lore:['[{"text": "arXiv:2108.09537", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing growth transform dynamical systems for spatio-temporal data sonification\\u00a7r\\n\\n\\u00a78\\u00a7oOindrila Chatterjee\\nShantanu Chakrabartty\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.09537\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 21 Aug 2021 16:25:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis article was submitted toPLoS One in March, 2021and is currently under peer review\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Yongming Li; Chengyu Liu; Pin Wang; Hehua Zhang; Anhai Wei', display:{Lore:['[{"text": "arXiv:2108.09922", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSubject Envelope based Multitype Reconstruction Algorithm of Speech Samples of Parkinson\'s Disease\\u00a7r\\n\\n\\u00a78\\u00a7oYongming Li\\nChengyu Liu\\nPin Wang\\nHehua Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.09922\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 23 Aug 2021 04:20:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 6 tables\\u00a7r"}']}
{title:'Badlani et al. (§72021§r)', author: 'Rohan Badlani; Adrian Łancucki; Kevin J. Shih; Rafael Valle; Wei Ping; Bryan Catanzaro', display:{Lore:['[{"text": "arXiv:2108.10447", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOne TTS Alignment To Rule Them All\\u00a7r\\n\\n\\u00a78\\u00a7oRohan Badlani\\nAdrian \\u0141ancucki\\nKevin J. Shih\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.10447\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 23 Aug 2021 23:45:48 GMT)\\u00a7r"}']}
{title:'Rafraf (§72021§r)', author: 'Hooman Rafraf', display:{Lore:['[{"text": "arXiv:2108.10449", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDifferential Music: Automated Music Generation Using LSTM Networks with Representation Based on Melodic and Harmonic Intervals\\u00a7r\\n\\n\\u00a78\\u00a7oHooman Rafraf\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.10449\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 23 Aug 2021 23:51:08 GMT)\\u00a7r"}']}
{title:'Tran et al. (§72021§r)', author: 'Thanh Tran; Nhat Truong Pham; Jan Lundgren', display:{Lore:['[{"text": "arXiv:2108.11089", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetecting Drill Failure in the Small Short-sound Drill Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oThanh Tran\\nNhat Truong Pham\\nJan Lundgren\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.11089\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 Nov 2021 13:09:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 10 figures, journal\\u00a7r"}']}
{title:'Zhao et al. (§72021§r)', author: 'Jingwei Zhao; Gus Xia', display:{Lore:['[{"text": "arXiv:2108.11213", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAccoMontage: Accompaniment Arrangement via Phrase Selection and Style Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oJingwei Zhao\\nGus Xia\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.11213\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 25 Aug 2021 13:02:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ISMIR 2021\\u00a7r"}']}
{title:'Rakotonirina (§72021§r)', author: 'Nathanaël Carraz Rakotonirina', display:{Lore:['[{"text": "arXiv:2108.11637", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Attention for Audio Super-Resolution\\u00a7r\\n\\n\\u00a78\\u00a7oNathana\\u00ebl Carraz Rakotonirina\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.11637\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 26 Aug 2021 08:05:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMLSP 2021\\u00a7r"}']}
{title:'Nemazi et al. (§72021§r)', author: 'Fabian Nemazi; Jon Nordby', display:{Lore:['[{"text": "arXiv:2108.11758", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetermining the origin of impulsive noise events using paired wireless sound sensors\\u00a7r\\n\\n\\u00a78\\u00a7oFabian Nemazi\\nJon Nordby\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.11758\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 23 Aug 2021 14:19:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for EuroNoise 2021\\u00a7r"}']}
{title:'Parra-Gallego et al. (§72021§r)', author: 'Luis Felipe Parra-Gallego; Juan Rafael Orozco-Arroyave', display:{Lore:['[{"text": "arXiv:2108.11981", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lClassification of Emotions and Evaluation of Customer Satisfaction from Speech in Real World Acoustic Environments\\u00a7r\\n\\n\\u00a78\\u00a7oLuis Felipe Parra-Gallego\\nJuan Rafael Orozco-Arroyave\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.11981\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 26 Aug 2021 18:23:56 GMT)\\u00a7r"}']}
{title:'Yan et al. (§72021§r)', author: 'Yuzi Yan; Wei-Qiang Zhang; Michael T. Johnson', display:{Lore:['[{"text": "arXiv:2108.12105", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFull Attention Bidirectional Deep Learning Structure for Single Channel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oYuzi Yan\\nWei-Qiang Zhang\\nMichael T. Johnson\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.12105\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 27 Aug 2021 03:19:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Qiongqiong Wang; Kong Aik Lee; Takafumi Koshinaka; Koji Okabe; Hitoshi Yamamoto', display:{Lore:['[{"text": "arXiv:2108.12128", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTask-aware Warping Factors in Mask-based Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oQiongqiong Wang\\nKong Aik Lee\\nTakafumi Koshinaka\\nKoji Okabe\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.12128\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 27 Aug 2021 05:57:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oEUSIPCO 2021 (the 29th European Signal Processing Conference)\\u00a7r"}']}
{title:'Hu et al. (§72021§r)', author: 'Shenghua Hu; Jing Wang; Yujun Wang; Wenjing Yang', display:{Lore:['[{"text": "arXiv:2108.12146", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSeparable Temporal Convolution plus Temporally Pooled Attention for Lightweight High-performance Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oShenghua Hu\\nJing Wang\\nYujun Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.12146\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 27 Aug 2021 07:27:33 GMT)\\u00a7r"}']}
{title:'Hernandez-Olivan et al. (§72021§r)', author: 'Carlos Hernandez-Olivan; Jose R. Beltran', display:{Lore:['[{"text": "arXiv:2108.12290", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Composition with Deep Learning: A Review\\u00a7r\\n\\n\\u00a78\\u00a7oCarlos Hernandez-Olivan\\nJose R. Beltran\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.12290\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 7 Sep 2021 16:54:33 GMT)\\u00a7r"}']}
{title:'McCallum (§72021§r)', author: 'Matthew C. McCallum', display:{Lore:['[{"text": "arXiv:2108.12955", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Learning of Deep Features for Music Segmentation\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew C. McCallum\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.12955\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8683407\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 Aug 2021 01:55:44 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Songhe Wang; Zheng Bao; Jingtong E', display:{Lore:['[{"text": "arXiv:2108.12973", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lArmor: A Benchmark for Meta-evaluation of Artificial Music\\u00a7r\\n\\n\\u00a78\\u00a7oSonghe Wang\\nZheng Bao\\nJingtong E\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.12973\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3474085.3475700\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 Aug 2021 03:09:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at ACM Multimedia 2021 (8 pages, 5 figures)\\u00a7r"}']}
{title:'Wu et al. (§72021§r)', author: 'Yanfeng Wu; Chenkai Guo; Junan Zhao; Xiao Jin; Jing Xu', display:{Lore:['[{"text": "arXiv:2108.13249", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRSKNet-MTSP: Effective and Portable Deep Architecture for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oYanfeng Wu\\nChenkai Guo\\nJunan Zhao\\nXiao Jin\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.13249\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 Aug 2021 14:08:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to Neurocomputing\\u00a7r"}']}
{title:'Dong et al. (§72021§r)', author: 'Mingyu Dong; Diqun Yan; Yongkang Gong; Rangding Wang', display:{Lore:['[{"text": "arXiv:2108.13562", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Example Devastation and Detection on Speech Recognition System by Adding Random Noise\\u00a7r\\n\\n\\u00a78\\u00a7oMingyu Dong\\nDiqun Yan\\nYongkang Gong\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.13562\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 17 Oct 2021 11:39:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o20 pages, 5 figures, Submitted to Computer Speech and Language\\u00a7r"}']}
{title:'Chen et al. (§72021§r)', author: 'Zhengyang Chen; Shuai Wang; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2108.13843", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Learning Based Domain Adaptation for Robust Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oZhengyang Chen\\nShuai Wang\\nYanmin Qian\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.13843\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9414261\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 31 Aug 2021 13:55:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in: ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\\u00a7r"}']}
{title:'Pahar et al. (§72021§r)', author: 'Madhurananda Pahar; Igor Miranda; Andreas Diacon; Thomas Niesler', display:{Lore:['[{"text": "arXiv:2109.00103", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic non-invasive Cough Detection based on Accelerometer and Audio Signals\\u00a7r\\n\\n\\u00a78\\u00a7oMadhurananda Pahar\\nIgor Miranda\\nAndreas Diacon\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.00103\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s11265-022-01748-5\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJournal of Signal Processing Systems, 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 31 Aug 2021 22:44:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2102.04997\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Hang Li; Yu Kang; Tianqiao Liu; Wenbiao Ding; Zitao Liu', display:{Lore:['[{"text": "arXiv:2109.00181", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCTAL: Pre-training Cross-modal Transformer for Audio-and-Language Representations\\u00a7r\\n\\n\\u00a78\\u00a7oHang Li\\nYu Kang\\nTianqiao Liu\\nWenbiao Ding\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.00181\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Sep 2021 04:18:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe 2021 Conference on Empirical Methods in Natural Language Processing\\u00a7r"}']}
{title:'Mizobuchi et al. (§72021§r)', author: 'Yusaku Mizobuchi; Daichi Kitamura; Tomohiko Nakamura; Hiroshi Saruwatari; Yu Takahashi; Kazunobu Kondo', display:{Lore:['[{"text": "arXiv:2109.00237", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrior Distribution Design for Music Bleeding-Sound Reduction Based on Nonnegative Matrix Factorization\\u00a7r\\n\\n\\u00a78\\u00a7oYusaku Mizobuchi\\nDaichi Kitamura\\nTomohiko Nakamura\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.00237\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Sep 2021 08:21:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted and willbe presented at APSIPA2021\\u00a7r"}']}
{title:'Hu et al. (§72021§r)', author: 'Shenghua Hu; Jing Wang; Yujun Wang; Lidong Yang; Wenjing Yang', display:{Lore:['[{"text": "arXiv:2109.00260", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Separable Temporal Convolution Neural Network with Attention for Small-Footprint Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oShenghua Hu\\nJing Wang\\nYujun Wang\\nLidong Yang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.00260\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Sep 2021 09:12:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2108.12146\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Andong Li; Wenzhe Liu; Chengshi Zheng; Xiaodong Li', display:{Lore:['[{"text": "arXiv:2109.00265", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmbedding and Beamforming: All-neural Causal Beamformer for Multichannel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oAndong Li\\nWenzhe Liu\\nChengshi Zheng\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.00265\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 2 Sep 2021 03:11:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022, first version\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Shibo Zhang; Ebrahim Nemati; Tousif Ahmed; Md Mahbubur Rahman; Jilong Kuang; Alex Gao', display:{Lore:['[{"text": "arXiv:2109.00630", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Novel Multi-Centroid Template Matching Algorithm and Its Application to Cough Detection\\u00a7r\\n\\n\\u00a78\\u00a7oShibo Zhang\\nEbrahim Nemati\\nTousif Ahmed\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.00630\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 4 Sep 2021 19:02:12 GMT)\\u00a7r"}']}
{title:'Dai et al. (§72021§r)', author: 'Shuqi Dai; Zeyu Jin; Celso Gomes; Roger B. Dannenberg', display:{Lore:['[{"text": "arXiv:2109.00663", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lControllable deep melody generation via hierarchical music structure representation\\u00a7r\\n\\n\\u00a78\\u00a7oShuqi Dai\\nZeyu Jin\\nCelso Gomes\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.00663\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 Sep 2021 01:31:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 9 figures, in Proc. of the 22nd Int. Society for Music Information Retrieval Conf.,Online, 2021\\u00a7r"}']}
{title:'Hasumi et al. (§72021§r)', author: 'Takuya Hasumi; Tomohiko Nakamura; Norihiro Takamune; Hiroshi Saruwatari; Daichi Kitamura; Yu Takahashi; Kazunobu Kondo', display:{Lore:['[{"text": "arXiv:2109.00704", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultichannel Audio Source Separation with Independent Deeply Learned Matrix Analysis Using Product of Source Models\\u00a7r\\n\\n\\u00a78\\u00a7oTakuya Hasumi\\nTomohiko Nakamura\\nNorihiro Takamune\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.00704\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 Sep 2021 04:31:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 5 figures, accepted for Asia-Pacific Signal and Information ProcessingAssociation Annual Summit and Conference 2021 (APSIPA ASC 2021)\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Sijia Li; Shiguang Liu; Dinesh Manocha', display:{Lore:['[{"text": "arXiv:2109.00748", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBinaural Audio Generation via Multi-task Learning\\u00a7r\\n\\n\\u00a78\\u00a7oSijia Li\\nShiguang Liu\\nDinesh Manocha\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.00748\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 Sep 2021 07:04:56 GMT)\\u00a7r"}']}
{title:'Hyrkas (§72021§r)', author: 'Jeremy Hyrkas', display:{Lore:['[{"text": "arXiv:2109.01948", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNetwork Modulation Synthesis: New Algorithms for Generating Musical Audio Using Autoencoder Networks\\u00a7r\\n\\n\\u00a78\\u00a7oJeremy Hyrkas\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.01948\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 4 Sep 2021 22:58:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to the International ComputerMusic Conference 2021(2020 Selected Papers)\\u00a7r"}']}
{title:'Zhao et al. (§72021§r)', author: 'Miao Zhao; Yufeng Ma; Min Liu; Minqiang Xu', display:{Lore:['[{"text": "arXiv:2109.01989", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe SpeakIn System for VoxCeleb Speaker Recognition Challange 2021\\u00a7r\\n\\n\\u00a78\\u00a7oMiao Zhao\\nYufeng Ma\\nMin Liu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.01989\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 5 Sep 2021 04:07:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH2021 VoxSRC2021 Workshop\\u00a7r"}']}
{title:'Yu et al. (§72021§r)', author: 'Guochen Yu; Yutian Wang; Hui Wang; Qin Zhang; Chengshi Zheng', display:{Lore:['[{"text": "arXiv:2109.02011", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Two-stage Complex Network using Cycle-consistent Generative Adversarial Networks for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oGuochen Yu\\nYutian Wang\\nHui Wang\\nQin Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.02011\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 5 Sep 2021 07:09:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Speech Communication\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Keke Wang; Xudong Mao; Hao Wu; Chen Ding; Chuxiang Shang; Rui Xia; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2109.02047", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe ByteDance Speaker Diarization System for the VoxCeleb Speaker Recognition Challenge 2021\\u00a7r\\n\\n\\u00a78\\u00a7oKeke Wang\\nXudong Mao\\nHao Wu\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.02047\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 5 Sep 2021 11:55:34 GMT)\\u00a7r"}']}
{title:'Rostami et al. (§72021§r)', author: 'Amir Mohammad Rostami; Mohammad Mehdi Homayounpour; Ahmad Nickabadi', display:{Lore:['[{"text": "arXiv:2109.02051", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Attention Branch Network with Combined Loss Function for Automatic Speaker Verification Spoof Detection\\u00a7r\\n\\n\\u00a78\\u00a7oAmir Mohammad Rostami\\nMohammad Mehdi Homayounpour\\nAhmad Nickabadi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.02051\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 19 Sep 2021 19:52:41 GMT)\\u00a7r"}']}
{title:'Slavíček et al. (§72021§r)', author: 'Josef Slavíček; Albert Swart; Michal Klčo; Niko Brümmer', display:{Lore:['[{"text": "arXiv:2109.02052", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Phonexia VoxCeleb Speaker Recognition Challenge 2021 System Description\\u00a7r\\n\\n\\u00a78\\u00a7oJosef Slav\\u00ed\\u010dek\\nAlbert Swart\\nMichal Kl\\u010do\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.02052\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 8 Sep 2021 08:37:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSecond place in the self-supervised track of VoxSRC-21: VoxCeleb Speaker Recognition Challenge\\u00a7r"}']}
{title:'Bonnici et al. (§72021§r)', author: 'Russell Sammut Bonnici; Charalampos Saitis; Martin Benning', display:{Lore:['[{"text": "arXiv:2109.02096", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTimbre Transfer with Variational Auto Encoding and Cycle-Consistent Adversarial Networks\\u00a7r\\n\\n\\u00a78\\u00a7oRussell Sammut Bonnici\\nCharalampos Saitis\\nMartin Benning\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.02096\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 10 Oct 2021 16:22:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 3 main figures, 4 tables\\u00a7r"}']}
{title:'Yesiler et al. (§72021§r)', author: 'Furkan Yesiler; Guillaume Doras; Rachel M. Bittner; Christopher J. Tralie; Joan Serrà', display:{Lore:['[{"text": "arXiv:2109.02472", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-based Musical Version Identification: Elements and Challenges\\u00a7r\\n\\n\\u00a78\\u00a7oFurkan Yesiler\\nGuillaume Doras\\nRachel M. Bittner\\nChristopher J. Tralie\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.02472\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/MSP.2021.3105941\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 Sep 2021 13:45:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to be published in IEEE Signal Processing Magazine\\u00a7r"}']}
{title:'Casey et al. (§72021§r)', author: 'Owen Casey; Rushit Dave; Naeem Seliya; Evelyn R Sowells Boone', display:{Lore:['[{"text": "arXiv:2109.02692", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMachine Learning: Challenges, Limitations, and Compatibility for Audio Restoration Processes\\u00a7r\\n\\n\\u00a78\\u00a7oOwen Casey\\nRushit Dave\\nNaeem Seliya\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.02692\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 Sep 2021 18:40:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 2 figures\\u00a7r"}']}
{title:'Teng et al. (§72021§r)', author: 'Zhongwei Teng; Quchen Fu; Jules White; Maria Powell; Douglas C. Schmidt', display:{Lore:['[{"text": "arXiv:2109.02773", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComplementing Handcrafted Features with Raw Waveform Using a Light-weight Auxiliary Model\\u00a7r\\n\\n\\u00a78\\u00a7oZhongwei Teng\\nQuchen Fu\\nJules White\\nMaria Powell\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.02773\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 Sep 2021 23:32:10 GMT)\\u00a7r"}']}
{title:'Fu et al. (§72021§r)', author: 'Quchen Fu; Zhongwei Teng; Jules White; Maria Powell; Douglas C. Schmidt', display:{Lore:['[{"text": "arXiv:2109.02774", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFastAudio: A Learnable Audio Front-End for Spoof Speech Detection\\u00a7r\\n\\n\\u00a78\\u00a7oQuchen Fu\\nZhongwei Teng\\nJules White\\nMaria Powell\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.02774\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 Sep 2021 23:32:10 GMT)\\u00a7r"}']}
{title:'Nguyen et al. (§72021§r)', author: 'Long H. Nguyen; Nhat Truong Pham; Van Huong Do; Liu Tai Nguyen; Thanh Tin Nguyen; Van Dung Do; Hai Nguyen; Ngoc Duy Nguyen', display:{Lore:['[{"text": "arXiv:2109.03219", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFruit-CoV: An Efficient Vision-based Framework for Speedy Detection and Diagnosis of SARS-CoV-2 Infections Through Recorded Cough Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oLong H. Nguyen\\nNhat Truong Pham\\nVan Huong Do\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.03219\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 Sep 2021 07:56:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages\\u00a7r"}']}
{title:'Liou et al. (§72021§r)', author: 'Yi-Syuan Liou; Wen-Chin Huang; Ming-Chi Yen; Shu-Wei Tsai; Yu-Huai Peng; Tomoki Toda; Yu Tsao; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:2109.03551", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTime Alignment using Lip Images for Frame-based Electrolaryngeal Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oYi-Syuan Liou\\nWen-Chin Huang\\nMing-Chi Yen\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.03551\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 8 Sep 2021 11:24:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to APSIPA ASC 2021\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Li Zhang; Huan Zhao; Qinling Meng; Yanli Chen; Min Liu; Lei Xie', display:{Lore:['[{"text": "arXiv:2109.03568", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBeijing ZKJ-NPU Speaker Verification System for VoxCeleb Speaker Recognition Challenge 2021\\u00a7r\\n\\n\\u00a78\\u00a7oLi Zhang\\nHuan Zhao\\nQinling Meng\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.03568\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 18 Nov 2021 16:06:39 GMT)\\u00a7r"}']}
{title:'Zheng et al. (§72021§r)', author: 'Siqi Zheng; Shiliang Zhang; Weilong Huang; Qian Chen; Hongbin Suo; Ming Lei; Jinwei Feng; Zhijie Yan', display:{Lore:['[{"text": "arXiv:2109.04049", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBeamTransformer: Microphone Array-based Overlapping Speech Detection\\u00a7r\\n\\n\\u00a78\\u00a7oSiqi Zheng\\nShiliang Zhang\\nWeilong Huang\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.04049\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Sep 2021 06:10:48 GMT)\\u00a7r"}']}
{title:'Togootogtokh et al. (§72021§r)', author: 'Enkhtogtokh Togootogtokh; Christian Klasen', display:{Lore:['[{"text": "arXiv:2109.04081", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeepEMO: Deep Learning for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oEnkhtogtokh Togootogtokh\\nChristian Klasen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.04081\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Sep 2021 07:51:57 GMT)\\u00a7r"}']}
{title:'Misawa et al. (§72021§r)', author: 'Sota Misawa; Norihiro Takamune; Tomohiko Nakamura; Daichi Kitamura; Hiroshi Saruwatari; Masakazu Une; Shoji Makino', display:{Lore:['[{"text": "arXiv:2109.04658", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Enhancement by Noise Self-Supervised Rank-Constrained Spatial Covariance Matrix Estimation via Independent Deeply Learned Matrix Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oSota Misawa\\nNorihiro Takamune\\nTomohiko Nakamura\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.04658\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 10 Sep 2021 04:25:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted for APSIPA2021\\u00a7r"}']}
{title:'Gong et al. (§72021§r)', author: 'Rong Gong; Carl Quillen; Dushyant Sharma; Andrew Goderre; José Laínez; Ljubomir Milanović', display:{Lore:['[{"text": "arXiv:2109.04783", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Attention Channel Combinator Frontend for End-to-End Multichannel Far-field Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oRong Gong\\nCarl Quillen\\nDushyant Sharma\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.04783\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 10 Sep 2021 11:03:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of Interspeech 2021\\u00a7r"}']}
{title:'Kong et al. (§72021§r)', author: 'Qiuqiang Kong; Yin Cao; Haohe Liu; Keunwoo Choi; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2109.05418", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDecoupling Magnitude and Phase Estimation with Deep ResUNet for Music Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oQiuqiang Kong\\nYin Cao\\nHaohe Liu\\nKeunwoo Choi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.05418\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInternational Society for Music Information Retrieval (ISMIR) 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 12 Sep 2021 03:42:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages\\u00a7r"}']}
{title:'Tang et al. (§72021§r)', author: 'Chuanxin Tang; Chong Luo; Zhiyuan Zhao; Dacheng Yin; Yucheng Zhao; Wenjun Zeng', display:{Lore:['[{"text": "arXiv:2109.05426", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero-Shot Text-to-Speech for Text-Based Insertion in Audio Narration\\u00a7r\\n\\n\\u00a78\\u00a7oChuanxin Tang\\nChong Luo\\nZhiyuan Zhao\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.05426\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 12 Sep 2021 04:17:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in Interspeech\'21\\u00a7r"}']}
{title:'Abarghooie et al. (§72021§r)', author: 'Reyhane Abarghooie; Zahra Sadat Zomorodian; Mohammad Tahsildoost; Zohreh Shaghaghian', display:{Lore:['[{"text": "arXiv:2109.06459", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Machine-learning Framework for Acoustic Design Assessment in Early Design Stages\\u00a7r\\n\\n\\u00a78\\u00a7oReyhane Abarghooie\\nZahra Sadat Zomorodian\\nMohammad Tahsildoost\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.06459\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Sep 2021 06:04:27 GMT)\\u00a7r"}']}
{title:'Sundstrom (§72021§r)', author: 'Jacob Sundstrom', display:{Lore:['[{"text": "arXiv:2109.08704", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Placement Agnosticism: Improving the Distance-based Amplitude Panning Algorithm\\u00a7r\\n\\n\\u00a78\\u00a7oJacob Sundstrom\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.08704\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 17 Sep 2021 18:10:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oI3DA 2021 International Conference\\u00a7r"}']}
{title:'Zhu et al. (§72021§r)', author: 'Wentao Zhu; Tianlong Kong; Shun Lu; Jixiang Li; Dawei Zhang; Feng Deng; Xiaorui Wang; Sen Yang; Ji Liu', display:{Lore:['[{"text": "arXiv:2109.08839", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeechNAS: Towards Better Trade-off between Latency and Accuracy for Large-Scale Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oWentao Zhu\\nTianlong Kong\\nShun Lu\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.08839\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 18 Sep 2021 05:31:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 3 figures, 3 tables. Accepted by ASRU2021\\u00a7r"}']}
{title:'Chang et al. (§72021§r)', author: 'Pei-Chun Chang; Yong-Sheng Chen; Chang-Hsing Lee', display:{Lore:['[{"text": "arXiv:2109.08910", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMS-SincResNet: Joint learning of 1D and 2D kernels using multi-scale SincNet and ResNet for music genre classification\\u00a7r\\n\\n\\u00a78\\u00a7oPei-Chun Chang\\nYong-Sheng Chen\\nChang-Hsing Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.08910\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 18 Sep 2021 11:39:53 GMT)\\u00a7r"}']}
{title:'Pham et al. (§72021§r)', author: 'Nhat Truong Pham; Duc Ngoc Minh Dang; Sy Dzung Nguyen', display:{Lore:['[{"text": "arXiv:2109.09026", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHybrid Data Augmentation and Deep Attention-based Dilated Convolutional-Recurrent Neural Networks for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oNhat Truong Pham\\nDuc Ngoc Minh Dang\\nSy Dzung Nguyen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.09026\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 18 Sep 2021 23:13:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 16 figures, 6 tables\\u00a7r"}']}
{title:'Kuzminykh et al. (§72021§r)', author: 'Ievgeniia Kuzminykh; Dan Shevchuk; Stavros Shiaeles; Bogdan Ghita', display:{Lore:['[{"text": "arXiv:2109.09906", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Interval Retrieval using Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oIevgeniia Kuzminykh\\nDan Shevchuk\\nStavros Shiaeles\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.09906\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-030-65726-0_21\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Sep 2021 01:32:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o20th International Conference on Next Generation Teletraffic and Wired/Wireless Advanced Networks andSystems, NEW2AN 2020 and 13th Conference on the Internet of Things and Smart Spaces, ruSMART 2020\\u00a7r"}']}
{title:'Pillay (§72021§r)', author: 'Ashwin Pillay', display:{Lore:['[{"text": "arXiv:2109.10455", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Audio Synthesis Framework Derived from Industrial Process Control\\u00a7r\\n\\n\\u00a78\\u00a7oAshwin Pillay\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.10455\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Sep 2021 23:20:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 24 figures\\u00a7r"}']}
{title:'Xie et al. (§72021§r)', author: 'Chao Xie; Yi-Chiao Wu; Patrick Lumban Tobing; Wen-Chin Huang; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2109.10608", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoisy-to-Noisy Voice Conversion Framework with Denoising Model\\u00a7r\\n\\n\\u00a78\\u00a7oChao Xie\\nYi-Chiao Wu\\nPatrick Lumban Tobing\\nWen-Chin Huang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.10608\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 Sep 2021 09:21:52 GMT)\\u00a7r"}']}
{title:'Saeki et al. (§72021§r)', author: 'Takaaki Saeki; Shinnosuke Takamichi; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2109.10724", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Latency Incremental Text-to-Speech Synthesis with Distilled Context Prediction Network\\u00a7r\\n\\n\\u00a78\\u00a7oTakaaki Saeki\\nShinnosuke Takamichi\\nHiroshi Saruwatari\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.10724\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 Sep 2021 13:29:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ASRU2021\\u00a7r"}']}
{title:'Chen et al. (§72021§r)', author: 'Szu-Jui Chen; Wei Xia; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:2109.11086", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScenario Aware Speech Recognition: Advancements for Apollo Fearless Steps     CHiME-4 Corpora\\u00a7r\\n\\n\\u00a78\\u00a7oSzu-Jui Chen\\nWei Xia\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.11086\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Sep 2021 00:43:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ASRU 2021\\u00a7r"}']}
{title:'Wong et al. (§72021§r)', author: 'Jeremy H. M. Wong; Yifan Gong', display:{Lore:['[{"text": "arXiv:2109.11140", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint speaker diarisation and tracking in switching state-space model\\u00a7r\\n\\n\\u00a78\\u00a7oJeremy H. M. Wong\\nYifan Gong\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.11140\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Sep 2021 04:43:58 GMT)\\u00a7r"}']}
{title:'Kawahara et al. (§72021§r)', author: 'Hideki Kawahara; Toshie Matsui Kohei; Yatabe Ken-Ichi Sakakibara Minoru Tsuzaki Masanori Morise Toshio Irino', display:{Lore:['[{"text": "arXiv:2109.11594", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImplementation of interactive tools for investigating fundamental frequency response of voiced sounds to auditory stimulation\\u00a7r\\n\\n\\u00a78\\u00a7oHideki Kawahara\\nToshie Matsui Kohei\\nYatabe Ken-Ichi Sakakibara Minoru Tsuzaki Masanori Morise Toshio Irino\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.11594\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Sep 2021 18:55:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for APSIPA ASC 2021\\u00a7r"}']}
{title:'Nandekar et al. (§72021§r)', author: 'Abhsihek Nandekar; Preeth Khona; Rajani M. B.; Anindya Sinha; Nithin Nagaraj', display:{Lore:['[{"text": "arXiv:2109.11782", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCausal Analysis of Carnatic Music: A Preliminary Study\\u00a7r\\n\\n\\u00a78\\u00a7oAbhsihek Nandekar\\nPreeth Khona\\nRajani M. B.\\nAnindya Sinha\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.11782\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Sep 2021 07:24:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o22 pages, 12 figures\\u00a7r"}']}
{title:'Champion et al. (§72021§r)', author: 'Pierre Champion; Denis Jouvet; Anthony Larcher', display:{Lore:['[{"text": "arXiv:2109.11946", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluating X-vector-based Speaker Anonymization under White-box Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oPierre Champion\\nDenis Jouvet\\nAnthony Larcher\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.11946\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n23rd International Conference on Speech and Computer - SPECOM\\n  2021, Sep 2021, Saint Petersburg, Russia\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 30 Sep 2021 09:04:53 GMT)\\u00a7r"}']}
{title:'Rust et al. (§72021§r)', author: 'Romana Rust; Achilleas Xydis; Kurt Heutschi; Nathanaël Perraudin; Gonzalo Casas; Chaoyu Du; Jürgen Strauss; Kurt Eggenschwiler; Fernando Perez-Cruz; Fabio Gramazio; Matthias Kohler', display:{Lore:['[{"text": "arXiv:2109.12014", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA data acquisition setup for data driven acoustic design\\u00a7r\\n\\n\\u00a78\\u00a7oRomana Rust\\nAchilleas Xydis\\nKurt Heutschi\\n+ 7 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.12014\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1177/1351010X20986901\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nBuilding Acoustics. February 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Sep 2021 15:20:02 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Xuechen Liu; Md Sahidullah; Tomi Kinnunen', display:{Lore:['[{"text": "arXiv:2109.12056", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParameterized Channel Normalization for Far-field Deep Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oXuechen Liu\\nMd Sahidullah\\nTomi Kinnunen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.12056\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Sep 2021 16:22:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at ASRU 2021\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Xuechen Liu; Md Sahidullah; Tomi Kinnunen', display:{Lore:['[{"text": "arXiv:2109.12058", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimized Power Normalized Cepstral Coefficients towards Robust Deep Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oXuechen Liu\\nMd Sahidullah\\nTomi Kinnunen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.12058\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Sep 2021 16:26:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at ASRU 2021\\u00a7r"}']}
{title:'Jot et al. (§72021§r)', author: 'Jean-Marc Jot; Rémi Audfray; Mark Hertensteiner; Brian Schmidt', display:{Lore:['[{"text": "arXiv:2109.12471", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRendering Spatial Sound for Interoperable Experiences in the Audio Metaverse\\u00a7r\\n\\n\\u00a78\\u00a7oJean-Marc Jot\\nR\\u00e9mi Audfray\\nMark Hertensteiner\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.12471\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 26 Sep 2021 01:24:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInternational Conference on Immersive and 3D Audio (i3DA), September 2021\\u00a7r"}']}
{title:'Imai (§72021§r)', author: 'Yusuke Imai', display:{Lore:['[{"text": "arXiv:2109.12475", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeneral Theory of Music by Icosahedron 3: Musical invariant and Melakarta raga\\u00a7r\\n\\n\\u00a78\\u00a7oYusuke Imai\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.12475\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 26 Sep 2021 01:55:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o31 pages, 34 figures\\u00a7r"}']}
{title:'Fuentes et al. (§72021§r)', author: 'Magdalena Fuentes; Justin Salamon; Pablo Zinemanas; Martín Rocamora; Genís Paja; Irán R. Román; Marius Miron; Xavier Serra; Juan Pablo Bello', display:{Lore:['[{"text": "arXiv:2109.12690", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.DB\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSoundata: A Python library for reproducible use of audio datasets\\u00a7r\\n\\n\\u00a78\\u00a7oMagdalena Fuentes\\nJustin Salamon\\nPablo Zinemanas\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.12690\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 4 Oct 2021 15:23:40 GMT)\\u00a7r"}']}
{title:'Wei et al. (§72021§r)', author: 'Yu-Lin Wei; Romit Roy Choudhury', display:{Lore:['[{"text": "arXiv:2109.13072", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEstimating Angle of Arrival (AoA) of multiple Echoes in a Steering Vector Space\\u00a7r\\n\\n\\u00a78\\u00a7oYu-Lin Wei\\nRomit Roy Choudhury\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.13072\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Sep 2021 14:21:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages, 20 figures\\u00a7r"}']}
{title:'Wei et al. (§72021§r)', author: 'Yu-Lin Wei; Rui Li; Abhinav Mehrotra; Romit Roy Choudhury; Nic Lane', display:{Lore:['[{"text": "arXiv:2109.13094", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInferring Facing Direction from Voice Signals\\u00a7r\\n\\n\\u00a78\\u00a7oYu-Lin Wei\\nRui Li\\nAbhinav Mehrotra\\nRomit Roy Choudhury\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.13094\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 29 Sep 2021 02:14:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 16 figures\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Haohe Liu; Qiuqiang Kong; Qiao Tian; Yan Zhao; DeLiang Wang; Chuanzeng Huang; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2109.13731", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoiceFixer: Toward General Speech Restoration with Neural Vocoder\\u00a7r\\n\\n\\u00a78\\u00a7oHaohe Liu\\nQiuqiang Kong\\nQiao Tian\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.13731\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 5 Oct 2021 15:52:27 GMT)\\u00a7r"}']}
{title:'Lee et al. (§72021§r)', author: 'Donmoon Lee; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2109.14508", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-domain Semi-Supervised Audio Event Classification Using Contrastive Regularization\\u00a7r\\n\\n\\u00a78\\u00a7oDonmoon Lee\\nKyogu Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.14508\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 29 Sep 2021 15:43:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, and 2 tables. Accepted paperat IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)2021\\u00a7r"}']}
{title:'Bahadi et al. (§72021§r)', author: 'Soufiyan Bahadi; Jean Rouat; Éric Plourde', display:{Lore:['[{"text": "arXiv:2109.14705", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptive Approach For Sparse Representations Using The Locally Competitive Algorithm For Audio\\u00a7r\\n\\n\\u00a78\\u00a7oSoufiyan Bahadi\\nJean Rouat\\n\\u00c9ric Plourde\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.14705\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/MLSP52302.2021.9596348\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2021 IEEE 31st International Workshop on Machine Learning for\\n  Signal Processing (MLSP)\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 29 Sep 2021 20:26:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published at IEEE Machine Learning for Signal Processing 2021\\u00a7r"}']}
{title:'Sun et al. (§72021§r)', author: 'Hongyi Sun; Xinyi Liu; Kecheng Xu; Jinghao Miao; Qi Luo', display:{Lore:['[{"text": "arXiv:2109.14797", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmergency Vehicles Audio Detection and Localization in Autonomous Driving\\u00a7r\\n\\n\\u00a78\\u00a7oHongyi Sun\\nXinyi Liu\\nKecheng Xu\\nJinghao Miao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.14797\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 2 Oct 2021 00:05:08 GMT)\\u00a7r"}']}
{title:'Yesiler et al. (§72021§r)', author: 'Furkan Yesiler; Marius Miron; Joan Serrà; Emilia Gómez', display:{Lore:['[{"text": "arXiv:2109.15188", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAssessing Algorithmic Biases for Musical Version Identification\\u00a7r\\n\\n\\u00a78\\u00a7oFurkan Yesiler\\nMarius Miron\\nJoan Serr\\u00e0\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.15188\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Sep 2021 15:04:02 GMT)\\u00a7r"}']}
{title:'Jain et al. (§72021§r)', author: 'Arjit Jain; Pranay Reddy Samala; Deepak Mittal; Preethi Jyoti; Maneesh Singh', display:{Lore:['[{"text": "arXiv:2110.00046", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpliceOut: A Simple and Efficient Audio Augmentation Method\\u00a7r\\n\\n\\u00a78\\u00a7oArjit Jain\\nPranay Reddy Samala\\nDeepak Mittal\\nPreethi Jyoti\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.00046\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 13 Oct 2021 15:05:15 GMT)\\u00a7r"}']}
{title:'Huo et al. (§72021§r)', author: 'Zhouyuan Huo; Dongseong Hwang; Khe Chai Sim; Shefali Garg; Ananya Misra; Nikhil Siddhartha; Trevor Strohman; Françoise Beaufays', display:{Lore:['[{"text": "arXiv:2110.00155", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIncremental Layer-wise Self-Supervised Learning for Efficient Speech Domain Adaptation On Device\\u00a7r\\n\\n\\u00a78\\u00a7oZhouyuan Huo\\nDongseong Hwang\\nKhe Chai Sim\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.00155\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Oct 2021 01:22:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Zhong-Qiu Wang; Gordon Wichern; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2110.00570", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Low-Distortion Target Estimates for Improved Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oZhong-Qiu Wang\\nGordon Wichern\\nJonathan Le Roux\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.00570\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Oct 2021 17:53:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oin submission\\u00a7r"}']}
{title:'Sudro et al. (§72021§r)', author: 'Protima Nomo Sudro; Rohit Sinha; S. R. Mahadeva Prasanna', display:{Lore:['[{"text": "arXiv:2110.00794", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProcessing Phoneme Specific Segments for Cleft Lip and Palate Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oProtima Nomo Sudro\\nRohit Sinha\\nS. R. Mahadeva Prasanna\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.00794\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Oct 2021 12:51:06 GMT)\\u00a7r"}']}
{title:'Ma et al. (§72021§r)', author: 'Yi Ma; Kong Aik Lee; Ville Hautamaki; Haizhou Li', display:{Lore:['[{"text": "arXiv:2110.00940", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPL-EESR: Perceptual Loss Based END-TO-END Robust Speaker Representation Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oYi Ma\\nKong Aik Lee\\nVille Hautamaki\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.00940\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 3 Oct 2021 07:05:29 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Zhiling Zhang; Zelin Zhou; Haifeng Tang; Guangwei Li; Mengyue Wu; Kenny Q. Zhu', display:{Lore:['[{"text": "arXiv:2110.01009", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnriching Ontology with Temporal Commonsense for Low-Resource Audio Tagging\\u00a7r\\n\\n\\u00a78\\u00a7oZhiling Zhang\\nZelin Zhou\\nHaifeng Tang\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.01009\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3459637.3482097\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 3 Oct 2021 14:19:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCIKM 2021\\u00a7r"}']}
{title:'Lai et al. (§72021§r)', author: 'Cheng-I Jeff Lai; Erica Cooper; Yang Zhang; Shiyu Chang; Kaizhi Qian; Yi-Lun Liao; Yung-Sung Chuang; Alexander H. Liu; Junichi Yamagishi; David Cox; James Glass', display:{Lore:['[{"text": "arXiv:2110.01147", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Interplay Between Sparsity, Naturalness, Intelligibility, and Prosody in Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oCheng-I Jeff Lai\\nErica Cooper\\nYang Zhang\\n+ 7 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.01147\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 28 Oct 2021 03:28:48 GMT)\\u00a7r"}']}
{title:'Eren et al. (§72021§r)', author: 'Ayşegül Özkaya Eren; Mustafa Sert', display:{Lore:['[{"text": "arXiv:2110.01210", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Captioning Using Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oAy\\u015feg\\u00fcl \\u00d6zkaya Eren\\nMustafa Sert\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.01210\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 7 Oct 2021 12:40:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to DCASE 2021 Challenge\\u00a7r"}']}
{title:'Michelson et al. (§72021§r)', author: 'Tzvi Michelson; Shmuel Peleg', display:{Lore:['[{"text": "arXiv:2110.01367", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-Visual Evaluation of Oratory Skills\\u00a7r\\n\\n\\u00a78\\u00a7oTzvi Michelson\\nShmuel Peleg\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.01367\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Sep 2021 11:38:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTransAI 2021\\u00a7r"}']}
{title:'Duarte et al. (§72021§r)', author: 'Julio Cesar Duarte; Sérgio Colcher', display:{Lore:['[{"text": "arXiv:2110.01425", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBuilding a Noisy Audio Dataset to Evaluate Machine Learning Approaches for Automatic Speech Recognition Systems\\u00a7r\\n\\n\\u00a78\\u00a7oJulio Cesar Duarte\\nS\\u00e9rgio Colcher\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.01425\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Oct 2021 13:08:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTech report series Monografias em Ci\\u00eanciada Computa\\u00e7\\u00e3o, september, 2021, Dep. Inform\\u00e1tica PUC-Rio, RJ, BRAZIL, ISSN 0103-9741\\u00a7r"}']}
{title:'Ye et al. (§72021§r)', author: 'Zhirong Ye; Xiangdong Wang; Hong Liu; Yueliang Qian; Rui Tao; Long Yan; Kazushige Ouchi', display:{Lore:['[{"text": "arXiv:2110.02011", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Event Detection Transformer: An Event-based End-to-End Model for Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oZhirong Ye\\nXiangdong Wang\\nHong Liu\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02011\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 12 Nov 2021 04:01:20 GMT)\\u00a7r"}']}
{title:'Wilson et al. (§72021§r)', author: 'Justin Wilson; Sunyeong Park; Seunghye J. Wilson; Ming C. Lin', display:{Lore:['[{"text": "arXiv:2110.02411", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Aging with Audio-Visual Style Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oJustin Wilson\\nSunyeong Park\\nSeunghye J. Wilson\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02411\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Oct 2021 23:33:28 GMT)\\u00a7r"}']}
{title:'Jung et al. (§72021§r)', author: 'Namkyu Jung; Geonmin Kim; Joon Son Chung', display:{Lore:['[{"text": "arXiv:2110.02791", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpell my name: keyword boosted speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oNamkyu Jung\\nGeonmin Kim\\nJoon Son Chung\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02791\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Oct 2021 14:16:57 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Rui Liu; Berrak Sisman; Haizhou Li', display:{Lore:['[{"text": "arXiv:2110.03156", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStrengthNet: Deep Learning-based Emotion Strength Assessment for Emotional Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oRui Liu\\nBerrak Sisman\\nHaizhou Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03156\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 8 Oct 2021 03:28:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022. 5 pages, 3 figures, 1 table. Our codes are available at: https://github.com/ttslr/StrengthNet\\u00a7r"}']}
{title:'Liang et al. (§72021§r)', author: 'Dawei Liang; Yangyang Shi; Yun Wang; Nayan Singhal; Alex Xiao; Jonathan Shaw; Edison Thomaz; Ozlem Kalinli; Mike Seltzer', display:{Lore:['[{"text": "arXiv:2110.03174", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransferring Voice Knowledge for Acoustic Event Detection: An Empirical Study\\u00a7r\\n\\n\\u00a78\\u00a7oDawei Liang\\nYangyang Shi\\nYun Wang\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03174\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Oct 2021 04:03:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Hortal et al. (§72021§r)', author: 'Enrique Hortal; Rodrigo Brechard Alarcia', display:{Lore:['[{"text": "arXiv:2110.03390", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGANtron: Emotional Speech Synthesis with Generative Adversarial Networks\\u00a7r\\n\\n\\u00a78\\u00a7oEnrique Hortal\\nRodrigo Brechard Alarcia\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03390\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Oct 2021 10:44:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 4 figures\\u00a7r"}']}
{title:'Scheidwasser-Clow et al. (§72021§r)', author: 'Neil Scheidwasser-Clow; Mikolaj Kegler; Pierre Beckmann; Milos Cernak', display:{Lore:['[{"text": "arXiv:2110.03414", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSERAB: A multi-lingual benchmark for speech emotion recognition\\u00a7r\\n\\n\\u00a78\\u00a7oNeil Scheidwasser-Clow\\nMikolaj Kegler\\nPierre Beckmann\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03414\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Oct 2021 13:01:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Kopru et al. (§72021§r)', author: 'Berkay Kopru; Engin Erzin', display:{Lore:['[{"text": "arXiv:2110.04091", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAffective Burst Detection from Speech using Kernel-fusion Dilated Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oBerkay Kopru\\nEngin Erzin\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04091\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Oct 2021 12:40:43 GMT)\\u00a7r"}']}
{title:'Casebeer et al. (§72021§r)', author: 'Jonah Casebeer; Nicholas J. Bryan; Paris Smaragdis', display:{Lore:['[{"text": "arXiv:2110.04284", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAuto-DSP: Learning to Optimize Acoustic Echo Cancellers\\u00a7r\\n\\n\\u00a78\\u00a7oJonah Casebeer\\nNicholas J. Bryan\\nParis Smaragdis\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04284\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Oct 2021 17:52:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the 2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). Source code and audio examples: https://jmcasebeer.github.io/projects/auto-dsp/\\u00a7r"}']}
{title:'Gong et al. (§72021§r)', author: 'Cheng Gong; Longbiao Wang; Zhenhua Ling; Ju Zhang; Jianwu Dang', display:{Lore:['[{"text": "arXiv:2110.04451", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing multiple reference audios and style embedding constraints for speech synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oCheng Gong\\nLongbiao Wang\\nZhenhua Ling\\nJu Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04451\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 9 Oct 2021 04:24:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages,3 figures submitted to ICASSP2022\\u00a7r"}']}
{title:'Rudovic et al. (§72021§r)', author: 'Ognjen Rudovic; Akanksha Bindal; Vineet Garg; Pramod Simha; Pranay Dighe; Sachin Kajarekar', display:{Lore:['[{"text": "arXiv:2110.04656", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreaming on-device detection of device directed speech from voice and touch-based invocation\\u00a7r\\n\\n\\u00a78\\u00a7oOgnjen Rudovic\\nAkanksha Bindal\\nVineet Garg\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04656\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 9 Oct 2021 22:33:42 GMT)\\u00a7r"}']}
{title:'Singh et al. (§72021§r)', author: 'Rita Singh; Ankit Shah; Hira Dhamyal', display:{Lore:['[{"text": "arXiv:2110.04678", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Overview of Techniques for Biomarker Discovery in Voice Signal\\u00a7r\\n\\n\\u00a78\\u00a7oRita Singh\\nAnkit Shah\\nHira Dhamyal\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04678\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 10 Oct 2021 01:39:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oLast two authors contributed equally to the paper\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Chao Wang; Zhonghao Li; Benlai Tang; Xiang Yin; Yuan Wan; Yibiao Yu; Zejun Ma', display:{Lore:['[{"text": "arXiv:2110.04754", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards High-fidelity Singing Voice Conversion with Acoustic Reference and Contrastive Predictive Coding\\u00a7r\\n\\n\\u00a78\\u00a7oChao Wang\\nZhonghao Li\\nBenlai Tang\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04754\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 10 Oct 2021 10:27:20 GMT)\\u00a7r"}']}
{title:'Kumar et al. (§72021§r)', author: 'Rajnish Kumar; Manjeet Dahiya', display:{Lore:['[{"text": "arXiv:2110.04765", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-task Learning with Metadata for Music Mood Classification\\u00a7r\\n\\n\\u00a78\\u00a7oRajnish Kumar\\nManjeet Dahiya\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04765\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 10 Oct 2021 11:36:34 GMT)\\u00a7r"}']}
{title:'Horiuchi et al. (§72021§r)', author: 'Ryosuke Horiuchi; Shoichi Koyama; Juliano G. C. Ribeiro; Natsuki Ueno; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2110.04972", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKernel Learning For Sound Field Estimation With L1 and L2 Regularizations\\u00a7r\\n\\n\\u00a78\\u00a7oRyosuke Horiuchi\\nShoichi Koyama\\nJuliano G. C. Ribeiro\\nNatsuki Ueno\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04972\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 12 Oct 2021 05:45:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEWorkshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)2021\\u00a7r"}']}
{title:'Zou et al. (§72021§r)', author: 'Yi Zou; Pei Zou; Yi Zhao; Kaixiang Zhang; Ran Zhang; Xiaorui Wang', display:{Lore:['[{"text": "arXiv:2110.05020", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMELONS: generating melody with long-term structure using transformers and structure graph\\u00a7r\\n\\n\\u00a78\\u00a7oYi Zou\\nPei Zou\\nYi Zhao\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05020\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 3 Nov 2021 08:49:42 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Shujun Liu; Hai Zhu; Kun Wang; Huajun Wang', display:{Lore:['[{"text": "arXiv:2110.05033", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPitch Preservation In Singing Voice Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oShujun Liu\\nHai Zhu\\nKun Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05033\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 12 Oct 2021 05:39:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Zhao et al. (§72021§r)', author: 'Miao Zhao; Yufeng Ma; Yiwei Ding; Yu Zheng; Min Liu; Minqiang Xu', display:{Lore:['[{"text": "arXiv:2110.05042", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-query multi-head attention pooling and Inter-topK penalty for speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oMiao Zhao\\nYufeng Ma\\nYiwei Ding\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05042\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 12 Oct 2021 03:12:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Wei Liu; Meng Sun; Xiongwei Zhang; Hugo Van hamme; Thomas Fang Zheng', display:{Lore:['[{"text": "arXiv:2110.05087", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Multi-Resolution Front-End for End-to-End Speech Anti-Spoofing\\u00a7r\\n\\n\\u00a78\\u00a7oWei Liu\\nMeng Sun\\nXiongwei Zhang\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05087\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Oct 2021 08:44:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Bittner et al. (§72021§r)', author: 'Rachel M. Bittner; Katherine Pasalo; Juan José Bosch; Gabriel Meseguer-Brocal; David Rubinstein', display:{Lore:['[{"text": "arXiv:2110.05580", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lvocadito: A dataset of solo vocals with f_0, note, and lyric annotations\\u00a7r\\n\\n\\u00a78\\u00a7oRachel M. Bittner\\nKatherine Pasalo\\nJuan Jos\\u00e9 Bosch\\nGabriel Meseguer-Brocal\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05580\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 29 Oct 2021 14:27:37 GMT)\\u00a7r"}']}
{title:'Watcharasupat et al. (§72021§r)', author: 'Karn N. Watcharasupat; Alexander Lerch', display:{Lore:['[{"text": "arXiv:2110.05587", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.IT\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a72math.IT\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluation of Latent Space Disentanglement in the Presence of Interdependent Attributes\\u00a7r\\n\\n\\u00a78\\u00a7oKarn N. Watcharasupat\\nAlexander Lerch\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05587\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Oct 2021 20:01:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to the Late-Breaking Demo Session of the 22nd International Society for Music Information Retrieval Conference\\u00a7r"}']}
{title:'Tai et al. (§72021§r)', author: 'Wenxin Tai; Jiajia Li; Yixiang Wang; Tian Lan; Qiao Liu', display:{Lore:['[{"text": "arXiv:2110.05713", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFoster Strengths and Circumvent Weaknesses: a Speech Enhancement Framework with Two-branch Collaborative Learning\\u00a7r\\n\\n\\u00a78\\u00a7oWenxin Tai\\nJiajia Li\\nYixiang Wang\\nTian Lan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05713\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Oct 2021 03:05:06 GMT)\\u00a7r"}']}
{title:'Sigel et al. (§72021§r)', author: 'Miles Sigel; Michael Zhou; Jiebo Luo', display:{Lore:['[{"text": "arXiv:2110.05765", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Sentiment Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oMiles Sigel\\nMichael Zhou\\nJiebo Luo\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05765\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Oct 2021 06:51:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oNSF REU: Computational Methods for Understanding Music, Media, and Minds, University of Rochester\\u00a7r"}']}
{title:'Fu et al. (§72021§r)', author: 'Szu-Wei Fu; Cheng Yu; Kuo-Hsuan Hung; Mirco Ravanelli; Yu Tsao', display:{Lore:['[{"text": "arXiv:2110.05866", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMetricGAN-U: Unsupervised speech enhancement/ dereverberation based only on noisy/ reverberated speech\\u00a7r\\n\\n\\u00a78\\u00a7oSzu-Wei Fu\\nCheng Yu\\nKuo-Hsuan Hung\\nMirco Ravanelli\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05866\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Oct 2021 10:01:32 GMT)\\u00a7r"}']}
{title:'Ye et al. (§72021§r)', author: 'Zhongjie Ye; Helin Wang; Dongchao Yang; Yuexian Zou', display:{Lore:['[{"text": "arXiv:2110.06100", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving the Performance of Automated Audio Captioning via Integrating the Acoustic and Semantic Information\\u00a7r\\n\\n\\u00a78\\u00a7oZhongjie Ye\\nHelin Wang\\nDongchao Yang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06100\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Oct 2021 15:49:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, accepted by DCASE 2021 workshop\\u00a7r"}']}
{title:'Huang et al. (§72021§r)', author: 'Wen-Chin Huang; Shu-Wen Yang; Tomoki Hayashi; Hung-Yi Lee; Shinji Watanabe; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2110.06280", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lS3PRL-VC: Open-source Voice Conversion Framework with Self-supervised Speech Representations\\u00a7r\\n\\n\\u00a78\\u00a7oWen-Chin Huang\\nShu-Wen Yang\\nTomoki Hayashi\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06280\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Oct 2021 19:01:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022. Code available at: https://github.com/s3prl/s3prl/tree/master/s3prl/downstream/a2o-vc-vcc2020\\u00a7r"}']}
{title:'Phan et al. (§72021§r)', author: 'Son Phan; Lam Pham', display:{Lore:['[{"text": "arXiv:2110.06323", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Annihilating Filter-Based DOA Estimation for Uniform Linear Array\\u00a7r\\n\\n\\u00a78\\u00a7oSon Phan\\nLam Pham\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06323\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Oct 2021 20:16:53 GMT)\\u00a7r"}']}
{title:'Holopainen (§72021§r)', author: 'Risto Holopainen', display:{Lore:['[{"text": "arXiv:2110.06371", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75nlin.AO\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAlgorithmic Composition by Autonomous Systems with Multiple Time-Scales\\u00a7r\\n\\n\\u00a78\\u00a7oRisto Holopainen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06371\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Oct 2021 21:32:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o28 pages, 3 figures. Submitted to Divergence Press\\u00a7r"}']}
{title:'Qin et al. (§72021§r)', author: 'Xiaoyi Qin; Na Li; Chao Weng; Dan Su; Ming Li', display:{Lore:['[{"text": "arXiv:2110.06534", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSimple Attention Module based Speaker Verification with Iterative noisy label detection\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoyi Qin\\nNa Li\\nChao Weng\\nDan Su\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06534\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Oct 2021 07:00:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP2022\\u00a7r"}']}
{title:'Mallol-Ragolta et al. (§72021§r)', author: 'Adria Mallol-Ragolta; Helena Cuesta; Emilia Gómez; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2110.06543", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEIHW-MTG DiCOVA 2021 Challenge System Report\\u00a7r\\n\\n\\u00a78\\u00a7oAdria Mallol-Ragolta\\nHelena Cuesta\\nEmilia G\\u00f3mez\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06543\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Oct 2021 07:38:54 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Li Zhang; Qing Wang; Lei Xie', display:{Lore:['[{"text": "arXiv:2110.06565", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDuality Temporal-channel-frequency Attention Enhanced Speaker Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oLi Zhang\\nQing Wang\\nLei Xie\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06565\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 15 Oct 2021 02:19:51 GMT)\\u00a7r"}']}
{title:'Pepino et al. (§72021§r)', author: 'Leonardo Pepino; Pablo Riera; Luciana Ferrer', display:{Lore:['[{"text": "arXiv:2110.06999", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStudy of positional encoding approaches for Audio Spectrogram Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oLeonardo Pepino\\nPablo Riera\\nLuciana Ferrer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06999\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747742\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Oct 2021 19:20:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022. 5 pages, 3 figures\\u00a7r"}']}
{title:'Michael et al. (§72021§r)', author: 'Jeffrey Josanne Michael; Nagendra Kumar Goel; Navneeth K; Jonas Robertson; Shravan Mishra', display:{Lore:['[{"text": "arXiv:2110.07027", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparison of SVD and factorized TDNN approaches for speech to text\\u00a7r\\n\\n\\u00a78\\u00a7oJeffrey Josanne Michael\\nNagendra Kumar Goel\\nNavneeth K\\nJonas Robertson\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.07027\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Oct 2021 20:54:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 1 figure, 3 tables\\u00a7r"}']}
{title:'Kiskin et al. (§72021§r)', author: 'Ivan Kiskin; Marianne Sinka; Adam D. Cobb; Waqas Rafique; Lawrence Wang; Davide Zilli; Benjamin Gutteridge; Rinita Dam; Theodoros Marinos; Yunpeng Li; Dickson Msaky; Emmanuel Kaindoa; Gerard Killeen; Eva Herreros-Moya; Kathy J. Willis; Stephen J. Roberts', display:{Lore:['[{"text": "arXiv:2110.07607", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHumBugDB: A Large-scale Acoustic Mosquito Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oIvan Kiskin\\nMarianne Sinka\\nAdam D. Cobb\\n+ 12 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.07607\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Oct 2021 14:18:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks. 10 pages main, 39 pages includingappendix. This paper accompanies the dataset found at https://zenodo.org/record"}','{"text": "/4904800 with corresponding code at https://github.com/HumBug-Mosquito/HumBugDB\\u00a7r"}']}
{title:'Vilamala et al. (§72021§r)', author: 'Marc Roig Vilamala; Tianwei Xing; Harrison Taylor; Luis Garcia; Mani Srivastava; Lance Kaplan; Alun Preece; Angelika Kimmig; Federico Cerutti', display:{Lore:['[{"text": "arXiv:2110.08090", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing DeepProbLog to perform Complex Event Processing on an Audio Stream\\u00a7r\\n\\n\\u00a78\\u00a7oMarc Roig Vilamala\\nTianwei Xing\\nHarrison Taylor\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.08090\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Oct 2021 13:33:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 3 figures\\u00a7r"}']}
{title:'Huang et al. (§72021§r)', author: 'Wen-Chin Huang; Bence Mark Halpern; Lester Phillip Violeta; Odette Scharenborg; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2110.08213", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Identity Preserving Normal to Dysarthric Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oWen-Chin Huang\\nBence Mark Halpern\\nLester Phillip Violeta\\nOdette Scharenborg\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.08213\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Oct 2021 17:18:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Ziteng Wang; Yueyue Na; Biao Tian; Qiang Fu', display:{Lore:['[{"text": "arXiv:2110.08437", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNN3A: Neural Network supported Acoustic Echo Cancellation, Noise Suppression and Automatic Gain Control for Real-Time Communications\\u00a7r\\n\\n\\u00a78\\u00a7oZiteng Wang\\nYueyue Na\\nBiao Tian\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.08437\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 16 Oct 2021 01:38:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP2022\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Ziteng Wang; Yueyue Na; Biao Tian; Qiang Fu', display:{Lore:['[{"text": "arXiv:2110.08439", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lControllable Multichannel Speech Dereverberation based on Deep Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oZiteng Wang\\nYueyue Na\\nBiao Tian\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.08439\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 16 Oct 2021 01:41:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP2022\\u00a7r"}']}
{title:'Lo et al. (§72021§r)', author: 'Tien-Hong Lo; Yao-Ting Sung; Berlin Chen', display:{Lore:['[{"text": "arXiv:2110.08731", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving End-To-End Modeling for Mispronunciation Detection with Effective Augmentation Mechanisms\\u00a7r\\n\\n\\u00a78\\u00a7oTien-Hong Lo\\nYao-Ting Sung\\nBerlin Chen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.08731\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 17 Oct 2021 06:11:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 2 figures, 4 tables, accepted to Asia-Pacific Signal and Information ProcessingAssociation Annual Summit and Conference (APSIPA ASC 2021)\\u00a7r"}']}
{title:'Chenna et al. (§72021§r)', author: 'Srivatsav Chenna; Nils Peters', display:{Lore:['[{"text": "arXiv:2110.08821", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStorage and Authentication of Audio Footage for IoAuT Devices Using Distributed Ledger Technology\\u00a7r\\n\\n\\u00a78\\u00a7oSrivatsav Chenna\\nNils Peters\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.08821\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 17 Oct 2021 13:47:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 3 Figures, 1 code listing\\u00a7r"}']}
{title:'Huang et al. (§72021§r)', author: 'Wen-Chin Huang; Erica Cooper; Junichi Yamagishi; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2110.09103", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLDNet: Unified Listener Dependent Modeling in MOS Prediction for Synthetic Speech\\u00a7r\\n\\n\\u00a78\\u00a7oWen-Chin Huang\\nErica Cooper\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09103\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Oct 2021 08:52:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022. Code available at: https://github.com/unilight/LDNet\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Lantian Li; Ruiqian Nai; Dong Wang', display:{Lore:['[{"text": "arXiv:2110.09116", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal Additive Margin Softmax for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oLantian Li\\nRuiqian Nai\\nDong Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09116\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Oct 2021 09:11:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Lu et al. (§72021§r)', author: 'Wei-Tsung Lu; Ju-Chiang Wang; Minz Won; Keunwoo Choi; Xuchen Song', display:{Lore:['[{"text": "arXiv:2110.09127", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpecTNT: a Time-Frequency Transformer for Music Audio\\u00a7r\\n\\n\\u00a78\\u00a7oWei-Tsung Lu\\nJu-Chiang Wang\\nMinz Won\\nKeunwoo Choi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09127\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInternational Society for Music Information Retrieval (ISMIR) 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Oct 2021 09:30:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages\\u00a7r"}']}
{title:'Delgado et al. (§72021§r)', author: 'Alejandro Delgado; SkoT McDonald; Ning Xu; Charalampos Saitis; Mark Sandler', display:{Lore:['[{"text": "arXiv:2110.09223", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Models for Query by Vocal Percussion: A Comparative Study\\u00a7r\\n\\n\\u00a78\\u00a7oAlejandro Delgado\\nSkoT McDonald\\nNing Xu\\nCharalampos Saitis\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09223\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Oct 2021 12:27:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in proceedings of the International ComputerMusic Conference (ICMC) 2021\\u00a7r"}']}
{title:'Mallol-Ragolta et al. (§72021§r)', author: 'Adria Mallol-Ragolta; Helena Cuesta; Emilia Gómez; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2110.09239", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEIHW-MTG: Second DiCOVA Challenge System Report\\u00a7r\\n\\n\\u00a78\\u00a7oAdria Mallol-Ragolta\\nHelena Cuesta\\nEmilia G\\u00f3mez\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09239\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Oct 2021 12:39:00 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Zhenyu Zhang; Yewei Gu; Xiaowei Yi; Xianfeng Zhao', display:{Lore:['[{"text": "arXiv:2110.09441", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFMFCC-A: A Challenging Mandarin Dataset for Synthetic Speech Detection\\u00a7r\\n\\n\\u00a78\\u00a7oZhenyu Zhang\\nYewei Gu\\nXiaowei Yi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09441\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Oct 2021 16:22:29 GMT)\\u00a7r"}']}
{title:'Kacprzak et al. (§72021§r)', author: 'Stanisław Kacprzak; Konrad Kowalczyk', display:{Lore:['[{"text": "arXiv:2110.09598", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Domain Adaptation with Paired Examples for Acoustic Scene Classification on Different Recording Devices\\u00a7r\\n\\n\\u00a78\\u00a7oStanis\\u0142aw Kacprzak\\nKonrad Kowalczyk\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09598\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.23919/EUSIPCO54536.2021.9616321\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2021 29th European Signal Processing Conference (EUSIPCO), Dublin,\\n  Ireland, 2021, pp. 1030-103\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Oct 2021 19:34:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in the Proceedings of the 29th European Signal Processing Conference (EUSIPCO), Dublin, Ireland, 2021\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Yu Wang; Nicholas J. Bryan; Justin Salamon; Mark Cartwright; Juan Pablo Bello', display:{Lore:['[{"text": "arXiv:2110.09600", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWho calls the shots? Rethinking Few-Shot Learning for Audio\\u00a7r\\n\\n\\u00a78\\u00a7oYu Wang\\nNicholas J. Bryan\\nJustin Salamon\\nMark Cartwright\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09600\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Oct 2021 19:47:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWASPAA 2021\\u00a7r"}']}
{title:'Comunità et al. (§72021§r)', author: 'Marco Comunità; Huy Phan; Joshua D. Reiss', display:{Lore:['[{"text": "arXiv:2110.09605", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Synthesis of Footsteps Sound Effects with Generative Adversarial Networks\\u00a7r\\n\\n\\u00a78\\u00a7oMarco Comunit\\u00e0\\nHuy Phan\\nJoshua D. Reiss\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09605\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 10 Dec 2021 12:11:13 GMT)\\u00a7r"}']}
{title:'Ma et al. (§72021§r)', author: 'Yufeng Ma; Miao Zhao; Yiwei Ding; Yu Zheng; Min Liu; Minqiang Xu', display:{Lore:['[{"text": "arXiv:2110.09720", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRep Works in Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oYufeng Ma\\nMiao Zhao\\nYiwei Ding\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09720\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Oct 2021 03:47:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Zhao et al. (§72021§r)', author: 'Huaibo Zhao; Yosuke Higuchi; Tetsuji Ogawa; Tetsunori Kobayashi', display:{Lore:['[{"text": "arXiv:2110.10402", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Investigation of Enhancing CTC Model for Triggered Attention-based Streaming ASR\\u00a7r\\n\\n\\u00a78\\u00a7oHuaibo Zhao\\nYosuke Higuchi\\nTetsuji Ogawa\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.10402\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Oct 2021 06:44:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to APSIPA 2021\\u00a7r"}']}
{title:'Cohen et al. (§72021§r)', author: 'Ariel Cohen; Inbal Rimon; Eran Aflalo; Haim Permuter', display:{Lore:['[{"text": "arXiv:2110.10491", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Study On Data Augmentation In Voice Anti-Spoofing\\u00a7r\\n\\n\\u00a78\\u00a7oAriel Cohen\\nInbal Rimon\\nEran Aflalo\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.10491\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Oct 2021 11:09:05 GMT)\\u00a7r"}']}
{title:'Sivaraman et al. (§72021§r)', author: 'Aswin Sivaraman; Scott Wisdom; Hakan Erdogan; John R. Hershey', display:{Lore:['[{"text": "arXiv:2110.10739", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdapting Speech Separation to Real-World Meetings Using Mixture Invariant Training\\u00a7r\\n\\n\\u00a78\\u00a7oAswin Sivaraman\\nScott Wisdom\\nHakan Erdogan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.10739\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Oct 2021 19:21:41 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Xuechen Liu; Md Sahidullah; Tomi Kinnunen', display:{Lore:['[{"text": "arXiv:2110.10983", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimizing Multi-Taper Features for Deep Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oXuechen Liu\\nMd Sahidullah\\nTomi Kinnunen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.10983\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2021.3122796\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Oct 2021 08:56:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in IEEE Signal Processing Letters\\u00a7r"}']}
{title:'Tarjano et al. (§72021§r)', author: 'Carlos Tarjano; Valdecy Pereira', display:{Lore:['[{"text": "arXiv:2110.11807", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSignal-Envelope: A C++ library with Python bindings for temporal envelope estimation\\u00a7r\\n\\n\\u00a78\\u00a7oCarlos Tarjano\\nValdecy Pereira\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.11807\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Oct 2021 14:27:02 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Wei Wang; Shuo Ren; Yao Qian; Shujie Liu; Yu Shi; Yanmin Qian; Michael Zeng', display:{Lore:['[{"text": "arXiv:2110.12138", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimizing Alignment of Speech and Language Latent Spaces for End-to-End Speech Recognition and Understanding\\u00a7r\\n\\n\\u00a78\\u00a7oWei Wang\\nShuo Ren\\nYao Qian\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.12138\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 23 Oct 2021 04:45:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Żelasko et al. (§72021§r)', author: 'Piotr Żelasko; Daniel Povey; Jan "Yenda" Trmal; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:2110.12561", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLhotse: a speech data representation library for the modern deep learning ecosystem\\u00a7r\\n\\n\\u00a78\\u00a7oPiotr \\u017belasko\\nDaniel Povey\\nJan \\"Yenda\\" Trmal\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.12561\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Oct 2021 00:32:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for presentation at NeurIPS 2021 Data-Centric AI (DCAI) Workshop\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Yanqing Liu; Zhihang Xu; Gang Wang; Kuan Chen; Bohan Li; Xu Tan; Jinzhu Li; Lei He; Sheng Zhao', display:{Lore:['[{"text": "arXiv:2110.12612", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDelightfulTTS: The Microsoft Speech Synthesis System for Blizzard Challenge 2021\\u00a7r\\n\\n\\u00a78\\u00a7oYanqing Liu\\nZhihang Xu\\nGang Wang\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.12612\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 19 Nov 2021 04:41:50 GMT)\\u00a7r"}']}
{title:'Giannakopoulos et al. (§72021§r)', author: 'Petros Giannakopoulos; Aggelos Pikrakis; Yannis Cotronis', display:{Lore:['[{"text": "arXiv:2110.12778", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Deep Reinforcement Learning Approach for Audio-based Navigation and Audio Source Localization in Multi-speaker Environments\\u00a7r\\n\\n\\u00a78\\u00a7oPetros Giannakopoulos\\nAggelos Pikrakis\\nYannis Cotronis\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.12778\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 27 Nov 2021 21:04:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2105.04488\\u00a7r"}']}
{title:'Lu et al. (§72021§r)', author: 'Wei-Tsung Lu; Meng-Hsuan Wu; Yuh-Ming Chiu; Li Su', display:{Lore:['[{"text": "arXiv:2110.12855", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lActions Speak Louder than Listening: Evaluating Music Style Transfer based on Editing Experience\\u00a7r\\n\\n\\u00a78\\u00a7oWei-Tsung Lu\\nMeng-Hsuan Wu\\nYuh-Ming Chiu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.12855\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3474085.3475529\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Oct 2021 12:20:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, Proceedings of the 29th ACM International Conference on Multimedia\\u00a7r"}']}
{title:'Manilow et al. (§72021§r)', author: "Ethan Manilow; Patrick O'Reilly; Prem Seetharaman; Bryan Pardo", display:{Lore:['[{"text": "arXiv:2110.13071", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Source Separation By Steering Pretrained Music Models\\u00a7r\\n\\n\\u00a78\\u00a7oEthan Manilow\\nPatrick O\'Reilly\\nPrem Seetharaman\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.13071\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Oct 2021 16:08:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Garcia et al. (§72021§r)', author: 'Hugo Flores Garcia; Aldo Aguilar; Ethan Manilow; Dmitry Vedenko; Bryan Pardo', display:{Lore:['[{"text": "arXiv:2110.13323", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning Tools for Audacity: Helping Researchers Expand the Artist\'s Toolkit\\u00a7r\\n\\n\\u00a78\\u00a7oHugo Flores Garcia\\nAldo Aguilar\\nEthan Manilow\\nDmitry Vedenko\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.13323\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 28 Oct 2021 22:57:34 GMT)\\u00a7r"}']}
{title:'Gimeno et al. (§72021§r)', author: 'Pablo Gimeno; Victoria Mingote; Alfonso Ortega; Antonio Miguel; Eduardo Lleida', display:{Lore:['[{"text": "arXiv:2110.14425", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeneralizing AUC Optimization to Multiclass Classification for Audio Segmentation With Limited Training Data\\u00a7r\\n\\n\\u00a78\\u00a7oPablo Gimeno\\nVictoria Mingote\\nAlfonso Ortega\\nAntonio Miguel\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.14425\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2021.3084501\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Signal Processing Letters, vol. 28, pp. 1135-1139, 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Oct 2021 13:36:04 GMT)\\u00a7r"}']}
{title:'Choi et al. (§72021§r)', author: 'Hyeong-Seok Choi; Juheon Lee; Wansoo Kim; Jie Hwan Lee; Hoon Heo; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2110.14513", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Analysis and Synthesis: Reconstructing Speech from Self-Supervised Representations\\u00a7r\\n\\n\\u00a78\\u00a7oHyeong-Seok Choi\\nJuheon Lee\\nWansoo Kim\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.14513\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 28 Oct 2021 13:36:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oNeuralInformation ProcessingSystems (NeurIPS) 2021\\u00a7r"}']}
{title:'Yuan et al. (§72021§r)', author: 'Yougen Yuan; Zhiqiang Lv; Shen Huang; Pengfei Hu', display:{Lore:['[{"text": "arXiv:2110.15316", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVRM-Phase I VKW system description of long-short video customizable keyword wakeup challenge\\u00a7r\\n\\n\\u00a78\\u00a7oYougen Yuan\\nZhiqiang Lv\\nShen Huang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.15316\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Oct 2021 08:42:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, in Chinese language, 3 tables, NCMMC 2021 conference paper\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Heming Wang; Yao Qian; Xiaofei Wang; Yiming Wang; Chengyi Wang; Shujie Liu; Takuya Yoshioka; Jinyu Li; DeLiang Wang', display:{Lore:['[{"text": "arXiv:2110.15430", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Noise Robustness of Contrastive Speech Representation Learning with Speech Reconstruction\\u00a7r\\n\\n\\u00a78\\u00a7oHeming Wang\\nYao Qian\\nXiaofei Wang\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.15430\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Oct 2021 20:39:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, submitted to ICASSP 2022\\u00a7r"}']}
{title:'Pérez-González-de-Martos … (§72021§r)', author: 'Alejandro Pérez-González-de-Martos; Albert Sanchis; Alfons Juan', display:{Lore:['[{"text": "arXiv:2110.15792", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVRAIN-UPV MLLP\'s system for the Blizzard Challenge 2021\\u00a7r\\n\\n\\u00a78\\u00a7oAlejandro P\\u00e9rez-Gonz\\u00e1lez-de-Martos\\nAlbert Sanchis\\nAlfons Juan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.15792\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Oct 2021 13:58:41 GMT)\\u00a7r"}']}
{title:'Muppidi et al. (§72021§r)', author: 'Aneesh Muppidi; Martin Radfar', display:{Lore:['[{"text": "arXiv:2111.00404", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Emotion Recognition Using Quaternion Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oAneesh Muppidi\\nMartin Radfar\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.00404\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 31 Oct 2021 04:06:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in ICASSP 2021\\u00a7r"}']}
{title:'Giri (§72021§r)', author: 'Ananya Giri', display:{Lore:['[{"text": "arXiv:2111.00436", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalysis of North Indian Classical Ragas Using Tonnetz\\u00a7r\\n\\n\\u00a78\\u00a7oAnanya Giri\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.00436\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 31 Oct 2021 09:05:48 GMT)\\u00a7r"}']}
{title:'Berthommier (§72021§r)', author: 'Frédéric Berthommier', display:{Lore:['[{"text": "arXiv:2111.00868", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.class-ph\\u00a7r, \\u00a7bq-bio.PE\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA mathematical model of the vowel space\\u00a7r\\n\\n\\u00a78\\u00a7oFr\\u00e9d\\u00e9ric Berthommier\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.00868\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 2 Nov 2021 10:37:13 GMT)\\u00a7r"}']}
{title:'Tiwari et al. (§72021§r)', author: 'Soham Tiwari; Kshitiz Lakhotia; Manjunath Mulimani', display:{Lore:['[{"text": "arXiv:2111.01205", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluating robustness of You Only Hear Once(YOHO) Algorithm on noisy audios in the VOICe Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oSoham Tiwari\\nKshitiz Lakhotia\\nManjunath Mulimani\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.01205\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Nov 2021 18:58:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 1 figure, 3 tables, Efficient Natural Language and Speech Processing Workshop, NeurIPS 2021\\u00a7r"}']}
{title:'Ching et al. (§72021§r)', author: 'Joann Ching; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2111.01216", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning To Generate Piano Music With Sustain Pedals\\u00a7r\\n\\n\\u00a78\\u00a7oJoann Ching\\nYi-Hsuan Yang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.01216\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Nov 2021 19:12:48 GMT)\\u00a7r"}']}
{title:'Gao et al. (§72021§r)', author: 'Teng Gao; Jian Zhou; Huabin Wang; Liang Tao; Hon Keung Kwan', display:{Lore:['[{"text": "arXiv:2111.01342", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention-Guided Generative Adversarial Network for Whisper to Normal Speech Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oTeng Gao\\nJian Zhou\\nHuabin Wang\\nLiang Tao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.01342\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Nov 2021 03:00:19 GMT)\\u00a7r"}']}
{title:'Pan et al. (§72021§r)', author: 'Qing Pan; Teng Gao; Jian Zhou; Huabin Wang; Liang Tao; Hon Keung Kwan', display:{Lore:['[{"text": "arXiv:2111.01430", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCycleGAN with Dual Adversarial Loss for Bone-Conducted Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oQing Pan\\nTeng Gao\\nJian Zhou\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.01430\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Nov 2021 08:42:50 GMT)\\u00a7r"}']}
{title:'Ooi et al. (§72021§r)', author: 'Kenneth Ooi; Karn N. Watcharasupat; Santi Peksi; Furi Andi Karnapi; Zhen-Ting Ong; Danny Chua; Hui-Wen Leow; Li-Long Kwok; Xin-Lei Ng; Zhen-Ann Loh; Woon-Seng Gan', display:{Lore:['[{"text": "arXiv:2111.02006", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Strongly-Labelled Polyphonic Dataset of Urban Sounds with Spatiotemporal Context\\u00a7r\\n\\n\\u00a78\\u00a7oKenneth Ooi\\nKarn N. Watcharasupat\\nSanti Peksi\\n+ 7 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.02006\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 2021 Asia-Pacific Signal and Information\\n  Processing Association Annual Summit and Conference (APSIPA ASC), 2021, pp.\\n  982-988\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 11 Nov 2021 14:43:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 8 figures, 3 tables. To be published in Proceedings of the 13th Asia Pacific Signal and Information Processing AssociationAnnual Summit and Conference, 2021\\u00a7r"}']}
{title:'Avdeeva et al. (§72021§r)', author: 'Anastasia Avdeeva; Aleksei Gusev; Igor Korsunov; Alexander Kozlov; Galina Lavrentyeva; Sergey Novoselov; Timur Pekhovsky; Andrey Shulipa; Alisa Vinogradova; Vladimir Volokhov; Evgeny Smirnov; Vasily Galyuk', display:{Lore:['[{"text": "arXiv:2111.02298", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSTC speaker recognition systems for the NIST SRE 2021\\u00a7r\\n\\n\\u00a78\\u00a7oAnastasia Avdeeva\\nAleksei Gusev\\nIgor Korsunov\\n+ 8 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.02298\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Nov 2021 15:31:01 GMT)\\u00a7r"}']}
{title:'Stamenovic et al. (§72021§r)', author: 'Marko Stamenovic; Nils L. Westhausen; Li-Chia Yang; Carl Jensen; Alex Pawlicki', display:{Lore:['[{"text": "arXiv:2111.02351", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeight, Block or Unit? Exploring Sparsity Tradeoffs for Speech Enhancement on Tiny Neural Accelerators\\u00a7r\\n\\n\\u00a78\\u00a7oMarko Stamenovic\\nNils L. Westhausen\\nLi-Chia Yang\\nCarl Jensen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.02351\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 Nov 2021 23:19:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in NeurIPS 2021Efficient Natural Langauge and Speech Processing Workshop as oral-spotlight presentation\\u00a7r"}']}
{title:'Fan et al. (§72021§r)', author: 'Peng Fan; Dongyue Guo; Yi Lin; Bo Yang; Jianwei Zhang', display:{Lore:['[{"text": "arXiv:2111.02654", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech recognition for air traffic control via feature learning and end-to-end training\\u00a7r\\n\\n\\u00a78\\u00a7oPeng Fan\\nDongyue Guo\\nYi Lin\\nBo Yang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.02654\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Nov 2021 06:38:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEEICASSP 2022\\u00a7r"}']}
{title:'Devaney (§72021§r)', author: 'Johanna Devaney', display:{Lore:['[{"text": "arXiv:2111.03895", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDigital Audio Processing Tools for Music Corpus Studies\\u00a7r\\n\\n\\u00a78\\u00a7oJohanna Devaney\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.03895\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 Nov 2021 16:48:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint of book chapter: Devaney, J. (InPress). Audio processing tools for music corpus studies. In D. Shanahan, A. Burgoyne, I. Quinn (Eds.), Oxford Handbook of Music and Corpus Studies. New York: Oxford University"}','{"text": "Press. The manuscript contains 6 figures and 3 tables\\u00a7r"}']}
{title:'Lin et al. (§72021§r)', author: 'Yu-Chen Lin; Cheng Yu; Yi-Te Hsu; Szu-Wei Fu; Yu Tsao; Tei-Wei Kuo', display:{Lore:['[{"text": "arXiv:2111.04436", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSEOFP-NET: Compression and Acceleration of Deep Neural Networks for Speech Enhancement Using Sign-Exponent-Only Floating-Points\\u00a7r\\n\\n\\u00a78\\u00a7oYu-Chen Lin\\nCheng Yu\\nYi-Te Hsu\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.04436\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Nov 2021 12:57:41 GMT)\\u00a7r"}']}
{title:'Ulkar et al. (§72021§r)', author: 'Mehmet Gorkem Ulkar; Osman Erman Okman', display:{Lore:['[{"text": "arXiv:2111.04988", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUltra-Low Power Keyword Spotting at the Edge\\u00a7r\\n\\n\\u00a78\\u00a7oMehmet Gorkem Ulkar\\nOsman Erman Okman\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.04988\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Nov 2021 08:24:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures\\u00a7r"}']}
{title:'Stanton et al. (§72021§r)', author: 'Daisy Stanton; Matt Shannon; Soroosh Mariooryad; RJ Skerry-Ryan; Eric Battenberg; Tom Bagby; David Kao', display:{Lore:['[{"text": "arXiv:2111.05095", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Generation\\u00a7r\\n\\n\\u00a78\\u00a7oDaisy Stanton\\nMatt Shannon\\nSoroosh Mariooryad\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.05095\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 7 Nov 2021 22:31:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 3 figures, 4 tables, appendix with 2 tables\\u00a7r"}']}
{title:'Puche et al. (§72021§r)', author: 'Aaron Valero Puche; Sukhan Lee', display:{Lore:['[{"text": "arXiv:2111.05174", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCAESynth: Real-Time Timbre Interpolation and Pitch Control with Conditional Autoencoders\\u00a7r\\n\\n\\u00a78\\u00a7oAaron Valero Puche\\nSukhan Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.05174\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Nov 2021 14:36:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMLSP 2021\\u00a7r"}']}
{title:'Jain et al. (§72021§r)', author: 'Navdeep Jain; Hongcheng Wang', display:{Lore:['[{"text": "arXiv:2111.05501", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInclusive Speaker Verification with Adaptive thresholding\\u00a7r\\n\\n\\u00a78\\u00a7oNavdeep Jain\\nHongcheng Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.05501\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Nov 2021 02:45:18 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72021§r)', author: 'Ziyang Chen; Xixi Hu; Andrew Owens', display:{Lore:['[{"text": "arXiv:2111.05846", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStructure from Silence: Learning Scene Structure from Ambient Sound\\u00a7r\\n\\n\\u00a78\\u00a7oZiyang Chen\\nXixi Hu\\nAndrew Owens\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.05846\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Nov 2021 18:55:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to CoRL 2021 (Oral Presentation)\\u00a7r"}']}
{title:'Andreu-Perez et al. (§72021§r)', author: 'Javier Andreu-Perez; Humberto Pérez-Espinosa; Eva Timonet; Mehrin Kiani; Manuel I. Girón-Pérez; Alma B. Benitez-Trinidad; Delaram Jarchi; Alejandro Rosales-Pérez; Nick Gatzoulis; Orion F. Reyes-Galaviz; Alejandro Torres-García; Carlos A. Reyes-García; Zulfiqar Ali; Francisco Rivas', display:{Lore:['[{"text": "arXiv:2111.05895", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Generic Deep Learning Based Cough Analysis System from Clinically Validated Samples for Point-of-Need Covid-19 Test and Severity Levels\\u00a7r\\n\\n\\u00a78\\u00a7oJavier Andreu-Perez\\nHumberto P\\u00e9rez-Espinosa\\nEva Timonet\\n+ 10 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.05895\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TSC.2021.3061402\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Transactions on Services Computing (2021)\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Nov 2021 19:39:26 GMT)\\u00a7r"}']}
{title:'Tan et al. (§72021§r)', author: 'Chih-Pin Tan; Chin-Jui Chang; Alvin W. Y. Su; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2111.06046", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Score Expansion with Variable-Length Infilling\\u00a7r\\n\\n\\u00a78\\u00a7oChih-Pin Tan\\nChin-Jui Chang\\nAlvin W. Y. Su\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.06046\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Nov 2021 04:27:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oGoing to published as a late-breaking demo paper at ISMIR 2021\\u00a7r"}']}
{title:'Lin et al. (§72021§r)', author: 'Hsin-Yi Lin; Huan-Hsin Tseng; Xugang Lu; Yu Tsao', display:{Lore:['[{"text": "arXiv:2111.06316", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Noise Adaptive Speech Enhancement by Discriminator-Constrained Optimal Transport\\u00a7r\\n\\n\\u00a78\\u00a7oHsin-Yi Lin\\nHuan-Hsin Tseng\\nXugang Lu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.06316\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Nov 2021 17:15:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at NeurIPS 2021\\u00a7r"}']}
{title:'Moustafa et al. (§72021§r)', author: 'Aly Moustafa; Salah A. Aly', display:{Lore:['[{"text": "arXiv:2111.06331", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards an Efficient Voice Identification Using Wav2Vec2.0 and HuBERT Based on the Quran Reciters Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oAly Moustafa\\nSalah A. Aly\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.06331\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Nov 2021 17:44:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 9 figures, 2 tables\\u00a7r"}']}
{title:'Kim et al. (§72021§r)', author: 'Byeonggeun Kim; Seunghan Yang; Jangho Kim; Simyung Chang', display:{Lore:['[{"text": "arXiv:2111.06531", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDomain Generalization on Efficient Acoustic Scene Classification using Residual Normalization\\u00a7r\\n\\n\\u00a78\\u00a7oByeonggeun Kim\\nSeunghan Yang\\nJangho Kim\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.06531\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Nov 2021 01:57:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of the Detection and Classification of Acoustic Scenes and Events 2021 Workshop (DCASE2021)\\u00a7r"}']}
{title:'Sen et al. (§72021§r)', author: 'Ovishake Sen; Al-Mahmud; Pias Roy', display:{Lore:['[{"text": "arXiv:2111.06625", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Convolutional Neural Network Based Approach to Recognize Bangla Spoken Digits from Speech Signal\\u00a7r\\n\\n\\u00a78\\u00a7oOvishake Sen\\nAl-Mahmud\\nPias Roy\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.06625\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Nov 2021 09:38:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 5 figures, 2021 International Conference on Electronics, Communications and Information Technology (ICECIT), 14to 16 September 2021, Khulna, Bangladesh\\u00a7r"}']}
{title:'Henkel et al. (§72021§r)', author: 'Florian Henkel; Stephanie Schwaiger; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2111.06643", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFully Automatic Page Turning on Real Scores\\u00a7r\\n\\n\\u00a78\\u00a7oFlorian Henkel\\nStephanie Schwaiger\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.06643\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Nov 2021 10:23:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oISMIR 2021 Late Breaking/Demo\\u00a7r"}']}
{title:'Daneshfar et al. (§72021§r)', author: 'Fatemeh Daneshfar; Seyed Jahanshah Kabudian', display:{Lore:['[{"text": "arXiv:2111.07094", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Emotion Recognition Using Deep Sparse Auto-Encoder Extreme Learning Machine with a New Weighting Scheme and Spectro-Temporal Features Along with Classical Feature Selection and A New Quantum-Inspired Dimension Reduction Method\\u00a7r\\n\\n\\u00a78\\u00a7oFatemeh Daneshfar\\nSeyed Jahanshah Kabudian\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.07094\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 13 Nov 2021 11:09:38 GMT)\\u00a7r"}']}
{title:'Xie et al. (§72021§r)', author: 'Chao Xie; Yi-Chiao Wu; Patrick Lumban Tobing; Wen-Chin Huang; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2111.07116", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDirect Noisy Speech Modeling for Noisy-to-Noisy Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oChao Xie\\nYi-Chiao Wu\\nPatrick Lumban Tobing\\nWen-Chin Huang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.07116\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 13 Nov 2021 13:47:37 GMT)\\u00a7r"}']}
{title:'Daneshfar et al. (§72021§r)', author: 'Fatemeh Daneshfar; Seyed Jahanshah Kabudian', display:{Lore:['[{"text": "arXiv:2111.07234", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Emotion Recognition System by Quaternion Nonlinear Echo State Network\\u00a7r\\n\\n\\u00a78\\u00a7oFatemeh Daneshfar\\nSeyed Jahanshah Kabudian\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.07234\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 14 Nov 2021 03:45:43 GMT)\\u00a7r"}']}
{title:'Han et al. (§72021§r)', author: 'Sangjun Han; Hyeongrae Ihm; Woohyung Lim', display:{Lore:['[{"text": "arXiv:2111.07657", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSymbolic Music Loop Generation with VQ-VAE\\u00a7r\\n\\n\\u00a78\\u00a7oSangjun Han\\nHyeongrae Ihm\\nWoohyung Lim\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.07657\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Nov 2021 10:30:13 GMT)\\u00a7r"}']}
{title:'Shakeel et al. (§72021§r)', author: 'Muhammad Shakeel; Katsutoshi Itoyama; Kenji Nishida; Kazuhiro Nakadai', display:{Lore:['[{"text": "arXiv:2111.07979", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SY\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SY\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMetric-based multimodal meta-learning for human movement identification via footstep recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMuhammad Shakeel\\nKatsutoshi Itoyama\\nKenji Nishida\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.07979\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Nov 2021 18:46:14 GMT)\\u00a7r"}']}
{title:"O'Connor et al. (§72021§r)", author: "Brendan O'Connor; Simon Dixon; George Fazekas", display:{Lore:['[{"text": "arXiv:2111.08196", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Exploratory Study on Perceptual Spaces of the Singing Voice\\u00a7r\\n\\n\\u00a78\\u00a7oBrendan O\'Connor\\nSimon Dixon\\nGeorge Fazekas\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.08196\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.30746/978-91-519-5560-5\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Nov 2021 02:48:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of the 2020 Joint Conference on AI Music Creativity (CSMC-MuMe 2020), Stockholm, Sweden, October 15-19, 2020\\u00a7r"}']}
{title:'Saqib et al. (§72021§r)', author: 'Usama Saqib; Antoine Deleforge; Jesper Jensen', display:{Lore:['[{"text": "arXiv:2111.08327", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetecting acoustic reflectors using a robot\'s ego-noise\\u00a7r\\n\\n\\u00a78\\u00a7oUsama Saqib\\nAntoine Deleforge\\nJesper Jensen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.08327\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE International Conference on Acoustics, Speech and Signal\\n  Processing (ICASSP), Jun 2021, Toronto, Canada\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Nov 2021 09:56:03 GMT)\\u00a7r"}']}
{title:'Zuiderveld et al. (§72021§r)', author: 'Jan Zuiderveld; Marco Federici; Erik J. Bekkers', display:{Lore:['[{"text": "arXiv:2111.08462", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Lightweight Controllable Audio Synthesis with Conditional Implicit Neural Representations\\u00a7r\\n\\n\\u00a78\\u00a7oJan Zuiderveld\\nMarco Federici\\nErik J. Bekkers\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.08462\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 2 Dec 2021 11:16:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to \\"Deep Generative Models and DownstreamApplications\\" (Oral) and\\"Machine Learning for Creativity and Design\\" (Poster) workshops at NeurIPS 2021\\u00a7r"}']}
{title:"O'Connor et al. (§72021§r)", author: "Brendan O'Connor; Simon Dixon; George Fazekas", display:{Lore:['[{"text": "arXiv:2111.08839", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero-shot Singing Technique Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oBrendan O\'Connor\\nSimon Dixon\\nGeorge Fazekas\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.08839\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Nov 2021 23:53:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of the 15th International Symposium on Computer Music Multidisciplinary Research (CMMR 2021), Tokyo, Japan, November15-16, 2021\\u00a7r"}']}
{title:'Zhou et al. (§72021§r)', author: 'Hengshun Zhou; Jun Du; Yuanyuan Zhang; Qing Wang; Qing-Feng Liu; Chin-Hui Lee', display:{Lore:['[{"text": "arXiv:2111.08910", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInformation Fusion in Attention Networks Using Adaptive and Multi-level Factorized Bilinear Pooling for Audio-visual Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHengshun Zhou\\nJun Du\\nYuanyuan Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.08910\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Nov 2021 05:22:22 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Yiwen Wang; Fan Li; Xiaoheng Zhang; Pin Wang; Yongming Li', display:{Lore:['[{"text": "arXiv:2111.09014", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSubject Enveloped Deep Sample Fuzzy Ensemble Learning Algorithm of Parkinson\'s Speech Data\\u00a7r\\n\\n\\u00a78\\u00a7oYiwen Wang\\nFan Li\\nXiaoheng Zhang\\nPin Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.09014\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Nov 2021 10:12:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages, 4 figures\\u00a7r"}']}
{title:'Ellinas et al. (§72021§r)', author: 'Nikolaos Ellinas; Georgios Vamvoukakis; Konstantinos Markopoulos; Aimilios Chalamandaris; Georgia Maniati; Panos Kakoulidis; Spyros Raptis; June Sig Sung; Hyoungmin Park; Pirros Tsiakoulis', display:{Lore:['[{"text": "arXiv:2111.09052", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHigh Quality Streaming Speech Synthesis with Low, Sentence-Length-Independent Latency\\u00a7r\\n\\n\\u00a78\\u00a7oNikolaos Ellinas\\nGeorgios Vamvoukakis\\nKonstantinos Markopoulos\\n+ 6 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.09052\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2020-2464\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Nov 2021 11:46:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH 2020\\u00a7r"}']}
{title:'Maniati et al. (§72021§r)', author: 'Georgia Maniati; Nikolaos Ellinas; Konstantinos Markopoulos; Georgios Vamvoukakis; June Sig Sung; Hyoungmin Park; Aimilios Chalamandaris; Pirros Tsiakoulis', display:{Lore:['[{"text": "arXiv:2111.09075", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-lingual Low Resource Speaker Adaptation Using Phonological Features\\u00a7r\\n\\n\\u00a78\\u00a7oGeorgia Maniati\\nNikolaos Ellinas\\nKonstantinos Markopoulos\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.09075\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-327\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Nov 2021 12:33:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH 2021\\u00a7r"}']}
{title:'Markopoulos et al. (§72021§r)', author: 'Konstantinos Markopoulos; Nikolaos Ellinas; Alexandra Vioni; Myrsini Christidou; Panos Kakoulidis; Georgios Vamvoukakis; Georgia Maniati; June Sig Sung; Hyoungmin Park; Pirros Tsiakoulis; Aimilios Chalamandaris', display:{Lore:['[{"text": "arXiv:2111.09146", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRapping-Singing Voice Synthesis based on Phoneme-level Prosody Control\\u00a7r\\n\\n\\u00a78\\u00a7oKonstantinos Markopoulos\\nNikolaos Ellinas\\nAlexandra Vioni\\n+ 7 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.09146\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/SSW.2021-21\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Nov 2021 14:31:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of 11th ISCA Speech Synthesis Workshop (SSW 11)\\u00a7r"}']}
{title:'Hussain et al. (§72021§r)', author: 'Tassadaq Hussain; Mandar Gogate; Kia Dashtipour; Amir Hussain', display:{Lore:['[{"text": "arXiv:2111.09642", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Intelligibility-Oriented Audio-Visual Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oTassadaq Hussain\\nMandar Gogate\\nKia Dashtipour\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.09642\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Nov 2021 11:47:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 4 figures\\u00a7r"}']}
{title:'Braun (§72021§r)', author: 'David Braun', display:{Lore:['[{"text": "arXiv:2111.09931", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDawDreamer: Bridging the Gap Between Digital Audio Workstations and Python Interfaces\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Braun\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.09931\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Nov 2021 20:07:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o3 pages with 0 figures. Included in the Late-Breaking Demo Session of the 22nd International Society for Music Information Retrieval Conference\\u00a7r"}']}
{title:'Christidou et al. (§72021§r)', author: 'Myrsini Christidou; Alexandra Vioni; Nikolaos Ellinas; Georgios Vamvoukakis; Konstantinos Markopoulos; Panos Kakoulidis; June Sig Sung; Hyoungmin Park; Aimilios Chalamandaris; Pirros Tsiakoulis', display:{Lore:['[{"text": "arXiv:2111.10168", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Prosodic Clustering for Multispeaker and Speaker-independent Phoneme-level Prosody Control\\u00a7r\\n\\n\\u00a78\\u00a7oMyrsini Christidou\\nAlexandra Vioni\\nNikolaos Ellinas\\n+ 6 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.10168\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-030-87802-3_11\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Nov 2021 11:43:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of SPECOM 2021\\u00a7r"}']}
{title:'Klapsas et al. (§72021§r)', author: 'Konstantinos Klapsas; Nikolaos Ellinas; June Sig Sung; Hyoungmin Park; Spyros Raptis', display:{Lore:['[{"text": "arXiv:2111.10173", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWord-Level Style Control for Expressive, Non-attentive Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oKonstantinos Klapsas\\nNikolaos Ellinas\\nJune Sig Sung\\nHyoungmin Park\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.10173\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-030-87802-3_31\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Nov 2021 12:03:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of SPECOM 2021\\u00a7r"}']}
{title:'Vioni et al. (§72021§r)', author: 'Alexandra Vioni; Myrsini Christidou; Nikolaos Ellinas; Georgios Vamvoukakis; Panos Kakoulidis; Taehoon Kim; June Sig Sung; Hyoungmin Park; Aimilios Chalamandaris; Pirros Tsiakoulis', display:{Lore:['[{"text": "arXiv:2111.10177", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProsodic Clustering for Phoneme-level Prosody Control in End-to-End Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oAlexandra Vioni\\nMyrsini Christidou\\nNikolaos Ellinas\\n+ 6 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.10177\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9413604\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Nov 2021 12:10:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of ICASSP 2021\\u00a7r"}']}
{title:'Colussi et al. (§72021§r)', author: 'Marco Colussi; Stavros Ntalampiras', display:{Lore:['[{"text": "arXiv:2111.10235", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInterpreting deep urban sound classification using Layer-wise Relevance Propagation\\u00a7r\\n\\n\\u00a78\\u00a7oMarco Colussi\\nStavros Ntalampiras\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.10235\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Nov 2021 14:15:45 GMT)\\u00a7r"}']}
{title:'López-Espejo et al. (§72021§r)', author: 'Iván López-Espejo; Zheng-Hua Tan; John Hansen; Jesper Jensen', display:{Lore:['[{"text": "arXiv:2111.10592", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Spoken Keyword Spotting: An Overview\\u00a7r\\n\\n\\u00a78\\u00a7oIv\\u00e1n L\\u00f3pez-Espejo\\nZheng-Hua Tan\\nJohn Hansen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.10592\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 20 Nov 2021 13:46:57 GMT)\\u00a7r"}']}
{title:'Manoret et al. (§72021§r)', author: 'Pongpak Manoret; Punnatorn Chotipurk; Sompoom Sunpaweravong; Chanati Jantrachotechatchawan; Kobchai Duangrattanalert', display:{Lore:['[{"text": "arXiv:2111.10783", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Detection of Depression from Stratified Samples of Audio Data\\u00a7r\\n\\n\\u00a78\\u00a7oPongpak Manoret\\nPunnatorn Chotipurk\\nSompoom Sunpaweravong\\nChanati Jantrachotechatchawan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.10783\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 21 Nov 2021 10:16:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o30 pages, 6 figures\\u00a7r"}']}
{title:'Singh et al. (§72021§r)', author: 'Arshdeep Singh; Raju Arvind; Padmanabhan Rajan', display:{Lore:['[{"text": "arXiv:2111.10897", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHealth Monitoring of Industrial machines using Scene-Aware Threshold Selection\\u00a7r\\n\\n\\u00a78\\u00a7oArshdeep Singh\\nRaju Arvind\\nPadmanabhan Rajan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.10897\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 21 Nov 2021 21:01:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, 1 Table\\u00a7r"}']}
{title:'Shao et al. (§72021§r)', author: 'Yiwen Shao; Shi-Xiong Zhang; Dong Yu', display:{Lore:['[{"text": "arXiv:2111.11023", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Channel Multi-Speaker ASR Using 3D Spatial Feature\\u00a7r\\n\\n\\u00a78\\u00a7oYiwen Shao\\nShi-Xiong Zhang\\nDong Yu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.11023\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Nov 2021 07:19:12 GMT)\\u00a7r"}']}
{title:'Zuhair et al. (§72021§r)', author: 'Aza Zuhair; Hossein Hassani', display:{Lore:['[{"text": "arXiv:2111.11063", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparing the Accuracy of Deep Neural Networks (DNN) and Convolutional Neural Network (CNN) in Music Genre Recognition (MGR): Experiments on Kurdish Music\\u00a7r\\n\\n\\u00a78\\u00a7oAza Zuhair\\nHossein Hassani\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.11063\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Nov 2021 09:21:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 5 figures, 3 tables\\u00a7r"}']}
{title:'Won et al. (§72021§r)', author: 'Minz Won; Janne Spijkervet; Keunwoo Choi', display:{Lore:['[{"text": "arXiv:2111.11636", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Classification: Beyond Supervised Learning, Towards Real-world Applications\\u00a7r\\n\\n\\u00a78\\u00a7oMinz Won\\nJanne Spijkervet\\nKeunwoo Choi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.11636\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5281/zenodo.5703779\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 3 Dec 2021 02:56:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis is a web book written for atutorial session of the 22nd International Society for Music Information Retrieval Conference, Nov 8-12, 2021. Please visit https://music-classification.github.io/tutorial/ for the "}','{"text": "original, web book format\\u00a7r"}']}
{title:'Zehren et al. (§72021§r)', author: 'Mickael Zehren; Marco Alunno; Paolo Bientinesi', display:{Lore:['[{"text": "arXiv:2111.11737", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lADTOF: A large dataset of non-synthetic music for automatic drum transcription\\u00a7r\\n\\n\\u00a78\\u00a7oMickael Zehren\\nMarco Alunno\\nPaolo Bientinesi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.11737\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5281/zenodo.5624527\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Nov 2021 09:16:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of the 22nd International Society for Music Information Retrieval Conference, ISMIR, Online, pp. 818-824\\u00a7r"}']}
{title:'Pons et al. (§72021§r)', author: 'Jordi Pons; Joan Serrà; Santiago Pascual; Giulio Cengarle; Daniel Arteaga; Davide Scaini', display:{Lore:['[{"text": "arXiv:2111.11773", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUpsampling layers for music source separation\\u00a7r\\n\\n\\u00a78\\u00a7oJordi Pons\\nJoan Serr\\u00e0\\nSantiago Pascual\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.11773\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Nov 2021 10:36:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDemo page: http://www.jordipons.me/apps/upsamplers/\\u00a7r"}']}
{title:'Soler et al. (§72021§r)', author: 'Jordi Laguarta Soler; Brian Subirana', display:{Lore:['[{"text": "arXiv:2111.11859", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLongitudinal Speech Biomarkers for Automated Alzheimer\'s Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJordi Laguarta Soler\\nBrian Subirana\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.11859\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3389/fcomp.2021.624694\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nFrontiers in Computer Science, 08 April 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Nov 2021 18:38:14 GMT)\\u00a7r"}']}
{title:'Sun et al. (§72021§r)', author: 'Haoran Sun; Lantian Li; Thomas Fang Zheng; Dong Wang', display:{Lore:['[{"text": "arXiv:2111.12324", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHow Speech is Recognized to Be Emotional - A Study Based on Information Decomposition\\u00a7r\\n\\n\\u00a78\\u00a7oHaoran Sun\\nLantian Li\\nThomas Fang Zheng\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.12324\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Nov 2021 08:15:53 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Di Wang; Lantian Li; Hongzhi Yu; Dong Wang', display:{Lore:['[{"text": "arXiv:2111.12326", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Study on Decoupled Probabilistic Linear Discriminant Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oDi Wang\\nLantian Li\\nHongzhi Yu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.12326\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Nov 2021 08:22:01 GMT)\\u00a7r"}']}
{title:'Han et al. (§72021§r)', author: 'Jiao Han; Yunqi Cai; Lantian Li; Guanyu Li; Dong Wang', display:{Lore:['[{"text": "arXiv:2111.12331", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn MAP Estimation for Between-Class Variance\\u00a7r\\n\\n\\u00a78\\u00a7oJiao Han\\nYunqi Cai\\nLantian Li\\nGuanyu Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.12331\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Nov 2021 08:26:21 GMT)\\u00a7r"}']}
{title:'Dubnov et al. (§72021§r)', author: 'Shlomo Dubnov; Kevin Huang; Cheng-i Wang', display:{Lore:['[{"text": "arXiv:2111.12588", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Cross-Cultural Analysis using Music Information Dynamics\\u00a7r\\n\\n\\u00a78\\u00a7oShlomo Dubnov\\nKevin Huang\\nCheng-i Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.12588\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Nov 2021 16:05:29 GMT)\\u00a7r"}']}
{title:'Gururani et al. (§72021§r)', author: 'Siddharth Gururani; Alexander Lerch', display:{Lore:['[{"text": "arXiv:2111.12761", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-Supervised Audio Classification with Partially Labeled Data\\u00a7r\\n\\n\\u00a78\\u00a7oSiddharth Gururani\\nAlexander Lerch\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.12761\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Nov 2021 19:48:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be presented at IEEE ISM 2021\\u00a7r"}']}
{title:'Jin et al. (§72021§r)', author: 'Wangkai Jin; Junyu Liu; Jianfeng Ren; Xiangjun Peng', display:{Lore:['[{"text": "arXiv:2111.12869", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPolyphonic Sound Event Detection Using Capsule Neural Network on Multi-Type-Multi-Scale Time-Frequency Representation\\u00a7r\\n\\n\\u00a78\\u00a7oWangkai Jin\\nJunyu Liu\\nJianfeng Ren\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.12869\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 Nov 2021 02:10:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder reviewed in ICASSP 2022\\u00a7r"}']}
{title:'Goren et al. (§72021§r)', author: 'Or Goren; Eliya Nachmani; Lior Wolf', display:{Lore:['[{"text": "arXiv:2111.12986", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA-Muze-Net: Music Generation by Composing the Harmony based on the Generated Melody\\u00a7r\\n\\n\\u00a78\\u00a7oOr Goren\\nEliya Nachmani\\nLior Wolf\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.12986\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 Nov 2021 09:45:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at MMM 2022\\u00a7r"}']}
{title:'Won et al. (§72021§r)', author: 'Minz Won; Keunwoo Choi; Xavier Serra', display:{Lore:['[{"text": "arXiv:2111.13457", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-Supervised Music Tagging Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oMinz Won\\nKeunwoo Choi\\nXavier Serra\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.13457\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Nov 2021 12:17:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInternational Society for Music Information Retrieval (ISMIR) 2021\\u00a7r"}']}
{title:'Du et al. (§72021§r)', author: 'Zhihao Du; Shiliang Zhang; Siqi Zheng; Weilong Huang; Ming Lei', display:{Lore:['[{"text": "arXiv:2111.13694", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Embedding-aware Neural Diarization for Flexible Number of Speakers with Textual Information\\u00a7r\\n\\n\\u00a78\\u00a7oZhihao Du\\nShiliang Zhang\\nSiqi Zheng\\nWeilong Huang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.13694\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 28 Nov 2021 12:51:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022, 5 pages, 2 figures\\u00a7r"}']}
{title:'Khanjani et al. (§72021§r)', author: 'Zahra Khanjani; Gabrielle Watson; Vandana P. Janeja', display:{Lore:['[{"text": "arXiv:2111.14203", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHow Deep Are the Fakes? Focusing on Audio Deepfake: A Survey\\u00a7r\\n\\n\\u00a78\\u00a7oZahra Khanjani\\nGabrielle Watson\\nVandana P. Janeja\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.14203\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 28 Nov 2021 18:28:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAbbreviated version of a longer survey under review\\u00a7r"}']}
{title:'Melek (§72021§r)', author: 'Negin Melek', display:{Lore:['[{"text": "arXiv:2111.14354", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lResponding to Challenge Call of Machine Learning Model Development in Diagnosing Respiratory Disease Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oNegin Melek\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.14354\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 Nov 2021 07:18:36 GMT)\\u00a7r"}']}
{title:'Xu et al. (§72021§r)', author: 'Junhao Xu; Jianwei Yu; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2111.14479", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMixed Precision DNN Qunatization for Overlapped Speech Separation and Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJunhao Xu\\nJianwei Yu\\nXunying Liu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.14479\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 Nov 2021 11:50:42 GMT)\\u00a7r"}']}
{title:'Fu et al. (§72021§r)', author: 'Changzeng Fu; Chaoran Liu; Carlos Toshinori Ishi; Hiroshi Ishiguro', display:{Lore:['[{"text": "arXiv:2111.15159", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCycleTransGAN-EVC: A CycleGAN-based Emotional Voice Conversion Model with Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oChangzeng Fu\\nChaoran Liu\\nCarlos Toshinori Ishi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.15159\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Nov 2021 06:33:57 GMT)\\u00a7r"}']}
{title:'Suzuki (§72021§r)', author: 'Masahiro Suzuki', display:{Lore:['[{"text": "arXiv:2112.00355", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScore Transformer: Generating Musical Score from Note-level Representation\\u00a7r\\n\\n\\u00a78\\u00a7oMasahiro Suzuki\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.00355\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3469877.3490612\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Dec 2021 09:08:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ACM Multimedia Asia 2021 (MMAsia \'21); Project page: https://score-transformer.github.io/\\u00a7r"}']}
{title:'Tan (§72021§r)', author: 'Hao Hao Tan', display:{Lore:['[{"text": "arXiv:2112.00702", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-supervised music emotion recognition using noisy student training and harmonic pitch class profiles\\u00a7r\\n\\n\\u00a78\\u00a7oHao Hao Tan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.00702\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 9 Dec 2021 15:11:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMediaEval 2021 submission for Emotion and Themes in Music\\u00a7r"}']}
{title:'Hu et al. (§72021§r)', author: 'Xiaolin Hu; Kai Li; Weiyi Zhang; Yi Luo; Jean-Marie Lemercier; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2112.02321", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Separation Using an Asynchronous Fully Recurrent Convolutional Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oXiaolin Hu\\nKai Li\\nWeiyi Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.02321\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 4 Dec 2021 12:44:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by NeurIPS 2021, Demo at https://cslikai.cn/project/AFRCNN\\u00a7r"}']}
{title:'Akuzawa et al. (§72021§r)', author: 'Kei Akuzawa; Kotaro Onishi; Keisuke Takiguchi; Kohki Mametani; Koichiro Mori', display:{Lore:['[{"text": "arXiv:2112.02796", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConditional Deep Hierarchical Variational Autoencoder for Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oKei Akuzawa\\nKotaro Onishi\\nKeisuke Takiguchi\\nKohki Mametani\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.02796\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 Dec 2021 05:54:11 GMT)\\u00a7r"}']}
{title:'AlBadawy et al. (§72021§r)', author: 'Ehab A. AlBadawy; Andrew Gibiansky; Qing He; Jilong Wu; Ming-Ching Chang; Siwei Lyu', display:{Lore:['[{"text": "arXiv:2112.03099", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVocBench: A Neural Vocoder Benchmark for Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oEhab A. AlBadawy\\nAndrew Gibiansky\\nQing He\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.03099\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 Dec 2021 15:09:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in icassp 2022\\u00a7r"}']}
{title:'Watson et al. (§72021§r)', author: 'Gabrielle Watson; Zahra Khanjani; Vandana P. Janeja', display:{Lore:['[{"text": "arXiv:2112.03351", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Deepfake Perceptions in College Going Populations\\u00a7r\\n\\n\\u00a78\\u00a7oGabrielle Watson\\nZahra Khanjani\\nVandana P. Janeja\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.03351\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 Dec 2021 20:53:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSummary of study findings\\u00a7r"}']}
{title:'Casebeer et al. (§72021§r)', author: 'Jonah Casebeer; Jacob Donley; Daniel Wong; Buye Xu; Anurag Kumar', display:{Lore:['[{"text": "arXiv:2112.04613", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNICE-Beam: Neural Integrated Covariance Estimators for Time-Varying Beamformers\\u00a7r\\n\\n\\u00a78\\u00a7oJonah Casebeer\\nJacob Donley\\nDaniel Wong\\nBuye Xu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.04613\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 8 Dec 2021 22:48:06 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Haohe Liu; Qiuqiang Kong; Jiafeng Liu', display:{Lore:['[{"text": "arXiv:2112.04685", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCWS-PResUNet: Music Source Separation with Channel-wise Subband Phase-aware ResUNet\\u00a7r\\n\\n\\u00a78\\u00a7oHaohe Liu\\nQiuqiang Kong\\nJiafeng Liu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.04685\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Dec 2021 03:42:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at MDX Workshop @ ISMIR 2021\\u00a7r"}']}
{title:'Zheng et al. (§72021§r)', author: 'Kaitong Zheng; Chengshi Zheng; Jinqiu Sang; Yulong Zhang; Xiaodong Li', display:{Lore:['[{"text": "arXiv:2112.04726", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoise-robust blind reverberation time estimation using noise-aware time-frequency masking\\u00a7r\\n\\n\\u00a78\\u00a7oKaitong Zheng\\nChengshi Zheng\\nJinqiu Sang\\nYulong Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.04726\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Dec 2021 06:58:41 GMT)\\u00a7r"}']}
{title:'Gómez-Cañón et al. (§72021§r)', author: 'Juan Sebastián Gómez-Cañón; Perfecto Herrera; Estefanía Cano; Emilia Gómez', display:{Lore:['[{"text": "arXiv:2112.04975", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPersonalized musically induced emotions of not-so-popular Colombian music\\u00a7r\\n\\n\\u00a78\\u00a7oJuan Sebasti\\u00e1n G\\u00f3mez-Ca\\u00f1\\u00f3n\\nPerfecto Herrera\\nEstefan\\u00eda Cano\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.04975\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nHCAI Human Centered AI Workshop at the 35th Conference on Neural\\n  Information Processing Systems (NeurIPS 2021)\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Dec 2021 15:12:49 GMT)\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Yi Li; Yang Sun; Kirill Horoshenkov; Syed Mohsen Naqvi', display:{Lore:['[{"text": "arXiv:2112.05036", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDomain Adaptation and Autoencoder Based Unsupervised Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oYi Li\\nYang Sun\\nKirill Horoshenkov\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.05036\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TAI.2021.3119927\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Transactions on Artificial Intelligence. (2021)\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Dec 2021 17:00:45 GMT)\\u00a7r"}']}
{title:'Hanssian (§72021§r)', author: 'Sevag Hanssian', display:{Lore:['[{"text": "arXiv:2112.05509", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic demixing with the sliCQ transform\\u00a7r\\n\\n\\u00a78\\u00a7oSevag Hanssian\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.05509\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Dec 2021 14:07:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2 pages, 3 figures. Published in the MDX21 workshop (satellite event of ISMIR 2021): https://mdx-workshop.github.io/proceedings/hanssian.pdf\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Anran Wang; Maruchi Kim; Hao Zhang; Shyamnath Gollakota', display:{Lore:['[{"text": "arXiv:2112.05893", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHybrid Neural Networks for On-device Directional Hearing\\u00a7r\\n\\n\\u00a78\\u00a7oAnran Wang\\nMaruchi Kim\\nHao Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.05893\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nAAAI 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 11 Dec 2021 01:29:12 GMT)\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Yi Li; Yang Sun; Syed Mohsen Naqvi', display:{Lore:['[{"text": "arXiv:2112.06052", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lU-shaped Transformer with Frequency-Band Aware Attention for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oYi Li\\nYang Sun\\nSyed Mohsen Naqvi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.06052\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3265839\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing (\\n  Volume: 31), 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 11 Dec 2021 19:18:34 GMT)\\u00a7r"}']}
{title:'Plantinga et al. (§72021§r)', author: 'Peter Plantinga; Deblin Bagchi; Eric Fosler-Lussier', display:{Lore:['[{"text": "arXiv:2112.06068", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerceptual Loss with Recognition Model for Single-Channel Enhancement and Robust ASR\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Plantinga\\nDeblin Bagchi\\nEric Fosler-Lussier\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.06068\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 11 Dec 2021 20:44:26 GMT)\\u00a7r"}']}
{title:'Tilkorn et al. (§72021§r)', author: 'H. Tilkorn; G. Mittag; S. Möller', display:{Lore:['[{"text": "arXiv:2112.06219", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisualising and Explaining Deep Learning Models for Speech Quality Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oH. Tilkorn\\nG. Mittag\\nS. M\\u00f6ller\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.06219\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 12 Dec 2021 12:50:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 6 figures, In Proceedings of the DAGA 2021 (the annual conference of the German Acoustical Society, DEGA)\\u00a7r"}']}
{title:'Stowell (§72021§r)', author: 'Dan Stowell', display:{Lore:['[{"text": "arXiv:2112.06725", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComputational bioacoustics with deep learning: a review and roadmap\\u00a7r\\n\\n\\u00a78\\u00a7oDan Stowell\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.06725\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.7717/peerj.13152\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Dec 2021 15:10:01 GMT)\\u00a7r"}']}
{title:'Kimura et al. (§72021§r)', author: 'Keisuke Kimura; Shoichi Koyama; Natsuki Ueno; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2112.06774", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMean-square-error-based secondary source placement in sound field synthesis with prior information on desired field\\u00a7r\\n\\n\\u00a78\\u00a7oKeisuke Kimura\\nShoichi Koyama\\nNatsuki Ueno\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.06774\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 10 Dec 2021 02:06:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEWorkshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)2021\\u00a7r"}']}
{title:'Li (§72021§r)', author: 'Zhuo Li', display:{Lore:['[{"text": "arXiv:2112.07134", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExplore Long-Range Context feature for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oZhuo Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.07134\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Dec 2021 03:08:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7orejected by interspeech2021\\u00a7r"}']}
{title:'Park et al. (§72021§r)', author: 'YeongHyeon Park; JoonSung Lee; Myung Jin Kim; Wonseok Park', display:{Lore:['[{"text": "arXiv:2112.07214", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoise Reduction and Driving Event Extraction Method for Performance Improvement on Driving Noise-based Surface Anomaly Detection\\u00a7r\\n\\n\\u00a78\\u00a7oYeongHyeon Park\\nJoonSung Lee\\nMyung Jin Kim\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.07214\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Dec 2021 07:50:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o3 pages, 3 figures, 2 tables\\u00a7r"}']}
{title:'Lella et al. (§72021§r)', author: 'Kranthi Kumar Lella; Alphonse Pja', display:{Lore:['[{"text": "arXiv:2112.07285", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic COVID-19 disease diagnosis using 1D convolutional neural network and augmentation with human respiratory sound based on parameters: cough, breath, and voice\\u00a7r\\n\\n\\u00a78\\u00a7oKranthi Kumar Lella\\nAlphonse Pja\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.07285\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3934/publichealth.2021019\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nAIMS Public Health. 2021;8(2):240-264\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Dec 2021 10:41:04 GMT)\\u00a7r"}']}
{title:'Sallandt et al. (§72021§r)', author: 'Henry Sallandt; Philipp Krah; Mathias Lemke', display:{Lore:['[{"text": "arXiv:2112.07349", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.flu-dyn\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSupervised Learning for Multi Zone Sound Field Reproduction under Harsh Environmental Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oHenry Sallandt\\nPhilipp Krah\\nMathias Lemke\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.07349\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Dec 2021 13:07:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint submitted for publication\\u00a7r"}']}
{title:'Lai et al. (§72021§r)', author: 'Yongquan Lai; Xin Tang; Yuanyuan Fu; Rui Fang', display:{Lore:['[{"text": "arXiv:2112.07463", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end speaker diarization with transformer\\u00a7r\\n\\n\\u00a78\\u00a7oYongquan Lai\\nXin Tang\\nYuanyuan Fu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.07463\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Dec 2021 15:23:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to icassp2022\\u00a7r"}']}
{title:'Lella et al. (§72021§r)', author: 'Kranthi Kumar Lella; Alphonse PJA', display:{Lore:['[{"text": "arXiv:2112.07670", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA literature review on COVID-19 disease diagnosis from respiratory sound data\\u00a7r\\n\\n\\u00a78\\u00a7oKranthi Kumar Lella\\nAlphonse PJA\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.07670\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3934/bioeng.2021013\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n[J]. AIMS Bioengineering, 2021, 8(2): 140-153\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Dec 2021 10:30:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2112.07285\\u00a7r"}']}
{title:'Hindawi et al. (§72021§r)', author: 'Noor Ahmad Al Hindawi; Ismail Shahin; Ali Bou Nassif', display:{Lore:['[{"text": "arXiv:2112.07940", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe exploitation of Multiple Feature Extraction Techniques for Speaker Identification in Emotional States under Disguised Voices\\u00a7r\\n\\n\\u00a78\\u00a7oNoor Ahmad Al Hindawi\\nIsmail Shahin\\nAli Bou Nassif\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.07940\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Dec 2021 07:56:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, accepted in the 14th International Conference on Developments in eSystems Engineering, 7-10 December, 2021\\u00a7r"}']}
{title:'Konev et al. (§72021§r)', author: 'A. A. Konev; V. S. Khlebnikov; A. Yu. Yakimuk', display:{Lore:['[{"text": "arXiv:2112.08027", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech frame implementation for speech analysis and recognition\\u00a7r\\n\\n\\u00a78\\u00a7oA. A. Konev\\nV. S. Khlebnikov\\nA. Yu. Yakimuk\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.08027\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Dec 2021 10:48:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 27 tables\\u00a7r"}']}
{title:'Zheng et al. (§72021§r)', author: 'Kaitong Zheng; Ruijie Meng; Chengshi Zheng; Xiaodong Li; Jinqiu Sang; Juanjuan Cai; Jie Wang', display:{Lore:['[{"text": "arXiv:2112.08561", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotionBox: a music-element-driven emotional music generation system using Recurrent Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oKaitong Zheng\\nRuijie Meng\\nChengshi Zheng\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.08561\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Dec 2021 01:50:07 GMT)\\u00a7r"}']}
{title:'Nagano et al. (§72021§r)', author: 'Tohru Nagano; Takashi Fukuda; Gakuto Kurata', display:{Lore:['[{"text": "arXiv:2112.08878", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKnowledge Distillation Leveraging Alternative Soft Targets from Non-Parallel Qualified Speech Data\\u00a7r\\n\\n\\u00a78\\u00a7oTohru Nagano\\nTakashi Fukuda\\nGakuto Kurata\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.08878\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Dec 2021 13:43:54 GMT)\\u00a7r"}']}
{title:'Gogate et al. (§72021§r)', author: 'Mandar Gogate; Kia Dashtipour; Amir Hussain', display:{Lore:['[{"text": "arXiv:2112.09060", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Robust Real-time Audio-Visual Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oMandar Gogate\\nKia Dashtipour\\nAmir Hussain\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.09060\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Dec 2021 17:54:45 GMT)\\u00a7r"}']}
{title:'Takamichi et al. (§72021§r)', author: 'Shinnosuke Takamichi; Ludwig Kürzinger; Takaaki Saeki; Sayaka Shiota; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2112.09323", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJTubeSpeech: corpus of Japanese speech collected from YouTube for speech recognition and speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oShinnosuke Takamichi\\nLudwig K\\u00fcrzinger\\nTakaaki Saeki\\nSayaka Shiota\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.09323\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 17 Dec 2021 05:09:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP2022\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Yi Li; Yang Sun; Syed Mohsen Naqvi', display:{Lore:['[{"text": "arXiv:2112.11142", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Learning based Monaural Speech Enhancement with Complex-Cycle-Consistent\\u00a7r\\n\\n\\u00a78\\u00a7oYi Li\\nYang Sun\\nSyed Mohsen Naqvi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.11142\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Dec 2021 12:23:43 GMT)\\u00a7r"}']}
{title:'Kawahara et al. (§72021§r)', author: 'Hideki Kawahara; Kohei Yatabe', display:{Lore:['[{"text": "arXiv:2112.11373", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSafeguarding test signals for acoustic measurement using arbitrary sounds\\u00a7r\\n\\n\\u00a78\\u00a7oHideki Kawahara\\nKohei Yatabe\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.11373\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Dec 2021 17:24:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 10 figures, submitted to Acoustical Science and Technology\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Yi Li; Yang Sun; Syed Mohsen Naqvi', display:{Lore:['[{"text": "arXiv:2112.11459", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Learning based Monaural Speech Enhancement with Multi-Task Pre-Training\\u00a7r\\n\\n\\u00a78\\u00a7oYi Li\\nYang Sun\\nSyed Mohsen Naqvi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.11459\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Dec 2021 13:08:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022. arXiv admin note: text overlapwith arXiv:2112.11142\\u00a7r"}']}
{title:'Shim et al. (§72021§r)', author: 'Hye-jin Shim; Jungwoo Heo; Jae-han Park; Ga-hui Lee; Ha-Jin Yu', display:{Lore:['[{"text": "arXiv:2112.12343", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGraph attentive feature aggregation for text-independent speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oHye-jin Shim\\nJungwoo Heo\\nJae-han Park\\nGa-hui Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.12343\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Dec 2021 03:36:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, 6 tables, submitted to ICASSP 2022\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Yuang Li; Yuntao Wang; Xin Liu; Yuanchun Shi; Shao-fu Shih', display:{Lore:['[{"text": "arXiv:2112.13156", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnabling Real-time On-chip Audio Super Resolution for Bone Conduction Microphones\\u00a7r\\n\\n\\u00a78\\u00a7oYuang Li\\nYuntao Wang\\nXin Liu\\nYuanchun Shi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.13156\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Dec 2021 23:03:04 GMT)\\u00a7r"}']}
{title:'Shahin et al. (§72021§r)', author: 'Ismail Shahin; Noor Hindawi; Ali Bou Nassif; Adi Alhudhaif; Kemal Polat', display:{Lore:['[{"text": "arXiv:2112.13350", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNovel Dual-Channel Long Short-Term Memory Compressed Capsule Networks for Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oIsmail Shahin\\nNoor Hindawi\\nAli Bou Nassif\\nAdi Alhudhaif\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.13350\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.eswa.2021.116080\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPublished in Expert Systems With Applications, 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 26 Dec 2021 10:37:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o19 pages, 11 figures\\u00a7r"}']}
{title:'Shahin et al. (§72021§r)', author: 'Ismail Shahin; Ali Bou Nassif; Nawel Nemmour; Ashraf Elnagar; Adi Alhudhaif; Kemal Polat', display:{Lore:['[{"text": "arXiv:2112.13353", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNovel Hybrid DNN Approaches for Speaker Verification in Emotional and Stressful Talking Environments\\u00a7r\\n\\n\\u00a78\\u00a7oIsmail Shahin\\nAli Bou Nassif\\nNawel Nemmour\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.13353\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s00521-021-06226-w\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPublished in Neural Computing and Applications. Vol. 33, issue 23,\\n  June 2021, pp. 16033-16055\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 26 Dec 2021 10:47:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o23 pages, 13 figures\\u00a7r"}']}
{title:'Khodaei et al. (§72021§r)', author: 'Mohammad Javad Khodaei; Amin Mehrvarz; Reza Ghaffarivardavagh; Nader Jalili', display:{Lore:['[{"text": "arXiv:2112.13453", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.class-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRetrieving Effective Acoustic Impedance and Refractive Index for Size Mismatch Samples\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Javad Khodaei\\nAmin Mehrvarz\\nReza Ghaffarivardavagh\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.13453\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 26 Dec 2021 21:50:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Tapia et al. (§72021§r)', author: 'Luis Sanchez Tapia; Antonio Gomez; Mario Esparza; Venkatesh Jatla; Marios Pattichis; Sylvia Celedón-Pattichis; Carlos LópezLeiva', display:{Lore:['[{"text": "arXiv:2112.13463", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBilingual Speech Recognition by Estimating Speaker Geometry from Video Data\\u00a7r\\n\\n\\u00a78\\u00a7oLuis Sanchez Tapia\\nAntonio Gomez\\nMario Esparza\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.13463\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nThe 19th International Conference on Computer Analysis of Images\\n  and Patterns (CAIP), 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 26 Dec 2021 23:29:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 6 figures\\u00a7r"}']}
{title:'Huizen et al. (§72021§r)', author: 'Roy Rudolf Huizen; Florentina Tatrin Kurniati', display:{Lore:['[{"text": "arXiv:2112.14930", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFeature extraction with mel scale separation method on noise audio recordings\\u00a7r\\n\\n\\u00a78\\u00a7oRoy Rudolf Huizen\\nFlorentina Tatrin Kurniati\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.14930\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.11591/ijeecs.v24.i2.pp815-824\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIJEECS, Vol. 24, No. 2, pp 815-824 (2021);\\n  http://ijeecs.iaescore.com/index.php/IJEECS/article/view/25626\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Dec 2021 05:24:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages\\u00a7r"}']}
{title:'Godwin et al. (§72021§r)', author: 'Toby Godwin; Georgios Rizos; Alice Baird; Najla D. Al Futaisi; Vincent Brisse; Bjoern W. Schuller', display:{Lore:['[{"text": "arXiv:2201.00052", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluating Deep Music Generation Methods Using Data Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oToby Godwin\\nGeorgios Rizos\\nAlice Baird\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.00052\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2021 IEEE 23rd International Workshop on Multimedia Signal\\n  Processing (MMSP)\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 31 Dec 2021 20:35:46 GMT)\\u00a7r"}']}
