{title:'Kumar et al. (§72022§r)', author: 'Anurag Kumar; Bhiksha Raj', display:{Lore:['[{"text": "arXiv:1707.02530", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep CNN Framework for Audio Event Recognition using Weakly Labeled Web Data\\u00a7r\\n\\n\\u00a78\\u00a7oAnurag Kumar\\nBhiksha Raj\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1707.02530\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 2 Oct 2022 01:27:14 GMT)\\u00a7r"}']}
{title:'Hou et al. (§72022§r)', author: 'Jen-Cheng Hou; Syu-Siang Wang; Ying-Hui Lai; Yu Tsao; Hsiu-Wen Chang; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:1709.00944", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-Visual Speech Enhancement Using Multimodal Deep Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oJen-Cheng Hou\\nSyu-Siang Wang\\nYing-Hui Lai\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1709.00944\\u00a7r\\n\\nVersion:\\u00a77v5 (Mon, 18 Apr 2022 11:47:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper is the same as arXiv:1703.10893v6. Apologies for the inconvenience. arXiv admin note: text overlapwith arXiv:1703.10893\\u00a7r"}']}
{title:'Marchegiani et al. (§72022§r)', author: 'Letizia Marchegiani; Paul Newman', display:{Lore:['[{"text": "arXiv:1810.04989", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.RO\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lListening for Sirens: Locating and Classifying Acoustic Alarms in City Scenes\\u00a7r\\n\\n\\u00a78\\u00a7oLetizia Marchegiani\\nPaul Newman\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.04989\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TITS.2022.3158076\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 28 Mar 2022 13:49:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 9 figures\\u00a7r"}']}
{title:'Marafioti et al. (§72022§r)', author: 'Andrés Marafioti; Nicki Holighaus; Piotr Majdak; Nathanaël Perraudin', display:{Lore:['[{"text": "arXiv:1810.12138", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio inpainting of music by means of neural networks\\u00a7r\\n\\n\\u00a78\\u00a7oAndr\\u00e9s Marafioti\\nNicki Holighaus\\nPiotr Majdak\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.12138\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 18 Feb 2022 15:42:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at the146th AES Convention [arXiv:1810.12138v2]. For the journal version, published in published in IEEE TASLP,see [arXiv:1810.12138v2]\\u00a7r"}']}
{title:'Tan et al. (§72022§r)', author: 'Zheng-Hua Tan; Achintya kr. Sarkar; Najim Dehak', display:{Lore:['[{"text": "arXiv:1906.03588", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lrVAD: An Unsupervised Segment-Based Robust Voice Activity Detection Method\\u00a7r\\n\\n\\u00a78\\u00a7oZheng-Hua Tan\\nAchintya kr. Sarkar\\nNajim Dehak\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.03588\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nComputer Speech & Language, volume 59, January 2020, Pages 1-21\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 11 Jan 2022 14:26:11 GMT)\\u00a7r"}']}
{title:'Zhao et al. (§72022§r)', author: 'Zishuo Zhao; Haoyun Wang', display:{Lore:['[{"text": "arXiv:1909.08444", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusical Instrument Classification via Low-Dimensional Feature Vectors\\u00a7r\\n\\n\\u00a78\\u00a7oZishuo Zhao\\nHaoyun Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.08444\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Jul 2022 15:57:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7olow quality as is my undergraduate work\\u00a7r"}']}
{title:'Naranjo-Alcazar et al. (§72022§r)', author: 'Javier Naranjo-Alcazar; Sergi Perez-Castanos; Pedro Zuccarrello; Ana M. Torres; Jose J. Lopez; Franscesc J. Ferri; Maximo Cobos', display:{Lore:['[{"text": "arXiv:2002.11561", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Open-set Recognition and Few-Shot Learning Dataset for Audio Event Classification in Domestic Environments\\u00a7r\\n\\n\\u00a78\\u00a7oJavier Naranjo-Alcazar\\nSergi Perez-Castanos\\nPedro Zuccarrello\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2002.11561\\u00a7r\\n\\nVersion:\\u00a77v8 (Mon, 11 Apr 2022 08:32:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEEAccess\\u00a7r"}']}
{title:'Nichols et al. (§72022§r)', author: 'Eric P. Nichols; Stefano Kalonaris; Gianluca Micchi; Anna Aljanaki', display:{Lore:['[{"text": "arXiv:2006.14221", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModeling Baroque Two-Part Counterpoint with Neural Machine Translation\\u00a7r\\n\\n\\u00a78\\u00a7oEric P. Nichols\\nStefano Kalonaris\\nGianluca Micchi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2006.14221\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 20 Jan 2022 01:34:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInternational Computer Music Conference 2021, 5 pages\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Bo-Yu Chen; Jordan B. L. Smith; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2008.02011", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Loop Combiner: Neural Network Models for Assessing the Compatibility of Loops\\u00a7r\\n\\n\\u00a78\\u00a7oBo-Yu Chen\\nJordan B. L. Smith\\nYi-Hsuan Yang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.02011\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 17 Feb 2022 15:30:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the 21st International Society for Music Information Retrieval Conference (ISMIR 2020)\\u00a7r"}']}
{title:'Molina (§72022§r)', author: 'David Daniel Albarracín Molina', display:{Lore:['[{"text": "arXiv:2008.04415", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CY\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptive music: Automated music composition and distribution\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Daniel Albarrac\\u00edn Molina\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.04415\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 25 Jan 2022 11:14:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o218 pages, 49 figures, 22 tables, 38 audio samples. Doctoral Thesis. Universidad de M\\u00e1laga, 2021. https://hdl.handle.net/10630/23508\\u00a7r"}']}
{title:'Koshy et al. (§72022§r)', author: 'Aranya Koshy; Shahin Tavakoli', display:{Lore:['[{"text": "arXiv:2008.12233", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.AP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring British Accents: Modelling the Trap-Bath Split with Functional Data Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oAranya Koshy\\nShahin Tavakoli\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.12233\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 30 Jan 2022 10:45:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o45 pages, 27 figures\\u00a7r"}']}
{title:'Fonseca et al. (§72022§r)', author: 'Eduardo Fonseca; Xavier Favory; Jordi Pons; Frederic Font; Xavier Serra', display:{Lore:['[{"text": "arXiv:2010.00475", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFSD50K: An Open Dataset of Human-Labeled Sound Events\\u00a7r\\n\\n\\u00a78\\u00a7oEduardo Fonseca\\nXavier Favory\\nJordi Pons\\nFrederic Font\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.00475\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 23 Apr 2022 20:12:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted version in TASLP. Main updates include: estimation of the amountof label noise in FSD50K,SNR comparison between FSD50K and AudioSet, improved description of evaluation metrics including equations, "}','{"text": "clarification of experimental methodology and some results, some content moved to Appendix for readability. https://ieeexplore.ieee.org/document/9645159\\u00a7r"}']}
{title:'Padi et al. (§72022§r)', author: 'Sarala Padi; Dinesh Manocha; Ram D. Sriram', display:{Lore:['[{"text": "arXiv:2010.09895", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Window Data Augmentation Approach for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSarala Padi\\nDinesh Manocha\\nRam D. Sriram\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.09895\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 16 Feb 2022 00:21:17 GMT)\\u00a7r"}']}
{title:'Hemati et al. (§72022§r)', author: 'Hamed Hemati; Damian Borth', display:{Lore:['[{"text": "arXiv:2011.06392", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing IPA-Based Tacotron for Data Efficient Cross-Lingual Speaker Adaptation and Pronunciation Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oHamed Hemati\\nDamian Borth\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.06392\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 31 Mar 2022 15:49:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint\\u00a7r"}']}
{title:'Naman et al. (§72022§r)', author: 'Anugunj Naman; Chetan Sinha; Liliana Mancini', display:{Lore:['[{"text": "arXiv:2101.01356", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFixed-MAML for Few Shot Classification in Multilingual Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAnugunj Naman\\nChetan Sinha\\nLiliana Mancini\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.01356\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-981-16-9650-3_37\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 30 May 2022 20:07:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCode at https://github.com/AnugunjNaman/Fixed-MAML\\u00a7r"}']}
{title:'Hsu et al. (§72022§r)', author: 'Fu-Shun Hsu; Shang-Ran Huang; Chien-Wen Huang; Chao-Jung Huang; Yuan-Ren Cheng; Chun-Chieh Chen; Jack Hsiao; Chung-Wei Chen; Li-Chin Chen; Yen-Chun Lai; Bi-Fang Hsu; Nian-Jhen Lin; Wan-Lin Tsai; Yi-Lin Wu; Tzu-Ling Tseng; Ching-Ting Tseng; Yi-Tsun Chen; Feipei Lai', display:{Lore:['[{"text": "arXiv:2102.03049", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBenchmarking of eight recurrent neural network variants for breath phase and adventitious sound detection on a self-developed open-access lung sound database-HF_Lung_V1\\u00a7r\\n\\n\\u00a78\\u00a7oFu-Shun Hsu\\nShang-Ran Huang\\nChien-Wen Huang\\n+ 14 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.03049\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1371/journal.pone.0254134\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPLoS ONE, 2021, 16(7): e0254134\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 12 Jul 2022 09:04:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o48 pages, 8 figures. Accepted by PLoS One\\u00a7r"}']}
{title:'Okamoto et al. (§72022§r)', author: 'Yuki Okamoto; Keisuke Imoto; Shinnosuke Takamichi; Ryosuke Yamanishi; Takahiro Fukumori; Yoichi Yamashita', display:{Lore:['[{"text": "arXiv:2102.05872", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOnoma-to-wave: Environmental sound synthesis from onomatopoeic words\\u00a7r\\n\\n\\u00a78\\u00a7oYuki Okamoto\\nKeisuke Imoto\\nShinnosuke Takamichi\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.05872\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 7 Feb 2022 06:00:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to APSIPA Transactions on Signal and Information Processing\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Fuling Chen; Roberto Togneri; Murray Maybery; Diana Tan', display:{Lore:['[{"text": "arXiv:2102.07982", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Gender Scoring and Independent Acoustic Characterization of Perceived Masculinity and Femininity\\u00a7r\\n\\n\\u00a78\\u00a7oFuling Chen\\nRoberto Togneri\\nMurray Maybery\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.07982\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 4 Aug 2022 14:05:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o24 pages, 7 figures, journal\\u00a7r"}']}
{title:'Mohaimenuzzaman et al. (§72022§r)', author: 'Md Mohaimenuzzaman; Christoph Bergmeir; Ian Thomas West; Bernd Meyer', display:{Lore:['[{"text": "arXiv:2103.03483", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnvironmental Sound Classification on the Edge: A Pipeline for Deep Acoustic Networks on Extremely Resource-Constrained Devices\\u00a7r\\n\\n\\u00a78\\u00a7oMd Mohaimenuzzaman\\nChristoph Bergmeir\\nIan Thomas West\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.03483\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.patcog.2022.109025\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPattern Recognition, p.109025 (2022)\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 20 Sep 2022 05:10:43 GMT)\\u00a7r"}']}
{title:'Llopis et al. (§72022§r)', author: 'Hermes Sampedro Llopis; Allan P. Engsig-Karup; Cheol-Ho Jeong; Finnur Pind; Jan S. Hesthaven', display:{Lore:['[{"text": "arXiv:2103.11730", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReduced basis methods for numerical room acoustic simulations with parametrized boundaries\\u00a7r\\n\\n\\u00a78\\u00a7oHermes Sampedro Llopis\\nAllan P. Engsig-Karup\\nCheol-Ho Jeong\\nFinnur Pind\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.11730\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 8 Mar 2022 10:05:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 figures and 2 tables\\u00a7r"}']}
{title:'Pham et al. (§72022§r)', author: 'Lam Pham; Chris Baume; Qiuqiang Kong; Tassadaq Hussain; Wenwu Wang; Mark Plumbley', display:{Lore:['[{"text": "arXiv:2104.01161", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Audio-Based Deep Learning Framework For BBC Television Programme Classification\\u00a7r\\n\\n\\u00a78\\u00a7oLam Pham\\nChris Baume\\nQiuqiang Kong\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01161\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 11 Feb 2022 09:38:56 GMT)\\u00a7r"}']}
{title:'Turrisi et al. (§72022§r)', author: 'Rosanna Turrisi; Leonardo Badino', display:{Lore:['[{"text": "arXiv:2104.02535", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimal Transport-based Adaptation in Dysarthric Speech Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oRosanna Turrisi\\nLeonardo Badino\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02535\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 14 Mar 2022 15:19:35 GMT)\\u00a7r"}']}
{title:'Meister et al. (§72022§r)', author: 'Julia A. Meister; Khuong An Nguyen; Zhiyuan Luo', display:{Lore:['[{"text": "arXiv:2104.07128", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio feature ranking for sound-based COVID-19 patient detection\\u00a7r\\n\\n\\u00a78\\u00a7oJulia A. Meister\\nKhuong An Nguyen\\nZhiyuan Luo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.07128\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-031-16474-3_13\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIn EPIA Conference on Artificial Intelligence (pp. 146-158).\\n  Springer, Cham (2022)\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 23 Nov 2022 10:47:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 3 figures, 6 tables\\u00a7r"}']}
{title:'Beguš et al. (§72022§r)', author: 'Gašper Beguš; Alan Zhou', display:{Lore:['[{"text": "arXiv:2104.09489", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInterpreting intermediate convolutional layers of generative CNNs trained on waveforms\\u00a7r\\n\\n\\u00a78\\u00a7oGa\\u0161per Begu\\u0161\\nAlan Zhou\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.09489\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2022.3209938\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol. 30, pp. 3214-3229, 2022\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 9 Sep 2022 19:52:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE/ACM Transactions on Audio Speech and Language Processing\\u00a7r"}']}
{title:'Cooper et al. (§72022§r)', author: 'Erica Cooper; Xin Wang; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2104.12292", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lText-to-Speech Synthesis Techniques for MIDI-to-Audio Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oErica Cooper\\nXin Wang\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.12292\\u00a7r\\n\\nVersion:\\u00a77v6 (Thu, 24 Feb 2022 07:42:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn the proceedings of ISCA Speech Synthesis Workshop 2021\\u00a7r"}']}
{title:'Dang et al. (§72022§r)', author: 'Feng Dang; Hangting Chen; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2104.13002", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDPT-FSNet: Dual-path Transformer Based Full-band and Sub-band Fusion Network for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oFeng Dang\\nHangting Chen\\nPengyuan Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.13002\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 25 Jan 2022 07:40:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Shih-Lun Wu; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2105.04090", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMuseMorphose: Full-Song and Fine-Grained Piano Music Style Transfer with One Transformer VAE\\u00a7r\\n\\n\\u00a78\\u00a7oShih-Lun Wu\\nYi-Hsuan Yang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.04090\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 19 Dec 2022 18:27:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for Publication at IEEE/ACMTransactions on Audio, Speech, and Language Processing (TASLP). Online supplemental materials are attached to the end of this arXiv version\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Shoule Wu; Ziqiang Shi', display:{Lore:['[{"text": "arXiv:2105.07583", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIt\\u00f4TTS and It\\u00f4Wave: Linear Stochastic Differential Equation Is All You Need For Audio Generation\\u00a7r\\n\\n\\u00a78\\u00a7oShoule Wu\\nZiqiang Shi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.07583\\u00a7r\\n\\nVersion:\\u00a77v5 (Sat, 29 Jan 2022 07:08:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe generated audio samples are available at https://wushoule.github.io/ItoAudio/\\u00a7r"}']}
{title:'Mannone et al. (§72022§r)', author: 'Maria Mannone; Davide Rocchesso', display:{Lore:['[{"text": "arXiv:2105.10781", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75quant-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQuanta in sound, the sound of quanta: a voice-informed quantum theoretical perspective on sound\\u00a7r\\n\\n\\u00a78\\u00a7oMaria Mannone\\nDavide Rocchesso\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.10781\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-030-95538-0_6\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 7 May 2022 16:30:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o34 pages, 16 figures. Pre-publication draft (2021) of: Mannone, M., Rocchesso, D. (2022). Quanta in Sound, the Sound of Quanta: A Voice-Informed Quantum Theoretical Perspective on Sound. In: Miranda, E. R. (eds) Quantum"}','{"text": "Computing in the Arts and Humanities. Springer, Cham\\u00a7r"}']}
{title:'Pandey et al. (§72022§r)', author: 'Ashutosh Pandey; DeLiang Wang', display:{Lore:['[{"text": "arXiv:2105.12831", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-attending RNN for Speech Enhancement to Improve Cross-corpus Generalization\\u00a7r\\n\\n\\u00a78\\u00a7oAshutosh Pandey\\nDeLiang Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.12831\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2022.3161143\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol. 30, pp. 1374-1385, 2022\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 18 Mar 2022 01:19:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing\\u00a7r"}']}
{title:'Lee et al. (§72022§r)', author: 'Sungho Lee; Hyeong-Seok Choi; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2105.13940", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDifferentiable Artificial Reverberation\\u00a7r\\n\\n\\u00a78\\u00a7oSungho Lee\\nHyeong-Seok Choi\\nKyogu Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.13940\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 20 Jul 2022 11:45:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to TASLP\\u00a7r"}']}
{title:'Fanzeres et al. (§72022§r)', author: 'Leonardo A. Fanzeres; Climent Nadeu', display:{Lore:['[{"text": "arXiv:2106.01266", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound-to-Imagination: An Exploratory Study on Unsupervised Crossmodal Translation Using Diverse Audiovisual Data\\u00a7r\\n\\n\\u00a78\\u00a7oLeonardo A. Fanzeres\\nCliment Nadeu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.01266\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 9 Mar 2022 16:49:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTitle changed. Added references. Overall revision of text, results unchanged. Figure 6 updated\\u00a7r"}']}
{title:'Verbitskiy et al. (§72022§r)', author: 'Sergey Verbitskiy; Vladimir Berikov; Viacheslav Vyshegorodtsev', display:{Lore:['[{"text": "arXiv:2106.01621", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lERANNs: Efficient Residual Audio Neural Networks for Audio Pattern Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSergey Verbitskiy\\nVladimir Berikov\\nViacheslav Vyshegorodtsev\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.01621\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.patrec.2022.07.012\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPattern Recognition Letters, 2022\\u00a7r\\n\\nVersion:\\u00a77v7 (Wed, 20 Jul 2022 13:08:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper has been submitted to Pattern Recognition Letters\\u00a7r"}']}
{title:'Lin et al. (§72022§r)', author: 'Yu-Chen Lin; Tsun-An Hsieh; Kuo-Hsuan Hung; Cheng Yu; Harinath Garudadri; Yu Tsao; Tei-Wei Kuo', display:{Lore:['[{"text": "arXiv:2106.05229", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Recovery for Real-World Self-powered Intermittent Devices\\u00a7r\\n\\n\\u00a78\\u00a7oYu-Chen Lin\\nTsun-An Hsieh\\nKuo-Hsuan Hung\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.05229\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 24 Jan 2022 12:59:40 GMT)\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Jingbei Li; Yi Meng; Chenyi Li; Zhiyong Wu; Helen Meng; Chao Weng; Dan Su', display:{Lore:['[{"text": "arXiv:2106.06233", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Speaking Styles in Conversational Text-to-Speech Synthesis with Graph-based Multi-modal Context Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oJingbei Li\\nYi Meng\\nChenyi Li\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.06233\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 31 Mar 2022 06:43:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Bie et al. (§72022§r)', author: 'Xiaoyu Bie; Simon Leglaive; Xavier Alameda-Pineda; Laurent Girin', display:{Lore:['[{"text": "arXiv:2106.12271", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Speech Enhancement using Dynamical Variational Auto-Encoders\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoyu Bie\\nSimon Leglaive\\nXavier Alameda-Pineda\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.12271\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2022.3207349.\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol. 30, pp. 2993-3007, 2022\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 30 Sep 2022 18:48:00 GMT)\\u00a7r"}']}
{title:'Valle-Pérez et al. (§72022§r)', author: 'Guillermo Valle-Pérez; Gustav Eje Henter; Jonas Beskow; André Holzapfel; Pierre-Yves Oudeyer; Simon Alexanderson', display:{Lore:['[{"text": "arXiv:2106.13871", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransflower: probabilistic autoregressive dance generation with multimodal attention\\u00a7r\\n\\n\\u00a78\\u00a7oGuillermo Valle-P\\u00e9rez\\nGustav Eje Henter\\nJonas Beskow\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.13871\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3478513.3480570\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 11 Jun 2022 22:48:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oArticlepresented at SIGGRAPH Asia 2021, and published in ACM Transactions on Graphics\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Haibin Wu; Po-chun Hsu; Ji Gao; Shanshan Zhang; Shen Huang; Jian Kang; Zhiyong Wu; Helen Meng; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2107.00309", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Sample Detection for Speaker Verification by Neural Vocoders\\u00a7r\\n\\n\\u00a78\\u00a7oHaibin Wu\\nPo-chun Hsu\\nJi Gao\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.00309\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 19 May 2022 18:43:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Yen et al. (§72022§r)', author: 'Hao Yen; Chao-Han Huck Yang; Hu Hu; Sabato Marco Siniscalchi; Qing Wang; Yuyang Wang; Xianjun Xia; Yuanjun Zhao; Yuzhong Wu; Yannan Wang; Jun Du; Chin-Hui Lee', display:{Lore:['[{"text": "arXiv:2107.01461", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Lottery Ticket Hypothesis Framework for Low-Complexity Device-Robust Neural Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oHao Yen\\nChao-Han Huck Yang\\nHu Hu\\n+ 8 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.01461\\u00a7r\\n\\nVersion:\\u00a77v4 (Sun, 1 May 2022 04:29:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 figures. DCASE 2021. The project started in November 2020. Revised version\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Qinghua Wu; Quanbo Shen; Jian Luan; YuJun Wang', display:{Lore:['[{"text": "arXiv:2107.03065", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMsdtron: a high-capability multi-speaker speech synthesis system for diverse data using characteristic information\\u00a7r\\n\\n\\u00a78\\u00a7oQinghua Wu\\nQuanbo Shen\\nJian Luan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.03065\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 11 Feb 2022 06:50:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP-2022\\u00a7r"}']}
{title:'Sheikh et al. (§72022§r)', author: 'Shakeel Ahmad Sheikh; Md Sahidullah; Fabrice Hirsch; Slim Ouni', display:{Lore:['[{"text": "arXiv:2107.04057", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMachine Learning for Stuttering Identification: Review, Challenges and Future Directions\\u00a7r\\n\\n\\u00a78\\u00a7oShakeel Ahmad Sheikh\\nMd Sahidullah\\nFabrice Hirsch\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.04057\\u00a7r\\n\\nVersion:\\u00a77v5 (Wed, 16 Nov 2022 15:28:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in Journal of Neurocomputing 2022 https://doi.org/10.1016/j.neucom.2022.10.015\\u00a7r"}']}
{title:'Maekaku et al. (§72022§r)', author: 'Takashi Maekaku; Xuankai Chang; Yuya Fujita; Li-Wei Chen; Shinji Watanabe; Alexander Rudnicky', display:{Lore:['[{"text": "arXiv:2107.05899", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Representation Learning Combining Conformer CPC with Deep Cluster for the ZeroSpeech Challenge 2021\\u00a7r\\n\\n\\u00a78\\u00a7oTakashi Maekaku\\nXuankai Chang\\nYuya Fujita\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.05899\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 16 Feb 2022 05:43:53 GMT)\\u00a7r"}']}
{title:'Chang et al. (§72022§r)', author: 'Jiangeng Chang; Shaoze Cui; Mengling Feng', display:{Lore:['[{"text": "arXiv:2107.06126", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiCOVA-Net: Diagnosing COVID-19 using Acoustics based on Deep Residual Network for the DiCOVA Challenge 2021\\u00a7r\\n\\n\\u00a78\\u00a7oJiangeng Chang\\nShaoze Cui\\nMengling Feng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.06126\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 4 May 2022 05:33:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 figures\\u00a7r"}']}
{title:'Peracha (§72022§r)', author: 'Omar Peracha', display:{Lore:['[{"text": "arXiv:2107.10388", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJS Fake Chorales: a Synthetic Dataset of Polyphonic Music with Human Annotation\\u00a7r\\n\\n\\u00a78\\u00a7oOmar Peracha\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.10388\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 2022 Sound and Music Computing Conference, SMC\\n  2022\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 31 Mar 2022 18:27:45 GMT)\\u00a7r"}']}
{title:'Toussaint et al. (§72022§r)', author: 'Wiebke Toussaint; Aaron Yi Ding', display:{Lore:['[{"text": "arXiv:2107.12049", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSVEva Fair: A Framework for Evaluating Fairness in Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oWiebke Toussaint\\nAaron Yi Ding\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.12049\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 4 Oct 2022 11:52:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis unpublished paper has been superseded by \\"Bias in Automated Speaker Recognition\\" (published at ACM FAccT2022): arXiv:2201.09486\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Huiyan Li; Haohong Lin; You Wang; Hengyang Wang; Ming Zhang; Han Gao; Qing Ai; Zhiyuan Luo; Guang Li', display:{Lore:['[{"text": "arXiv:2108.00190", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSequence-to-Sequence Voice Reconstruction for Silent Speech in a Tonal Language\\u00a7r\\n\\n\\u00a78\\u00a7oHuiyan Li\\nHaohong Lin\\nYou Wang\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.00190\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 1 Jun 2022 14:52:15 GMT)\\u00a7r"}']}
{title:'Hung et al. (§72022§r)', author: 'Tun-Min Hung; Bo-Yu Chen; Yen-Tung Yeh; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2108.01576", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Benchmarking Initiative for Audio-Domain Music Generation Using the Freesound Loop Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oTun-Min Hung\\nBo-Yu Chen\\nYen-Tung Yeh\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.01576\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 22 Sep 2022 16:17:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe paper has been accepted for publication at ISMIR 2021\\u00a7r"}']}
{title:'Zaïdi et al. (§72022§r)', author: 'Julian Zaïdi; Hugo Seuté; Benjamin van Niekerk; Marc-André Carbonneau', display:{Lore:['[{"text": "arXiv:2108.02271", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDaft-Exprt: Cross-Speaker Prosody Transfer on Any Text for Expressive Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oJulian Za\\u00efdi\\nHugo Seut\\u00e9\\nBenjamin van Niekerk\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.02271\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10761\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech (2022) 4591-4595\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 5 Apr 2022 18:06:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022, 5 pages, 5 figures, 2 tables\\u00a7r"}']}
{title:'Dehghani et al. (§72022§r)', author: 'Arash Dehghani; Seyyed Ali Seyyedsalehi', display:{Lore:['[{"text": "arXiv:2108.03818", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTime-Frequency Localization Using Deep Convolutional Maxout Neural Network in Persian Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oArash Dehghani\\nSeyyed Ali Seyyedsalehi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.03818\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s11063-022-11006-1\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nNeural Process Lett (2022)\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 31 Aug 2022 03:50:11 GMT)\\u00a7r"}']}
{title:'Mohaimenuzzaman et al. (§72022§r)', author: 'Md Mohaimenuzzaman; Christoph Bergmeir; Bernd Meyer', display:{Lore:['[{"text": "arXiv:2108.06128", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices\\u00a7r\\n\\n\\u00a78\\u00a7oMd Mohaimenuzzaman\\nChristoph Bergmeir\\nBernd Meyer\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.06128\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ACCESS.2022.3140807\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nin IEEE Access, vol. 10, 2022\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 17 Jan 2022 05:30:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages\\u00a7r"}']}
{title:'Jin et al. (§72022§r)', author: 'Xutong Jin; Sheng Li; Guoping Wang; Dinesh Manocha', display:{Lore:['[{"text": "arXiv:2108.07425", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeuralSound: Learning-based Modal Sound Synthesis With Acoustic Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oXutong Jin\\nSheng Li\\nGuoping Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.07425\\u00a7r\\n\\nVersion:\\u00a77v4 (Sat, 28 May 2022 04:38:07 GMT)\\u00a7r"}']}
{title:'Sitaula et al. (§72022§r)', author: 'Chiranjibi Sitaula; Jinyuan He; Archana Priyadarshi; Mark Tracy; Omid Kavehei; Murray Hinder; Anusha Withana; Alistair McEwan; Faezeh Marzbanrad', display:{Lore:['[{"text": "arXiv:2108.07467", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeonatal Bowel Sound Detection Using Convolutional Neural Network and Laplace Hidden Semi-Markov Model\\u00a7r\\n\\n\\u00a78\\u00a7oChiranjibi Sitaula\\nJinyuan He\\nArchana Priyadarshi\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.07467\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2022.3178225\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  2022\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 1 Jun 2022 01:38:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in IEEE/ACM Transactionson Audio Speech and Language Processing journal\\u00a7r"}']}
{title:'Imai (§72022§r)', author: 'Yusuke Imai', display:{Lore:['[{"text": "arXiv:2108.10294", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeneral Theory of Music by Icosahedron 2: Analysis of musical pieces by the exceptional musical icosahedra\\u00a7r\\n\\n\\u00a78\\u00a7oYusuke Imai\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.10294\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 2 Aug 2022 06:14:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o32 pages, 51 figures\\u00a7r"}']}
{title:'Dai et al. (§72022§r)', author: 'Dengxin Dai; Arun Balajee Vasudevan; Jiri Matas; Luc Van Gool', display:{Lore:['[{"text": "arXiv:2109.02763", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBinaural SoundNet: Predicting Semantics, Depth and Motion with Binaural Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oDengxin Dai\\nArun Balajee Vasudevan\\nJiri Matas\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.02763\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 27 Feb 2022 13:30:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by TPAMI. arXiv admin note: substantial text overlap with arXiv:2003.04210\\u00a7r"}']}
{title:'Grumiaux et al. (§72022§r)', author: 'Pierre-Amaury Grumiaux; Srđan Kitić; Laurent Girin; Alexandre Guérin', display:{Lore:['[{"text": "arXiv:2109.03465", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Survey of Sound Source Localization with Deep Learning Methods\\u00a7r\\n\\n\\u00a78\\u00a7oPierre-Amaury Grumiaux\\nSr\\u0111an Kiti\\u0107\\nLaurent Girin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.03465\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0011809\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 17 Jun 2022 11:44:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in The Journal of the Acoustical Society of America\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Xueyao Zhang; Jinchao Zhang; Yao Qiu; Li Wang; Jie Zhou', display:{Lore:['[{"text": "arXiv:2109.06441", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStructure-Enhanced Pop Music Generation via Harmony-Aware Learning\\u00a7r\\n\\n\\u00a78\\u00a7oXueyao Zhang\\nJinchao Zhang\\nYao Qiu\\nLi Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.06441\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3503161.3548084\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 12 Jul 2022 10:11:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ACM MM 2022\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Tao Li; Xinsheng Wang; Qicong Xie; Zhichao Wang; Lei Xie', display:{Lore:['[{"text": "arXiv:2109.06733", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-speaker emotion disentangling and transfer for end-to-end speech synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oTao Li\\nXinsheng Wang\\nQicong Xie\\nZhichao Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.06733\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 8 Apr 2022 08:06:26 GMT)\\u00a7r"}']}
{title:'Zhu et al. (§72022§r)', author: 'Yunyao Zhu; Stephen Hahn; Simon Mak; Yue Jiang; Cynthia Rudin', display:{Lore:['[{"text": "arXiv:2109.07623", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBacHMMachine: An Interpretable and Scalable Model for Algorithmic Harmonization for Four-part Baroque Chorales\\u00a7r\\n\\n\\u00a78\\u00a7oYunyao Zhu\\nStephen Hahn\\nSimon Mak\\nYue Jiang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.07623\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 23 Feb 2022 02:15:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 7 figures\\u00a7r"}']}
{title:'Iqbal et al. (§72022§r)', author: 'Turab Iqbal; Yin Cao; Andrew Bailey; Mark D. Plumbley; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2109.09227", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lARCA23K: An audio dataset for investigating open-set label noise\\u00a7r\\n\\n\\u00a78\\u00a7oTurab Iqbal\\nYin Cao\\nAndrew Bailey\\nMark D. Plumbley\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.09227\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 27 Feb 2022 09:35:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the Detection and Classification of Acoustic Scenes and Events 2021 Workshop (DCASE2021)\\u00a7r"}']}
{title:'Ju et al. (§72022§r)', author: 'Zeqian Ju; Peiling Lu; Xu Tan; Rui Wang; Chen Zhang; Songruoyao Wu; Kejun Zhang; Xiangyang Li; Tao Qin; Tie-Yan Liu', display:{Lore:['[{"text": "arXiv:2109.09617", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTeleMelody: Lyric-to-Melody Generation with a Template-Based Two-Stage Method\\u00a7r\\n\\n\\u00a78\\u00a7oZeqian Ju\\nPeiling Lu\\nXu Tan\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.09617\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 14 Nov 2022 03:29:54 GMT)\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Rui Li; Dong Pu; Minnie Huang; Bill Huang', display:{Lore:['[{"text": "arXiv:2109.11115", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnet-TTS: Improving Unseen Speaker and Style Transfer in One-shot Voice Cloning\\u00a7r\\n\\n\\u00a78\\u00a7oRui Li\\nDong Pu\\nMinnie Huang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.11115\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 24 Feb 2022 10:33:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 5 figures, Accepted to IEEE ICASSP 2022\\u00a7r"}']}
{title:'Yu et al. (§72022§r)', author: 'Guochen Yu; Andong Li; Yutian Wang; Yinuo Guo; Hui Wang; Chengshi Zheng', display:{Lore:['[{"text": "arXiv:2109.12591", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint magnitude estimation and phase recovery using Cycle-in-Cycle GAN for non-parallel speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oGuochen Yu\\nAndong Li\\nYutian Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.12591\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 14 Feb 2022 12:12:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccecpted by ICASSP 2022\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Li Li; Hirokazu Kameoka; Shoji Makino', display:{Lore:['[{"text": "arXiv:2109.13496", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFastMVAE2: On improving and accelerating the fast variational autoencoder-based source separation algorithm for determined mixtures\\u00a7r\\n\\n\\u00a78\\u00a7oLi Li\\nHirokazu Kameoka\\nShoji Makino\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.13496\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 7 Sep 2022 04:51:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmit to IEEE/ACM TASLP, under review\\u00a7r"}']}
{title:'Luong et al. (§72022§r)', author: 'Manh Luong; Viet Anh Tran', display:{Lore:['[{"text": "arXiv:2109.13675", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFlowVocoder: A small Footprint Neural Vocoder based Normalizing flow for Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oManh Luong\\nViet Anh Tran\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.13675\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 25 Mar 2022 08:01:52 GMT)\\u00a7r"}']}
{title:'Popov et al. (§72022§r)', author: 'Vadim Popov; Ivan Vovk; Vladimir Gogoryan; Tasnima Sadekova; Mikhail Kudinov; Jiansheng Wei', display:{Lore:['[{"text": "arXiv:2109.13821", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme\\u00a7r\\n\\n\\u00a78\\u00a7oVadim Popov\\nIvan Vovk\\nVladimir Gogoryan\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.13821\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 4 Aug 2022 10:25:40 GMT)\\u00a7r"}']}
{title:'Vaessen et al. (§72022§r)', author: 'Nik Vaessen; David A. van Leeuwen', display:{Lore:['[{"text": "arXiv:2109.15053", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFine-tuning wav2vec2 for speaker recognition\\u00a7r\\n\\n\\u00a78\\u00a7oNik Vaessen\\nDavid A. van Leeuwen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.15053\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746952\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 6 May 2022 09:41:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to ICASSP 2022\\u00a7r"}']}
{title:'Beguš et al. (§72022§r)', author: 'Gašper Beguš; Alan Zhou', display:{Lore:['[{"text": "arXiv:2110.02375", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInterpreting intermediate convolutional layers in unsupervised acoustic word classification\\u00a7r\\n\\n\\u00a78\\u00a7oGa\\u0161per Begu\\u0161\\nAlan Zhou\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02375\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746849\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2022 - 2022 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 3 Feb 2022 18:25:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2022\\u00a7r"}']}
{title:'Tae et al. (§72022§r)', author: 'Jaesung Tae; Hyeongju Kim; Taesu Kim', display:{Lore:['[{"text": "arXiv:2110.02584", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEdiTTS: Score-based Editing for Controllable Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oJaesung Tae\\nHyeongju Kim\\nTaesu Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02584\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 9 Jul 2022 17:22:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 3 figures, 3 tables, INTERSPEECH 2022\\u00a7r"}']}
{title:'Hidaka et al. (§72022§r)', author: 'Shunsuke Hidaka; Kohei Wakamiya; Tokihiko Kaburagi', display:{Lore:['[{"text": "arXiv:2110.02878", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Investigation of the Effectiveness of Phase for Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oShunsuke Hidaka\\nKohei Wakamiya\\nTokihiko Kaburagi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02878\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746037\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP (2022) 3708-3712\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 15 Feb 2022 07:20:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Verma (§72022§r)', author: 'Prateek Verma', display:{Lore:['[{"text": "arXiv:2110.03183", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention is All You Need? Good Embeddings with Statistics are enough:Large Scale Audio Understanding without Transformers/ Convolutions/ BERTs/ Mixers/ Attention/ RNNs or ....\\u00a7r\\n\\n\\u00a78\\u00a7oPrateek Verma\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03183\\u00a7r\\n\\nVersion:\\u00a77v5 (Sun, 30 Jan 2022 02:25:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE Copyright: written as told\\u00a7r"}']}
{title:'Tonami et al. (§72022§r)', author: 'Noriyuki Tonami; Keisuke Imoto; Ryotaro Nagase; Yuki Okamoto; Takahiro Fukumori; Yoichi Yamashita', display:{Lore:['[{"text": "arXiv:2110.03243", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Event Detection Guided by Semantic Contexts of Scenes\\u00a7r\\n\\n\\u00a78\\u00a7oNoriyuki Tonami\\nKeisuke Imoto\\nRyotaro Nagase\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03243\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 17 Feb 2022 06:03:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022\\u00a7r"}']}
{title:'Hoang et al. (§72022§r)', author: 'Truong Hoang; Lam Pham; Dat Ngo; Hoang D. Nguyen', display:{Lore:['[{"text": "arXiv:2110.03251", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Cough-based deep learning framework for detecting COVID-19\\u00a7r\\n\\n\\u00a78\\u00a7oTruong Hoang\\nLam Pham\\nDat Ngo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03251\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/EMBC48229.2022.9871179\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nEMBC 44 (2022) 3422-3425\\u00a7r\\n\\nVersion:\\u00a77v4 (Sat, 1 Oct 2022 03:57:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCOVID-19, EMBC-2022, DiCOVA, top 2nd, benchmark on Spec> 0.95\\u00a7r"}']}
{title:'Gu et al. (§72022§r)', author: 'Jianju Gu; Longbiao Cheng; Dingding Yao; Junfeng Li; Yonghong Yan', display:{Lore:['[{"text": "arXiv:2110.03272", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Novel Blind Source Separation Framework Towards Maximum Signal-To-Interference Ratio\\u00a7r\\n\\n\\u00a78\\u00a7oJianju Gu\\nLongbiao Cheng\\nDingding Yao\\nJunfeng Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03272\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 8 Mar 2022 02:23:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Binbin Zhang; Hang Lv; Pengcheng Guo; Qijie Shao; Chao Yang; Lei Xie; Xin Xu; Hui Bu; Xiaoyu Chen; Chenchen Zeng; Di Wu; Zhendong Peng', display:{Lore:['[{"text": "arXiv:2110.03370", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWenetSpeech: A 10000+ Hours Multi-domain Mandarin Corpus for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oBinbin Zhang\\nHang Lv\\nPengcheng Guo\\n+ 8 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03370\\u00a7r\\n\\nVersion:\\u00a77v5 (Wed, 23 Feb 2022 06:42:31 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'You Jin Kim; Hee-Soo Heo; Jee-weon Jung; Youngki Kwon; Bong-Jin Lee; Joon Son Chung', display:{Lore:['[{"text": "arXiv:2110.03380", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdvancing the dimensionality reduction of speaker embeddings for speaker diarisation: disentangling noise and informing speech activity\\u00a7r\\n\\n\\u00a78\\u00a7oYou Jin Kim\\nHee-Soo Heo\\nJee-weon Jung\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03380\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 3 Nov 2022 09:21:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper was submitted to ICASSP 2023\\u00a7r"}']}
{title:'Ren et al. (§72022§r)', author: 'Zhao Ren; Thanh Tam Nguyen; Wolfgang Nejdl', display:{Lore:['[{"text": "arXiv:2110.03536", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrototype Learning for Interpretable Respiratory Sound Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oZhao Ren\\nThanh Tam Nguyen\\nWolfgang Nejdl\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03536\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 7 Feb 2022 09:55:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTechnical reportof the paper accepted by IEEE ICASSP 2022\\u00a7r"}']}
{title:'Bous et al. (§72022§r)', author: 'Frederik Bous; Laurent Benaroya; Nicolas Obin; Axel Roebel', display:{Lore:['[{"text": "arXiv:2110.03744", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Reenactment with F0 and timing constraints and adversarial learning of conversions\\u00a7r\\n\\n\\u00a78\\u00a7oFrederik Bous\\nLaurent Benaroya\\nNicolas Obin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03744\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 31 May 2022 10:46:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2107.12346\\u00a7r"}']}
{title:'Pahar et al. (§72022§r)', author: 'Madhurananda Pahar; Marisa Klopper; Byron Reeve; Rob Warren; Grant Theron; Andreas Diacon; Thomas Niesler', display:{Lore:['[{"text": "arXiv:2110.03771", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWake-Cough: cough spotting and cougher identification for personalised long-term cough monitoring\\u00a7r\\n\\n\\u00a78\\u00a7oMadhurananda Pahar\\nMarisa Klopper\\nByron Reeve\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03771\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 10 Sep 2022 17:38:24 GMT)\\u00a7r"}']}
{title:'Ratnarajah et al. (§72022§r)', author: 'Anton Ratnarajah; Shi-Xiong Zhang; Meng Yu; Zhenyu Tang; Dinesh Manocha; Dong Yu', display:{Lore:['[{"text": "arXiv:2110.04057", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFAST-RIR: Fast neural diffuse room impulse response generator\\u00a7r\\n\\n\\u00a78\\u00a7oAnton Ratnarajah\\nShi-Xiong Zhang\\nMeng Yu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04057\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 6 Feb 2022 03:04:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022. More results and source code is available at https://anton-jeran.github.io/FRIR/\\u00a7r"}']}
{title:'Lin et al. (§72022§r)', author: 'Qingjian Lin; Lin Yang; Xuyang Wang; Xiaoyi Qin; Junjie Wang; Ming Li', display:{Lore:['[{"text": "arXiv:2110.04438", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Lightweight Applications: Asymmetric Enroll-Verify Structure for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oQingjian Lin\\nLin Yang\\nXuyang Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04438\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 26 Jan 2022 04:25:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Dongchao Yang; Helin Wang; Yuexian Zou; Zhongjie Ye; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2110.04474", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Mutual learning framework for Few-shot Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oDongchao Yang\\nHelin Wang\\nYuexian Zou\\nZhongjie Ye\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04474\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 4 Jun 2022 02:20:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2022. arXiv admin note: text overlapwith arXiv:2106.12252 by other authors\\u00a7r"}']}
{title:'He et al. (§72022§r)', author: 'Yunchao He; Jian Luan; Yujun Wang', display:{Lore:['[{"text": "arXiv:2110.04486", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPAMA-TTS: Progression-Aware Monotonic Attention for Stable Seq2Seq TTS With Accurate Phoneme Duration Control\\u00a7r\\n\\n\\u00a78\\u00a7oYunchao He\\nJian Luan\\nYujun Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04486\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 18 Mar 2022 07:32:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022. 5 pages, 4 figures, 3 tables. Audio samples are available at: https://pama-tts.github.io/\\u00a7r"}']}
{title:'Shor et al. (§72022§r)', author: 'Joel Shor; Aren Jansen; Wei Han; Daniel Park; Yu Zhang', display:{Lore:['[{"text": "arXiv:2110.04621", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUniversal Paralinguistic Speech Representations Using Self-Supervised Conformers\\u00a7r\\n\\n\\u00a78\\u00a7oJoel Shor\\nAren Jansen\\nWei Han\\nDaniel Park\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04621\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747197\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2022-2022 IEEE\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 13 Dec 2022 14:07:12 GMT)\\u00a7r"}']}
{title:'Zhou et al. (§72022§r)', author: 'Zelin Zhou; Zhiling Zhang; Xuenan Xu; Zeyu Xie; Mengyue Wu; Kenny Q. Zhu', display:{Lore:['[{"text": "arXiv:2110.04684", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCan Audio Captions Be Evaluated with Image Caption Metrics?\\u00a7r\\n\\n\\u00a78\\u00a7oZelin Zhou\\nZhiling Zhang\\nXuenan Xu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04684\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 27 Jan 2022 12:10:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2022\\u00a7r"}']}
{title:'Luong et al. (§72022§r)', author: 'Hieu-Thi Luong; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2110.04946", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLaughNet: synthesizing laughter utterances from waveform silhouettes and a single laughter example\\u00a7r\\n\\n\\u00a78\\u00a7oHieu-Thi Luong\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04946\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 26 Jan 2022 01:40:13 GMT)\\u00a7r"}']}
{title:'Takahashi et al. (§72022§r)', author: 'Naoya Takahashi; Mayank Kumar Singh; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2110.05054", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSource Mixing and Separation Robust Audio Steganography\\u00a7r\\n\\n\\u00a78\\u00a7oNaoya Takahashi\\nMayank Kumar Singh\\nYuki Mitsufuji\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05054\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 18 Feb 2022 01:26:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022\\u00a7r"}']}
{title:'Takahashi et al. (§72022§r)', author: 'Naoya Takahashi; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2110.05059", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAmicable examples for informed source separation\\u00a7r\\n\\n\\u00a78\\u00a7oNaoya Takahashi\\nYuki Mitsufuji\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05059\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 18 Feb 2022 01:24:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022\\u00a7r"}']}
{title:'Koutini et al. (§72022§r)', author: 'Khaled Koutini; Jan Schlüter; Hamid Eghbal-zadeh; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2110.05069", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Training of Audio Transformers with Patchout\\u00a7r\\n\\n\\u00a78\\u00a7oKhaled Koutini\\nJan Schl\\u00fcter\\nHamid Eghbal-zadeh\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05069\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-227\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 29 Mar 2022 12:25:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022. Source code: https://github.com/kkoutini/PaSST\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Zhengyang Chen; Sanyuan Chen; Yu Wu; Yao Qian; Chengyi Wang; Shujie Liu; Yanmin Qian; Michael Zeng', display:{Lore:['[{"text": "arXiv:2110.05777", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLarge-scale Self-Supervised Speech Representation Learning for Automatic Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oZhengyang Chen\\nSanyuan Chen\\nYu Wu\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05777\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 24 Jan 2022 12:07:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Neekhara et al. (§72022§r)', author: 'Paarth Neekhara; Jason Li; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2110.05798", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdapting TTS models For New Speakers using Transfer Learning\\u00a7r\\n\\n\\u00a78\\u00a7oPaarth Neekhara\\nJason Li\\nBoris Ginsburg\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05798\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Apr 2022 01:31:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Quan et al. (§72022§r)', author: 'Changsheng Quan; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2110.05966", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-channel Narrow-band Deep Speech Separation with Full-band Permutation Invariant Training\\u00a7r\\n\\n\\u00a78\\u00a7oChangsheng Quan\\nXiaofei Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05966\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 12 Apr 2022 09:56:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by ICASSP 2022\\u00a7r"}']}
{title:'Liang et al. (§72022§r)', author: 'Chengdong Liang; Yijiang Chen; Jiadi Yao; Xiao-Lei Zhang', display:{Lore:['[{"text": "arXiv:2110.05975", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Channel Far-Field Speaker Verification with Large-Scale Ad-hoc Microphone Arrays\\u00a7r\\n\\n\\u00a78\\u00a7oChengdong Liang\\nYijiang Chen\\nJiadi Yao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05975\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 28 Mar 2022 11:51:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Mahanta et al. (§72022§r)', author: 'Saranga Kingkor Mahanta; Darsh Kaushik; Shubham Jain; Hoang Van Truong; Koushik Guha', display:{Lore:['[{"text": "arXiv:2110.06123", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCOVID-19 Diagnosis from Cough Acoustics using ConvNets and Data Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oSaranga Kingkor Mahanta\\nDarsh Kaushik\\nShubham Jain\\nHoang Van Truong\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06123\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 3 May 2022 16:09:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDiCOVA, top 1st, This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\\u00a7r"}']}
{title:'Yu et al. (§72022§r)', author: 'Guochen Yu; Andong Li; Chengshi Zheng; Yinuo Guo; Yutian Wang; Hui Wang', display:{Lore:['[{"text": "arXiv:2110.06467", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDual-branch Attention-In-Attention Transformer for single-channel speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oGuochen Yu\\nAndong Li\\nChengshi Zheng\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06467\\u00a7r\\n\\nVersion:\\u00a77v5 (Mon, 14 Feb 2022 11:59:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Koyama et al. (§72022§r)', author: 'Yuichiro Koyama; Naoki Murata; Stefan Uhlich; Giorgio Fabbro; Shusuke Takahashi; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2110.06494", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Source Separation with Deep Equilibrium Models\\u00a7r\\n\\n\\u00a78\\u00a7oYuichiro Koyama\\nNaoki Murata\\nStefan Uhlich\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06494\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 28 Apr 2022 06:42:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, accepted for publication in IEEE ICASSP 2022\\u00a7r"}']}
{title:'Koyama et al. (§72022§r)', author: 'Yuichiro Koyama; Kazuhide Shigemi; Masafumi Takahashi; Kazuki Shimada; Naoya Takahashi; Emiru Tsunoo; Shusuke Takahashi; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2110.06501", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatial Data Augmentation with Simulated Room Impulse Responses for Sound Event Localization and Detection\\u00a7r\\n\\n\\u00a78\\u00a7oYuichiro Koyama\\nKazuhide Shigemi\\nMasafumi Takahashi\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06501\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 28 Apr 2022 06:28:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, accepted for publication in IEEE ICASSP 2022\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Bo-Yu Chen; Wei-Han Hsu; Wei-Hsiang Liao; Marco A. Martínez Ramírez; Yuki Mitsufuji; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2110.06525", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic DJ Transitions with Differentiable Audio Effects and Generative Adversarial Networks\\u00a7r\\n\\n\\u00a78\\u00a7oBo-Yu Chen\\nWei-Han Hsu\\nWei-Hsiang Liao\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06525\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 17 Feb 2022 15:26:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published at ICASSP 2022\\u00a7r"}']}
{title:'Guo et al. (§72022§r)', author: 'Yina Guo; Xiaofei Zhang; Zhenying Gong; Anhong Wang; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2110.06634", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end translation of human neural activity to speech with a dual-dual generative adversarial network\\u00a7r\\n\\n\\u00a78\\u00a7oYina Guo\\nXiaofei Zhang\\nZhenying Gong\\nAnhong Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06634\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 26 Mar 2022 14:45:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 13 figures\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Hsuan-Yu Chen; Xuanjun Chen; Jyh-Shing Roger Jang', display:{Lore:['[{"text": "arXiv:2110.06707", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSinger separation for karaoke content generation\\u00a7r\\n\\n\\u00a78\\u00a7oHsuan-Yu Chen\\nXuanjun Chen\\nJyh-Shing Roger Jang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06707\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 12 Jan 2022 07:44:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Haitong Zhang; Yue Lin', display:{Lore:['[{"text": "arXiv:2110.07210", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImprove Cross-lingual Voice Cloning Using Low-quality Code-switched Data\\u00a7r\\n\\n\\u00a78\\u00a7oHaitong Zhang\\nYue Lin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.07210\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 17 Nov 2022 06:54:56 GMT)\\u00a7r"}']}
{title:'Barahona-Ríos et al. (§72022§r)', author: 'Adrián Barahona-Ríos; Tom Collins', display:{Lore:['[{"text": "arXiv:2110.07311", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpecSinGAN: Sound Effect Variation Synthesis Using Single-Image GANs\\u00a7r\\n\\n\\u00a78\\u00a7oAdri\\u00e1n Barahona-R\\u00edos\\nTom Collins\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.07311\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 5 Apr 2022 09:23:09 GMT)\\u00a7r"}']}
{title:'Srivastava et al. (§72022§r)', author: 'Sangeeta Srivastava; Yun Wang; Andros Tjandra; Anurag Kumar; Chunxi Liu; Kritika Singh; Yatharth Saraf', display:{Lore:['[{"text": "arXiv:2110.07313", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConformer-Based Self-Supervised Learning for Non-Speech Audio Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oSangeeta Srivastava\\nYun Wang\\nAndros Tjandra\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.07313\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 7 Jan 2022 02:30:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages. Submitted to ICASSP in Oct 2021\\u00a7r"}']}
{title:'Yu et al. (§72022§r)', author: 'Fan Yu; Shiliang Zhang; Yihui Fu; Lei Xie; Siqi Zheng; Zhihao Du; Weilong Huang; Pengcheng Guo; Zhijie Yan; Bin Ma; Xin Xu; Hui Bu', display:{Lore:['[{"text": "arXiv:2110.07393", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lM2MeT: The ICASSP 2022 Multi-Channel Multi-Party Meeting Transcription Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oFan Yu\\nShiliang Zhang\\nYihui Fu\\n+ 8 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.07393\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 25 Feb 2022 06:48:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Haichuan Yang; Yuan Shangguan; Dilin Wang; Meng Li; Pierce Chuang; Xiaohui Zhang; Ganesh Venkatesh; Ozlem Kalinli; Vikas Chandra', display:{Lore:['[{"text": "arXiv:2110.08352", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOmni-sparsity DNN: Fast Sparsity Optimization for On-Device Streaming E2E ASR via Supernet\\u00a7r\\n\\n\\u00a78\\u00a7oHaichuan Yang\\nYuan Shangguan\\nDilin Wang\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.08352\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Jul 2022 05:14:37 GMT)\\u00a7r"}']}
{title:'Oglic et al. (§72022§r)', author: 'Dino Oglic; Zoran Cvetkovic; Peter Sollich; Steve Renals; Bin Yu', display:{Lore:['[{"text": "arXiv:2110.08634", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Robust Waveform-Based Acoustic Models\\u00a7r\\n\\n\\u00a78\\u00a7oDino Oglic\\nZoran Cvetkovic\\nPeter Sollich\\nSteve Renals\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.08634\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2022.3172632\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  2022\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 29 Jun 2022 11:18:06 GMT)\\u00a7r"}']}
{title:'Zhuang et al. (§72022§r)', author: 'Xiaobin Zhuang; Huiran Yu; Weifeng Zhao; Tao Jiang; Peng Hu', display:{Lore:['[{"text": "arXiv:2110.09121", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKaraTuner: Towards end to end natural pitch correction for singing voice in karaoke\\u00a7r\\n\\n\\u00a78\\u00a7oXiaobin Zhuang\\nHuiran Yu\\nWeifeng Zhao\\nTao Jiang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09121\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Jun 2022 02:17:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in Proc. Interspeech 2022, Incheon, South Korea\\u00a7r"}']}
{title:'He et al. (§72022§r)', author: 'Mutian He; Jingzhou Yang; Lei He; Frank K. Soong', display:{Lore:['[{"text": "arXiv:2110.09698", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Lexicon Reader: Reduce Pronunciation Errors in End-to-end TTS by Leveraging External Textual Knowledge\\u00a7r\\n\\n\\u00a78\\u00a7oMutian He\\nJingzhou Yang\\nLei He\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09698\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 24 Jun 2022 13:21:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures; accepted by Interspeech 2022\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Fengyu Yang; Jian Luan; Yujun Wang', display:{Lore:['[{"text": "arXiv:2110.09780", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Emotional Speech Synthesis by Using SUS-Constrained VAE and Text Encoder Aggregation\\u00a7r\\n\\n\\u00a78\\u00a7oFengyu Yang\\nJian Luan\\nYujun Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09780\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 28 Jan 2022 07:58:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by ICASSP2022\\u00a7r"}']}
{title:'Gong et al. (§72022§r)', author: 'Yuan Gong; Cheng-I Jeff Lai; Yu-An Chung; James Glass', display:{Lore:['[{"text": "arXiv:2110.09784", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSSAST: Self-Supervised Audio Spectrogram Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oYuan Gong\\nCheng-I Jeff Lai\\nYu-An Chung\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09784\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 10 Feb 2022 21:39:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at AAAI2022. Code at https://github.com/YuanGongND/ssast\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Haozhe Chen; Weiming Zhang; Kunlin Liu; Kejiang Chen; Han Fang; Nenghai Yu', display:{Lore:['[{"text": "arXiv:2110.09814", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Pattern based Black-box Model Watermarking for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHaozhe Chen\\nWeiming Zhang\\nKunlin Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09814\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747044\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 2 May 2022 09:42:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures. Acceptted by 2022 IEEEInternational Conference on Acoustics, Speech and Signal Processing (ICASSP)\\u00a7r"}']}
{title:'van Wyk et al. (§72022§r)', author: 'Jacques van Wyk; Jaco Versfeld; Johan du Preez', display:{Lore:['[{"text": "arXiv:2110.10010", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTemporal separation of whale vocalizations from background oceanic noise using a power calculation\\u00a7r\\n\\n\\u00a78\\u00a7oJacques van Wyk\\nJaco Versfeld\\nJohan du Preez\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.10010\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.ecoinf.2022.101627\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 20 Mar 2022 11:16:41 GMT)\\u00a7r"}']}
{title:'Tzinis et al. (§72022§r)', author: 'Efthymios Tzinis; Yossi Adi; Vamsi K. Ithapu; Buye Xu; Anurag Kumar', display:{Lore:['[{"text": "arXiv:2110.10103", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContinual self-training with bootstrapped remixing for speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oEfthymios Tzinis\\nYossi Adi\\nVamsi K. Ithapu\\nBuye Xu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.10103\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747463\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2022 - 2022 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 30 Jan 2022 00:54:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proc. ICASSP 2022, May 22-27, 2022, Singapore\\u00a7r"}']}
{title:'Gao et al. (§72022§r)', author: 'Chenyang Gao; Yue Gu; Ivan Marsic', display:{Lore:['[{"text": "arXiv:2110.10593", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProgressive Learning for Stabilizing Label Selection in Speech Separation with Mapping-based Method\\u00a7r\\n\\n\\u00a78\\u00a7oChenyang Gao\\nYue Gu\\nIvan Marsic\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.10593\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 21 Mar 2022 14:55:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Pandey et al. (§72022§r)', author: 'Ashutosh Pandey; Buye Xu; Anurag Kumar; Jacob Donley; Paul Calamia; DeLiang Wang', display:{Lore:['[{"text": "arXiv:2110.10757", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTPARN: Triple-path Attentive Recurrent Network for Time-domain Multichannel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oAshutosh Pandey\\nBuye Xu\\nAnurag Kumar\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.10757\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Apr 2022 13:07:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in ICASSP 2022\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Ho-Hsiang Wu; Prem Seetharaman; Kundan Kumar; Juan Pablo Bello', display:{Lore:['[{"text": "arXiv:2110.11499", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWav2CLIP: Learning Robust Audio Representations From CLIP\\u00a7r\\n\\n\\u00a78\\u00a7oHo-Hsiang Wu\\nPrem Seetharaman\\nKundan Kumar\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.11499\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 15 Feb 2022 13:06:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCopyright 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising "}','{"text": "or promotional purposes, creating newcollective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Pandey et al. (§72022§r)', author: 'Ashutosh Pandey; Buye Xu; Anurag Kumar; Jacob Donley; Paul Calamia; DeLiang Wang', display:{Lore:['[{"text": "arXiv:2110.11844", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTime-domain Ad-hoc Array Speech Enhancement Using a Triple-path Network\\u00a7r\\n\\n\\u00a78\\u00a7oAshutosh Pandey\\nBuye Xu\\nAnurag Kumar\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.11844\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 4 Jul 2022 19:55:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in INTERSPEECH 2022\\u00a7r"}']}
{title:'Pandey et al. (§72022§r)', author: 'Asutosh Pandey; Buye Xu; Anurag Kumar; Jacob Donley; Paul Calamia; DeLiang Wang', display:{Lore:['[{"text": "arXiv:2110.13130", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultichannel Speech Enhancement without Beamforming\\u00a7r\\n\\n\\u00a78\\u00a7oAsutosh Pandey\\nBuye Xu\\nAnurag Kumar\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.13130\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Apr 2022 12:54:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in ICASSP 2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Ruiteng Zhang; Jianguo Wei; Wenhuan Lu; Lin Zhang; Yantao Ji; Junhai Xu; Xugang Lu', display:{Lore:['[{"text": "arXiv:2110.13465", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCS-Rep: Making Speaker Verification Networks Embracing Re-parameterization\\u00a7r\\n\\n\\u00a78\\u00a7oRuiteng Zhang\\nJianguo Wei\\nWenhuan Lu\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.13465\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 4 Apr 2022 02:20:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Geraghty et al. (§72022§r)', author: 'Jack Geraghty; Jiazheng Li; Alessandro Ragano; Andrew Hines', display:{Lore:['[{"text": "arXiv:2110.13589", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAQP: An Open Modular Python Platform for Objective Speech and Audio Quality Metrics\\u00a7r\\n\\n\\u00a78\\u00a7oJack Geraghty\\nJiazheng Li\\nAlessandro Ragano\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.13589\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3524273.3532885\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 30 Jun 2022 10:53:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 3 figures, accepted and presented at ACM MMSys22, June, 2022, Athlone, Ireland\\u00a7r"}']}
{title:'Choi et al. (§72022§r)', author: 'Kwanghee Choi; Martin Kersner; Jacob Morton; Buru Chang', display:{Lore:['[{"text": "arXiv:2110.14131", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTemporal Knowledge Distillation for On-device Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oKwanghee Choi\\nMartin Kersner\\nJacob Morton\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.14131\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 5 Feb 2022 15:44:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Shijun Wang; Dimche Kostadinov; Damian Borth', display:{Lore:['[{"text": "arXiv:2110.14422", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero-shot Voice Conversion via Self-supervised Prosody Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oShijun Wang\\nDimche Kostadinov\\nDamian Borth\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.14422\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 31 May 2022 14:05:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in: 2022 International Joint Conference on Neural Networks (IJCNN)\\u00a7r"}']}
{title:'Marmoret et al. (§72022§r)', author: 'Axel Marmoret; Florian Voorwinden; Valentin Leplat; Jérémy E. Cohen; Frédéric Bimbot', display:{Lore:['[{"text": "arXiv:2110.14434", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NA\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a72math.NA\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNonnegative Tucker Decomposition with Beta-divergence for Music Structure Analysis of Audio Signals\\u00a7r\\n\\n\\u00a78\\u00a7oAxel Marmoret\\nFlorian Voorwinden\\nValentin Leplat\\nJ\\u00e9r\\u00e9my E. Cohen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.14434\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 2 Aug 2022 12:15:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 2 figures, 1 table, 1 algorithm. To be published in GRETSI2022. The algorithm is available athttps://gitlab.inria.fr/amarmore/nonnegative-factorization\\u00a7r"}']}
{title:'Marmoret et al. (§72022§r)', author: 'Axel Marmoret; Jérémy E. Cohen; Frédéric Bimbot', display:{Lore:['[{"text": "arXiv:2110.14437", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring single-song autoencoding schemes for audio-based music structure analysis\\u00a7r\\n\\n\\u00a78\\u00a7oAxel Marmoret\\nJ\\u00e9r\\u00e9my E. Cohen\\nFr\\u00e9d\\u00e9ric Bimbot\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.14437\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 7 Mar 2022 20:42:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 4 figures, 2 tables. Rejected from ICASSP 2022, an extended version is available at arXiv:2202.04981\\u00a7r"}']}
{title:'Zaidi et al. (§72022§r)', author: 'Mohd Abbas Zaidi; Beomseok Lee; Sangha Kim; Chanwoo Kim', display:{Lore:['[{"text": "arXiv:2110.15729", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDecision Attentive Regularization to Improve Simultaneous Speech Translation Systems\\u00a7r\\n\\n\\u00a78\\u00a7oMohd Abbas Zaidi\\nBeomseok Lee\\nSangha Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.15729\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 17 Jun 2022 07:08:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 1 table\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Jaechang Kim; Yunjoo Lee; Seunghoon Hong; Jungseul Ok', display:{Lore:['[{"text": "arXiv:2111.00195", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Continuous Representation of Audio for Arbitrary Scale Super Resolution\\u00a7r\\n\\n\\u00a78\\u00a7oJaechang Kim\\nYunjoo Lee\\nSeunghoon Hong\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.00195\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 30 Mar 2022 10:05:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022. The source code is available at https://github.com/ml-postech/LISA\\u00a7r"}']}
{title:'Heydari et al. (§72022§r)', author: 'Mojtaba Heydari; Matthew McCallum; Andreas Ehmann; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2111.00704", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Novel 1D State Space for Efficient Music Rhythmic Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oMojtaba Heydari\\nMatthew McCallum\\nAndreas Ehmann\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.00704\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 20 Feb 2022 15:32:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), May. 2022\\u00a7r"}']}
{title:'Xu et al. (§72022§r)', author: 'Shengyuan Xu; Wenxiao Zhao; Jing Guo', display:{Lore:['[{"text": "arXiv:2111.00962", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRefineGAN: Universally Generating Waveform Better than Ground Truth with Highly Accurate Pitch and Intensity Responses\\u00a7r\\n\\n\\u00a78\\u00a7oShengyuan Xu\\nWenxiao Zhao\\nJing Guo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.00962\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 20 Mar 2022 09:43:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH2022\\u00a7r"}']}
{title:'Kohler et al. (§72022§r)', author: 'Jonas Kohler; Maarten C. Ottenhoff; Sophocles Goulis; Miguel Angrick; Albert J. Colon; Louis Wagner; Simon Tousseyn; Pieter L. Kubben; Christian Herff', display:{Lore:['[{"text": "arXiv:2111.01457", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynthesizing Speech from Intracranial Depth Electrodes using an Encoder-Decoder Framework\\u00a7r\\n\\n\\u00a78\\u00a7oJonas Kohler\\nMaarten C. Ottenhoff\\nSophocles Goulis\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.01457\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.51628/001c.57524\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 31 Oct 2022 13:15:13 GMT)\\u00a7r"}']}
{title:'Guo et al. (§72022§r)', author: 'Dongyue Guo; Jianwei Zhang; Bo Yang; Yi Lin', display:{Lore:['[{"text": "arXiv:2111.02041", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparative Study of Speaker Role Identification in Air Traffic Communication Using Deep Learning Approaches\\u00a7r\\n\\n\\u00a78\\u00a7oDongyue Guo\\nJianwei Zhang\\nBo Yang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.02041\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 22 Aug 2022 08:20:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to the ACM TALLIP for possible publication\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Yu-Wen Chen; Yu Tsao', display:{Lore:['[{"text": "arXiv:2111.02585", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInQSS: a speech intelligibility and quality assessment model using a multi-task learning network\\u00a7r\\n\\n\\u00a78\\u00a7oYu-Wen Chen\\nYu Tsao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.02585\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 1 Jul 2022 01:54:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by Insterspeech 2022\\u00a7r"}']}
{title:'Gardner et al. (§72022§r)', author: 'Josh Gardner; Ian Simon; Ethan Manilow; Curtis Hawthorne; Jesse Engel', display:{Lore:['[{"text": "arXiv:2111.03017", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMT3: Multi-Task Multitrack Music Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oJosh Gardner\\nIan Simon\\nEthan Manilow\\nCurtis Hawthorne\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.03017\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 15 Mar 2022 17:13:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICLR 2022 camera-ready version\\u00a7r"}']}
{title:'Kawahara et al. (§72022§r)', author: 'Hideki Kawahara; Kohei Yatabe; Ken-Ichi Sakakibara; Tatsuya Kitamura; Hideki Banno; Masanori Morise', display:{Lore:['[{"text": "arXiv:2111.03629", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lObjective measurement of pitch extractors\' responses to frequency modulated sounds and two reference pitch extraction methods for analyzing voice pitch responses to auditory stimulation\\u00a7r\\n\\n\\u00a78\\u00a7oHideki Kawahara\\nKohei Yatabe\\nKen-Ichi Sakakibara\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.03629\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 28 Jun 2022 01:20:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP2022 rejected this. The substantially revised version was submitted to Interspeech2022 and accepted. It is arXiv:2204.00911\\u00a7r"}']}
{title:'Balasubramanian et al. (§72022§r)', author: 'Sivakumar Balasubramanian; Aditya Jajodia; Gowtham Srinivasan', display:{Lore:['[{"text": "arXiv:2111.03971", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards noise robust trigger-word detection with contrastive learning pre-task for fast on-boarding of new trigger-words\\u00a7r\\n\\n\\u00a78\\u00a7oSivakumar Balasubramanian\\nAditya Jajodia\\nGowtham Srinivasan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.03971\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 27 Jul 2022 14:29:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICMLA\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'Sung-Feng Huang; Chyi-Jiunn Lin; Da-Rong Liu; Yi-Chen Chen; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2111.04040", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMeta-TTS: Meta-Learning for Few-Shot Speaker Adaptive Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oSung-Feng Huang\\nChyi-Jiunn Lin\\nDa-Rong Liu\\nYi-Chen Chen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.04040\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2022.3167258\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol. 30, pp. 1558-1571, 2022\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 29 Jul 2022 13:53:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE/ACM Transactions on Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Shih et al. (§72022§r)', author: 'Yi-Jen Shih; Shih-Lun Wu; Frank Zalkow; Meinard Müller; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2111.04093", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTheme Transformer: Symbolic Music Generation with Theme-Conditioned Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oYi-Jen Shih\\nShih-Lun Wu\\nFrank Zalkow\\nMeinard M\\u00fcller\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.04093\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 21 Mar 2022 07:01:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oto be published at IEEE Transactions on Multimedia\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Haibin Wu; Bo Zheng; Xu Li; Xixin Wu; Hung-yi Lee; Helen Meng', display:{Lore:['[{"text": "arXiv:2111.04330", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCharacterizing the adversarial vulnerability of speech self-supervised learning\\u00a7r\\n\\n\\u00a78\\u00a7oHaibin Wu\\nBo Zheng\\nXu Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.04330\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 29 Mar 2022 16:32:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Lazzarini et al. (§72022§r)', author: 'Victor Lazzarini; Joseph Timoney', display:{Lore:['[{"text": "arXiv:2111.05592", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving the Chamberlin Digital State Variable Filter\\u00a7r\\n\\n\\u00a78\\u00a7oVictor Lazzarini\\nJoseph Timoney\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.05592\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 18 Feb 2022 17:12:40 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Qiquan Zhang; Qi Song; Zhaoheng Ni; Aaron Nicolson; Haizhou Li', display:{Lore:['[{"text": "arXiv:2111.07518", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTime-Frequency Attention for Monaural Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oQiquan Zhang\\nQi Song\\nZhaoheng Ni\\nAaron Nicolson\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.07518\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 9 Mar 2022 09:02:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, Accepted and presented at ICASSP 2022\\u00a7r"}']}
{title:'Shan et al. (§72022§r)', author: 'Siyuan Shan; Lamtharn Hantrakul; Jitong Chen; Matt Avent; David Trevelyan', display:{Lore:['[{"text": "arXiv:2111.10003", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDifferentiable Wavetable Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oSiyuan Shan\\nLamtharn Hantrakul\\nJitong Chen\\nMatt Avent\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.10003\\u00a7r\\n\\nVersion:\\u00a77v4 (Sun, 13 Feb 2022 07:33:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022, Demo: https://lamtharnhantrakul.github.io/diffwts.github.io/\\u00a7r"}']}
{title:'Cornell et al. (§72022§r)', author: 'Samuele Cornell; Thomas Balestri; Thibaud Sénéchal', display:{Lore:['[{"text": "arXiv:2111.10639", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImplicit Acoustic Echo Cancellation for Keyword Spotting and Device-Directed Speech Detection\\u00a7r\\n\\n\\u00a78\\u00a7oSamuele Cornell\\nThomas Balestri\\nThibaud S\\u00e9n\\u00e9chal\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.10639\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 4 Oct 2022 15:34:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be presented at SLT 2022\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Heeseung Kim; Sungwon Kim; Sungroh Yoon', display:{Lore:['[{"text": "arXiv:2111.11755", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGuided-TTS: A Diffusion Model for Text-to-Speech via Classifier Guidance\\u00a7r\\n\\n\\u00a78\\u00a7oHeeseung Kim\\nSungwon Kim\\nSungroh Yoon\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.11755\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 10 Jun 2022 15:01:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 5 figures, ICML\'2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Luyu Wang; Pauline Luc; Yan Wu; Adria Recasens; Lucas Smaira; Andrew Brock; Andrew Jaegle; Jean-Baptiste Alayrac; Sander Dieleman; Joao Carreira; Aaron van den Oord', display:{Lore:['[{"text": "arXiv:2111.12124", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Learning Universal Audio Representations\\u00a7r\\n\\n\\u00a78\\u00a7oLuyu Wang\\nPauline Luc\\nYan Wu\\n+ 7 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.12124\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 23 Jun 2022 04:23:47 GMT)\\u00a7r"}']}
{title:'McKinney et al. (§72022§r)', author: 'Alex F. McKinney; Benjamin Cauchi', display:{Lore:['[{"text": "arXiv:2111.12531", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-Intrusive Binaural Speech Intelligibility Prediction from Discrete Latent Representations\\u00a7r\\n\\n\\u00a78\\u00a7oAlex F. McKinney\\nBenjamin Cauchi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.12531\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2022.3161115\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 21 Mar 2022 22:31:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages + 1 refs; 1 figure; accepted at IEEE SPL (to appear)\\u00a7r"}']}
{title:'Ye et al. (§72022§r)', author: 'Zhirong Ye; Xiangdong Wang; Hong Liu; Yueliang Qian; Rui Tao; Long Yan; Kazushige Ouchi', display:{Lore:['[{"text": "arXiv:2111.15222", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSP-SEDT: Self-supervised Pre-training for Sound Event Detection Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oZhirong Ye\\nXiangdong Wang\\nHong Liu\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.15222\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Apr 2022 10:27:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to interspeech 2022; added experiments for section 4\\u00a7r"}']}
{title:'Okamoto et al. (§72022§r)', author: 'Yuki Okamoto; Shota Horiguchi; Masaaki Yamamoto; Keisuke Imoto; Yohei Kawaguchi', display:{Lore:['[{"text": "arXiv:2112.00209", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnvironmental Sound Extraction Using Onomatopoeic Words\\u00a7r\\n\\n\\u00a78\\u00a7oYuki Okamoto\\nShota Horiguchi\\nMasaaki Yamamoto\\nKeisuke Imoto\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.00209\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 17 Feb 2022 04:41:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP2022\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Shuang Wu; Shijian Lu; Li Cheng', display:{Lore:['[{"text": "arXiv:2112.01806", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic-to-Dance Generation with Optimal Transport\\u00a7r\\n\\n\\u00a78\\u00a7oShuang Wu\\nShijian Lu\\nLi Cheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.01806\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 4 May 2022 05:37:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIJCAI 2022\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Xiaoliang Wu; Ajitha Rajan', display:{Lore:['[{"text": "arXiv:2112.01821", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCatch Me If You Can: Blackbox Adversarial Attacks on Automatic Speech Recognition using Frequency Masking\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoliang Wu\\nAjitha Rajan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.01821\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 12 Apr 2022 12:27:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 7 figures and 3 tables\\u00a7r"}']}
{title:'Manco et al. (§72022§r)', author: 'Ilaria Manco; Emmanouil Benetos; Elio Quinton; Gyorgy Fazekas', display:{Lore:['[{"text": "arXiv:2112.04214", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning music audio representations via weak language supervision\\u00a7r\\n\\n\\u00a78\\u00a7oIlaria Manco\\nEmmanouil Benetos\\nElio Quinton\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.04214\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 17 Feb 2022 10:30:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022\\u00a7r"}']}
{title:'Dang et al. (§72022§r)', author: 'Trung Dang; Dung Tran; Peter Chin; Kazuhito Koishida', display:{Lore:['[{"text": "arXiv:2112.04424", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTraining Robust Zero-Shot Voice Conversion Models with Self-supervised Features\\u00a7r\\n\\n\\u00a78\\u00a7oTrung Dang\\nDung Tran\\nPeter Chin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.04424\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 10 Feb 2022 20:06:03 GMT)\\u00a7r"}']}
{title:'Qu et al. (§72022§r)', author: 'Leyuan Qu; Cornelius Weber; Stefan Wermter', display:{Lore:['[{"text": "arXiv:2112.04748", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLipSound2: Self-Supervised Pre-Training for Lip-to-Speech Reconstruction and Lip Reading\\u00a7r\\n\\n\\u00a78\\u00a7oLeyuan Qu\\nCornelius Weber\\nStefan Wermter\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.04748\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TNNLS.2022.3191677\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 12 Sep 2022 11:34:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oACCEPTED IN IEEE Transactions on Neural Networks and Learning Systems\\u00a7r"}']}
{title:'Ahmed et al. (§72022§r)', author: 'Md. Rayhan Ahmed; Salekul Islam; Ph. D; A. K. M. Muzahidul Islam; Ph. D; Swakkhar Shatabda; Ph. D', display:{Lore:['[{"text": "arXiv:2112.05666", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Ensemble 1D-CNN-LSTM-GRU Model with Data Augmentation for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMd. Rayhan Ahmed\\nSalekul Islam\\nPh. D\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.05666\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 22 Nov 2022 16:57:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper is currently under revision process at expert systems with applications journal\\u00a7r"}']}
{title:'Ma et al. (§72022§r)', author: 'Guodong Ma; Pengfei Hu; Nurmemet Yolwas; Shen Huang; Hao Huang', display:{Lore:['[{"text": "arXiv:2112.06721", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPM-MMUT: Boosted Phone-Mask Data Augmentation using Multi-Modeling Unit Training for Phonetic-Reduction-Robust E2E Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGuodong Ma\\nPengfei Hu\\nNurmemet Yolwas\\nShen Huang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.06721\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 3 Jul 2022 01:14:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Chiquier et al. (§72022§r)', author: 'Mia Chiquier; Chengzhi Mao; Carl Vondrick', display:{Lore:['[{"text": "arXiv:2112.07076", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-Time Neural Voice Camouflage\\u00a7r\\n\\n\\u00a78\\u00a7oMia Chiquier\\nChengzhi Mao\\nCarl Vondrick\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.07076\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 16 Feb 2022 20:42:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Ke Chen; Xingjian Du; Bilei Zhu; Zejun Ma; Taylor Berg-Kirkpatrick; Shlomo Dubnov', display:{Lore:['[{"text": "arXiv:2112.07891", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero-shot Audio Source Separation through Query-based Learning from Weakly-labeled Data\\u00a7r\\n\\n\\u00a78\\u00a7oKe Chen\\nXingjian Du\\nBilei Zhu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.07891\\u00a7r\\n\\nVersion:\\u00a77v4 (Sat, 12 Feb 2022 06:42:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint versionfor Association for the Advancement of Artificial Intelligence Conference, AAAI 2022\\u00a7r"}']}
{title:'Zhao et al. (§72022§r)', author: 'Yanpeng Zhao; Jack Hessel; Youngjae Yu; Ximing Lu; Rowan Zellers; Yejin Choi', display:{Lore:['[{"text": "arXiv:2112.08995", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConnecting the Dots between Audio and Text without Parallel Data through Visual Knowledge Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oYanpeng Zhao\\nJack Hessel\\nYoungjae Yu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.08995\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 2 May 2022 18:22:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to NAACL 2022. Our code isavailable at https://github.com/zhaoyanpeng/vipant\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Yusong Wu; Ethan Manilow; Yi Deng; Rigel Swavely; Kyle Kastner; Tim Cooijmans; Aaron Courville; Cheng-Zhi Anna Huang; Jesse Engel', display:{Lore:['[{"text": "arXiv:2112.09312", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMIDI-DDSP: Detailed Control of Musical Performance via Hierarchical Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oYusong Wu\\nEthan Manilow\\nYi Deng\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.09312\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 17 Mar 2022 22:33:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by International Conference on Learning Representations (ICLR)2022\\u00a7r"}']}
{title:'Shi et al. (§72022§r)', author: 'Jing Shi; Xuankai Chang; Tomoki Hayashi; Yen-Ju Lu; Shinji Watanabe; Bo Xu', display:{Lore:['[{"text": "arXiv:2112.09382", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiscretization and Re-synthesis: an alternative method to solve the Cocktail Party Problem\\u00a7r\\n\\n\\u00a78\\u00a7oJing Shi\\nXuankai Chang\\nTomoki Hayashi\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.09382\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 9 Jan 2022 14:28:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, https://shincling.github.io/discreteSeparation/\\u00a7r"}']}
{title:'Dair et al. (§72022§r)', author: "Zachary Dair; Ryan Donovan; Ruairi O'Reilly", display:{Lore:['[{"text": "arXiv:2112.09596", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLinguistic and Gender Variation in Speech Emotion Recognition using Spectral Features\\u00a7r\\n\\n\\u00a78\\u00a7oZachary Dair\\nRyan Donovan\\nRuairi O\'Reilly\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.09596\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n29th AICS Vol-3105 (2021) 141-152\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 26 Oct 2022 18:22:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at AICS 2021 Conference - Machine Learning for Time Series Section Published in CEUR Vol-3105 http://ceur-ws.org/Vol-3105/paper34.pdf This publicationhas emanated from research supported in part by a Grant "}','{"text": "from Science Foundation Ireland under Grant number 18/CRT/6222 Associated source code https://github.com/ZacDair/SER_Platform_AICS 12 Pages, 5 Figures\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Dongchao Yang; Helin Wang; Yuexian Zou; Fan Cui; Yujun Wang', display:{Lore:['[{"text": "arXiv:2112.10153", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetect what you want: Target Sound Detection\\u00a7r\\n\\n\\u00a78\\u00a7oDongchao Yang\\nHelin Wang\\nYuexian Zou\\nFan Cui\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.10153\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 7 Jul 2022 07:20:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to DCASE workshop2022\\u00a7r"}']}
{title:'Gao et al. (§72022§r)', author: 'Changfeng Gao; Gaofeng Cheng; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2112.12522", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Variant Consistency based Self-supervised Learning for Robust Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oChangfeng Gao\\nGaofeng Cheng\\nPengyuan Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.12522\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 4 May 2022 08:20:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 3 figures\\u00a7r"}']}
{title:'Kumpawat et al. (§72022§r)', author: 'Jayesh Kumpawat; Shubhajit Dey', display:{Lore:['[{"text": "arXiv:2112.13450", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic scene classification using auditory datasets\\u00a7r\\n\\n\\u00a78\\u00a7oJayesh Kumpawat\\nShubhajit Dey\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.13450\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 15 Jul 2022 17:38:45 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Ziyu Wang; Dejing Xu; Gus Xia; Ying Shan', display:{Lore:['[{"text": "arXiv:2112.15110", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-to-symbolic Arrangement via Cross-modal Music Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oZiyu Wang\\nDejing Xu\\nGus Xia\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.15110\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 22 Feb 2022 13:13:40 GMT)\\u00a7r"}']}
{title:'Hasan (§72022§r)', author: 'Nahian Ibn Hasan', display:{Lore:['[{"text": "arXiv:2201.00124", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBird Species Classification And Acoustic Features Selection Based on Distributed Neural Network with Two Stage Windowing of Short-Term Features\\u00a7r\\n\\n\\u00a78\\u00a7oNahian Ibn Hasan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.00124\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 1 Jan 2022 05:42:20 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Haoxu Wang; Yan Jia; Zeqing Zhao; Xuyang Wang; Junjie Wang; Ming Li', display:{Lore:['[{"text": "arXiv:2201.00167", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerating Adversarial Samples For Training Wake-up Word Detection Systems Against Confusing Words\\u00a7r\\n\\n\\u00a78\\u00a7oHaoxu Wang\\nYan Jia\\nZeqing Zhao\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.00167\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 1 Jan 2022 11:01:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2011.01460\\u00a7r"}']}
{title:'Chi et al. (§72022§r)', author: 'Nathan A. Chi; Peter Washington; Aaron Kline; Arman Husic; Cathy Hou; Chloe He; Kaitlyn Dunlap; Dennis Wall', display:{Lore:['[{"text": "arXiv:2201.00927", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lClassifying Autism from Crowdsourced Semi-Structured Speech Recordings: A Machine Learning Approach\\u00a7r\\n\\n\\u00a78\\u00a7oNathan A. Chi\\nPeter Washington\\nAaron Kline\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.00927\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 4 Jan 2022 01:31:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o17 pages, 4 figures, submitted to JMIR Pediatrics and Parenting\\u00a7r"}']}
{title:'Dang et al. (§72022§r)', author: 'Ting Dang; Jing Han; Tong Xia; Dimitris Spathis; Erika Bondareva; Chloë Siegele-Brown; Jagmohan Chauhan; Andreas Grammenos; Apinan Hasthanasombat; Andres Floto; Pietro Cicuta; Cecilia Mascolo', display:{Lore:['[{"text": "arXiv:2201.01232", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Longitudinal Cough, Breath, and Voice Data for COVID-19 Progression Prediction via Sequential Deep Learning: Model Development and Validation\\u00a7r\\n\\n\\u00a78\\u00a7oTing Dang\\nJing Han\\nTong Xia\\n+ 8 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.01232\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.2196/37004\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 22 Jun 2022 12:53:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUpdated title. Revised format according to journal requirements\\u00a7r"}']}
{title:'Shi et al. (§72022§r)', author: 'Bowen Shi; Wei-Ning Hsu; Abdelrahman Mohamed', display:{Lore:['[{"text": "arXiv:2201.01763", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Self-Supervised Audio-Visual Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oBowen Shi\\nWei-Ning Hsu\\nAbdelrahman Mohamed\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.01763\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 14 Jul 2022 23:05:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2022\\u00a7r"}']}
{title:'Dias et al. (§72022§r)', author: 'Fábio Felix Dias; Moacir Antonelli Ponti; Rosane Minghim', display:{Lore:['[{"text": "arXiv:2201.02099", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImplementing simple spectral denoising for environmental audio recordings\\u00a7r\\n\\n\\u00a78\\u00a7oF\\u00e1bio Felix Dias\\nMoacir Antonelli Ponti\\nRosane Minghim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.02099\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 6 Jan 2022 15:30:33 GMT)\\u00a7r"}']}
{title:'Natsiou et al. (§72022§r)', author: "Anastasia Natsiou; Sean O'Leary", display:{Lore:['[{"text": "arXiv:2201.02483", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA sinusoidal signal reconstruction method for the inversion of the mel-spectrogram\\u00a7r\\n\\n\\u00a78\\u00a7oAnastasia Natsiou\\nSean O\'Leary\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.02483\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 7 Jan 2022 15:03:06 GMT)\\u00a7r"}']}
{title:'Natsiou et al. (§72022§r)', author: "Anastasia Natsiou; Sean O'Leary", display:{Lore:['[{"text": "arXiv:2201.02490", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio representations for deep learning in sound synthesis: A review\\u00a7r\\n\\n\\u00a78\\u00a7oAnastasia Natsiou\\nSean O\'Leary\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.02490\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 7 Jan 2022 15:08:47 GMT)\\u00a7r"}']}
{title:'Mari et al. (§72022§r)', author: 'Alessandro Mari; Arash Salarian', display:{Lore:['[{"text": "arXiv:2201.02805", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA novel audio representation using space filling curves\\u00a7r\\n\\n\\u00a78\\u00a7oAlessandro Mari\\nArash Salarian\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.02805\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 8 Jan 2022 11:01:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or "}','{"text": "promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Nassif et al. (§72022§r)', author: 'Ali Bou Nassif; Ismail Shahin; Ashraf Elnagar; Divya Velayudhan; Adi Alhudhaif; Kemal Polat', display:{Lore:['[{"text": "arXiv:2201.02994", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotional Speaker Identification using a Novel Capsule Nets Model\\u00a7r\\n\\n\\u00a78\\u00a7oAli Bou Nassif\\nIsmail Shahin\\nAshraf Elnagar\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.02994\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.eswa.2021.116469\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 9 Jan 2022 12:37:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 8 figures\\u00a7r"}']}
{title:'Pham et al. (§72022§r)', author: 'Lam Pham; Dat Ngo; Truong Hoang; Alexander Schindler; Ian McLoughlin', display:{Lore:['[{"text": "arXiv:2201.03054", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Ensemble of Deep Learning Frameworks Applied For Predicting Respiratory Anomalies\\u00a7r\\n\\n\\u00a78\\u00a7oLam Pham\\nDat Ngo\\nTruong Hoang\\nAlexander Schindler\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.03054\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 9 Jan 2022 16:59:48 GMT)\\u00a7r"}']}
{title:'Xiao et al. (§72022§r)', author: 'Feiyang Xiao; Jian Guan; Haiyan Lan; Qiaoxi Zhu; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2201.03217", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLocal Information Assisted Attention-free Decoder for Audio Captioning\\u00a7r\\n\\n\\u00a78\\u00a7oFeiyang Xiao\\nJian Guan\\nHaiyan Lan\\nQiaoxi Zhu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.03217\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2022.3189536\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 3 Jul 2022 04:26:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE Signal Processing Letters\\u00a7r"}']}
{title:'Cerutti et al. (§72022§r)', author: 'Gianmarco Cerutti; Lukas Cavigelli; Renzo Andri; Michele Magno; Elisabetta Farella; Luca Benini', display:{Lore:['[{"text": "arXiv:2201.03386", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.AR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSub-mW Keyword Spotting on an MCU: Analog Binary Feature Extraction and Binary Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oGianmarco Cerutti\\nLukas Cavigelli\\nRenzo Andri\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.03386\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 Jan 2022 15:10:58 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Yoonjeon Kim; Joel Jang; Sumin Shin', display:{Lore:['[{"text": "arXiv:2201.03809", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic2Video: Automatic Generation of Music Video with fusion of audio and text\\u00a7r\\n\\n\\u00a78\\u00a7oYoonjeon Kim\\nJoel Jang\\nSumin Shin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.03809\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 9 Jun 2022 06:43:39 GMT)\\u00a7r"}']}
{title:'Zhou et al. (§72022§r)', author: 'Kun Zhou; Berrak Sisman; Rajib Rana; Björn W. Schuller; Haizhou Li', display:{Lore:['[{"text": "arXiv:2201.03967", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotion Intensity and its Control for Emotional Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oKun Zhou\\nBerrak Sisman\\nRajib Rana\\nBj\\u00f6rn W. Schuller\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.03967\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TAFFC.2022.3175578\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 18 Jul 2022 07:50:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE Transactions on Affective Computing\\u00a7r"}']}
{title:'Brown et al. (§72022§r)', author: 'Andrew Brown; Jaesung Huh; Joon Son Chung; Arsha Nagrani; Daniel Garcia-Romero; Andrew Zisserman', display:{Lore:['[{"text": "arXiv:2201.04583", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoxSRC 2021: The Third VoxCeleb Speaker Recognition Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oAndrew Brown\\nJaesung Huh\\nJoon Son Chung\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.04583\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 16 Nov 2022 10:38:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2012.06867\\u00a7r"}']}
{title:'Prananta et al. (§72022§r)', author: 'Luke Prananta; Bence Mark Halpern; Siyuan Feng; Odette Scharenborg', display:{Lore:['[{"text": "arXiv:2201.04908", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Effectiveness of Time Stretching for Enhancing Dysarthric Speech for Improved Dysarthric Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oLuke Prananta\\nBence Mark Halpern\\nSiyuan Feng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.04908\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 13 Jan 2022 11:56:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oExtended version of paper to be submitted to Interspeech 2022. 6 pages, 2 tables\\u00a7r"}']}
{title:'Mancusi et al. (§72022§r)', author: 'Michele Mancusi; Nicola Zonca; Emanuele Rodolà; Silvia Zuffi', display:{Lore:['[{"text": "arXiv:2201.05013", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFish sounds: towards the evaluation of marine acoustic biodiversity through data-driven audio source separation\\u00a7r\\n\\n\\u00a78\\u00a7oMichele Mancusi\\nNicola Zonca\\nEmanuele Rodol\\u00e0\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.05013\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 14 Jan 2022 10:51:08 GMT)\\u00a7r"}']}
{title:'Devaney (§72022§r)', author: 'Johanna Devaney', display:{Lore:['[{"text": "arXiv:2201.05244", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBeyond chord vocabularies: Exploiting pitch-relationships in a chord estimation metric\\u00a7r\\n\\n\\u00a78\\u00a7oJohanna Devaney\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.05244\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nLate-Breaking Demo Session of the 22nd International Society for\\n  Music Information Retrieval Conference (2021)\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 13 Jan 2022 23:34:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oExtended abstract, 3 pages, 2 tables\\u00a7r"}']}
{title:'Linke et al. (§72022§r)', author: 'Simon Linke; Rolf Bader; Robert Mores', display:{Lore:['[{"text": "arXiv:2201.05452", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75nlin.AO\\u00a7r, \\u00a75physics.app-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultiphonic modeling using Impulse Pattern Formulation (IPF)\\u00a7r\\n\\n\\u00a78\\u00a7oSimon Linke\\nRolf Bader\\nRobert Mores\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.05452\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 Jan 2022 13:55:29 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Youde Liu; Jian Guan; Qiaoxi Zhu; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2201.05510", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnomalous Sound Detection using Spectral-Temporal Information Fusion\\u00a7r\\n\\n\\u00a78\\u00a7oYoude Liu\\nJian Guan\\nQiaoxi Zhu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.05510\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 29 Apr 2022 02:30:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at ICASSP 2022\\u00a7r"}']}
{title:'Geng et al. (§72022§r)', author: 'Mengzhe Geng; Shansong Liu; Jianwei Yu; Xurong Xie; Shoukang Hu; Zi Ye; Zengrui Jin; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2201.05554", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpectro-Temporal Deep Features for Disordered Speech Assessment and Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMengzhe Geng\\nShansong Liu\\nJianwei Yu\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.05554\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-60\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 Jan 2022 16:56:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH 2021\\u00a7r"}']}
{title:'Geng et al. (§72022§r)', author: 'Mengzhe Geng; Xurong Xie; Shansong Liu; Jianwei Yu; Shoukang Hu; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2201.05562", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigation of Data Augmentation Techniques for Disordered Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMengzhe Geng\\nXurong Xie\\nShansong Liu\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.05562\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2020-1161\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 Jan 2022 17:09:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH 2020\\u00a7r"}']}
{title:'Qiu et al. (§72022§r)', author: 'Jibao Qiu; C. L. Philip Chen; Tong Zhang', display:{Lore:['[{"text": "arXiv:2201.05782", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Novel Multi-Task Learning Method for Symbolic Music Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJibao Qiu\\nC. L. Philip Chen\\nTong Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.05782\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 15 Jan 2022 07:45:10 GMT)\\u00a7r"}']}
{title:'Ng et al. (§72022§r)', author: 'Dianwen Ng; Yunqi Chen; Biao Tian; Qiang Fu; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2201.05863", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConvMixer: Feature Interactive Convolution with Curriculum Learning for Small Footprint and Noisy Far-field Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oDianwen Ng\\nYunqi Chen\\nBiao Tian\\nQiang Fu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.05863\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747025\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 15 Jan 2022 14:38:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Dellaferrera et al. (§72022§r)', author: 'Giorgia Dellaferrera; Toshitake Asabuki; Tomoki Fukai', display:{Lore:['[{"text": "arXiv:2201.06123", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModeling the Repetition-based Recovering of Acoustic and Visual Sources with Dendritic Neurons\\u00a7r\\n\\n\\u00a78\\u00a7oGiorgia Dellaferrera\\nToshitake Asabuki\\nTomoki Fukai\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.06123\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3389/fnins.2022.855753\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nFrontiers in Neuroscience 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 16 Jan 2022 19:35:59 GMT)\\u00a7r"}']}
{title:'Shrawankar (§72022§r)', author: 'Urmila Shrawankar', display:{Lore:['[{"text": "arXiv:2201.06209", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparative Study of Acoustic Echo Cancellation Algorithms for Speech Recognition System in Noisy Environment\\u00a7r\\n\\n\\u00a78\\u00a7oUrmila Shrawankar\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.06209\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Jan 2022 04:28:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 Pages\\u00a7r"}']}
{title:'Sarkar et al. (§72022§r)', author: 'Achintya kr. Sarkar; Zheng-Hua Tan', display:{Lore:['[{"text": "arXiv:2201.06426", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Training Targets and Activation Functions for Deep Representation Learning in Text-Dependent Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oAchintya kr. Sarkar\\nZheng-Hua Tan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.06426\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Jan 2022 14:32:51 GMT)\\u00a7r"}']}
{title:'Lei et al. (§72022§r)', author: 'Yi Lei; Shan Yang; Xinsheng Wang; Lei Xie', display:{Lore:['[{"text": "arXiv:2201.06460", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMsEmoTTS: Multi-scale emotion transfer, prediction, and control for emotional speech synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYi Lei\\nShan Yang\\nXinsheng Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.06460\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Jan 2022 15:13:18 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Yu Wang; Xinsheng Wang; Pengcheng Zhu; Jie Wu; Hanzhao Li; Heyang Xue; Yongmao Zhang; Lei Xie; Mengxiao Bi', display:{Lore:['[{"text": "arXiv:2201.07429", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.DB\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOpencpop: A High-Quality Open Source Chinese Popular Song Corpus for Singing Voice Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYu Wang\\nXinsheng Wang\\nPengcheng Zhu\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.07429\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 20 Jan 2022 02:08:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7owill be submitted to Interspeech 2022\\u00a7r"}']}
{title:'Ma et al. (§72022§r)', author: 'Dabiao Ma; Yitong Zhang; Meng Li; Feng Ye', display:{Lore:['[{"text": "arXiv:2201.07438", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMHTTS: Fast multi-head text-to-speech for spontaneous speech with imperfect transcription\\u00a7r\\n\\n\\u00a78\\u00a7oDabiao Ma\\nYitong Zhang\\nMeng Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.07438\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 4 Feb 2022 08:30:54 GMT)\\u00a7r"}']}
{title:'Sridhar et al. (§72022§r)', author: 'Kusha Sridhar; Carlos Busso', display:{Lore:['[{"text": "arXiv:2201.07876", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Personalization of an Emotion Recognition System: The Unique Properties of the Externalization of Valence in Speech\\u00a7r\\n\\n\\u00a78\\u00a7oKusha Sridhar\\nCarlos Busso\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.07876\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TAFFC.2022.3187336\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Transactions on Affective Computing, vol. 13, no. 4, pp.\\n  1959-1972, October-December 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 19 Jan 2022 22:14:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 Figures and 5 tables\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'J. Yang; Lei He', display:{Lore:['[{"text": "arXiv:2201.08124", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-Lingual Text-to-Speech Using Multi-Task Learning and Speaker Classifier Joint Training\\u00a7r\\n\\n\\u00a78\\u00a7oJ. Yang\\nLei He\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.08124\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 20 Jan 2022 12:02:58 GMT)\\u00a7r"}']}
{title:'Retta et al. (§72022§r)', author: 'Ephrem A. Retta; Richard Sutcliffe; Eiad Almekhlafi; Yosef K. Enku; Eyob Alemu; Tigist D. Gemechu; Michael A. Berwo; Mustafa Mhamed; Jun Feng', display:{Lore:['[{"text": "arXiv:2201.08448", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKinit Classification in Ethiopian Chants, Azmaris and Modern Music: A New Dataset and CNN Benchmark\\u00a7r\\n\\n\\u00a78\\u00a7oEphrem A. Retta\\nRichard Sutcliffe\\nEiad Almekhlafi\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.08448\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1371/journal.pone.0284560\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 20 Jan 2022 20:48:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 4 tables, 3 figures\\u00a7r"}']}
{title:'Hu et al. (§72022§r)', author: 'Zhejing Hu; Yan Liu; Gong Chen; Yongxu Liu', display:{Lore:['[{"text": "arXiv:2201.08526", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCan Machines Generate Personalized Music? A Hybrid Favorite-aware Method for User Preference Music Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oZhejing Hu\\nYan Liu\\nGong Chen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.08526\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 Jan 2022 03:24:27 GMT)\\u00a7r"}']}
{title:'Rho et al. (§72022§r)', author: 'Daniel Rho; Jinhyeok Park; Jong Hwan Ko', display:{Lore:['[{"text": "arXiv:2201.09032", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNAS-VAD: Neural Architecture Search for Voice Activity Detection\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Rho\\nJinhyeok Park\\nJong Hwan Ko\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.09032\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-975\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 29 Mar 2022 08:16:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Kamble et al. (§72022§r)', author: 'Madhu R. Kamble; Jose Patino; Maria A. Zuluaga; Massimiliano Todisco', display:{Lore:['[{"text": "arXiv:2201.09110", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring auditory acoustic features for the diagnosis of the Covid-19\\u00a7r\\n\\n\\u00a78\\u00a7oMadhu R. Kamble\\nJose Patino\\nMaria A. Zuluaga\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.09110\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 22 Jan 2022 18:22:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ICASSP 2022\\u00a7r"}']}
{title:'Jiang et al. (§72022§r)', author: 'Xue Jiang; Xiulian Peng; Chengyu Zheng; Huaying Xue; Yuan Zhang; Yan Lu', display:{Lore:['[{"text": "arXiv:2201.09429", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Neural Speech Coding for Real-Time Communications\\u00a7r\\n\\n\\u00a78\\u00a7oXue Jiang\\nXiulian Peng\\nChengyu Zheng\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.09429\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 15 Feb 2022 05:11:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2022 (Accepted)\\u00a7r"}']}
{title:'An et al. (§72022§r)', author: 'Xiaochun An; Frank K. Soong; Lei Xie', display:{Lore:['[{"text": "arXiv:2201.09472", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDisentangling Style and Speaker Attributes for TTS Style Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oXiaochun An\\nFrank K. Soong\\nLei Xie\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.09472\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jan 2022 06:05:20 GMT)\\u00a7r"}']}
{title:'Hutiri et al. (§72022§r)', author: 'Wiebke Toussaint Hutiri; Aaron Ding', display:{Lore:['[{"text": "arXiv:2201.09486", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CY\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBias in Automated Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oWiebke Toussaint Hutiri\\nAaron Ding\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.09486\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3531146.3533089\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2022 ACM Conference on Fairness, Accountability, and Transparency\\n  (FAccT \'22)\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 20 Jun 2022 00:34:09 GMT)\\u00a7r"}']}
{title:'Raissi et al. (§72022§r)', author: 'Tina Raissi; Eugen Beck; Ralf Schlüter; Hermann Ney', display:{Lore:['[{"text": "arXiv:2201.09692", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Factored Hybrid HMM Acoustic Modeling without State Tying\\u00a7r\\n\\n\\u00a78\\u00a7oTina Raissi\\nEugen Beck\\nRalf Schl\\u00fcter\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.09692\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jan 2022 13:55:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for presentation at IEEE ICASSP 2022\\u00a7r"}']}
{title:'Kanervisto et al. (§72022§r)', author: 'Anssi Kanervisto; Ville Hautamäki; Tomi Kinnunen; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2201.09709", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimizing Tandem Speaker Verification and Anti-Spoofing Systems\\u00a7r\\n\\n\\u00a78\\u00a7oAnssi Kanervisto\\nVille Hautam\\u00e4ki\\nTomi Kinnunen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.09709\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3138681\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nin IEEE/ACM Transactions on Audio, Speech, and Language\\n  Processing, vol. 30, pp. 477-488, 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jan 2022 14:27:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in IEEE/ACM Transactionson Audio, Speech, and Language Processing. Published version available at: https://ieeexplore.ieee.org/document/9664367\\u00a7r"}']}
{title:'Guo et al. (§72022§r)', author: 'Haohan Guo; Zhiping Zhou; Fanbo Meng; Kai Liu', display:{Lore:['[{"text": "arXiv:2201.10130", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Adversarial Waveform Generation based Singing Voice Conversion with Harmonic Signals\\u00a7r\\n\\n\\u00a78\\u00a7oHaohan Guo\\nZhiping Zhou\\nFanbo Meng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.10130\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Jan 2022 07:06:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Baranwal et al. (§72022§r)', author: 'Neha Baranwal; Sharatkumar Chilaka', display:{Lore:['[{"text": "arXiv:2201.10198", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Mispronunciation detection system using a hybrid CTC-ATT based approach for L2 English speakers\\u00a7r\\n\\n\\u00a78\\u00a7oNeha Baranwal\\nSharatkumar Chilaka\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.10198\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Jan 2022 09:28:54 GMT)\\u00a7r"}']}
{title:'Jung et al. (§72022§r)', author: 'Jee-weon Jung; Hemlata Tak; Hye-jin Shim; Hee-Soo Heo; Bong-Jin Lee; Soo-Whan Chung; Hong-Goo Kang; Ha-Jin Yu; Nicholas Evans; Tomi Kinnunen', display:{Lore:['[{"text": "arXiv:2201.10283", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSASV Challenge 2022: A Spoofing Aware Speaker Verification Challenge Evaluation Plan\\u00a7r\\n\\n\\u00a78\\u00a7oJee-weon Jung\\nHemlata Tak\\nHye-jin Shim\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.10283\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 2 Mar 2022 12:46:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oEvaluation plan of the SASV Challenge 2022. See this webpage for more information: https://sasv-challenge.github.io\\u00a7r"}']}
{title:'Qi et al. (§72022§r)', author: 'Jun Qi; Javier Tejedor', display:{Lore:['[{"text": "arXiv:2201.10609", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting Hybrid Models of Tensor-Train Networks for Spoken Command Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJun Qi\\nJavier Tejedor\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.10609\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Jan 2022 05:57:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in Proc. ICASSP 2022\\u00a7r"}']}
{title:'Du et al. (§72022§r)', author: 'Hongqiang Du; Lei Xie; Haizhou Li', display:{Lore:['[{"text": "arXiv:2201.10693", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoise-robust voice conversion with domain adversarial training\\u00a7r\\n\\n\\u00a78\\u00a7oHongqiang Du\\nLei Xie\\nHaizhou Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.10693\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Jan 2022 00:57:47 GMT)\\u00a7r"}']}
{title:'Takamichi et al. (§72022§r)', author: 'Shinnosuke Takamichi; Wataru Nakata; Naoko Tanji; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2201.10896", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJ-MAC: Japanese multi-speaker audiobook corpus for speech synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oShinnosuke Takamichi\\nWataru Nakata\\nNaoko Tanji\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.10896\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Jan 2022 12:22:53 GMT)\\u00a7r"}']}
{title:'Frusque et al. (§72022§r)', author: 'Gaetan Frusque; Olga Fink', display:{Lore:['[{"text": "arXiv:2201.11069", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearnable Wavelet Packet Transform for Data-Adapted Spectrograms\\u00a7r\\n\\n\\u00a78\\u00a7oGaetan Frusque\\nOlga Fink\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.11069\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747491\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2022 - 2022 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Jan 2022 17:28:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 3 figures, accepted to ICASSP 2022 conference\\u00a7r"}']}
{title:'Kadriu (§72022§r)', author: 'Kastriot Kadriu', display:{Lore:['[{"text": "arXiv:2201.11178", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRapid solution for searching similar audio items\\u00a7r\\n\\n\\u00a78\\u00a7oKastriot Kadriu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.11178\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Jan 2022 20:30:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 5 figures, 2 pseudo-code blocks\\u00a7r"}']}
{title:'Żelasko et al. (§72022§r)', author: 'Piotr Żelasko; Siyuan Feng; Laureano Moro Velazquez; Ali Abavisani; Saurabhchand Bhati; Odette Scharenborg; Mark Hasegawa-Johnson; Najim Dehak', display:{Lore:['[{"text": "arXiv:2201.11207", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiscovering Phonetic Inventories with Crosslingual Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oPiotr \\u017belasko\\nSiyuan Feng\\nLaureano Moro Velazquez\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.11207\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 28 Jan 2022 03:11:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in Computer Speech and Language\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Chunyong Yang; Pengfei Liu; Yanli Chen; Hongbin Wang; Min Liu', display:{Lore:['[{"text": "arXiv:2201.11400", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe MSXF TTS System for ICASSP 2022 ADD Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oChunyong Yang\\nPengfei Liu\\nYanli Chen\\nHongbin Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.11400\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Jan 2022 09:30:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDeep Synthesis Detection Challenge 2022\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Shuang Wu; Zhenguang Li; Shijian Lu; Li Cheng', display:{Lore:['[{"text": "arXiv:2201.11999", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDual Learning Music Composition and Dance Choreography\\u00a7r\\n\\n\\u00a78\\u00a7oShuang Wu\\nZhenguang Li\\nShijian Lu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.11999\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3474085.3475180\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Jan 2022 09:20:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oACMMM 2021 (Oral)\\u00a7r"}']}
{title:'Bhosale et al. (§72022§r)', author: 'Swapnil Bhosale; Rupayan Chakraborty; Sunil Kumar Kopparapu', display:{Lore:['[{"text": "arXiv:2201.12352", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Audio Captioning using Attention weighted Event based Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oSwapnil Bhosale\\nRupayan Chakraborty\\nSunil Kumar Kopparapu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.12352\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Jan 2022 05:54:19 GMT)\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Shoule Wu; Ziqiang Shi', display:{Lore:['[{"text": "arXiv:2201.12519", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIt\\u00f4Wave: It\\u00f4 Stochastic Differential Equation Is All You Need For Wave Generation\\u00a7r\\n\\n\\u00a78\\u00a7oShoule Wu\\nZiqiang Shi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.12519\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Apr 2022 01:18:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2022. arXiv admin note: substantial text overlap with arXiv:2105.07583\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Ziyi Chen; Hua Hua; Yuxiang Zhang; Ming Li; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2201.12567", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe HCCL-DKU system for fake audio generation task of the 2022 ICASSP ADD Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oZiyi Chen\\nHua Hua\\nYuxiang Zhang\\nMing Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.12567\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 29 Jan 2022 11:47:01 GMT)\\u00a7r"}']}
{title:'Grachten et al. (§72022§r)', author: 'Maarten Grachten; Carlos Cancino-Chacón; Thassilo Gadermaier', display:{Lore:['[{"text": "arXiv:2201.13144", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lpartitura: A Python Package for Handling Symbolic Musical Data\\u00a7r\\n\\n\\u00a78\\u00a7oMaarten Grachten\\nCarlos Cancino-Chac\\u00f3n\\nThassilo Gadermaier\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.13144\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Jan 2022 11:40:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis preprint is a slightly updated and reformatted version ofthe work presented at the Late Breaking/DemoSession of the 20th International Society for Music Information Retrieval Conference (ISMIR 2019), Delft, The "}','{"text": "Netherlands\\u00a7r"}']}
{title:'Kawamura et al. (§72022§r)', author: 'Masaya Kawamura; Tomohiko Nakamura; Daichi Kitamura; Hiroshi Saruwatari; Yu Takahashi; Kazunobu Kondo', display:{Lore:['[{"text": "arXiv:2202.00200", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDifferentiable Digital Signal Processing Mixture Model for Synthesis Parameter Extraction from Mixture of Harmonic Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oMasaya Kawamura\\nTomohiko Nakamura\\nDaichi Kitamura\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.00200\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Feb 2022 03:38:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, to appear in 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2022)\\u00a7r"}']}
{title:'Kang et al. (§72022§r)', author: 'Zhiqi Kang; Mostafa Sadeghi; Radu Horaud; Xavier Alameda-Pineda; Jacob Donley; Anurag Kumar', display:{Lore:['[{"text": "arXiv:2202.00538", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe impact of removing head movements on audio-visual speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oZhiqi Kang\\nMostafa Sadeghi\\nRadu Horaud\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.00538\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 2 Feb 2022 11:05:39 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Ke Chen; Xingjian Du; Bilei Zhu; Zejun Ma; Taylor Berg-Kirkpatrick; Shlomo Dubnov', display:{Lore:['[{"text": "arXiv:2202.00874", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHTS-AT: A Hierarchical Token-Semantic Audio Transformer for Sound Classification and Detection\\u00a7r\\n\\n\\u00a78\\u00a7oKe Chen\\nXingjian Du\\nBilei Zhu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.00874\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Feb 2022 04:49:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint versionfor ICASSP 2022, Singapore\\u00a7r"}']}
{title:'M et al. (§72022§r)', author: 'Gurunath Reddy M; K. Sreenivasa Rao; Partha Pratim Das', display:{Lore:['[{"text": "arXiv:2202.01078", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMelody Extraction from Polyphonic Music by Deep Learning Approaches: A Review\\u00a7r\\n\\n\\u00a78\\u00a7oGurunath Reddy M\\nK. Sreenivasa Rao\\nPartha Pratim Das\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.01078\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Feb 2022 15:07:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o72 pages\\u00a7r"}']}
{title:'Islam et al. (§72022§r)', author: 'Zubayer Islam; Mohamed Abdel-Aty', display:{Lore:['[{"text": "arXiv:2202.01367", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-time Emergency Vehicle Event Detection Using Audio Data\\u00a7r\\n\\n\\u00a78\\u00a7oZubayer Islam\\nMohamed Abdel-Aty\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.01367\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 3 Feb 2022 01:40:19 GMT)\\u00a7r"}']}
{title:'Cao et al. (§72022§r)', author: 'Chunxiao Cao; Zili An; Zhong Ren; Dinesh Manocha; Kun Zhou', display:{Lore:['[{"text": "arXiv:2202.01582", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Psychoacoustic Quality Criterion for Path-Traced Sound Propagation\\u00a7r\\n\\n\\u00a78\\u00a7oChunxiao Cao\\nZili An\\nZhong Ren\\nDinesh Manocha\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.01582\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 8 Oct 2022 12:51:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 10 figures. To bepublished in IEEE TVCG\\u00a7r"}']}
{title:'Ye et al. (§72022§r)', author: 'Shuaishuai Ye; Peiyao Wang; Shunfei Chen; Xinhui Hu; Xinkang Xu', display:{Lore:['[{"text": "arXiv:2202.01614", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe RoyalFlush System of Speech Recognition for M2MeT Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oShuaishuai Ye\\nPeiyao Wang\\nShunfei Chen\\nXinhui Hu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.01614\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 24 Feb 2022 10:37:41 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Tianchi Liu; Rohan Kumar Das; Kong Aik Lee; Haizhou Li', display:{Lore:['[{"text": "arXiv:2202.01624", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMFA: TDNN with Multi-scale Frequency-channel Attention for Text-independent Speaker Verification with Short Utterances\\u00a7r\\n\\n\\u00a78\\u00a7oTianchi Liu\\nRohan Kumar Das\\nKong Aik Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.01624\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 15 Feb 2022 17:09:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'Jiawen Huang; Emmanouil Benetos; Sebastian Ewert', display:{Lore:['[{"text": "arXiv:2202.01646", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Lyrics Alignment through Joint Pitch Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJiawen Huang\\nEmmanouil Benetos\\nSebastian Ewert\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.01646\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 3 Feb 2022 15:43:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proc. ICASSP 2022\\u00a7r"}']}
{title:'Lee et al. (§72022§r)', author: 'Wo Jae Lee; Karim Helwani; Arvindh Krishnaswamy; Srikanth Tenneti', display:{Lore:['[{"text": "arXiv:2202.01784", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Audio Anomaly Detection\\u00a7r\\n\\n\\u00a78\\u00a7oWo Jae Lee\\nKarim Helwani\\nArvindh Krishnaswamy\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.01784\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nRobustML Workshop - ICLR 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 3 Feb 2022 17:19:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted paper at RobustML Workshop@ICLR 2021\\u00a7r"}']}
{title:'Thomé et al. (§72022§r)', author: 'Carl Thomé; Sebastian Piwell; Oscar Utterbäck', display:{Lore:['[{"text": "arXiv:2202.02112", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusical Audio Similarity with Self-supervised Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oCarl Thom\\u00e9\\nSebastian Piwell\\nOscar Utterb\\u00e4ck\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.02112\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Feb 2022 12:51:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oISMIR LBD 2021\\u00a7r"}']}
{title:'Thomé et al. (§72022§r)', author: 'Carl Thomé; Sven Ahlbäck', display:{Lore:['[{"text": "arXiv:2202.02115", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPolyphonic pitch detection with convolutional recurrent neural networks\\u00a7r\\n\\n\\u00a78\\u00a7oCarl Thom\\u00e9\\nSven Ahlb\\u00e4ck\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.02115\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Feb 2022 12:58:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMIREX 2017\\u00a7r"}']}
{title:'Zhao et al. (§72022§r)', author: 'Xujiang Zhao; Xuchao Zhang; Wei Cheng; Wenchao Yu; Yuncong Chen; Haifeng Chen; Feng Chen', display:{Lore:['[{"text": "arXiv:2202.02441", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSEED: Sound Event Early Detection via Evidential Uncertainty\\u00a7r\\n\\n\\u00a78\\u00a7oXujiang Zhao\\nXuchao Zhang\\nWei Cheng\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.02441\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 13 Feb 2022 02:46:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2022\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Wenzhe Liu; Andong Li; Chengshi Zheng; Xiaodong Li', display:{Lore:['[{"text": "arXiv:2202.02500", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Neural Beam Filter for Real-time Multi-channel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oWenzhe Liu\\nAndong Li\\nChengshi Zheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.02500\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 5 Feb 2022 07:00:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures\\u00a7r"}']}
{title:'Kang et al. (§72022§r)', author: 'Tianqu Kang; Anh-Dung Dinh; Binghong Wang; Tianyuan Du; Yijia Chen; Kevin Chau', display:{Lore:['[{"text": "arXiv:2202.02545", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimization of a Real-Time Wavelet-Based Algorithm for Improving Speech Intelligibility\\u00a7r\\n\\n\\u00a78\\u00a7oTianqu Kang\\nAnh-Dung Dinh\\nBinghong Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.02545\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 21 Jul 2022 19:58:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages, 7 figures, 4 tables\\u00a7r"}']}
{title:'Richard et al. (§72022§r)', author: 'Alexander Richard; Peter Dodds; Vamsi Krishna Ithapu', display:{Lore:['[{"text": "arXiv:2202.03416", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Impulse Responses: Estimating and Parameterizing Filters with Deep Networks\\u00a7r\\n\\n\\u00a78\\u00a7oAlexander Richard\\nPeter Dodds\\nVamsi Krishna Ithapu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.03416\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Feb 2022 18:57:23 GMT)\\u00a7r"}']}
{title:'Tompkins et al. (§72022§r)', author: 'Daniel Tompkins; Kshitiz Kumar; Jian Wu', display:{Lore:['[{"text": "arXiv:2202.03514", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMaximizing Audio Event Detection Model Performance on Small Datasets Through Knowledge Transfer, Data Augmentation, And Pretraining: An Ablation Study\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Tompkins\\nKshitiz Kumar\\nJian Wu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.03514\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Feb 2022 20:57:40 GMT)\\u00a7r"}']}
{title:'Yu et al. (§72022§r)', author: 'Fan Yu; Shiliang Zhang; Pengcheng Guo; Yihui Fu; Zhihao Du; Siqi Zheng; Weilong Huang; Lei Xie; Zheng-Hua Tan; DeLiang Wang; Yanmin Qian; Kong Aik Lee; Zhijie Yan; Bin Ma; Xin Xu; Hui Bu', display:{Lore:['[{"text": "arXiv:2202.03647", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSummary On The ICASSP 2022 Multi-Channel Multi-Party Meeting Transcription Grand Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oFan Yu\\nShiliang Zhang\\nPengcheng Guo\\n+ 12 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.03647\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 25 Feb 2022 06:51:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Morais et al. (§72022§r)', author: 'Edmilson Morais; Ron Hoory; Weizhong Zhu; Itai Gat; Matheus Damasceno; Hagai Aronowitz', display:{Lore:['[{"text": "arXiv:2202.03896", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Emotion Recognition using Self-Supervised Features\\u00a7r\\n\\n\\u00a78\\u00a7oEdmilson Morais\\nRon Hoory\\nWeizhong Zhu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.03896\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Feb 2022 00:50:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, 2 tables, ICASSP 2022\\u00a7r"}']}
{title:'Shen et al. (§72022§r)', author: 'Chen Shen; Yi Liu; Wenzhi Fan; Bin Wang; Shixue Wen; Yao Tian; Jun Zhang; Jingsheng Yang; Zejun Ma', display:{Lore:['[{"text": "arXiv:2202.04261", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Volcspeech system for the ICASSP 2022 multi-channel multi-party meeting transcription challenge\\u00a7r\\n\\n\\u00a78\\u00a7oChen Shen\\nYi Liu\\nWenzhi Fan\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.04261\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 10 Feb 2022 02:58:07 GMT)\\u00a7r"}']}
{title:'Kwak et al. (§72022§r)', author: 'Il-Youp Kwak; Sunmook Choi; Jonghoon Yang; Yerin Lee; Seungsang Oh', display:{Lore:['[{"text": "arXiv:2202.04328", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCAU_KU team\'s submission to ADD 2022 Challenge task 1: Low-quality fake audio detection through frequency feature masking\\u00a7r\\n\\n\\u00a78\\u00a7oIl-Youp Kwak\\nSunmook Choi\\nJonghoon Yang\\nYerin Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.04328\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 9 Feb 2022 08:27:41 GMT)\\u00a7r"}']}
{title:'Ahrens (§72022§r)', author: 'Jens Ahrens', display:{Lore:['[{"text": "arXiv:2202.04393", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBinaural Audio Rendering in the Spherical Harmonic Domain: A Summary of the Mathematics and its Pitfalls\\u00a7r\\n\\n\\u00a78\\u00a7oJens Ahrens\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.04393\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 14 Sep 2022 10:56:20 GMT)\\u00a7r"}']}
{title:'Makris et al. (§72022§r)', author: 'Dimos Makris; Guo Zixun; Maximos Kaliakatsos-Papakostas; Dorien Herremans', display:{Lore:['[{"text": "arXiv:2202.04464", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConditional Drums Generation using Compound Word Representations\\u00a7r\\n\\n\\u00a78\\u00a7oDimos Makris\\nGuo Zixun\\nMaximos Kaliakatsos-Papakostas\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.04464\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 21 Feb 2022 11:32:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for the11th International Conference on Artificial Intelligence in Music, Sound, Art and Design (EvoMUSART), 2022\\u00a7r"}']}
{title:'Passos et al. (§72022§r)', author: 'Leandro Aparecido Passos; João Paulo Papa; Javier Del Ser; Amir Hussain; Ahsan Adeel', display:{Lore:['[{"text": "arXiv:2202.04528", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultimodal Audio-Visual Information Fusion using Canonical-Correlated Graph Neural Network for Energy-Efficient Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oLeandro Aparecido Passos\\nJo\\u00e3o Paulo Papa\\nJavier Del Ser\\nAmir Hussain\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.04528\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.inffus.2022.09.006\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 16 Sep 2022 14:44:22 GMT)\\u00a7r"}']}
{title:'Tsiamas et al. (§72022§r)', author: 'Ioannis Tsiamas; Gerard I. Gállego; José A. R. Fonollosa; Marta R. Costa-jussà', display:{Lore:['[{"text": "arXiv:2202.04774", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSHAS: Approaching optimal Segmentation for End-to-End Speech Translation\\u00a7r\\n\\n\\u00a78\\u00a7oIoannis Tsiamas\\nGerard I. G\\u00e1llego\\nJos\\u00e9 A. R. Fonollosa\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.04774\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 6 Jul 2022 15:43:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022. For an additional 2-page Appendix refer to v1\\u00a7r"}']}
{title:'Tian et al. (§72022§r)', author: 'Jingguang Tian; Xinhui Hu; Xinkang Xu', display:{Lore:['[{"text": "arXiv:2202.04814", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRoyalflush Speaker Diarization System for ICASSP 2022 Multi-channel Multi-party Meeting Transcription Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oJingguang Tian\\nXinhui Hu\\nXinkang Xu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.04814\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 18 Feb 2022 09:07:34 GMT)\\u00a7r"}']}
{title:'Samui et al. (§72022§r)', author: 'Suman Samui; Indrajit Chakrabarti; Soumya K. Ghosh', display:{Lore:['[{"text": "arXiv:2202.04882", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAuditory Model based Phase-Aware Bayesian Spectral Amplitude Estimator for Single-Channel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oSuman Samui\\nIndrajit Chakrabarti\\nSoumya K. Ghosh\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.04882\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Feb 2022 07:44:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEE\\u00a7r"}']}
{title:'Berengueres et al. (§72022§r)', author: 'Jose Berengueres; Maryam Al Kuwaiti; Ahmed Yasir; Kenjiro Tadakuma', display:{Lore:['[{"text": "arXiv:2202.04958", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound masking degrades perception of self-location during stepping: A case for sound-transparent spacesuits for Mars\\u00a7r\\n\\n\\u00a78\\u00a7oJose Berengueres\\nMaryam Al Kuwaiti\\nAhmed Yasir\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.04958\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Feb 2022 11:23:22 GMT)\\u00a7r"}']}
{title:'Marmoret et al. (§72022§r)', author: 'Axel Marmoret; Jérémy E. Cohen; Frédéric Bimbot', display:{Lore:['[{"text": "arXiv:2202.04981", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBarwise Compression Schemes for Audio-Based Music Structure Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oAxel Marmoret\\nJ\\u00e9r\\u00e9my E. Cohen\\nFr\\u00e9d\\u00e9ric Bimbot\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.04981\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 15 Apr 2022 15:52:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at the 2022 Sound and Music Computing (SMC) conference, 8 pages, 6 figures, 1 table, code available at https://gitlab.inria.fr/amarmore/barwisemusiccompression. arXiv admin note: substantial text overlap with "}','{"text": "arXiv:2110.14437\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Haoran Wu; Axel Marmoret; Jérémy E. Cohen', display:{Lore:['[{"text": "arXiv:2202.04989", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-Supervised Convolutive NMF for Automatic Piano Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oHaoran Wu\\nAxel Marmoret\\nJ\\u00e9r\\u00e9my E. Cohen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.04989\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Apr 2022 10:29:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at the 2022 Sound and Music Computing (SMC) conference, 7 pages, 5 figures, 3 tables, code available at https://github.com/cohenjer/TransSSCNMF\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Xuechen Liu; Md Sahidullah; Tomi Kinnunen', display:{Lore:['[{"text": "arXiv:2202.05236", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearnable Nonlinear Compression for Robust Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oXuechen Liu\\nMd Sahidullah\\nTomi Kinnunen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.05236\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Feb 2022 18:44:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2022\\u00a7r"}']}
{title:'Samui (§72022§r)', author: 'Suman Samui', display:{Lore:['[{"text": "arXiv:2202.05272", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSingle-channel speech enhancement by using psychoacoustical model inspired fusion framework\\u00a7r\\n\\n\\u00a78\\u00a7oSuman Samui\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.05272\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Feb 2022 13:13:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2202.04882\\u00a7r"}']}
{title:'Ritter et al. (§72022§r)', author: 'Frank E. Ritter; Mathieu Brener', display:{Lore:['[{"text": "arXiv:2202.05332", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Initial Description of Capabilities and Constraints for a Computational Auditory System (an Artificial Ear) for Cognitive Architectures\\u00a7r\\n\\n\\u00a78\\u00a7oFrank E. Ritter\\nMathieu Brener\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.05332\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Feb 2022 21:23:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 2 figures, 2 tables Keywords: computational auditory system, artificial ear, cognitive architecture\\u00a7r"}']}
{title:'Miao et al. (§72022§r)', author: 'Yuantian Miao; Chao Chen; Lei Pan; Jun Zhang; Yang Xiang', display:{Lore:['[{"text": "arXiv:2202.05416", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFAAG: Fast Adversarial Audio Generation through Interactive Attack Optimisation\\u00a7r\\n\\n\\u00a78\\u00a7oYuantian Miao\\nChao Chen\\nLei Pan\\nJun Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.05416\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Feb 2022 02:46:42 GMT)\\u00a7r"}']}
{title:'Bardelli et al. (§72022§r)', author: 'S. Bardelli; Claudia Ferretti; Luca Andrea Ludovico; Giorgio Presti; Maurizio Rinaldi', display:{Lore:['[{"text": "arXiv:2202.05539", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a75astro-ph.IM\\u00a7r, \\u00a7acs.CY\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.ed-ph\\u00a7r, \\u00a75physics.soc-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Sonification of the zCOSMOS Galaxy Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oS. Bardelli\\nClaudia Ferretti\\nLuca Andrea Ludovico\\nGiorgio Presti\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.05539\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.22201/ia.14052059p.2022.54.10\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nproceedings of \\"Interactive Cultural Heritage and Arts\\", Held as\\n  Part of the 23rd HCI International Conference, in Lecture Notes in Computer\\n  Science book series (LNCS, volume 12794), 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Feb 2022 10:36:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages, 6 figures\\u00a7r"}']}
{title:'Ngo et al. (§72022§r)', author: 'Dat Ngo; Lam Pham; Truong Hoang; Sefki Kolozali; Delaram Jarchi', display:{Lore:['[{"text": "arXiv:2202.05626", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-Based Deep Learning Frameworks for Detecting COVID-19\\u00a7r\\n\\n\\u00a78\\u00a7oDat Ngo\\nLam Pham\\nTruong Hoang\\nSefki Kolozali\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.05626\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 2 Mar 2022 10:12:44 GMT)\\u00a7r"}']}
{title:'Wolff et al. (§72022§r)', author: 'Daniel Wolff; Rémi Mignot; Axel Roebel', display:{Lore:['[{"text": "arXiv:2202.05718", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Defect Detection in Music with Deep Networks\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Wolff\\nR\\u00e9mi Mignot\\nAxel Roebel\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.05718\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 22nd International Society for Music\\n  Information Retrieval Conference, Online, 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Feb 2022 15:56:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages\\u00a7r"}']}
{title:'Hussain et al. (§72022§r)', author: 'Tassadaq Hussain; Muhammad Diyan; Mandar Gogate; Kia Dashtipour; Ahsan Adeel; Yu Tsao; Amir Hussain', display:{Lore:['[{"text": "arXiv:2202.05756", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Novel Speech Intelligibility Enhancement Model based on CanonicalCorrelation and Deep Learning\\u00a7r\\n\\n\\u00a78\\u00a7oTassadaq Hussain\\nMuhammad Diyan\\nMandar Gogate\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.05756\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Feb 2022 16:48:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2202.04172\\u00a7r"}']}
{title:'Poltronieri et al. (§72022§r)', author: 'Andrea Poltronieri; Aldo Gangemi', display:{Lore:['[{"text": "arXiv:2202.05817", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.DL\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe HaMSE Ontology: Using Semantic Technologies to support Music Representation Interoperability and Musicological Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oAndrea Poltronieri\\nAldo Gangemi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.05817\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5281/zenodo.6047780\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Feb 2022 18:26:24 GMT)\\u00a7r"}']}
{title:'Gondi (§72022§r)', author: 'Santosh Gondi', display:{Lore:['[{"text": "arXiv:2202.05993", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWav2Vec2.0 on the Edge: Performance Evaluation\\u00a7r\\n\\n\\u00a78\\u00a7oSantosh Gondi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.05993\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 12 Feb 2022 05:49:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages\\u00a7r"}']}
{title:'Dong et al. (§72022§r)', author: 'Hao-Wen Dong; Cong Zhou; Taylor Berg-Kirkpatrick; Julian McAuley', display:{Lore:['[{"text": "arXiv:2202.06034", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Performer: Score-to-Audio Music Performance Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oHao-Wen Dong\\nCong Zhou\\nTaylor Berg-Kirkpatrick\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.06034\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 21 Feb 2022 03:29:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2022 final version with appendix\\u00a7r"}']}
{title:'Wei et al. (§72022§r)', author: 'Shiqi Wei; Gus Xia', display:{Lore:['[{"text": "arXiv:2202.06180", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning long-term music representations via hierarchical contextual constraints\\u00a7r\\n\\n\\u00a78\\u00a7oShiqi Wei\\nGus Xia\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.06180\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 13 Feb 2022 01:44:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ISMIR2021\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Shimin Zhang; Ziteng Wang; Jiayao Sun; Yihui Fu; Biao Tian; Qiang Fu; Lei Xie', display:{Lore:['[{"text": "arXiv:2202.06850", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Task Deep Residual Echo Suppression with Echo-aware Loss\\u00a7r\\n\\n\\u00a78\\u00a7oShimin Zhang\\nZiteng Wang\\nJiayao Sun\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.06850\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746733\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 21 Feb 2022 01:56:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2022\\u00a7r"}']}
{title:'Heymans et al. (§72022§r)', author: 'Walter Heymans; Marelie H. Davel; Charl van Heerden', display:{Lore:['[{"text": "arXiv:2202.07219", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-style Training for South African Call Centre Audio\\u00a7r\\n\\n\\u00a78\\u00a7oWalter Heymans\\nMarelie H. Davel\\nCharl van Heerden\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.07219\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-030-95070-5_8\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nArtificial Intelligence Research 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Feb 2022 06:22:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 8 tables, Southern African Conference for Artificial Intelligence Research 2021, Part ofthe Communications in Computer and Information Science book series (CCIS, volume 1551, pp 111-124), Springer\\u00a7r"}']}
{title:'Borsos et al. (§72022§r)', author: 'Zalán Borsos; Matt Sharifi; Marco Tagliasacchi', display:{Lore:['[{"text": "arXiv:2202.07273", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeechPainter: Text-conditioned Speech Inpainting\\u00a7r\\n\\n\\u00a78\\u00a7oZal\\u00e1n Borsos\\nMatt Sharifi\\nMarco Tagliasacchi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.07273\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 30 Mar 2022 17:23:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Prusa et al. (§72022§r)', author: 'Zdenek Prusa; Nicki Holighaus', display:{Lore:['[{"text": "arXiv:2202.07382", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MS\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhase Vocoder Done Right\\u00a7r\\n\\n\\u00a78\\u00a7oZdenek Prusa\\nNicki Holighaus\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.07382\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Feb 2022 13:20:14 GMT)\\u00a7r"}']}
{title:'Rajbamshi et al. (§72022§r)', author: 'Shristi Rajbamshi; Georg Tauböck; Peter Balazs; Nicki Holighaus', display:{Lore:['[{"text": "arXiv:2202.07479", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Inpainting via \\u2113_1-Minimization and Dictionary Learning\\u00a7r\\n\\n\\u00a78\\u00a7oShristi Rajbamshi\\nGeorg Taub\\u00f6ck\\nPeter Balazs\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.07479\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Feb 2022 14:47:07 GMT)\\u00a7r"}']}
{title:'Haider et al. (§72022§r)', author: 'Daniel Haider; Peter Balazs; Nicki Holighaus', display:{Lore:['[{"text": "arXiv:2202.07484", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhase-Based Signal Representations for Scattering\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Haider\\nPeter Balazs\\nNicki Holighaus\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.07484\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Feb 2022 14:51:58 GMT)\\u00a7r"}']}
{title:'Průša et al. (§72022§r)', author: 'Zdeněk Průša; Nicki Holighaus', display:{Lore:['[{"text": "arXiv:2202.07498", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MS\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-iterative Filter Bank Phase (Re)Construction\\u00a7r\\n\\n\\u00a78\\u00a7oZden\\u011bk Pr\\u016f\\u0161a\\nNicki Holighaus\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.07498\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.23919/EUSIPCO.2017.8081342\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Feb 2022 15:11:54 GMT)\\u00a7r"}']}
{title:'Kong et al. (§72022§r)', author: 'Zhifeng Kong; Wei Ping; Ambrish Dantrey; Bryan Catanzaro', display:{Lore:['[{"text": "arXiv:2202.07790", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Denoising in the Waveform Domain with Self-Attention\\u00a7r\\n\\n\\u00a78\\u00a7oZhifeng Kong\\nWei Ping\\nAmbrish Dantrey\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.07790\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 7 Jul 2022 00:18:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in ICASSP 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Listen to audio samples from CleanUNet at: https://cleanunet.github.io/\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Bing Yang; Hong Liu; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2202.07841", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Deep Direct-Path Relative Transfer Function for Binaural Sound Source Localization\\u00a7r\\n\\n\\u00a78\\u00a7oBing Yang\\nHong Liu\\nXiaofei Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.07841\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Feb 2022 03:30:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by TASLP 2021\\u00a7r"}']}
{title:'Wei et al. (§72022§r)', author: 'Kun Wei; Yike Zhang; Sining Sun; Lei Xie; Long Ma', display:{Lore:['[{"text": "arXiv:2202.07855", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConversational Speech Recognition By Learning Conversation-level Characteristics\\u00a7r\\n\\n\\u00a78\\u00a7oKun Wei\\nYike Zhang\\nSining Sun\\nLei Xie\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.07855\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 17 Feb 2022 16:24:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2022\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Bing Yang; Hong Liu; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2202.07859", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSRP-DNN: Learning Direct-Path Phase Difference for Multiple Moving Sound Source Localization\\u00a7r\\n\\n\\u00a78\\u00a7oBing Yang\\nHong Liu\\nXiaofei Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.07859\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Feb 2022 05:00:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Tao Wang; Ruibo Fu; Jiangyan Yi; Jianhua Tao; Zhengqi Wen', display:{Lore:['[{"text": "arXiv:2202.07907", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSinging-Tacotron: Global duration control attention and dynamic filter for End-to-end singing voice synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oTao Wang\\nRuibo Fu\\nJiangyan Yi\\nJianhua Tao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.07907\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Feb 2022 07:35:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 7 figures\\u00a7r"}']}
{title:'Yu et al. (§72022§r)', author: 'Guochen Yu; Andong Li; Hui Wang; Yutian Wang; Yuxuan Ke; Chengshi Zheng', display:{Lore:['[{"text": "arXiv:2202.07931", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDBT-Net: Dual-branch federative magnitude and phase estimation with attention-in-attention transformer for monaural speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oGuochen Yu\\nAndong Li\\nHui Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.07931\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 30 Jul 2022 12:27:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages;Accepted by IEEE/ACM Trans. Audio. Speech, Lang. Process\\u00a7r"}']}
{title:'Gusó et al. (§72022§r)', author: 'Enric Gusó; Jordi Pons; Santiago Pascual; Joan Serrà', display:{Lore:['[{"text": "arXiv:2202.07968", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn loss functions and evaluation metrics for music source separation\\u00a7r\\n\\n\\u00a78\\u00a7oEnric Gus\\u00f3\\nJordi Pons\\nSantiago Pascual\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.07968\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Feb 2022 10:22:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022\\u00a7r"}']}
{title:'Gupta et al. (§72022§r)', author: 'Vikram Gupta; Rini Sharon; Ramit Sawhney; Debdoot Mukherjee', display:{Lore:['[{"text": "arXiv:2202.07991", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lADIMA: Abuse Detection In Multilingual Audio\\u00a7r\\n\\n\\u00a78\\u00a7oVikram Gupta\\nRini Sharon\\nRamit Sawhney\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.07991\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Feb 2022 11:09:50 GMT)\\u00a7r"}']}
{title:'Yi et al. (§72022§r)', author: 'Jiangyan Yi; Ruibo Fu; Jianhua Tao; Shuai Nie; Haoxin Ma; Chenglong Wang; Tao Wang; Zhengkun Tian; Ye Bai; Cunhang Fan; Shan Liang; Shiming Wang; Shuai Zhang; Xinrui Yan; Le Xu; Zhengqi Wen; Haizhou Li; Zheng Lian; Bin Liu', display:{Lore:['[{"text": "arXiv:2202.08433", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lADD 2022: the First Audio Deep Synthesis Detection Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oJiangyan Yi\\nRuibo Fu\\nJianhua Tao\\n+ 15 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.08433\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 26 Feb 2022 07:06:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Zhou et al. (§72022§r)', author: 'Hengshun Zhou; Jun Du; Chao-Han Huck Yang; Shifu Xiong; Chin-Hui Lee', display:{Lore:['[{"text": "arXiv:2202.08509", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Study of Designing Compact Audio-Visual Wake Word Spotting System Based on Iterative Fine-Tuning in Neural Network Pruning\\u00a7r\\n\\n\\u00a78\\u00a7oHengshun Zhou\\nJun Du\\nChao-Han Huck Yang\\nShifu Xiong\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.08509\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Feb 2022 08:26:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022. H. Zhou et al\\u00a7r"}']}
{title:'Tzinis et al. (§72022§r)', author: 'Efthymios Tzinis; Yossi Adi; Vamsi Krishna Ithapu; Buye Xu; Paris Smaragdis; Anurag Kumar', display:{Lore:['[{"text": "arXiv:2202.08862", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRemixIT: Continual self-training of speech enhancement models via bootstrapped remixing\\u00a7r\\n\\n\\u00a78\\u00a7oEfthymios Tzinis\\nYossi Adi\\nVamsi Krishna Ithapu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.08862\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JSTSP.2022.3200911\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJ-STSP-SLSAP-00040-2022\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 3 Aug 2022 07:02:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in IEEE Journal of Selected Topics in Signal Processing\\u00a7r"}']}
{title:'Venkatesh et al. (§72022§r)', author: 'Satvik Venkatesh; David Moffat; Eduardo Reck Miranda', display:{Lore:['[{"text": "arXiv:2202.08898", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWord Embeddings for Automatic Equalization in Audio Mixing\\u00a7r\\n\\n\\u00a78\\u00a7oSatvik Venkatesh\\nDavid Moffat\\nEduardo Reck Miranda\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.08898\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.17743/jaes.2022.0047\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJ.Audio.Eng.Soc. 70 (2022) 753-763\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 19 Sep 2022 21:28:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 5 Figures, 3 tables; More experimental validation;Published in the Journal of the Audio Engineering Society\\u00a7r"}']}
{title:'Cho et al. (§72022§r)', author: 'Yongbaek Cho; Changhoon Kim; Yezhou Yang; Yi Ren', display:{Lore:['[{"text": "arXiv:2202.08900", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttributable-Watermarking of Speech Generative Models\\u00a7r\\n\\n\\u00a78\\u00a7oYongbaek Cho\\nChanghoon Kim\\nYezhou Yang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.08900\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 15 Mar 2022 08:16:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2022\\u00a7r"}']}
{title:'Padi et al. (§72022§r)', author: 'Sarala Padi; Seyed Omid Sadjadi; Dinesh Manocha; Ram D. Sriram', display:{Lore:['[{"text": "arXiv:2202.08974", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultimodal Emotion Recognition using Transfer Learning from Speaker Recognition and BERT-based models\\u00a7r\\n\\n\\u00a78\\u00a7oSarala Padi\\nSeyed Omid Sadjadi\\nDinesh Manocha\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.08974\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Feb 2022 00:23:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2108.02510\\u00a7r"}']}
{title:'Coppock et al. (§72022§r)', author: 'Harry Coppock; Alican Akman; Christian Bergler; Maurice Gerczuk; Chloë Brown; Jagmohan Chauhan; Andreas Grammenos; Apinan Hasthanasombat; Dimitris Spathis; Tong Xia; Pietro Cicuta; Jing Han; Shahin Amiriparian; Alice Baird; Lukas Stappen; Sandra Ottl; Panagiotis Tzirakis; Anton Batliner; Cecilia Mascolo; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2202.08981", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Summary of the ComParE COVID-19 Challenges\\u00a7r\\n\\n\\u00a78\\u00a7oHarry Coppock\\nAlican Akman\\nChristian Bergler\\n+ 16 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.08981\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Feb 2022 18:50:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages, 13 figures\\u00a7r"}']}
{title:'Stappen et al. (§72022§r)', author: 'Lukas Stappen; Manuel Milling; Valentin Munst; Korakot Hoffmann; Bjorn W. Schuller', display:{Lore:['[{"text": "arXiv:2202.09102", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPredicting Sex and Stroke Success \\u2013 Computer-aided Player Grunt Analysis in Tennis Matches\\u00a7r\\n\\n\\u00a78\\u00a7oLukas Stappen\\nManuel Milling\\nValentin Munst\\nKorakot Hoffmann\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.09102\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Feb 2022 10:06:46 GMT)\\u00a7r"}']}
{title:'Weiß et al. (§72022§r)', author: 'Christof Weiß; Geoffroy Peeters', display:{Lore:['[{"text": "arXiv:2202.09198", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep-Learning Architectures for Multi-Pitch Estimation: Towards Reliable Evaluation\\u00a7r\\n\\n\\u00a78\\u00a7oChristof Wei\\u00df\\nGeoffroy Peeters\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.09198\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Feb 2022 13:52:21 GMT)\\u00a7r"}']}
{title:'Goel et al. (§72022§r)', author: 'Karan Goel; Albert Gu; Chris Donahue; Christopher Ré', display:{Lore:['[{"text": "arXiv:2202.09729", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIt\'s Raw! Audio Generation with State-Space Models\\u00a7r\\n\\n\\u00a78\\u00a7oKaran Goel\\nAlbert Gu\\nChris Donahue\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.09729\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 20 Feb 2022 04:45:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o23 pages, 7 figures, 7 tables\\u00a7r"}']}
{title:'Avramidis et al. (§72022§r)', author: 'Kleanthis Avramidis; Christos Garoufis; Athanasia Zlatintsi; Petros Maragos', display:{Lore:['[{"text": "arXiv:2202.09750", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Affective Representations of Music-Induced EEG through Multimodal Supervision and latent Domain Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oKleanthis Avramidis\\nChristos Garoufis\\nAthanasia Zlatintsi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.09750\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 20 Feb 2022 07:32:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, IEEE ICASSP 2022\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Yu-Hua Chen; Wen-Yi Hsiao; Tsu-Kuang Hsieh; Jyh-Shing Roger Jang; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2202.09907", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7ltowards automatic transcription of polyphonic electric guitar music:a new dataset and a multi-loss transformer model\\u00a7r\\n\\n\\u00a78\\u00a7oYu-Hua Chen\\nWen-Yi Hsiao\\nTsu-Kuang Hsieh\\nJyh-Shing Roger Jang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.09907\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 20 Feb 2022 20:47:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oto be published at ICASSP 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Tao Wang; Jiangyan Yi; Ruibo Fu; Jianhua Tao; Zhengqi Wen', display:{Lore:['[{"text": "arXiv:2202.09950", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCampNet: Context-Aware Mask Prediction for End-to-End Text-Based Speech Editing\\u00a7r\\n\\n\\u00a78\\u00a7oTao Wang\\nJiangyan Yi\\nRuibo Fu\\nJianhua Tao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.09950\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 22 Mar 2022 12:45:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ounder review, 14 pages, 14 figures, demo page is available at https://hairuo55.github.io/CampNet\\u00a7r"}']}
{title:'Tang et al. (§72022§r)', author: 'Huaizhen Tang; Xulong Zhang; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2202.10020", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAVQVC: One-shot Voice Conversion by Vector Quantization with applying contrastive learning\\u00a7r\\n\\n\\u00a78\\u00a7oHuaizhen Tang\\nXulong Zhang\\nJianzong Wang\\nNing Cheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.10020\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Feb 2022 07:23:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages paper\\u00a7r"}']}
{title:'Huynh et al. (§72022§r)', author: 'Ngoc Dung Huynh; Mohamed Reda Bouadjenek; Imran Razzak; Kevin Lee; Chetan Arora; Ali Hassani; Arkady Zaslavsky', display:{Lore:['[{"text": "arXiv:2202.10594", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Attacks on Speech Recognition Systems for Mission-Critical Applications: A Survey\\u00a7r\\n\\n\\u00a78\\u00a7oNgoc Dung Huynh\\nMohamed Reda Bouadjenek\\nImran Razzak\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.10594\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Feb 2022 00:29:40 GMT)\\u00a7r"}']}
{title:'Zhao et al. (§72022§r)', author: 'Botao Zhao; Xulong Zhang; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2202.10712", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lnnSpeech: Speaker-Guided Conditional Variational Autoencoder for Zero-shot Multi-speaker Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oBotao Zhao\\nXulong Zhang\\nJianzong Wang\\nNing Cheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.10712\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Feb 2022 07:43:30 GMT)\\u00a7r"}']}
{title:'Ye et al. (§72022§r)', author: 'Jianhao Ye; Hongbin Zhou; Zhiba Su; Wendi He; Kaimeng Ren; Lin Li; Heng Lu', display:{Lore:['[{"text": "arXiv:2202.10729", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Cross-lingual Speech Synthesis with Triplet Training Scheme\\u00a7r\\n\\n\\u00a78\\u00a7oJianhao Ye\\nHongbin Zhou\\nZhiba Su\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.10729\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Feb 2022 08:40:43 GMT)\\u00a7r"}']}
{title:'Yu et al. (§72022§r)', author: 'Yinfeng Yu; Wenbing Huang; Fuchun Sun; Changan Chen; Yikai Wang; Xiaohong Liu', display:{Lore:['[{"text": "arXiv:2202.10910", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Adversarial Audio-Visual Navigation\\u00a7r\\n\\n\\u00a78\\u00a7oYinfeng Yu\\nWenbing Huang\\nFuchun Sun\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.10910\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Feb 2022 14:19:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work aims to do an adversarial sound intervention for robustaudio-visual navigation\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Qiqi Wang; Xulong Zhang; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2202.10976", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDRVC: A Framework of Any-to-Any Voice Conversion with Self-Supervised Learning\\u00a7r\\n\\n\\u00a78\\u00a7oQiqi Wang\\nXulong Zhang\\nJianzong Wang\\nNing Cheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.10976\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Feb 2022 15:30:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at ICASSP 2022\\u00a7r"}']}
{title:'Chhaglani et al. (§72022§r)', author: 'Bhawana Chhaglani; Camellia Zakaria; Adam Lechowicz; Prashant Shenoy; Jeremy Gummeson', display:{Lore:['[{"text": "arXiv:2202.11136", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFlowSense: Monitoring Airflow in Building Ventilation Systems Using Audio Sensing\\u00a7r\\n\\n\\u00a78\\u00a7oBhawana Chhaglani\\nCamellia Zakaria\\nAdam Lechowicz\\nPrashant Shenoy\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.11136\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3517258\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Feb 2022 19:22:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o26 pages, 12 figures, Will appear in March issue of the IMWUT 2022 journal\\u00a7r"}']}
{title:'Si et al. (§72022§r)', author: 'Shijing Si; Jianzong Wang; Junqing Peng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2202.11424", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Speaker Age Estimation with Label Distribution Learning\\u00a7r\\n\\n\\u00a78\\u00a7oShijing Si\\nJianzong Wang\\nJunqing Peng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.11424\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 23 Feb 2022 11:11:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the 47th IEEE InternationalConference on Acoustics, Speech, and Signal Processing (ICASSP 2022)\\u00a7r"}']}
{title:'Parekh et al. (§72022§r)', author: "Jayneel Parekh; Sanjeel Parekh; Pavlo Mozharovskyi; Florence d'Alché-Buc; Gaël Richard", display:{Lore:['[{"text": "arXiv:2202.11479", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lListen to Interpret: Post-hoc Interpretability for Audio Networks with NMF\\u00a7r\\n\\n\\u00a78\\u00a7oJayneel Parekh\\nSanjeel Parekh\\nPavlo Mozharovskyi\\nFlorence d\'Alch\\u00e9-Buc\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.11479\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 24 Oct 2022 15:11:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at NeurIPS 2022\\u00a7r"}']}
{title:'Shamsabadi et al. (§72022§r)', author: 'Ali Shahin Shamsabadi; Brij Mohan Lal Srivastava; Aurélien Bellet; Nathalie Vauquier; Emmanuel Vincent; Mohamed Maouche; Marc Tommasi; Nicolas Papernot', display:{Lore:['[{"text": "arXiv:2202.11823", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDifferentially Private Speaker Anonymization\\u00a7r\\n\\n\\u00a78\\u00a7oAli Shahin Shamsabadi\\nBrij Mohan Lal Srivastava\\nAur\\u00e9lien Bellet\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.11823\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 6 Oct 2022 09:16:42 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Doyeon Kim; Hyewon Han; Hyeon-Kyeong Shin; Soo-Whan Chung; Hong-Goo Kang', display:{Lore:['[{"text": "arXiv:2202.11918", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhase Continuity: Learning Derivatives of Phase Spectrum for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oDoyeon Kim\\nHyewon Han\\nHyeon-Kyeong Shin\\nSoo-Whan Chung\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.11918\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Feb 2022 06:15:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Nutan Chen; Djalel Benbouzid; Francesco Ferroni; Mathis Nitschke; Luciano Pinna; Patrick van der Smagt', display:{Lore:['[{"text": "arXiv:2202.12243", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFlat Latent Manifolds for Human-machine Co-creation of Music\\u00a7r\\n\\n\\u00a78\\u00a7oNutan Chen\\nDjalel Benbouzid\\nFrancesco Ferroni\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.12243\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 10 Aug 2022 06:03:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o3rd Conference on AI MusicCreativity (AIMC 2022)\\u00a7r"}']}
{title:'Simonetta et al. (§72022§r)', author: 'Federico Simonetta; Federico Avanzini; Stavros Ntalampiras', display:{Lore:['[{"text": "arXiv:2202.12257", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Perceptual Measure for Evaluating the Resynthesis of Automatic Music Transcriptions\\u00a7r\\n\\n\\u00a78\\u00a7oFederico Simonetta\\nFederico Avanzini\\nStavros Ntalampiras\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.12257\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s11042-022-12476-0\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 7 Mar 2022 12:18:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Multimedia Tools and Applications (2022); supplementary materials are in the latex sources\\u00a7r"}']}
{title:'Baskar et al. (§72022§r)', author: 'Murali Karthick Baskar; Andrew Rosenberg; Bhuvana Ramabhadran; Yu Zhang; Pedro Moreno', display:{Lore:['[{"text": "arXiv:2202.12719", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAsk2Mask: Guided Data Selection for Masked Speech Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oMurali Karthick Baskar\\nAndrew Rosenberg\\nBhuvana Ramabhadran\\nYu Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.12719\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Feb 2022 17:34:54 GMT)\\u00a7r"}']}
{title:'Miao et al. (§72022§r)', author: 'Xiaoxiao Miao; Xin Wang; Erica Cooper; Junichi Yamagishi; Natalia Tomashenko', display:{Lore:['[{"text": "arXiv:2202.13097", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLanguage-Independent Speaker Anonymization Approach using Self-Supervised Pre-Trained Models\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoxiao Miao\\nXin Wang\\nErica Cooper\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.13097\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 27 Apr 2022 11:30:11 GMT)\\u00a7r"}']}
{title:'Sha et al. (§72022§r)', author: 'Yu Sha; Johannes Faber; Shuiping Gou; Bo Liu; Wei Li; Stefan Schramm; Horst Stoecker; Thomas Steckenreiter; Domagoj Vnucec; Nadine Wetzstein; Andreas Widl; Kai Zhou', display:{Lore:['[{"text": "arXiv:2202.13226", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn acoustic signal cavitation detection framework based on XGBoost with adaptive selection feature engineering\\u00a7r\\n\\n\\u00a78\\u00a7oYu Sha\\nJohannes Faber\\nShuiping Gou\\n+ 8 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.13226\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.measurement.2022.110897\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nMeasurement 192 (2022), 110897\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 1 Mar 2022 13:39:05 GMT)\\u00a7r"}']}
{title:'Sha et al. (§72022§r)', author: 'Yu Sha; Johannes Faber; Shuiping Gou; Bo Liu; Wei Li; Stefan Schramm; Horst Stoecker; Thomas Steckenreiter; Domagoj Vnucec; Nadine Wetzstein; Andreas Widl; Kai Zhou', display:{Lore:['[{"text": "arXiv:2202.13245", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRegional-Local Adversarially Learned One-Class Classifier Anomalous Sound Detection in Global Long-Term Space\\u00a7r\\n\\n\\u00a78\\u00a7oYu Sha\\nJohannes Faber\\nShuiping Gou\\n+ 8 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.13245\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3534678.3539133\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nKDD \'22: Proceedings of the 28th ACM SIGKDD Conference on\\n  Knowledge Discovery and Data Mining, August 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 26 Feb 2022 22:54:17 GMT)\\u00a7r"}']}
{title:'Kalantari et al. (§72022§r)', author: 'Leila Kalantari; Jose Principe; Kathryn E. Sieving', display:{Lore:['[{"text": "arXiv:2202.13255", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierarchical Linear Dynamical System for Representing Notes from Recorded Audio\\u00a7r\\n\\n\\u00a78\\u00a7oLeila Kalantari\\nJose Principe\\nKathryn E. Sieving\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.13255\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 27 Feb 2022 00:27:58 GMT)\\u00a7r"}']}
{title:'Faundez-Zanuy et al. (§72022§r)', author: 'Marcos Faundez-Zanuy; Mattias Nilsson; W. Bastiaan Kleijn', display:{Lore:['[{"text": "arXiv:2202.13865", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the relevance of bandwidth extension for speaker identification\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos Faundez-Zanuy\\nMattias Nilsson\\nW. Bastiaan Kleijn\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.13865\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2002 11th European Signal Processing Conference, 2002, pp. 1-4\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Feb 2022 09:14:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages\\u00a7r"}']}
{title:'Chang et al. (§72022§r)', author: 'Xuankai Chang; Niko Moritz; Takaaki Hori; Shinji Watanabe; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2203.00232", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExtended Graph Temporal Classification for Multi-Speaker End-to-End ASR\\u00a7r\\n\\n\\u00a78\\u00a7oXuankai Chang\\nNiko Moritz\\nTakaaki Hori\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.00232\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Mar 2022 05:02:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP2022\\u00a7r"}']}
{title:'Yu et al. (§72022§r)', author: 'Guochen Yu; Yuansheng Guan; Weixin Meng; Chengshi Zheng; Hui Wang', display:{Lore:['[{"text": "arXiv:2203.00472", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDMF-Net: A decoupling-style multi-band fusion model for full-band speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oGuochen Yu\\nYuansheng Guan\\nWeixin Meng\\nChengshi Zheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.00472\\u00a7r\\n\\nVersion:\\u00a77v4 (Sat, 30 Jul 2022 13:00:48 GMT)\\u00a7r"}']}
{title:'Faundez-Zanuy (§72022§r)', author: 'Marcos Faundez-Zanuy', display:{Lore:['[{"text": "arXiv:2203.00513", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA comparative study of several parameterizations for speaker recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos Faundez-Zanuy\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.00513\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2000 10th European Signal Processing Conference, 2000\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Feb 2022 00:31:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Yufeng Yang; Peidong Wang; DeLiang Wang', display:{Lore:['[{"text": "arXiv:2203.00725", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Conformer Based Acoustic Model for Robust Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYufeng Yang\\nPeidong Wang\\nDeLiang Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.00725\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 20 Oct 2022 02:27:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures\\u00a7r"}']}
{title:'Cheng et al. (§72022§r)', author: 'Pengyu Cheng; Zhenhua Ling', display:{Lore:['[{"text": "arXiv:2203.00951", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Adaption with Intuitive Prosodic Features for Statistical Parametric Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oPengyu Cheng\\nZhenhua Ling\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.00951\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Mar 2022 09:00:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICDSP2022\\u00a7r"}']}
{title:'Guo et al. (§72022§r)', author: 'Haohan Guo; Hui Lu; Xixin Wu; Helen Meng', display:{Lore:['[{"text": "arXiv:2203.01080", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Multi-Scale Time-Frequency Spectrogram Discriminator for GAN-based Non-Autoregressive TTS\\u00a7r\\n\\n\\u00a78\\u00a7oHaohan Guo\\nHui Lu\\nXixin Wu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.01080\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 22 Mar 2022 02:15:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Sha et al. (§72022§r)', author: 'Yu Sha; Johannes Faber; Shuiping Gou; Bo Liu; Wei Li; Stefan Schramm; Horst Stoecker; Thomas Steckenreiter; Domagoj Vnucec; Nadine Wetzstein; Andreas Widl; Kai Zhou', display:{Lore:['[{"text": "arXiv:2203.01118", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA multi-task learning for cavitation detection and cavitation intensity recognition of valve acoustic signals\\u00a7r\\n\\n\\u00a78\\u00a7oYu Sha\\nJohannes Faber\\nShuiping Gou\\n+ 8 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.01118\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.engappai.2022.104904\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nEngineering Applications of Artificial Intelligence, 113 (2022),\\n  104904\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Apr 2022 16:44:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2202.13226\\u00a7r"}']}
{title:'Faundez-Zanuy et al. (§72022§r)', author: 'Marcos Faundez-Zanuy; Jordi Sole-Casals', display:{Lore:['[{"text": "arXiv:2203.01164", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker recognition improvement using blind inversion of distortions\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos Faundez-Zanuy\\nJordi Sole-Casals\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.01164\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nEUSIPCO 2004, Vienna\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 23 Feb 2022 23:49:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Shuo Liu; Adria Mallol-Ragolta; Emilia Parada-Cabeleiro; Kun Qian; Xin Jing; Alexander Kathan; Bin Hu; Bjoern W. Schuller', display:{Lore:['[{"text": "arXiv:2203.01205", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Self-supervised Learning: A Survey\\u00a7r\\n\\n\\u00a78\\u00a7oShuo Liu\\nAdria Mallol-Ragolta\\nEmilia Parada-Cabeleiro\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.01205\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Mar 2022 15:58:29 GMT)\\u00a7r"}']}
{title:'Shih et al. (§72022§r)', author: 'Kevin J. Shih; Rafael Valle; Rohan Badlani; João Felipe Santos; Bryan Catanzaro', display:{Lore:['[{"text": "arXiv:2203.01786", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerative Modeling for Low Dimensional Speech Attributes with Neural Spline Flows\\u00a7r\\n\\n\\u00a78\\u00a7oKevin J. Shih\\nRafael Valle\\nRohan Badlani\\nJo\\u00e3o Felipe Santos\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.01786\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 27 Jun 2022 05:58:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o22 pages, 11 figures, 3 tables\\u00a7r"}']}
{title:'Faundez-Zanuy (§72022§r)', author: 'Marcos Faundez-Zanuy', display:{Lore:['[{"text": "arXiv:2203.02020", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNonlinear predictive models computation in ADPCM schemes\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos Faundez-Zanuy\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.02020\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2000 10th European Signal Processing Conference, 2000, pp. 1-4\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 3 Mar 2022 21:03:38 GMT)\\u00a7r"}']}
{title:'Xiong et al. (§72022§r)', author: 'Junwen Xiong; Yu Zhou; Peng Zhang; Lei Xie; Wei Huang; Yufei Zha', display:{Lore:['[{"text": "arXiv:2203.02216", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLook&Listen: Multi-Modal Correlation Learning for Active Speaker Detection and Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oJunwen Xiong\\nYu Zhou\\nPeng Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.02216\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 7 Jul 2022 13:06:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 8figures\\u00a7r"}']}
{title:'Kaneko et al. (§72022§r)', author: 'Takuhiro Kaneko; Kou Tanaka; Hirokazu Kameoka; Shogo Seki', display:{Lore:['[{"text": "arXiv:2203.02395", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7liSTFTNet: Fast and Lightweight Mel-Spectrogram Vocoder Incorporating Inverse Short-Time Fourier Transform\\u00a7r\\n\\n\\u00a78\\u00a7oTakuhiro Kaneko\\nKou Tanaka\\nHirokazu Kameoka\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.02395\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Mar 2022 16:05:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022. Project page: https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet/\\u00a7r"}']}
{title:'Tang et al. (§72022§r)', author: 'Larry Tang; Po Hao Chou; Yi Yu Zheng; Ziqian Ge; Ankit Shah; Bhiksha Raj', display:{Lore:['[{"text": "arXiv:2203.02483", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOntological Learning from Weak Labels\\u00a7r\\n\\n\\u00a78\\u00a7oLarry Tang\\nPo Hao Chou\\nYi Yu Zheng\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.02483\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Mar 2022 18:29:46 GMT)\\u00a7r"}']}
{title:'Xiong et al. (§72022§r)', author: 'Junwen Xiong; Peng Zhang; Lei Xie; Wei Huang; Yufei Zha; Yanning Zhang', display:{Lore:['[{"text": "arXiv:2203.02655", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-visual speech separation based on joint feature representation with cross-modal attention\\u00a7r\\n\\n\\u00a78\\u00a7oJunwen Xiong\\nPeng Zhang\\nLei Xie\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.02655\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 5 Mar 2022 04:39:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Tao Wang; Ruibo Fu; Jiangyan Yi; Jianhua Tao; Zhengqi Wen', display:{Lore:['[{"text": "arXiv:2203.02678", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeuralDPS: Neural Deterministic Plus Stochastic Model with Multiband Excitation for Noise-Controllable Waveform Generation\\u00a7r\\n\\n\\u00a78\\u00a7oTao Wang\\nRuibo Fu\\nJiangyan Yi\\nJianhua Tao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.02678\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2022.3140480\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 5 Mar 2022 08:15:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 12 figures; Accepted to TASLP. Demo page https://hairuo55.github.io/NeuralDPS. arXiv admin note: text overlap with arXiv:1906.09573 by other authors\\u00a7r"}']}
{title:'Eisenberg et al. (§72022§r)', author: 'Aviad Eisenberg; Sharon Gannot; Shlomo E. Chazan', display:{Lore:['[{"text": "arXiv:2203.02941", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSingle microphone speaker extraction using unified time-frequency Siamese-Unet\\u00a7r\\n\\n\\u00a78\\u00a7oAviad Eisenberg\\nSharon Gannot\\nShlomo E. Chazan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.02941\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 6 Mar 2022 11:45:30 GMT)\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Lantian Li; Di Wang; Wenqiang Du; Dong Wang', display:{Lore:['[{"text": "arXiv:2203.02942", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lC-P Map: A Novel Evaluation Toolkit for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oLantian Li\\nDi Wang\\nWenqiang Du\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.02942\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 6 Mar 2022 11:47:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Odyssey 2022\\u00a7r"}']}
{title:'Sofer et al. (§72022§r)', author: 'Amit Sofer; Shlomo E. Chazan', display:{Lore:['[{"text": "arXiv:2203.02944", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCNN self-attention voice activity detector\\u00a7r\\n\\n\\u00a78\\u00a7oAmit Sofer\\nShlomo E. Chazan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.02944\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 6 Mar 2022 11:52:00 GMT)\\u00a7r"}']}
{title:'Xing et al. (§72022§r)', author: 'Qingyu Xing; Xiaohan Ma', display:{Lore:['[{"text": "arXiv:2203.02967", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVariational Auto-Encoder based Mandarin Speech Cloning\\u00a7r\\n\\n\\u00a78\\u00a7oQingyu Xing\\nXiaohan Ma\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.02967\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 6 Mar 2022 14:01:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Insterspeech 2022\\u00a7r"}']}
{title:'Turian et al. (§72022§r)', author: 'Joseph Turian; Jordie Shier; Humair Raj Khan; Bhiksha Raj; Björn W. Schuller; Christian J. Steinmetz; Colin Malloy; George Tzanetakis; Gissel Velarde; Kirk McNally; Max Henry; Nicolas Pinto; Camille Noufi; Christian Clough; Dorien Herremans; Eduardo Fonseca; Jesse Engel; Justin Salamon; Philippe Esling; Pranay Manocha; Shinji Watanabe; Zeyu Jin; Yonatan Bisk', display:{Lore:['[{"text": "arXiv:2203.03022", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHEAR: Holistic Evaluation of Audio Representations\\u00a7r\\n\\n\\u00a78\\u00a7oJoseph Turian\\nJordie Shier\\nHumair Raj Khan\\n+ 19 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.03022\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 29 May 2022 17:51:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oto appear in Proceedings of Machine Learning Research (PMLR): NeurIPS 2021 Competition Track\\u00a7r"}']}
{title:'Faundez-Zanuy (§72022§r)', author: 'Marcos Faundez-Zanuy', display:{Lore:['[{"text": "arXiv:2203.03190", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker recognition by means of a combination of linear and nonlinear predictive models\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos Faundez-Zanuy\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.03190\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n6th European Conference on EUROSPEEECH 1999 Budapest, Hungary,\\n  September 5-9, 1999\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Mar 2022 07:57:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages\\u00a7r"}']}
{title:'Desai et al. (§72022§r)', author: 'Jay Desai; Houwei Cao; Ravi Shah', display:{Lore:['[{"text": "arXiv:2203.03428", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention-based Region of Interest (ROI) Detection for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJay Desai\\nHouwei Cao\\nRavi Shah\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.03428\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 3 Mar 2022 22:01:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper written in 2019\\u00a7r"}']}
{title:'Bhatia et al. (§72022§r)', author: 'Karan Bhatia; Ansh Agrawal; Priyanka Singh; Arun Kumar Singh', display:{Lore:['[{"text": "arXiv:2203.03706", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetection of AI Synthesized Hindi Speech\\u00a7r\\n\\n\\u00a78\\u00a7oKaran Bhatia\\nAnsh Agrawal\\nPriyanka Singh\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.03706\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Mar 2022 21:13:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 Pages, 6 Figures, 4 Tables\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Weidong Chen; Xiaofen Xing; Xiangmin Xu; Jianxin Pang; Lan Du', display:{Lore:['[{"text": "arXiv:2203.03812", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeechFormer: A Hierarchical Efficient Framework Incorporating the Characteristics of Speech\\u00a7r\\n\\n\\u00a78\\u00a7oWeidong Chen\\nXiaofen Xing\\nXiangmin Xu\\nJianxin Pang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.03812\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 10 Mar 2022 01:43:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4figures. This paper was submitted to Insterspeech 2022\\u00a7r"}']}
{title:'Marinozzi et al. (§72022§r)', author: 'Stefano Marinozzi; Marcos Faundez-Zanuy', display:{Lore:['[{"text": "arXiv:2203.03932", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDigital Speech Algorithms for Speaker De-Identification\\u00a7r\\n\\n\\u00a78\\u00a7oStefano Marinozzi\\nMarcos Faundez-Zanuy\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.03932\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/CogInfoCom.2014.7020470\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2014 5th IEEE Conference on Cognitive Infocommunications\\n  (CogInfoCom), 2014, pp. 317-320\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Mar 2022 08:57:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages\\u00a7r"}']}
{title:'Montesinos et al. (§72022§r)', author: 'Juan F. Montesinos; Venkatesh S. Kadandale; Gloria Haro', display:{Lore:['[{"text": "arXiv:2203.04099", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oJuan F. Montesinos\\nVenkatesh S. Kadandale\\nGloria Haro\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.04099\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 19 Jul 2022 16:54:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ECCV 2022\\u00a7r"}']}
{title:'Faundez-Zanuy et al. (§72022§r)', author: 'Marcos Faundez-Zanuy; Enric Sesa-Nogueras; Stefano Marinozzi', display:{Lore:['[{"text": "arXiv:2203.04638", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Identification Experiments Under Gender De-Identification\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos Faundez-Zanuy\\nEnric Sesa-Nogueras\\nStefano Marinozzi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.04638\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/CCST.2015.7389702\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2015 International Carnahan Conference on Security Technology\\n  (ICCST), 2015, pp. 1-6\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 9 Mar 2022 10:47:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages. arXiv admin note:substantial text overlap with arXiv:2203.03932\\u00a7r"}']}
{title:'Chang et al. (§72022§r)', author: 'Yi Chang; Sofiane Laridi; Zhao Ren; Gregory Palmer; Björn W. Schuller; Marco Fisichella', display:{Lore:['[{"text": "arXiv:2203.04696", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Federated Learning Against Adversarial Attacks for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYi Chang\\nSofiane Laridi\\nZhao Ren\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.04696\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 9 Mar 2022 13:19:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 6 figures, 3 tables\\u00a7r"}']}
{title:'Caulley (§72022§r)', author: 'Desmond Caulley', display:{Lore:['[{"text": "arXiv:2203.04880", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Environmental Feature Representation in I-vector Space for Room Verification and Metadata Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oDesmond Caulley\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.04880\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 9 Mar 2022 16:55:45 GMT)\\u00a7r"}']}
{title:'Caulley et al. (§72022§r)', author: 'Desmond Caulley; Yufeng Yang; David Anderson', display:{Lore:['[{"text": "arXiv:2203.05333", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEACELEB: An East Asian Language Speaking Celebrity Dataset for Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDesmond Caulley\\nYufeng Yang\\nDavid Anderson\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.05333\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Mar 2022 12:29:35 GMT)\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'Kuan-Po Huang; Yuan-Kuei Wu; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2203.05882", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving the transferability of speech separation by meta-learning\\u00a7r\\n\\n\\u00a78\\u00a7oKuan-Po Huang\\nYuan-Kuei Wu\\nHung-yi Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.05882\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Mar 2022 12:38:09 GMT)\\u00a7r"}']}
{title:'Islam et al. (§72022§r)', author: 'Zubayer Islam; Mohamed Abdel-Aty', display:{Lore:['[{"text": "arXiv:2203.06059", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Convolutional Neural Network for Roadway Incident Surveillance Using Audio Data\\u00a7r\\n\\n\\u00a78\\u00a7oZubayer Islam\\nMohamed Abdel-Aty\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.06059\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 9 Mar 2022 13:42:56 GMT)\\u00a7r"}']}
{title:'Schuller et al. (§72022§r)', author: 'Björn W. Schuller; Alican Akman; Yi Chang; Harry Coppock; Alexander Gebhard; Alexander Kathan; Esther Rituerto-González; Andreas Triantafyllopoulos; Florian B. Pokorny', display:{Lore:['[{"text": "arXiv:2203.06064", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lClimate Change     Computer Audition: A Call to Action and Overview on Audio Intelligence to Help Save the Planet\\u00a7r\\n\\n\\u00a78\\u00a7oBj\\u00f6rn W. Schuller\\nAlican Akman\\nYi Chang\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.06064\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Mar 2022 13:32:31 GMT)\\u00a7r"}']}
{title:'Yun et al. (§72022§r)', author: 'Jihoon Yun; Sangeeta Srivastava; Dhrubojyoti Roy; Nathan Stohs; Charlie Mydlarz; Mahin Salman; Bea Steers; Juan Pablo Bello; Anish Arora', display:{Lore:['[{"text": "arXiv:2203.06220", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.NI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInfrastructure-free, Deep Learned Urban Noise Monitoring at \\u223c100mW\\u00a7r\\n\\n\\u00a78\\u00a7oJihoon Yun\\nSangeeta Srivastava\\nDhrubojyoti Roy\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.06220\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Mar 2022 19:44:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ICCPS 2022\\u00a7r"}']}
{title:'Teng et al. (§72022§r)', author: 'Zhongwei Teng; Quchen Fu; Jules White; Maria E. Powell; Douglas C. Schmidt', display:{Lore:['[{"text": "arXiv:2203.06517", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSA-SASV: An End-to-End Spoof-Aggregated Spoofing-Aware Speaker Verification System\\u00a7r\\n\\n\\u00a78\\u00a7oZhongwei Teng\\nQuchen Fu\\nJules White\\nMaria E. Powell\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.06517\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 24 Mar 2022 18:02:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUpdate Experiment Results in ASV2019 protocol\\u00a7r"}']}
{title:'C et al. (§72022§r)', author: 'Mohan Rao B C; Vinayak Arkachaari; Harsha M N; Sushmitha M N; Gayathri Ramesh K K; Ullas M S; Pathi Mohan Rao; Sudha G; Narayana Darapaneni', display:{Lore:['[{"text": "arXiv:2203.06583", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBi-Sampling Approach to Classify Music Mood leveraging Raga-Rasa Association in Indian Classical Music\\u00a7r\\n\\n\\u00a78\\u00a7oMohan Rao B C\\nVinayak Arkachaari\\nHarsha M N\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.06583\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 13 Mar 2022 06:12:27 GMT)\\u00a7r"}']}
{title:'Gong et al. (§72022§r)', author: 'Yuan Gong; Sameer Khurana; Andrew Rouditchenko; James Glass', display:{Lore:['[{"text": "arXiv:2203.06760", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oYuan Gong\\nSameer Khurana\\nAndrew Rouditchenko\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.06760\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 13 Mar 2022 21:14:04 GMT)\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Andong Li; Chengshi Zheng; Ziyang Zhang; Xiaodong Li', display:{Lore:['[{"text": "arXiv:2203.07179", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMDNet: Learning Monaural Speech Enhancement from Deep Prior Gradient\\u00a7r\\n\\n\\u00a78\\u00a7oAndong Li\\nChengshi Zheng\\nZiyang Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.07179\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 16 Mar 2022 07:45:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech2022\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Andong Li; Guochen Yu; Chengshi Zheng; Xiaodong Li', display:{Lore:['[{"text": "arXiv:2203.07195", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTaylorBeamformer: Learning All-Neural Beamformer for Multi-Channel Speech Enhancement from Taylor\'s Approximation Theory\\u00a7r\\n\\n\\u00a78\\u00a7oAndong Li\\nGuochen Yu\\nChengshi Zheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.07195\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 16 Mar 2022 07:51:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech2022\\u00a7r"}']}
{title:'Pan et al. (§72022§r)', author: 'Xichen Pan; Peiyu Chen; Yichen Gong; Helong Zhou; Xinbing Wang; Zhouhan Lin', display:{Lore:['[{"text": "arXiv:2203.07996", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXichen Pan\\nPeiyu Chen\\nYichen Gong\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.07996\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 26 Mar 2022 04:11:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oACL2022 Main Conference\\u00a7r"}']}
{title:'Zhao et al. (§72022§r)', author: 'Yueqi Zhao; Michael M. Fogler', display:{Lore:['[{"text": "arXiv:2203.08073", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a75physics.data-an\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCan A Neural Network Hear the Shape of A Drum?\\u00a7r\\n\\n\\u00a78\\u00a7oYueqi Zhao\\nMichael M. Fogler\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.08073\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 25 Apr 2022 23:16:15 GMT)\\u00a7r"}']}
{title:'Choi et al. (§72022§r)', author: 'Won-Gook Choi; Joon-Hyuk Chang; Jae-Mo Yang; Han-Gil Moon', display:{Lore:['[{"text": "arXiv:2203.08439", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInstance-level loss based multiple-instance learning framework for acoustic scene classification\\u00a7r\\n\\n\\u00a78\\u00a7oWon-Gook Choi\\nJoon-Hyuk Chang\\nJae-Mo Yang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.08439\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 30 Jun 2022 03:10:37 GMT)\\u00a7r"}']}
{title:'Morshed et al. (§72022§r)', author: 'Mashrur M. Morshed; Ahmad Omar Ahsan; Hasan Mahmud; Md. Kamrul Hasan', display:{Lore:['[{"text": "arXiv:2203.08490", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Audio Representations with MLPs\\u00a7r\\n\\n\\u00a78\\u00a7oMashrur M. Morshed\\nAhmad Omar Ahsan\\nHasan Mahmud\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.08490\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Mar 2022 09:33:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn submission to Proceedings of MachineLearning Research (PMLR): NeurIPS 2021 Competition Track\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Ruiteng Zhang; Jianguo Wei; Xugang Lu; Wenhuan Lu; Di Jin; Junhai Xu; Lin Zhang; Yantao Ji; Jianwu Dang', display:{Lore:['[{"text": "arXiv:2203.09098", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTMS: A Temporal Multi-scale Backbone Design for Speaker Embedding\\u00a7r\\n\\n\\u00a78\\u00a7oRuiteng Zhang\\nJianguo Wei\\nXugang Lu\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.09098\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Mar 2022 05:49:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDue to the limitation \\"The abstract field cannot be longer than 1,920 characters\\", the abstract here is shorter than that in thePDF file\\u00a7r"}']}
{title:'Yao et al. (§72022§r)', author: 'Dong Yao; Zhou Zhao; Shengyu Zhang; Jieming Zhu; Yudong Zhu; Rui Zhang; Xiuqiang He', display:{Lore:['[{"text": "arXiv:2203.09129", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContrastive Learning with Positive-Negative Frame Mask for Music Representation\\u00a7r\\n\\n\\u00a78\\u00a7oDong Yao\\nZhou Zhao\\nShengyu Zhang\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.09129\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3485447.3512011\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 3 Apr 2022 04:06:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by WWW2022\\u00a7r"}']}
{title:'Martinez et al. (§72022§r)', author: 'Angel Mario Castro Martinez; Constantin Spille; Jana Roßbach; Birger Kollmeier; Bernd T. Meyer', display:{Lore:['[{"text": "arXiv:2203.09148", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrediction of speech intelligibility with DNN-based performance measures\\u00a7r\\n\\n\\u00a78\\u00a7oAngel Mario Castro Martinez\\nConstantin Spille\\nJana Ro\\u00dfbach\\nBirger Kollmeier\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.09148\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.csl.2021.101329\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nComputer Speech & Language, 74, p.101329 (2022)\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Mar 2022 08:05:38 GMT)\\u00a7r"}']}
{title:'Faundez-Zanuy et al. (§72022§r)', author: 'Marcos Faundez-Zanuy; Daniel Rodríguez-Porcheron', display:{Lore:['[{"text": "arXiv:2203.09231", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker recognition using residual signal of linear and nonlinear prediction models\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos Faundez-Zanuy\\nDaniel Rodr\\u00edguez-Porcheron\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.09231\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n5th International Conference on spoken language processing. Vol.2\\n  pp.121-124. ICSLP 1998. ISBN 1-876346-17-5\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Mar 2022 10:36:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, published in 5th International Conference on spoken language processing. Vol.2 pp.121-124. ICSLP1998. ISBN 1-876346-17-5\\u00a7r"}']}
{title:'Mekyska et al. (§72022§r)', author: 'Jiri Mekyska; Zoltan Galaz; Zdenek Mzourek; Zdenek Smekal; Irena Rektorova; Ilona Eliasova; Milena Kostalova; Martina Mrackova; Dagmar Berankov; Marcos Faundez-Zanuy; Karmele Lopez-de-Ipiña; Jesus B. Alonso-Hernandez', display:{Lore:['[{"text": "arXiv:2203.09295", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAssessing Progress of Parkinson s Disease Using Acoustic Analysis of Phonation\\u00a7r\\n\\n\\u00a78\\u00a7oJiri Mekyska\\nZoltan Galaz\\nZdenek Mzourek\\n+ 8 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.09295\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/IWOBI.2015.7160153\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n4th IEEE IWOBI 2015, pp. 115-122, 10-12 June, 2015 Donostia-San\\n  Sebastian. ISBN: 978-84-606-8733-7\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Mar 2022 12:57:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages published in the 4th IEEE IWOBI 2015, pp. 115-122, 10-12 June, 2015 Donostia-San Sebastian. ISBN: 978-84-606-8733-7\\u00a7r"}']}
{title:'Mekyska et al. (§72022§r)', author: 'Jiri Mekyska; Eva Janousova; Pedro Gomez-Vilda; Zdenek Smekal; Irena Rektorova; Ilona Eliasova; Milena Kostalova; Martina Mrackova; Jesus B. Alonso-Hernandez; Marcos Faundez-Zanuy; Karmele López-de-Ipiña', display:{Lore:['[{"text": "arXiv:2203.09402", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust and Complex Approach of Pathological Speech Signal Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oJiri Mekyska\\nEva Janousova\\nPedro Gomez-Vilda\\n+ 7 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.09402\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.neucom.2015.02.085\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nNeurocomputing, Volume 167, 2015, Pages 94-111, ISSN 0925-2312\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Mar 2022 15:54:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o41 pages, published in Neurocomputing, Volume167, 2015, Pages 94-111, ISSN 0925-2312\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Haitong Zhang; Yue Lin', display:{Lore:['[{"text": "arXiv:2203.09708", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImprove few-shot voice cloning using multi-modal learning\\u00a7r\\n\\n\\u00a78\\u00a7oHaitong Zhang\\nYue Lin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.09708\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Mar 2022 02:57:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2022 IEEE International Conference on Acoustics, Speech and Signal Processing\\u00a7r"}']}
{title:'Xiao et al. (§72022§r)', author: 'Ruitong Xiao; Haitong Zhang; Yue Lin', display:{Lore:['[{"text": "arXiv:2203.09722", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDGC-vector: A new speaker embedding for zero-shot voice conversion\\u00a7r\\n\\n\\u00a78\\u00a7oRuitong Xiao\\nHaitong Zhang\\nYue Lin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.09722\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Mar 2022 03:38:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2022 IEEE International Conference on Acoustics, Speech and Signal Processing\\u00a7r"}']}
{title:'Du et al. (§72022§r)', author: 'Zhihao Du; Shiliang Zhang; Siqi Zheng; Zhijie Yan', display:{Lore:['[{"text": "arXiv:2203.09767", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Embedding-aware Neural Diarization: an Efficient Framework for Overlapping Speech Diarization in Meeting Scenarios\\u00a7r\\n\\n\\u00a78\\u00a7oZhihao Du\\nShiliang Zhang\\nSiqi Zheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.09767\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 31 Mar 2022 03:17:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022, 5 parges, 2 figure\\u00a7r"}']}
{title:'Biolková et al. (§72022§r)', author: 'Marie Biolková; Bac Nguyen', display:{Lore:['[{"text": "arXiv:2203.09849", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Predictor for Black-Box Adversarial Attacks on Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMarie Biolkov\\u00e1\\nBac Nguyen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.09849\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Mar 2022 10:37:20 GMT)\\u00a7r"}']}
{title:'López-De-Ipiña et al. (§72022§r)', author: 'Karmele López-De-Ipiña; Unai Martinez de Lizarduy; Nora Barroso; Miriam Ecay-Torres; Pablo Martinez-Lage; Fernando Torres; Marcos Faundez-Zanuy', display:{Lore:['[{"text": "arXiv:2203.09878", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic analysis of Categorical Verbal Fluency for Mild Cognitive Impartment detection: a non-linear language independent approach\\u00a7r\\n\\n\\u00a78\\u00a7oKarmele L\\u00f3pez-De-Ipi\\u00f1a\\nUnai Martinez de Lizarduy\\nNora Barroso\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.09878\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/IWOBI.2015.7160151\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2015 4th International Work Conference on Bioinspired Intelligence\\n  (IWOBI), pp. 101-104\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Mar 2022 11:40:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, published in 2015 4th International Work Conference on Bioinspired Intelligence(IWOBI), pp. 101-104\\u00a7r"}']}
{title:'Mucha et al. (§72022§r)', author: 'Jan Mucha; Zoltan Galaz; Jiri Mekyska; Tomas Kiska; Vojtech Zvoncak; Zdenek Smekal; Ilona Eliasova; Martina Mrackova; Milena Kostalova; Irena Rektorova; Marcos Faundez-Zanuy; Jesus B. Alonso-Hernandez', display:{Lore:['[{"text": "arXiv:2203.09880", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIdentification of Hypokinetic Dysarthria Using Acoustic Analysis of Poem Recitation\\u00a7r\\n\\n\\u00a78\\u00a7oJan Mucha\\nZoltan Galaz\\nJiri Mekyska\\n+ 8 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.09880\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Mar 2022 11:45:01 GMT)\\u00a7r"}']}
{title:'Bittner et al. (§72022§r)', author: 'Rachel M. Bittner; Juan José Bosch; David Rubinstein; Gabriel Meseguer-Brocal; Sebastian Ewert', display:{Lore:['[{"text": "arXiv:2203.09893", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Lightweight Instrument-Agnostic Model for Polyphonic Note Transcription and Multipitch Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oRachel M. Bittner\\nJuan Jos\\u00e9 Bosch\\nDavid Rubinstein\\nGabriel Meseguer-Brocal\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.09893\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 12 May 2022 16:24:07 GMT)\\u00a7r"}']}
{title:'Matsunaga et al. (§72022§r)', author: 'Yuta Matsunaga; Takaaki Saeki; Shinnosuke Takamichi; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2203.09961", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPersonalized Filled-pause Generation with Group-wise Prediction Models\\u00a7r\\n\\n\\u00a78\\u00a7oYuta Matsunaga\\nTakaaki Saeki\\nShinnosuke Takamichi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.09961\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 22 Apr 2022 10:43:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to LREC 2022\\u00a7r"}']}
{title:'Seo et al. (§72022§r)', author: 'Hyungjoo Seo; Sahil Bhandary Karnoor; Romit Roy Choudhury', display:{Lore:['[{"text": "arXiv:2203.10072", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRoSS: Utilizing Robotic Rotation for Audio Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oHyungjoo Seo\\nSahil Bhandary Karnoor\\nRomit Roy Choudhury\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.10072\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Mar 2022 17:38:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 13 figures\\u00a7r"}']}
{title:'Aldeneh et al. (§72022§r)', author: 'Zakaria Aldeneh; Masha Fedzechkina; Skyler Seto; Katherine Metcalf; Miguel Sarabia; Nicholas Apostoloff; Barry-John Theobald', display:{Lore:['[{"text": "arXiv:2203.10117", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the role of Lip Articulation in Visual Speech Perception\\u00a7r\\n\\n\\u00a78\\u00a7oZakaria Aldeneh\\nMasha Fedzechkina\\nSkyler Seto\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.10117\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 10 Nov 2022 17:38:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Hu et al. (§72022§r)', author: 'Jinbo Hu; Yin Cao; Ming Wu; Qiuqiang Kong; Feiran Yang; Mark D. Plumbley; Jun Yang', display:{Lore:['[{"text": "arXiv:2203.10228", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Track-Wise Ensemble Event Independent Network for Polyphonic Sound Event Localization and Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJinbo Hu\\nYin Cao\\nMing Wu\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.10228\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 19 Mar 2022 03:00:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 2 figures, submitted to IEEE ICASSP 2022\\u00a7r"}']}
{title:'Srivastava et al. (§72022§r)', author: 'Sangeeta Srivastava; Ho-Hsiang Wu; Joao Rulff; Magdalena Fuentes; Mark Cartwright; Claudio Silva; Anish Arora; Juan Pablo Bello', display:{Lore:['[{"text": "arXiv:2203.10425", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Study on Robustness to Perturbations for Representations of Environmental Sound\\u00a7r\\n\\n\\u00a78\\u00a7oSangeeta Srivastava\\nHo-Hsiang Wu\\nJoao Rulff\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.10425\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 6 Jul 2022 20:44:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in EUSIPCO 2022\\u00a7r"}']}
{title:'Xue et al. (§72022§r)', author: 'Jinlong Xue; Yayue Deng; Yichen Han; Ya Li; Jianqing Sun; Jiaen Liang', display:{Lore:['[{"text": "arXiv:2203.10473", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lECAPA-TDNN for Multi-speaker Text-to-speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oJinlong Xue\\nYayue Deng\\nYichen Han\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.10473\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 26 Mar 2022 16:39:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, submitted to interspeech2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Zewang Zhang; Yibin Zheng; Xinhui Li; Li Lu', display:{Lore:['[{"text": "arXiv:2203.10750", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses\\u00a7r\\n\\n\\u00a78\\u00a7oZewang Zhang\\nYibin Zheng\\nXinhui Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.10750\\u00a7r\\n\\nVersion:\\u00a77v5 (Sat, 25 Jun 2022 07:48:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at InterSpeech2022\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Juntae Kim; Sung Min Ban', display:{Lore:['[{"text": "arXiv:2203.10793", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhase-Aware Spoof Speech Detection Based on Res2Net with Phase Network\\u00a7r\\n\\n\\u00a78\\u00a7oJuntae Kim\\nSung Min Ban\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.10793\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Mar 2022 08:15:51 GMT)\\u00a7r"}']}
{title:'Mekyska et al. (§72022§r)', author: 'Jiri Mekyska; Zdenek Smekal; Zoltan Galaz; Zdenek Mzourek; Irena Rektorova; Marcos Faundez-Zanuy; Karmele Lopez-De-Ipina', display:{Lore:['[{"text": "arXiv:2203.10830", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerceptual Features as Markers of Parkinson\'s Disease: The Issue of Clinical Interpretability\\u00a7r\\n\\n\\u00a78\\u00a7oJiri Mekyska\\nZdenek Smekal\\nZoltan Galaz\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.10830\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-319-28109-4_9\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nNOLISP 2015, In Recent Advances in Nonlinear Speech Processing.\\n  Smart Innovation, Systems and Technologies, vol 48. Springer, Cham\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Mar 2022 09:46:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, published in International Conference on NONLINEAR SPEECH PROCESSING, NOLISP 2015 jointly organized with the 25th Italian Workshop on Neural Networks, WIRN 2015, held at May 2015, Vietri sul Mare, Salerno, "}','{"text": "Italy\\u00a7r"}']}
{title:'López-de-Ipiña et al. (§72022§r)', author: 'K. López-de-Ipiña; Marcos Faundez-Zanuy; Jordi Solé-Casals; Fernando Zelarin; Pilar Calvo', display:{Lore:['[{"text": "arXiv:2203.10837", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-class versus One-class classifier in spontaneous speech analysis oriented to Alzheimer Disease diagnosis\\u00a7r\\n\\n\\u00a78\\u00a7oK. L\\u00f3pez-de-Ipi\\u00f1a\\nMarcos Faundez-Zanuy\\nJordi Sol\\u00e9-Casals\\nFernando Zelarin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.10837\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-319-28109-4_7\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nRecent Advances in Nonlinear Speech Processing. Smart Innovation,\\n  Systems and Technologies, vol 48. Springer, Cham 2015\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Mar 2022 09:57:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, published in International Conference on NONLINEAR SPEECH PROCESSING, NOLISP 2015 jointly organized with the 25th Italian Workshop on Neural Networks, WIRN 2015, held at May 2015, Vietri sul Mare, Salerno, "}','{"text": "Italy\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Xuechen Liu; Md Sahidullah; Tomi Kinnunen', display:{Lore:['[{"text": "arXiv:2203.10992", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpoofing-Aware Speaker Verification with Unsupervised Domain Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oXuechen Liu\\nMd Sahidullah\\nTomi Kinnunen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.10992\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 26 Apr 2022 05:59:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Speaker Odyssey 2022\\u00a7r"}']}
{title:'Zandi et al. (§72022§r)', author: 'Navid H. Zandi; Awny M. El-Mohandes; Rong Zheng', display:{Lore:['[{"text": "arXiv:2203.11138", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIndividualizing Head-Related Transfer Functions for Binaural Acoustic Applications\\u00a7r\\n\\n\\u00a78\\u00a7oNavid H. Zandi\\nAwny M. El-Mohandes\\nRong Zheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.11138\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Mar 2022 17:13:55 GMT)\\u00a7r"}']}
{title:'Liang et al. (§72022§r)', author: 'Dawei Liang; Zifan Xu; Yinuo Chen; Rebecca Adaimi; David Harwath; Edison Thomaz', display:{Lore:['[{"text": "arXiv:2203.11294", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomated detection of foreground speech with wearable sensing in everyday home environments: A transfer learning approach\\u00a7r\\n\\n\\u00a78\\u00a7oDawei Liang\\nZifan Xu\\nYinuo Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.11294\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Mar 2022 19:06:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'Wen-Chin Huang; Erica Cooper; Yu Tsao; Hsin-Min Wang; Tomoki Toda; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2203.11389", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe VoiceMOS Challenge 2022\\u00a7r\\n\\n\\u00a78\\u00a7oWen-Chin Huang\\nErica Cooper\\nYu Tsao\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.11389\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 4 Jul 2022 01:13:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Ye et al. (§72022§r)', author: 'Zhe Ye; Jiahao Chen; Diqun Yan', display:{Lore:['[{"text": "arXiv:2203.11499", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lResidual-Guided Non-Intrusive Speech Quality Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oZhe Ye\\nJiahao Chen\\nDiqun Yan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.11499\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Mar 2022 07:19:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Jain et al. (§72022§r)', author: 'Rishabh Jain; Mariam Yiwere; Dan Bigioi; Peter Corcoran; Horia Cucu', display:{Lore:['[{"text": "arXiv:2203.11562", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Text-to-Speech Pipeline, Evaluation Methodology, and Initial Fine-Tuning Results for Child Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oRishabh Jain\\nMariam Yiwere\\nDan Bigioi\\nPeter Corcoran\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.11562\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 4 Apr 2022 12:24:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEEACCESS\\u00a7r"}']}
{title:'Seibold et al. (§72022§r)', author: 'Matthias Seibold; Armando Hoch; Mazda Farshad; Nassir Navab; Philipp Fürnstahl', display:{Lore:['[{"text": "arXiv:2203.11570", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConditional Generative Data Augmentation for Clinical Audio Datasets\\u00a7r\\n\\n\\u00a78\\u00a7oMatthias Seibold\\nArmando Hoch\\nMazda Farshad\\nNassir Navab\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.11570\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 24 Jun 2022 08:21:35 GMT)\\u00a7r"}']}
{title:'Hou et al. (§72022§r)', author: 'Yuanbo Hou; Zhaoyi Liu; Bo Kang; Yun Wang; Dick Botteldooren', display:{Lore:['[{"text": "arXiv:2203.11573", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCT-SAT: Contextual Transformer for Sequential Audio Tagging\\u00a7r\\n\\n\\u00a78\\u00a7oYuanbo Hou\\nZhaoyi Liu\\nBo Kang\\nYun Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.11573\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Mar 2022 09:53:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to interspeech 2022\\u00a7r"}']}
{title:'Lopez-de-Ipiña et al. (§72022§r)', author: 'Karmele Lopez-de-Ipiña; Unai Martinez de Lizarduy; Pilar Calvo; Blanca Beita; Joseba García-Melero; Miriam Ecay-Torres; Ainara Estanga; Marcos Faundez-Zanuy', display:{Lore:['[{"text": "arXiv:2203.11606", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalysis of Disfluencies for automatic detection of Mild Cognitive Impartment: a deep learning approach\\u00a7r\\n\\n\\u00a78\\u00a7oKarmele Lopez-de-Ipi\\u00f1a\\nUnai Martinez de Lizarduy\\nPilar Calvo\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.11606\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/IWOBI.2017.7985526\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2017 International Conference and Workshop on Bioinspired\\n  Intelligence (IWOBI), 2017, pp. 1-4\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Mar 2022 10:42:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, published in 2017 International Conference and Workshop on Bioinspired Intelligence(IWOBI), 2017, pp. 1-4, 10-12 July Funchal (Portugal)\\u00a7r"}']}
{title:'Faundez-Zanuy et al. (§72022§r)', author: 'Marcos Faundez-Zanuy; Francesc Vallverdu; Enric Monte', display:{Lore:['[{"text": "arXiv:2203.11612", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNonlinear prediction with neural nets in ADPCM\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos Faundez-Zanuy\\nFrancesc Vallverdu\\nEnric Monte\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.11612\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.1998.674438\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 1998 IEEE International Conference on\\n  Acoustics, Speech and Signal Processing, ICASSP \'98 (Cat. No.98CH36181),\\n  1998, pp. 345-348 vol.1\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Mar 2022 10:59:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, published in Proceedings of the 1998 IEEE InternationalConference on Acoustics, Speech and Signal Processing, ICASSP \'98 (Cat. No.98CH36181) Seattle, WA, USA. arXiv admin note: text overlap with "}','{"text": "arXiv:2203.01818\\u00a7r"}']}
{title:'Rodriguez-Porcheron et al… (§72022§r)', author: 'Daniel Rodriguez-Porcheron; Marcos Faundez-Zanuy', display:{Lore:['[{"text": "arXiv:2203.11614", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker recognition with a MLP classifier and LPCC codebook\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Rodriguez-Porcheron\\nMarcos Faundez-Zanuy\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.11614\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.1999.759872\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n1999 IEEE International Conference on Acoustics, Speech, and\\n  Signal Processing. Proceedings. ICASSP99 (Cat. No.99CH36258), 1999, pp.\\n  1005-1008 vol.2\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Mar 2022 11:11:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, published in 1999 IEEE InternationalConference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 (Cat. No.99CH36258) Phoenix, AZ, USA\\u00a7r"}']}
{title:'Gupta et al. (§72022§r)', author: 'Tarun Gupta; Duc-Tuan Truong; Tran The Anh; Chng Eng Siong', display:{Lore:['[{"text": "arXiv:2203.11774", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEstimation of speaker age and height from speech signal using bi-encoder transformer mixture model\\u00a7r\\n\\n\\u00a78\\u00a7oTarun Gupta\\nDuc-Tuan Truong\\nTran The Anh\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.11774\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Mar 2022 14:39:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Feng et al. (§72022§r)', author: 'Meng Feng; Chieh-Chi Kao; Qingming Tang; Ming Sun; Viktor Rozgic; Spyros Matsoukas; Chao Wang', display:{Lore:['[{"text": "arXiv:2203.11997", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFederated Self-Supervised Learning for Acoustic Event Classification\\u00a7r\\n\\n\\u00a78\\u00a7oMeng Feng\\nChieh-Chi Kao\\nQingming Tang\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.11997\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Mar 2022 18:49:52 GMT)\\u00a7r"}']}
{title:'Conner et al. (§72022§r)', author: 'Michael Conner; Lucas Gral; Kevin Adams; David Hunger; Reagan Strelow; Alexander Neuwirth', display:{Lore:['[{"text": "arXiv:2203.12105", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Generation Using an LSTM\\u00a7r\\n\\n\\u00a78\\u00a7oMichael Conner\\nLucas Gral\\nKevin Adams\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.12105\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 23 Mar 2022 00:13:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in MICS2022\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Juncheng B Li; Shuhui Qu; Xinjian Li; Po-Yao Huang; Florian Metze', display:{Lore:['[{"text": "arXiv:2203.12122", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Adversarial Robustness of Large-scale Audio Visual Learning\\u00a7r\\n\\n\\u00a78\\u00a7oJuncheng B Li\\nShuhui Qu\\nXinjian Li\\nPo-Yao Huang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.12122\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2022 International Conference on Acoustics, Speech, and Signal\\n  Processing (ICASSP 2022)\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 21 Apr 2022 06:35:14 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Jun Chen; Zilin Wang; Deyi Tuo; Zhiyong Wu; Shiyin Kang; Helen Meng', display:{Lore:['[{"text": "arXiv:2203.12188", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFullSubNet+: Channel Attention FullSubNet with Complex Spectrograms for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oJun Chen\\nZilin Wang\\nDeyi Tuo\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.12188\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 26 Mar 2022 19:20:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Lei et al. (§72022§r)', author: 'Shun Lei; Yixuan Zhou; Liyang Chen; Zhiyong Wu; Shiyin Kang; Helen Meng', display:{Lore:['[{"text": "arXiv:2203.12201", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Expressive Speaking Style Modelling with Hierarchical Context Information for Mandarin Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oShun Lei\\nYixuan Zhou\\nLiyang Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.12201\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Apr 2022 12:07:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Watcharasupat et al. (§72022§r)', author: 'Karn N. Watcharasupat; Sureenate Jaratjarungkiat; Bhan Lam; Sujinat Jitwiriyanont; Kanyanut Akaratham; Kenneth Ooi; Zhen-Ting Ong; Titima Suthiwan; Nitipong Pichetpan; Monthita Rojtinnakorn; Woon-Seng Gan', display:{Lore:['[{"text": "arXiv:2203.12245", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.AP\\u00a7r, \\u00a7cstat.ME\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQuantitative Evaluation Approach for Translation of Perceptual Soundscape Attributes: Initial Application to the Thai Language\\u00a7r\\n\\n\\u00a78\\u00a7oKarn N. Watcharasupat\\nSureenate Jaratjarungkiat\\nBhan Lam\\n+ 7 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.12245\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.apacoust.2022.108962\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nAppl. Acoust., vol. 200, p. 108962, Nov. 2022\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 6 Jun 2022 05:11:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review for Applied Acoustics (Special Issue on Soundscape Attributes Translation: Current Projects and Challenges)\\u00a7r"}']}
{title:'Faundez-Zanuy (§72022§r)', author: 'Marcos Faundez-Zanuy', display:{Lore:['[{"text": "arXiv:2203.12306", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA combination between VQ and covariance matrices for speaker recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos Faundez-Zanuy\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.12306\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2001.940865\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2001 IEEE International Conference on Acoustics, Speech, and\\n  Signal Processing. Proceedings (Cat. No.01CH37221), 2001, pp. 453-456 vol.1\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 23 Mar 2022 10:06:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, published in 2001 IEEE InternationalConference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.01CH37221), Salt Lake City, UT, USA\\u00a7r"}']}
{title:'Pham et al. (§72022§r)', author: 'Lam Pham; Khoa Dinh; Dat Ngo; Hieu Tang; Alexander Schindler', display:{Lore:['[{"text": "arXiv:2203.12314", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWider or Deeper Neural Network Architecture for Acoustic Scene Classification with Mismatched Recording Devices\\u00a7r\\n\\n\\u00a78\\u00a7oLam Pham\\nKhoa Dinh\\nDat Ngo\\nHieu Tang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.12314\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 23 Mar 2022 10:27:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper was submitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Close et al. (§72022§r)', author: 'George Close; Thomas Hain; Stefan Goetze', display:{Lore:['[{"text": "arXiv:2203.12369", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMetricGAN+/-: Increasing Robustness of Noise Reduction on Unseen Data\\u00a7r\\n\\n\\u00a78\\u00a7oGeorge Close\\nThomas Hain\\nStefan Goetze\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.12369\\u00a7r\\n\\nVersion:\\u00a77v5 (Wed, 15 Jun 2022 14:13:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, Accepted to EUSIPCO 2022\\u00a7r"}']}
{title:'Guo (§72022§r)', author: 'Rui Guo', display:{Lore:['[{"text": "arXiv:2203.12736", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn interactive music infilling interface for pop music composition\\u00a7r\\n\\n\\u00a78\\u00a7oRui Guo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.12736\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 23 Mar 2022 21:40:01 GMT)\\u00a7r"}']}
{title:'Zhao et al. (§72022§r)', author: 'Xintao Zhao; Feng Liu; Changhe Song; Zhiyong Wu; Shiyin Kang; Deyi Tuo; Helen Meng', display:{Lore:['[{"text": "arXiv:2203.12813", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDisentangleing Content and Fine-grained Prosody Information via Hybrid ASR Bottleneck Features for Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oXintao Zhao\\nFeng Liu\\nChanghe Song\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.12813\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Mar 2022 02:24:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:"D'Alessandro et al. (§72022§r)", author: "Guido D'Alessandro; Marcos Faundez Zanuy; Francesco Piazza", display:{Lore:['[{"text": "arXiv:2203.12894", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA new subband non linear prediction coding algorithm for narrowband speech signal: The nADPCMB MLT coding scheme\\u00a7r\\n\\n\\u00a78\\u00a7oGuido D\'Alessandro\\nMarcos Faundez Zanuy\\nFrancesco Piazza\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.12894\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2002.5743969\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2002 IEEE International Conference on Acoustics, Speech, and\\n  Signal Processing, 2002, pp. I-1025-I-1028\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Mar 2022 07:26:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, published in 2002 IEEE InternationalConference on Acoustics, Speech, and Signal Processing Orlando, FL, USA\\u00a7r"}']}
{title:'Faundez-Zanuy (§72022§r)', author: 'Marcos Faundez-Zanuy', display:{Lore:['[{"text": "arXiv:2203.12896", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWide band sub-band speech coding using nonlinear prediction\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos Faundez-Zanuy\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.12896\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2003.1202324\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2003 IEEE International Conference on Acoustics, Speech, and\\n  Signal Processing, 2003. Proceedings. (ICASSP \'03)., 2003, pp. II-181\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Mar 2022 07:28:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, published in 2003 IEEE InternationalConference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP \'03) Hong Kong, China\\u00a7r"}']}
{title:'Saeki et al. (§72022§r)', author: 'Takaaki Saeki; Shinnosuke Takamichi; Tomohiko Nakamura; Naoko Tanji; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2203.12937", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelfRemaster: Self-Supervised Speech Restoration with Analysis-by-Synthesis Approach Using Channel Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oTakaaki Saeki\\nShinnosuke Takamichi\\nTomohiko Nakamura\\nNaoko Tanji\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.12937\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Jun 2022 18:43:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Ramoneda et al. (§72022§r)', author: 'Pedro Ramoneda; Nazif Can Tamer; Vsevolod Eremenko; Xavier Serra; Marius Miron', display:{Lore:['[{"text": "arXiv:2203.13010", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScore difficulty analysis for piano performance education based on fingering\\u00a7r\\n\\n\\u00a78\\u00a7oPedro Ramoneda\\nNazif Can Tamer\\nVsevolod Eremenko\\nXavier Serra\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.13010\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Mar 2022 12:00:04 GMT)\\u00a7r"}']}
{title:'Siyao et al. (§72022§r)', author: 'Li Siyao; Weijiang Yu; Tianpei Gu; Chunze Lin; Quan Wang; Chen Qian; Chen Change Loy; Ziwei Liu', display:{Lore:['[{"text": "arXiv:2203.13055", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory\\u00a7r\\n\\n\\u00a78\\u00a7oLi Siyao\\nWeijiang Yu\\nTianpei Gu\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.13055\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 25 Mar 2022 03:07:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by CVPR 2022. Code and video link: https://github.com/lisiyao21/Bailando/\\u00a7r"}']}
{title:'Harasim et al. (§72022§r)', author: 'Daniel Harasim; Giovanni Affatato; Fabian C. Moss', display:{Lore:['[{"text": "arXiv:2203.13158", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lmidiVERTO: A Web Application to Visualize Tonality in Real Time\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Harasim\\nGiovanni Affatato\\nFabian C. Moss\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.13158\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Mar 2022 16:26:48 GMT)\\u00a7r"}']}
{title:'Sadhu et al. (§72022§r)', author: 'Samik Sadhu; Hynek Hermansky', display:{Lore:['[{"text": "arXiv:2203.13216", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComplex Frequency Domain Linear Prediction: A Tool to Compute Modulation Spectrum of Speech\\u00a7r\\n\\n\\u00a78\\u00a7oSamik Sadhu\\nHynek Hermansky\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.13216\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 31 Mar 2022 19:51:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Karas et al. (§72022§r)', author: 'Vincent Karas; Mani Kumar Tellamekala; Adria Mallol-Ragolta; Michel Valstar; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2203.13285", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContinuous-Time Audiovisual Fusion with Recurrence vs. Attention for In-The-Wild Affect Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oVincent Karas\\nMani Kumar Tellamekala\\nAdria Mallol-Ragolta\\nMichel Valstar\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.13285\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.48550/arXiv.2203.13285\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 29 Mar 2022 16:02:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 1 figures, addedreferences and an overview figure\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Juncheng B Li; Shuhui Qu; Po-Yao Huang; Florian Metze', display:{Lore:['[{"text": "arXiv:2203.13448", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudioTagging Done Right: 2nd comparison of deep learning methods for environmental sound classification\\u00a7r\\n\\n\\u00a78\\u00a7oJuncheng B Li\\nShuhui Qu\\nPo-Yao Huang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.13448\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInterSpeech 2022\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 3 Apr 2022 00:48:38 GMT)\\u00a7r"}']}
{title:'Ge et al. (§72022§r)', author: 'Yunjie Ge; Qian Wang; Jingfeng Zhang; Juntao Zhou; Yunzhu Zhang; Chao Shen', display:{Lore:['[{"text": "arXiv:2203.13497", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWaveFuzz: A Clean-Label Poisoning Attack to Protect Your Voice\\u00a7r\\n\\n\\u00a78\\u00a7oYunjie Ge\\nQian Wang\\nJingfeng Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.13497\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Mar 2022 08:14:37 GMT)\\u00a7r"}']}
{title:'Ghosh et al. (§72022§r)', author: 'Sreyan Ghosh; Ashish Seth; and Deepak Mittal; Maneesh Singh; S. Umesh', display:{Lore:['[{"text": "arXiv:2203.13628", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeLoRes: Decorrelating Latent Spaces for Low-Resource Audio Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oSreyan Ghosh\\nAshish Seth\\nand Deepak Mittal\\nManeesh Singh\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.13628\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 26 Jun 2022 05:59:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to AAAI2022 workshop on Self-supervised Learning for Audio and Speech Processing\\u00a7r"}']}
{title:'Lou et al. (§72022§r)', author: 'Siyu Lou; Xuenan Xu; Mengyue Wu; Kai Yu', display:{Lore:['[{"text": "arXiv:2203.13645", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-text Retrieval in Context\\u00a7r\\n\\n\\u00a78\\u00a7oSiyu Lou\\nXuenan Xu\\nMengyue Wu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.13645\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 29 Mar 2022 04:32:47 GMT)\\u00a7r"}']}
{title:'Lee et al. (§72022§r)', author: 'Hung-Shin Lee; Pin-Tuan Huang; Yao-Fei Cheng; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:2203.13687", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChain-based Discriminative Autoencoders for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHung-Shin Lee\\nPin-Tuan Huang\\nYao-Fei Cheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.13687\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 15 Jun 2022 14:20:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in Interspeech 2022\\u00a7r"}']}
{title:'Lee et al. (§72022§r)', author: 'Hung-Shin Lee; Pin-Yuan Chen; Yao-Fei Cheng; Yu Tsao; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:2203.13696", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech-enhanced and Noise-aware Networks for Robust Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHung-Shin Lee\\nPin-Yuan Chen\\nYao-Fei Cheng\\nYu Tsao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.13696\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 23 Nov 2022 02:04:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in ISCSLP 2022\\u00a7r"}']}
{title:'Grondin et al. (§72022§r)', author: 'François Grondin; Marc-Antoine Maheux; Jean-Samuel Lauzon; Jonathan Vincent; François Michaud', display:{Lore:['[{"text": "arXiv:2203.14409", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSMP-PHAT: Lightweight DoA Estimation by Merging Microphone Pairs\\u00a7r\\n\\n\\u00a78\\u00a7oFran\\u00e7ois Grondin\\nMarc-Antoine Maheux\\nJean-Samuel Lauzon\\nJonathan Vincent\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14409\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 27 Mar 2022 22:44:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Hernandez-Olivan et al. (§72022§r)', author: 'Carlos Hernandez-Olivan; Jorge Abadias Puyuelo; Jose R. Beltran', display:{Lore:['[{"text": "arXiv:2203.14641", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSubjective Evaluation of Deep Learning Models for Symbolic Music Composition\\u00a7r\\n\\n\\u00a78\\u00a7oCarlos Hernandez-Olivan\\nJorge Abadias Puyuelo\\nJose R. Beltran\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14641\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 3 Apr 2022 11:27:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWorkshop on Generative AI and HCI, CHI 2022\\u00a7r"}']}
{title:'Nakano et al. (§72022§r)', author: 'Yoshifumi Nakano; Takaaki Saeki; Shinnosuke Takamichi; Katsuhito Sudoh; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2203.14725", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lvTTS: visual-text to speech\\u00a7r\\n\\n\\u00a78\\u00a7oYoshifumi Nakano\\nTakaaki Saeki\\nShinnosuke Takamichi\\nKatsuhito Sudoh\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14725\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Mar 2022 13:10:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to interspech 2022\\u00a7r"}']}
{title:'Saito et al. (§72022§r)', author: 'Yuki Saito; Yuto Nishimura; Shinnosuke Takamichi; Kentaro Tachibana; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2203.14757", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSTUDIES: Corpus of Japanese Empathetic Dialogue Speech Towards Friendly Voice Agent\\u00a7r\\n\\n\\u00a78\\u00a7oYuki Saito\\nYuto Nishimura\\nShinnosuke Takamichi\\nKentaro Tachibana\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14757\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 16 Jun 2022 09:19:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, Accepted for INTERSPEECH2022, project page: http://sython.org/Corpus/STUDIES\\u00a7r"}']}
{title:'Miao et al. (§72022§r)', author: 'Xiaoxiao Miao; Xin Wang; Erica Cooper; Junichi Yamagishi; Natalia Tomashenko', display:{Lore:['[{"text": "arXiv:2203.14834", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalyzing Language-Independent Speaker Anonymization Framework under Unseen Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoxiao Miao\\nXin Wang\\nErica Cooper\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14834\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Mar 2022 15:14:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmit to Interspeech2022\\u00a7r"}']}
{title:'Novoselov et al. (§72022§r)', author: 'Sergey Novoselov; Galina Lavrentyeva; Anastasia Avdeeva; Vladimir Volokhov; Aleksei Gusev', display:{Lore:['[{"text": "arXiv:2203.15095", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Speaker Recognition with Transformers Using wav2vec 2.0\\u00a7r\\n\\n\\u00a78\\u00a7oSergey Novoselov\\nGalina Lavrentyeva\\nAnastasia Avdeeva\\nVladimir Volokhov\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15095\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Mar 2022 20:59:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech2022. arXiv admin note: text overlap with arXiv:2111.02298\\u00a7r"}']}
{title:'Lavrentyeva et al. (§72022§r)', author: 'Galina Lavrentyeva; Sergey Novoselov; Andrey Shulipa; Marina Volkova; Aleksandr Kozlov', display:{Lore:['[{"text": "arXiv:2203.15106", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigation of Different Calibration Methods for Deep Speaker Embedding based Verification Systems\\u00a7r\\n\\n\\u00a78\\u00a7oGalina Lavrentyeva\\nSergey Novoselov\\nAndrey Shulipa\\nMarina Volkova\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15106\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Mar 2022 21:22:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech2022\\u00a7r"}']}
{title:'Manilow et al. (§72022§r)', author: 'Ethan Manilow; Curtis Hawthorne; Cheng-Zhi Anna Huang; Bryan Pardo; Jesse Engel', display:{Lore:['[{"text": "arXiv:2203.15140", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Source Separation by Explicitly Modeling Dependencies Between Sources\\u00a7r\\n\\n\\u00a78\\u00a7oEthan Manilow\\nCurtis Hawthorne\\nCheng-Zhi Anna Huang\\nBryan Pardo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15140\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Mar 2022 23:21:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at ICASSP 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Fangyuan Wang; Bo Xu', display:{Lore:['[{"text": "arXiv:2203.15206", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lShifted Chunk Encoder for Transformer Based Streaming End-to-End ASR\\u00a7r\\n\\n\\u00a78\\u00a7oFangyuan Wang\\nBo Xu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15206\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 26 Sep 2022 12:16:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper has been accepted by iconip 2022, update the latest version\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Yang Zhang; Zhiqiang Lv; Haibin Wu; Shanshan Zhang; Pengfei Hu; Zhiyong Wu; Hung-yi Lee; Helen Meng', display:{Lore:['[{"text": "arXiv:2203.15249", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMFA-Conformer: Multi-scale Feature Aggregation Conformer for Automatic Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oYang Zhang\\nZhiqiang Lv\\nHaibin Wu\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15249\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 11 Nov 2022 02:51:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by INTERSPEECH 2022\\u00a7r"}']}
{title:'Ghosh et al. (§72022§r)', author: 'Shankhanil Ghosh; Chhanda Saha; Naagamani Molakathaala', display:{Lore:['[{"text": "arXiv:2203.15253", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeuraGen-A Low-Resource Neural Network based approach for Gender Classification\\u00a7r\\n\\n\\u00a78\\u00a7oShankhanil Ghosh\\nChhanda Saha\\nNaagamani Molakathaala\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15253\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Mar 2022 05:57:24 GMT)\\u00a7r"}']}
{title:'Furukawa et al. (§72022§r)', author: 'Kei Furukawa; Takeshi Kishiyama; Satoshi Nakamura', display:{Lore:['[{"text": "arXiv:2203.15276", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lApplying Syntaxx2013Prosody Mapping Hypothesis and Prosodic Well-Formedness Constraints to Neural Sequence-to-Sequence Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oKei Furukawa\\nTakeshi Kishiyama\\nSatoshi Nakamura\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15276\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Mar 2022 06:45:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Chen Chen; Nana Hou; Yuchen Hu; Shashank Shirol; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2203.15321", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoise-robust Speech Recognition with 10 Minutes Unparalleled In-domain Data\\u00a7r\\n\\n\\u00a78\\u00a7oChen Chen\\nNana Hou\\nYuchen Hu\\nShashank Shirol\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15321\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Mar 2022 08:06:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2022\\u00a7r"}']}
{title:'Zou et al. (§72022§r)', author: 'Heqing Zou; Yuke Si; Chen Chen; Deepu Rajan; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2203.15326", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Emotion Recognition with Co-Attention based Multi-level Acoustic Information\\u00a7r\\n\\n\\u00a78\\u00a7oHeqing Zou\\nYuke Si\\nChen Chen\\nDeepu Rajan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15326\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Mar 2022 08:17:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Ebrat et al. (§72022§r)', author: 'Danial Ebrat; Farzad Didehvar; Milad Dadgar', display:{Lore:['[{"text": "arXiv:2203.15335", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIranian Modal Music (Dastgah) detection using deep neural networks\\u00a7r\\n\\n\\u00a78\\u00a7oDanial Ebrat\\nFarzad Didehvar\\nMilad Dadgar\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15335\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 19 Oct 2022 00:55:03 GMT)\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Haibin Wu; Lingwei Meng; Jiawen Kang; Jinchao Li; Xu Li; Xixin Wu; Hung-yi Lee; Helen Meng', display:{Lore:['[{"text": "arXiv:2203.15377", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpoofing-Aware Speaker Verification by Multi-Level Fusion\\u00a7r\\n\\n\\u00a78\\u00a7oHaibin Wu\\nLingwei Meng\\nJiawen Kang\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15377\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Mar 2022 09:16:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'van Rijn et al. (§72022§r)', author: 'Pol van Rijn; Silvan Mertes; Dominik Schiller; Piotr Dura; Hubert Siuzdak; Peter M. C. Harrison; Elisabeth André; Nori Jacoby', display:{Lore:['[{"text": "arXiv:2203.15379", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoiceMe: Personalized voice generation in TTS\\u00a7r\\n\\n\\u00a78\\u00a7oPol van Rijn\\nSilvan Mertes\\nDominik Schiller\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15379\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 11 Jul 2022 16:22:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech\'22. Audio and video samples are available at: https://polvanrijn.github.io/VoiceMe/\\u00a7r"}']}
{title:'Violeta et al. (§72022§r)', author: 'Lester Phillip Violeta; Wen-Chin Huang; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2203.15431", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating Self-supervised Pretraining Frameworks for Pathological Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oLester Phillip Violeta\\nWen-Chin Huang\\nTomoki Toda\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15431\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 29 Jun 2022 09:43:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Binbin Zhang; Di Wu; Zhendong Peng; Xingchen Song; Zhuoyuan Yao; Hang Lv; Lei Xie; Chao Yang; Fuping Pan; Jianwei Niu', display:{Lore:['[{"text": "arXiv:2203.15455", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeNet 2.0: More Productive End-to-End Speech Recognition Toolkit\\u00a7r\\n\\n\\u00a78\\u00a7oBinbin Zhang\\nDi Wu\\nZhendong Peng\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15455\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 5 Jul 2022 07:47:22 GMT)\\u00a7r"}']}
{title:'Tran et al. (§72022§r)', author: 'Mai Lan Tran; Dongjin Lee; Jae-Hun Jung', display:{Lore:['[{"text": "arXiv:2203.15468", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMachine Composition of Korean Music via Topological Data Analysis and Artificial Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oMai Lan Tran\\nDongjin Lee\\nJae-Hun Jung\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15468\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Mar 2022 12:11:31 GMT)\\u00a7r"}']}
{title:'Yadav et al. (§72022§r)', author: 'Sarthak Yadav; Neil Zeghidour', display:{Lore:['[{"text": "arXiv:2203.15519", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning neural audio features without supervision\\u00a7r\\n\\n\\u00a78\\u00a7oSarthak Yadav\\nNeil Zeghidour\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15519\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Mar 2022 12:59:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Chen Chen; Nana Hou; Yuchen Hu; Heqing Zou; Xiaofeng Qi; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2203.15526", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInteractive Audio-text Representation for Automated Audio Captioning with Contrastive Learning\\u00a7r\\n\\n\\u00a78\\u00a7oChen Chen\\nNana Hou\\nYuchen Hu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15526\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 12 Apr 2022 06:50:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Moutti et al. (§72022§r)', author: 'Maria Moutti; Sofia Eleftheriou; Panagiotis Koromilas; Theodoros Giannakopoulos', display:{Lore:['[{"text": "arXiv:2203.15568", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Dataset for Speech Emotion Recognition in Greek Theatrical Plays\\u00a7r\\n\\n\\u00a78\\u00a7oMaria Moutti\\nSofia Eleftheriou\\nPanagiotis Koromilas\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15568\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 27 Mar 2022 21:55:59 GMT)\\u00a7r"}']}
{title:'Lee et al. (§72022§r)', author: 'Hung-Shin Lee; Yu Tsao; Shyh-Kang Jeng; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:2203.15576", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSubspace-based Representation and Learning for Phonotactic Spoken Language Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHung-Shin Lee\\nYu Tsao\\nShyh-Kang Jeng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15576\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Mar 2022 07:01:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in IEEE/ACM Trans. Audio, Speech, Lang. Process.,2020, vol. 28, pp. 3065-3079\\u00a7r"}']}
{title:'Sun et al. (§72022§r)', author: 'Jingyu Sun; Guiping Zhong; Dinghao Zhou; Baoxiang Li; Yiran Zhong', display:{Lore:['[{"text": "arXiv:2203.15609", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLocality Matters: A Locality-Biased Linear Attention for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJingyu Sun\\nGuiping Zhong\\nDinghao Zhou\\nBaoxiang Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15609\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Mar 2022 14:20:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, submitted to interspeech 2022\\u00a7r"}']}
{title:'Sun et al. (§72022§r)', author: 'Jingyu Sun; Guiping Zhong; Dinghao Zhou; Baoxiang Li', display:{Lore:['[{"text": "arXiv:2203.15613", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDynamic Latency for CTC-Based Streaming Automatic Speech Recognition With Emformer\\u00a7r\\n\\n\\u00a78\\u00a7oJingyu Sun\\nGuiping Zhong\\nDinghao Zhou\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15613\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Mar 2022 14:31:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, submitted to interspeech 2022\\u00a7r"}']}
{title:'Chevi et al. (§72022§r)', author: 'Rendi Chevi; Radityo Eko Prasojo; Alham Fikri Aji; Andros Tjandra; Sakriani Sakti', display:{Lore:['[{"text": "arXiv:2203.15643", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNix-TTS: Lightweight and End-to-End Text-to-Speech via Module-wise Distillation\\u00a7r\\n\\n\\u00a78\\u00a7oRendi Chevi\\nRadityo Eko Prasojo\\nAlham Fikri Aji\\nAndros Tjandra\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15643\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 5 Nov 2022 12:43:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at SLT 2022 (https://slt2022.org/).Associated materials can be seen in https://github.com/rendchevi/nix-tts\\u00a7r"}']}
{title:'Saeki et al. (§72022§r)', author: 'Takaaki Saeki; Kentaro Tachibana; Ryuichi Yamamoto', display:{Lore:['[{"text": "arXiv:2203.15683", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDRSpeech: Degradation-Robust Text-to-Speech Synthesis with Frame-Level and Utterance-Level Acoustic Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oTakaaki Saeki\\nKentaro Tachibana\\nRyuichi Yamamoto\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15683\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 29 Jun 2022 13:38:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Ming et al. (§72022§r)', author: 'Chen Ming; James A. Simmons', display:{Lore:['[{"text": "arXiv:2203.15770", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTarget Geometry Estimation Using Deep Neural Networks in Sonar Sensing\\u00a7r\\n\\n\\u00a78\\u00a7oChen Ming\\nJames A. Simmons\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15770\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Mar 2022 17:27:57 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Zijiang Yang; Xin Jing; Andreas Triantafyllopoulos; Meishu Song; Ilhan Aslan; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2203.15873", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Overview     Analysis of Sequence-to-Sequence Emotional Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oZijiang Yang\\nXin Jing\\nAndreas Triantafyllopoulos\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15873\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Mar 2022 19:41:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Jia et al. (§72022§r)', author: 'Junteng Jia; Jay Mahadeokar; Weiyi Zheng; Yuan Shangguan; Ozlem Kalinli; Frank Seide', display:{Lore:['[{"text": "arXiv:2203.15966", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFederated Domain Adaptation for ASR with Full Self-Supervision\\u00a7r\\n\\n\\u00a78\\u00a7oJunteng Jia\\nJay Mahadeokar\\nWeiyi Zheng\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15966\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 5 Apr 2022 17:57:30 GMT)\\u00a7r"}']}
{title:'Yi et al. (§72022§r)', author: 'Gaoxiong Yi; Wei Xiao; Yiming Xiao; Babak Naderi; Sebastian Möller; Wafaa Wardah; Gabriel Mittag; Ross Cutler; Zhuohuang Zhang; Donald S. Williamson; Fei Chen; Fuzheng Yang; Shidong Shang', display:{Lore:['[{"text": "arXiv:2203.16032", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConferencingSpeech 2022 Challenge: Non-intrusive Objective Speech Quality Assessment (NISQA) Challenge for Online Conferencing Applications\\u00a7r\\n\\n\\u00a78\\u00a7oGaoxiong Yi\\nWei Xiao\\nYiming Xiao\\n+ 9 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16032\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 1 Apr 2022 03:22:04 GMT)\\u00a7r"}']}
{title:'Yu et al. (§72022§r)', author: 'Guochen Yu; Andong Li; Wenzhe Liu; Chengshi Zheng; Yutian Wang; Hui Wang', display:{Lore:['[{"text": "arXiv:2203.16033", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimizing Shoulder to Shoulder: A Coordinated Sub-Band Fusion Model for Real-Time Full-Band Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oGuochen Yu\\nAndong Li\\nWenzhe Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16033\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 15 Jun 2022 09:25:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2203.00472\\u00a7r"}']}
{title:'Long et al. (§72022§r)', author: 'Ziang Long; Yunling Zheng; Meng Yu; Jack Xin', display:{Lore:['[{"text": "arXiv:2203.16037", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Zero-Shot Many to Many Voice Conversion with Self-Attention VAE\\u00a7r\\n\\n\\u00a78\\u00a7oZiang Long\\nYunling Zheng\\nMeng Yu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16037\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 22 Aug 2022 07:11:58 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Fan-Lin Wang; Hung-Shin Lee; Yu Tsao; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:2203.16040", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDisentangling the Impacts of Language and Channel Variability on Speech Separation Networks\\u00a7r\\n\\n\\u00a78\\u00a7oFan-Lin Wang\\nHung-Shin Lee\\nYu Tsao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16040\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 20 Jun 2022 02:26:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in Interspeech 2022\\u00a7r"}']}
{title:'Jin et al. (§72022§r)', author: 'Zhenhao Jin; Xiang Hao; Xiangdong Su', display:{Lore:['[{"text": "arXiv:2203.16054", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCoarse-to-Fine Recursive Speech Separation for Unknown Number of Speakers\\u00a7r\\n\\n\\u00a78\\u00a7oZhenhao Jin\\nXiang Hao\\nXiangdong Su\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16054\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Mar 2022 04:45:34 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Yikang Wang; Hiromitsu Nishizaki', display:{Lore:['[{"text": "arXiv:2203.16085", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCombination of Time-domain, Frequency-domain, and Cepstral-domain Acoustic Features for Speech Commands Classification\\u00a7r\\n\\n\\u00a78\\u00a7oYikang Wang\\nHiromitsu Nishizaki\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16085\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 17 Jun 2022 02:01:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'Kuan Po Huang; Yu-Kuan Fu; Yu Zhang; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2203.16104", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Distortion Robustness of Self-supervised Speech Processing Tasks with Domain Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oKuan Po Huang\\nYu-Kuan Fu\\nYu Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16104\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 25 Jul 2022 00:58:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2022\\u00a7r"}']}
{title:'Chang et al. (§72022§r)', author: 'Yi Chang; Zhao Ren; Thanh Tam Nguyen; Wolfgang Nejdl; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2203.16141", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExample-based Explanations with Adversarial Attacks for Respiratory Sound Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oYi Chang\\nZhao Ren\\nThanh Tam Nguyen\\nWolfgang Nejdl\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16141\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Mar 2022 08:28:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Müller et al. (§72022§r)', author: 'Nicolas M. Müller; Pavel Czempin; Franziska Dieckmann; Adam Froghyar; Konstantin Böttinger', display:{Lore:['[{"text": "arXiv:2203.16263", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDoes Audio Deepfake Detection Generalize?\\u00a7r\\n\\n\\u00a78\\u00a7oNicolas M. M\\u00fcller\\nPavel Czempin\\nFranziska Dieckmann\\nAdam Froghyar\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16263\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 21 Apr 2022 09:28:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Simonetta et al. (§72022§r)', author: 'Federico Simonetta; Stavros Ntalampiras; Federico Avanzini', display:{Lore:['[{"text": "arXiv:2203.16294", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustics-specific Piano Velocity Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oFederico Simonetta\\nStavros Ntalampiras\\nFederico Avanzini\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16294\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 29 Jun 2022 15:34:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted at MMSP 2022\\u00a7r"}']}
{title:'Xiao et al. (§72022§r)', author: 'Yang Xiao; Nana Hou; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2203.16361", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRainbow Keywords: Efficient Incremental Learning for Online Spoken Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oYang Xiao\\nNana Hou\\nEng Siong Chng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16361\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 30 Jun 2022 11:07:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Xue et al. (§72022§r)', author: 'Heyang Xue; Xinsheng Wang; Yongmao Zhang; Lei Xie; Pengcheng Zhu; Mengxiao Bi', display:{Lore:['[{"text": "arXiv:2203.16408", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearn2Sing 2.0: Diffusion and Mutual Information-Based Target Speaker SVS by Learning from Singing Teacher\\u00a7r\\n\\n\\u00a78\\u00a7oHeyang Xue\\nXinsheng Wang\\nYongmao Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16408\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 26 May 2022 06:38:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Xiang et al. (§72022§r)', author: 'Ziyue Xiang; Paolo Bestagini; Stefano Tubaro; Edward J. Delp', display:{Lore:['[{"text": "arXiv:2203.16499", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lForensic Analysis and Localization of Multiply Compressed MP3 Audio Using Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oZiyue Xiang\\nPaolo Bestagini\\nStefano Tubaro\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16499\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747639\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 28 Apr 2022 21:01:22 GMT)\\u00a7r"}']}
{title:'Elbanna et al. (§72022§r)', author: 'Gasser Elbanna; Alice Biryukov; Neil Scheidwasser-Clow; Lara Orlandic; Pablo Mainar; Mikolaj Kegler; Pierre Beckmann; Milos Cernak', display:{Lore:['[{"text": "arXiv:2203.16637", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHybrid Handcrafted and Learnable Audio Representation for Analysis of Speech Under Cognitive and Physical Load\\u00a7r\\n\\n\\u00a78\\u00a7oGasser Elbanna\\nAlice Biryukov\\nNeil Scheidwasser-Clow\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16637\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10498\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 30 Jun 2022 17:03:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to InterSpeech 2022\\u00a7r"}']}
{title:'Peng et al. (§72022§r)', author: 'Yu-Huai Peng; Hung-Shin Lee; Pin-Tuan Huang; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:2203.16646", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeneration of Speaker Representations Using Heterogeneous Training Batch Assembly\\u00a7r\\n\\n\\u00a78\\u00a7oYu-Huai Peng\\nHung-Shin Lee\\nPin-Tuan Huang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16646\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Mar 2022 19:59:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in APSIPA ASC 2021\\u00a7r"}']}
{title:'Tavi et al. (§72022§r)', author: 'Lauri Tavi; Tomi Kinnunen; Rosa González Hautamäki', display:{Lore:['[{"text": "arXiv:2203.16738", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving speaker de-identification with functional data analysis of f0 trajectories\\u00a7r\\n\\n\\u00a78\\u00a7oLauri Tavi\\nTomi Kinnunen\\nRosa Gonz\\u00e1lez Hautam\\u00e4ki\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16738\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.specom.2022.03.010\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Mar 2022 01:34:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Speech Communication. March 2022\\u00a7r"}']}
{title:'Yamamoto et al. (§72022§r)', author: 'Ayako Yamamoto; Toshio Irino; Shoko Araki; Kenichi Arai; Atsunori Ogawa; Keisuke Kinoshita; Tomohiro Nakatani', display:{Lore:['[{"text": "arXiv:2203.16760", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEffective data screening technique for crowdsourced speech intelligibility experiments: Evaluation with IRM-based speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oAyako Yamamoto\\nToshio Irino\\nShoko Araki\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16760\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.23919/APSIPAASC55919.2022.9979946\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. APSIPA ASC 2022\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 20 Aug 2022 02:30:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper was submitted to APSIPA ASC 2022 (https://www.apsipa2022.org).The original title [v1] was \\"Subjective intelligibility of speech sounds enhanced by ideal ratio mask via crowdsourced remote experiments with "}','{"text": "effective data screening.\\"\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Li Wang; Rongzhi Gu; Weiji Zhuang; Peng Gao; Yujun Wang; Yuexian Zou', display:{Lore:['[{"text": "arXiv:2203.16772", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Decoupling Features Through Orthogonality Regularization\\u00a7r\\n\\n\\u00a78\\u00a7oLi Wang\\nRongzhi Gu\\nWeiji Zhuang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16772\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Mar 2022 03:18:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2022\\u00a7r"}']}
{title:'Yu et al. (§72022§r)', author: 'Fan Yu; Zhihao Du; Shiliang Zhang; Yuxiao Lin; Lei Xie', display:{Lore:['[{"text": "arXiv:2203.16834", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparative Study on Speaker-attributed Automatic Speech Recognition in Multi-party Meetings\\u00a7r\\n\\n\\u00a78\\u00a7oFan Yu\\nZhihao Du\\nShiliang Zhang\\nYuxiao Lin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16834\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 1 Jul 2022 16:53:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by INTERSPEECH 2022, 5 pages, 2 figures\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Jingbei Li; Yi Meng; Zhiyong Wu; Helen Meng; Qiao Tian; Yuping Wang; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2203.16838", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeuFA: Neural Network Based End-to-End Forced Alignment with Bidirectional Attention Mechanism\\u00a7r\\n\\n\\u00a78\\u00a7oJingbei Li\\nYi Meng\\nZhiyong Wu\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16838\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Mar 2022 06:45:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Xixin Wu; Shoukang Hu; Zhiyong Wu; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2203.16928", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Architecture Search for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXixin Wu\\nShoukang Hu\\nZhiyong Wu\\nXunying Liu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16928\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Mar 2022 10:16:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Siuzdak et al. (§72022§r)', author: 'Hubert Siuzdak; Piotr Dura; Pol van Rijn; Nori Jacoby', display:{Lore:['[{"text": "arXiv:2203.16930", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWavThruVec: Latent speech representation as intermediate features for neural speech synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oHubert Siuzdak\\nPiotr Dura\\nPol van Rijn\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16930\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10797\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 11 Jul 2022 12:13:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022. Audio samples are available at: https://charactr-platform.github.io/WavThruVec/\\u00a7r"}']}
{title:'Kashkin et al. (§72022§r)', author: 'A. Kashkin; I. Karpukhin; S. Shishkin', display:{Lore:['[{"text": "arXiv:2203.16937", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHiFi-VC: High Quality ASR-Based Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oA. Kashkin\\nI. Karpukhin\\nS. Shishkin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16937\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Mar 2022 10:45:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Faundez-Zanuy et al. (§72022§r)', author: 'Marcos Faundez-Zanuy; Enric Monte; Francesc Vallverdú', display:{Lore:['[{"text": "arXiv:2203.16962", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA comparative study between linear and nonlinear speech prediction\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos Faundez-Zanuy\\nEnric Monte\\nFrancesc Vallverd\\u00fa\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16962\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/BFb0032575\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n1997 International Workshop on Artificial Neural Networks (IWANN),\\n  Lanzarore (Spain)\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Mar 2022 11:32:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, published in Mira,J., Moreno-D\\u00edaz, R., Cabestany, J. (eds) Biological and Artificial Computation: From Neuroscience to Technology. IWANN 1997. Lecture Notes in Computer Science, vol 1240. Springer, Berlin, "}','{"text": "Heidelberg\\u00a7r"}']}
{title:'Grinberg et al. (§72022§r)', author: 'Petr Grinberg; Vladislav Shikhov', display:{Lore:['[{"text": "arXiv:2203.16970", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparative Study of Fusion Methods for SASV Challenge 2022\\u00a7r\\n\\n\\u00a78\\u00a7oPetr Grinberg\\nVladislav Shikhov\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16970\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Mar 2022 11:43:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper is submitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Zhou et al. (§72022§r)', author: 'Guanxing Zhou; Hao Liang; Xinghao Ding; Yue Huang; Xiaotong Tu; Saqlain Abbas', display:{Lore:['[{"text": "arXiv:2203.16988", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic-Net: A Novel Neural Network for Sound Localization and Quantification\\u00a7r\\n\\n\\u00a78\\u00a7oGuanxing Zhou\\nHao Liang\\nXinghao Ding\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16988\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Mar 2022 12:20:09 GMT)\\u00a7r"}']}
{title:'Jing et al. (§72022§r)', author: 'Xin Jing; Shuo Liu; Emilia Parada-Cabaleiro; Andreas Triantafyllopoulos; Meishu Song; Zijiang Yang; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2203.17012", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Temporal-oriented Broadcast ResNet for COVID-19 Detection\\u00a7r\\n\\n\\u00a78\\u00a7oXin Jing\\nShuo Liu\\nEmilia Parada-Cabaleiro\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.17012\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Mar 2022 13:11:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages,submitted to Intesspeech 2022\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Chengxin Chen; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2203.17023", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCTA-RNN: Channel and Temporal-wise Attention RNN Leveraging Pre-trained ASR Embeddings for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oChengxin Chen\\nPengyuan Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.17023\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Mar 2022 13:32:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, submitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Liao et al. (§72022§r)', author: 'Yen-Lun Liao; Xuanjun Chen; Chung-Che Wang; Jyh-Shing Roger Jang', display:{Lore:['[{"text": "arXiv:2203.17031", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Speaker Distillation for Countermeasure Model on Automatic Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oYen-Lun Liao\\nXuanjun Chen\\nChung-Che Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.17031\\u00a7r\\n\\nVersion:\\u00a77v6 (Sun, 2 Oct 2022 16:05:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ISCA SPSC 2022\\u00a7r"}']}
{title:'Halpern et al. (§72022§r)', author: 'Bence Mark Halpern; Teja Rebernik; Thomas Tienkamp; Rob van Son; Michiel van den Brekel; Martijn Wieling; Max Witjes; Odette Scharenborg', display:{Lore:['[{"text": "arXiv:2203.17072", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lManipulation of oral cancer speech using neural articulatory synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oBence Mark Halpern\\nTeja Rebernik\\nThomas Tienkamp\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.17072\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Mar 2022 14:40:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 tables, 1 figure. Submitted to Interspeech 2022\\u00a7r"}']}
{title:'Novikova (§72022§r)', author: 'Jekaterina Novikova', display:{Lore:['[{"text": "arXiv:2203.17110", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImpact of Environmental Noise on Alzheimer\'s Disease Detection from Speech: Should You Let a Baby Cry?\\u00a7r\\n\\n\\u00a78\\u00a7oJekaterina Novikova\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.17110\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 14 Sep 2022 17:33:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oW-NUT at COLING 2022\\u00a7r"}']}
{title:'Ao et al. (§72022§r)', author: 'Junyi Ao; Ziqiang Zhang; Long Zhou; Shujie Liu; Haizhou Li; Tom Ko; Lirong Dai; Jinyu Li; Yao Qian; Furu Wei', display:{Lore:['[{"text": "arXiv:2203.17113", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data\\u00a7r\\n\\n\\u00a78\\u00a7oJunyi Ao\\nZiqiang Zhang\\nLong Zhou\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.17113\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 20 Jun 2022 07:00:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Chao et al. (§72022§r)', author: 'Rong Chao; Cheng Yu; Szu-Wei Fu; Xugang Lu; Yu Tsao', display:{Lore:['[{"text": "arXiv:2203.17152", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerceptual Contrast Stretching on Target Feature for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oRong Chao\\nCheng Yu\\nSzu-Wei Fu\\nXugang Lu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.17152\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 15 Jul 2022 08:22:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Mirheidari et al. (§72022§r)', author: 'Bahman Mirheidari; André Bittar; Nicholas Cummins; Johnny Downs; Helen L. Fisher; Heidi Christensen', display:{Lore:['[{"text": "arXiv:2203.17242", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Detection of Expressed Emotion from Five-Minute Speech Samples: Challenges and Opportunities\\u00a7r\\n\\n\\u00a78\\u00a7oBahman Mirheidari\\nAndr\\u00e9 Bittar\\nNicholas Cummins\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.17242\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Mar 2022 16:26:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'de Korte et al. (§72022§r)', author: 'Marcel de Korte; Jaebok Kim; Aki Kunikoshi; Adaeze Adigwe; Esther Klabbers', display:{Lore:['[{"text": "arXiv:2204.00061", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData-augmented cross-lingual synthesis in a teacher-student framework\\u00a7r\\n\\n\\u00a78\\u00a7oMarcel de Korte\\nJaebok Kim\\nAki Kunikoshi\\nAdaeze Adigwe\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00061\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Mar 2022 20:01:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Fara et al. (§72022§r)', author: 'Salvatore Fara; Stefano Goria; Emilia Molimpakis; Nicholas Cummins', display:{Lore:['[{"text": "arXiv:2204.00088", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech and the n-Back task as a lens into depression. How combining both may allow us to isolate different core symptoms of depression\\u00a7r\\n\\n\\u00a78\\u00a7oSalvatore Fara\\nStefano Goria\\nEmilia Molimpakis\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00088\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Mar 2022 09:12:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Rouat et al. (§72022§r)', author: 'Jean Rouat; Ramin Pichevar; Stéphane Loiselle', display:{Lore:['[{"text": "arXiv:2204.00094", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerceptive, non-linear Speech Processing and Spiking Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oJean Rouat\\nRamin Pichevar\\nSt\\u00e9phane Loiselle\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00094\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/11520153_14\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Mar 2022 21:11:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7opreprint of the 2005 published paper: Perceptive, Non-linear Speech Processing andSpiking Neural Networks. In: Chollet, G., Esposito, A., Faundez-Zanuy, M., Marinaro, M. (eds) Nonlinear Speech Modeling and "}','{"text": "Applications. NN 2004. Lecture Notes in Computer Science, vol 3445. Springer, Berlin, Heidelberg\\u00a7r"}']}
{title:'Faundez-Zanuy (§72022§r)', author: 'Marcos Faundez-Zanuy', display:{Lore:['[{"text": "arXiv:2204.00245", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptive hybrid speech coding with a MLP LPC structure\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos Faundez-Zanuy\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00245\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/BFb0100549\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n1999 International Work-Conference on Artificial Neural Networks\\n  (IWANN) Alicante, Spain\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Apr 2022 07:09:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, published in Mira,J., S\\u00e1nchez-Andr\\u00e9s, J.V. (eds) Engineering Applications of Bio-Inspired Artificial Neural Networks. IWANN 1999. Lecture Notes in Computer Science, vol 1607. Springer, Berlin, Heidelberg. arXiv "}','{"text": "admin note: substantial text overlap with arXiv:2203.11612\\u00a7r"}']}
{title:'Faundez-Zanuy et al. (§72022§r)', author: 'Marcos Faundez-Zanuy; Adam Slupinski', display:{Lore:['[{"text": "arXiv:2204.00311", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker verification in mismatch training and testing conditions\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos Faundez-Zanuy\\nAdam Slupinski\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00311\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n6th international conference on spoken language processing (ICSLP\\n  2000), Vol. II, pp.322-325, 2000\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Apr 2022 09:45:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, published in 6th international conference on spoken language processing (ICSLP 2000), Vol. II, pp.322-325. ICSLP 2000, ISBN 7-80150-144-4/G.18Beijing (China). October 16-20, 2000. arXiv admin note: substantial "}','{"text": "text overlap with arXiv:2203.00513\\u00a7r"}']}
{title:'Chelotti et al. (§72022§r)', author: 'José O. Chelotti; Sebastián R. Vanrell; Luciano S. Martinez-Rau; Julio R. Galli; Santiago A. Utsumi; Alejandra M. Planisich; Suyai A. Almirón; Diego H. Milone; Leonardo L. Giovanini; H. Leonardo Rufiner', display:{Lore:['[{"text": "arXiv:2204.00331", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing segment-based features of jaw movements to recognize foraging activities in grazing cattle\\u00a7r\\n\\n\\u00a78\\u00a7oJos\\u00e9 O. Chelotti\\nSebasti\\u00e1n R. Vanrell\\nLuciano S. Martinez-Rau\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00331\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.biosystemseng.2023.03.014\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 15 Nov 2022 22:52:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint submitted to journal\\u00a7r"}']}
{title:'Chang et al. (§72022§r)', author: 'Xuankai Chang; Takashi Maekaku; Yuya Fujita; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2204.00540", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Integration of Speech Recognition, Speech Enhancement, and Self-Supervised Learning Representation\\u00a7r\\n\\n\\u00a78\\u00a7oXuankai Chang\\nTakashi Maekaku\\nYuya Fujita\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00540\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Apr 2022 16:02:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Rose et al. (§72022§r)', author: 'Richard Rose; Olivier Siohan', display:{Lore:['[{"text": "arXiv:2204.00652", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end multi-talker audio-visual ASR using an active speaker attention module\\u00a7r\\n\\n\\u00a78\\u00a7oRichard Rose\\nOlivier Siohan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00652\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Apr 2022 18:42:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 3 tables, 28 citations\\u00a7r"}']}
{title:'Baskar et al. (§72022§r)', author: 'Murali Karthick Baskar; Tim Herzig; Diana Nguyen; Mireia Diez; Tim Polzehl; Lukáš Burget; Jan "Honza\'\' Černocký', display:{Lore:['[{"text": "arXiv:2204.00770", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker adaptation for Wav2vec2 based dysarthric ASR\\u00a7r\\n\\n\\u00a78\\u00a7oMurali Karthick Baskar\\nTim Herzig\\nDiana Nguyen\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00770\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Apr 2022 05:46:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Ma et al. (§72022§r)', author: 'Guodong Ma; Pengfei Hu; Jian Kang; Shen Huang; Hao Huang', display:{Lore:['[{"text": "arXiv:2204.00819", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Phone Mask Training for Phonetic-Reduction-Robust E2E Uyghur Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGuodong Ma\\nPengfei Hu\\nJian Kang\\nShen Huang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00819\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-964\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nINTERSPEECH 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Apr 2022 09:04:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2021\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Helin Wang; Dongchao Yang; Chao Weng; Jianwei Yu; Yuexian Zou', display:{Lore:['[{"text": "arXiv:2204.00821", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Target Sound Extraction with Timestamp Information\\u00a7r\\n\\n\\u00a78\\u00a7oHelin Wang\\nDongchao Yang\\nChao Weng\\nJianwei Yu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00821\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Apr 2022 09:09:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to interspeech2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Jianrong Wang; Jinyu Liu; Longxuan Zhao; Shanyu Wang; Ruiguo Yu; Li Liu', display:{Lore:['[{"text": "arXiv:2204.00873", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic-to-articulatory Inversion based on Speech Decomposition and Auxiliary Feature\\u00a7r\\n\\n\\u00a78\\u00a7oJianrong Wang\\nJinyu Liu\\nLongxuan Zhao\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00873\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Apr 2022 14:47:19 GMT)\\u00a7r"}']}
{title:'Kawahara et al. (§72022§r)', author: 'Hideki Kawahara; Kohei Yatabe; Ken-Ichi Sakakibara; Tatsuya Kitamura; Hideki Banno; Masanori Morise', display:{Lore:['[{"text": "arXiv:2204.00902", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn objective test tool for pitch extractors\' response attributes\\u00a7r\\n\\n\\u00a78\\u00a7oHideki Kawahara\\nKohei Yatabe\\nKen-Ichi Sakakibara\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00902\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 24 Jun 2022 09:12:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 9 figures, submitted to Interspeech2022. arXiv admin note: text overlap with arXiv:2111.03629\\u00a7r"}']}
{title:'Lavault et al. (§72022§r)', author: 'Antoine Lavault; Axel Roebel; Matthieu Voiry', display:{Lore:['[{"text": "arXiv:2204.00907", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStyleWaveGAN: Style-based synthesis of drum sounds with extensive controls using generative adversarial networks\\u00a7r\\n\\n\\u00a78\\u00a7oAntoine Lavault\\nAxel Roebel\\nMatthieu Voiry\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00907\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5281/zenodo.6573360\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Apr 2022 17:27:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in Sound andMusic Computing 2022\\u00a7r"}']}
{title:'Kawahara et al. (§72022§r)', author: 'Hideki Kawahara; Kohei Yatabe; Ken-Ichi Sakakibara; Tatsuya Kitamura; Hideki Banno; Masanori Morise', display:{Lore:['[{"text": "arXiv:2204.00911", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMeasuring pitch extractors\' response to frequency-modulated multi-component signals\\u00a7r\\n\\n\\u00a78\\u00a7oHideki Kawahara\\nKohei Yatabe\\nKen-Ichi Sakakibara\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00911\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Apr 2022 17:43:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 9 figures, The following article has been submitted to/accepted by The Acoustical Society of America. After it is published, it will be found at http://asa.scitation.org/journal/jas\\u00a7r"}']}
{title:'Zhou et al. (§72022§r)', author: 'Yixuan Zhou; Changhe Song; Xiang Li; Luwen Zhang; Zhiyong Wu; Yanyao Bian; Dan Su; Helen Meng', display:{Lore:['[{"text": "arXiv:2204.00990", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContent-Dependent Fine-Grained Speaker Embedding for Zero-Shot Speaker Adaptation in Text-to-Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYixuan Zhou\\nChanghe Song\\nXiang Li\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00990\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 11 Nov 2022 09:00:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Shafiei et al. (§72022§r)', author: 'Sepideh Shafiei; S. Hakam', display:{Lore:['[{"text": "arXiv:2204.01009", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Computational Analysis of Pitch Drift in Unaccompanied Solo Singing using DBSCAN Clustering\\u00a7r\\n\\n\\u00a78\\u00a7oSepideh Shafiei\\nS. Hakam\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.01009\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 3 Apr 2022 06:54:13 GMT)\\u00a7r"}']}
{title:'Rallabandi et al. (§72022§r)', author: 'Sai Sirisha Rallabandi; Sebastian Möller', display:{Lore:['[{"text": "arXiv:2204.01115", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn incorporating social speaker characteristics in synthetic speech\\u00a7r\\n\\n\\u00a78\\u00a7oSai Sirisha Rallabandi\\nSebastian M\\u00f6ller\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.01115\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 3 Apr 2022 16:51:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Faundez-Zanuy (§72022§r)', author: 'Marcos Faundez-Zanuy', display:{Lore:['[{"text": "arXiv:2204.01294", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn The Model Size Selection For Speaker Identification\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos Faundez-Zanuy\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.01294\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2001 A Speaker Odyssey - The Speaker Recognition Workshop June\\n  18-22, 2001, Crete, Greece\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Apr 2022 08:04:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, published in Speaker odyssey 2001,The speaker recognition workshop. 189-194 Crete (Greece)\\u00a7r"}']}
{title:'Faundez-Zanuy (§72022§r)', author: 'Marcos Faundez-Zanuy', display:{Lore:['[{"text": "arXiv:2204.01295", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNonlinear Vectorial Prediction with Neural Nets\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos Faundez-Zanuy\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.01295\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5555/646370.688874\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nLecture Notes in Computer Science LNCS 2085 Vol. II, pages\\n  754-761. IWANN 2001, Granada (Spain) ISSN 0302-9743\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Apr 2022 08:04:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, published in Proceedings of the 6th International Work Conference on Artificial and Natural Neural Networks: Bio inspired Applications ofConnectionism Part II June 2001 Pages 754 761\\u00a7r"}']}
{title:'Boeddeker et al. (§72022§r)', author: 'Christoph Boeddeker; Tobias Cord-Landwehr; Thilo von Neumann; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2204.01338", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Initialization Scheme for Meeting Separation with Spatial Mixture Models\\u00a7r\\n\\n\\u00a78\\u00a7oChristoph Boeddeker\\nTobias Cord-Landwehr\\nThilo von Neumann\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.01338\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Apr 2022 09:21:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Vial et al. (§72022§r)', author: 'Pierre-Hugo Vial; Paul Magron; Thomas Oberlin; Cédric Févotte', display:{Lore:['[{"text": "arXiv:2204.01360", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning the Proximity Operator in Unfolded ADMM for Phase Retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oPierre-Hugo Vial\\nPaul Magron\\nThomas Oberlin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.01360\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2022.3189275\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Apr 2022 10:09:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 5 figures, submitted to IEEE SPL\\u00a7r"}']}
{title:'Sheikh et al. (§72022§r)', author: 'Shakeel Ahmad Sheikh; Md Sahidullah; Fabrice Hirsch; Slim Ouni', display:{Lore:['[{"text": "arXiv:2204.01564", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntroducing ECAPA-TDNN and Wav2Vec2.0 Embeddings to Stuttering Detection\\u00a7r\\n\\n\\u00a78\\u00a7oShakeel Ahmad Sheikh\\nMd Sahidullah\\nFabrice Hirsch\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.01564\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Apr 2022 15:12:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Jianrong Wang; Zixuan Wang; Xiaosheng Hu; Xuewei Li; Qiang Fang; Li Liu', display:{Lore:['[{"text": "arXiv:2204.01672", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lResidual-guided Personalized Speech Synthesis based on Face Image\\u00a7r\\n\\n\\u00a78\\u00a7oJianrong Wang\\nZixuan Wang\\nXiaosheng Hu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.01672\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Apr 2022 15:27:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2022\\u00a7r"}']}
{title:'Tang et al. (§72022§r)', author: 'Zhenyu Tang; Rohith Aralikatti; Anton Ratnarajah; Dinesh Manocha', display:{Lore:['[{"text": "arXiv:2204.01787", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGWA: A Large High-Quality Acoustic Dataset for Audio Processing\\u00a7r\\n\\n\\u00a78\\u00a7oZhenyu Tang\\nRohith Aralikatti\\nAnton Ratnarajah\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.01787\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3528233.3530731\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 20 Jun 2022 22:14:38 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Bingqing Chen; Luca Bondi; Samarjit Das', display:{Lore:['[{"text": "arXiv:2204.01905", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning to Adapt to Domain Shifts with Few-shot Samples in Anomalous Sound Detection\\u00a7r\\n\\n\\u00a78\\u00a7oBingqing Chen\\nLuca Bondi\\nSamarjit Das\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.01905\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Apr 2022 00:22:25 GMT)\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Guinan Li; Jianwei Yu; Jiajun Deng; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2204.01977", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-visual multi-channel speech separation, dereverberation and recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGuinan Li\\nJianwei Yu\\nJiajun Deng\\nXunying Liu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.01977\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 8 Apr 2022 08:07:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Du et al. (§72022§r)', author: 'Ye-Qian Du; Jie Zhang; Qiu-Shi Zhu; Li-Rong Dai; Ming-Hui Wu; Xin Fang; Zhou-Wang Yang', display:{Lore:['[{"text": "arXiv:2204.02023", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Complementary Joint Training Approach Using Unpaired Speech and Text for Low-Resource Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYe-Qian Du\\nJie Zhang\\nQiu-Shi Zhu\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02023\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Apr 2022 07:02:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Faundez-Zanuy et al. (§72022§r)', author: 'Marcos Faundez-Zanuy; Mattias Nilsson; W. Bastiaan Kleijn', display:{Lore:['[{"text": "arXiv:2204.02040", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Relevance of Bandwidth Extension for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos Faundez-Zanuy\\nMattias Nilsson\\nW. Bastiaan Kleijn\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02040\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n7th International Conference on Spoken Language Processing\\n  (ICSLP2002), September 16-20, 2002\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Apr 2022 08:12:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages published in 7th International Conference on Spoken Language Processing, September 16-20, 2002,Denver, Colorado, USA. arXiv admin note: text overlap with arXiv:2202.13865\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Dongchao Yang; Helin Wang; Yuexian Zou; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2204.02088", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Mixed supervised Learning Framework for Target Sound Detection\\u00a7r\\n\\n\\u00a78\\u00a7oDongchao Yang\\nHelin Wang\\nYuexian Zou\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02088\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 19 Jul 2022 16:28:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to DCASE workshop\\u00a7r"}']}
{title:'Faundez-Zanuy (§72022§r)', author: 'Marcos Faundez-Zanuy', display:{Lore:['[{"text": "arXiv:2204.02101", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-Linear Speech coding with MLP, RBF and Elman based prediction\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos Faundez-Zanuy\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02101\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/3-540-44869-1_85\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInternational Work-Conference on Artificial Neural Networks IWANN\\n  2003, LNCS 2687 Menorca (Spain)\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Apr 2022 10:48:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, published in Mira,J., \\u00c1lvarez, J.R. (eds) Artificial Neural Nets Problem Solving Methods. IWANN 2003. Lecture Notes in Computer Science, vol 2687. Springer, Berlin, Heidelberg\\u00a7r"}']}
{title:'Heggan et al. (§72022§r)', author: 'Calum Heggan; Sam Budgett; Timothy Hospedales; Mehrdad Yaghoobi', display:{Lore:['[{"text": "arXiv:2204.02121", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMetaAudio: A Few-Shot Audio Classification Benchmark\\u00a7r\\n\\n\\u00a78\\u00a7oCalum Heggan\\nSam Budgett\\nTimothy Hospedales\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02121\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 10 Apr 2022 09:53:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages with 1 figure and 2 main results tables. V1 Preprint\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Dongchao Yang; Helin Wang; Zhongjie Ye; Yuexian Zou; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2204.02143", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRaDur: A Reference-aware and Duration-robust Network for Target Sound Detection\\u00a7r\\n\\n\\u00a78\\u00a7oDongchao Yang\\nHelin Wang\\nZhongjie Ye\\nYuexian Zou\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02143\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Apr 2022 12:08:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to interspeech2022\\u00a7r"}']}
{title:'Saeki et al. (§72022§r)', author: 'Takaaki Saeki; Detai Xin; Wataru Nakata; Tomoki Koriyama; Shinnosuke Takamichi; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2204.02152", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022\\u00a7r\\n\\n\\u00a78\\u00a7oTakaaki Saeki\\nDetai Xin\\nWataru Nakata\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02152\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 29 Jun 2022 13:42:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Georges et al. (§72022§r)', author: 'Marc-Antoine Georges; Julien Diard; Laurent Girin; Jean-Luc Schwartz; Thomas Hueber', display:{Lore:['[{"text": "arXiv:2204.02269", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRepeat after me: Self-supervised learning of acoustic-to-articulatory mapping by vocal imitation\\u00a7r\\n\\n\\u00a78\\u00a7oMarc-Antoine Georges\\nJulien Diard\\nLaurent Girin\\nJean-Luc Schwartz\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02269\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Apr 2022 15:02:49 GMT)\\u00a7r"}']}
{title:'Imoto et al. (§72022§r)', author: 'Keisuke Imoto; Yuka Komatsu; Shunsuke Tsubaki; Tatsuya Komatsu', display:{Lore:['[{"text": "arXiv:2204.02279", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHow Information on Acoustic Scenes and Sound Events Mutually Benefits Event Detection and Scene Classification Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oKeisuke Imoto\\nYuka Komatsu\\nShunsuke Tsubaki\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02279\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Apr 2022 15:19:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Faundez-Zanuy (§72022§r)', author: 'Marcos Faundez-Zanuy', display:{Lore:['[{"text": "arXiv:2204.02400", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhat can predictive speech coders learn from speaker recognizers?\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos Faundez-Zanuy\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02400\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nNon-Linear Speech Processing (NOLISP) 2003\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Apr 2022 10:57:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, published in ITRWon Non-Linear Speech Processing (NOLISP 03), May 20-23, 2003, LeCroisic, France, paper 001. arXiv admin note: text overlap with arXiv:2204.02101\\u00a7r"}']}
{title:'Nayak et al. (§72022§r)', author: 'Prateeth Nayak; Takuya Higuchi; Anmol Gupta; Shivesh Ranjan; Stephen Shum; Siddharth Sigtia; Erik Marchi; Varun Lakshminarasimhan; Minsik Cho; Saurabh Adya; Chandra Dhir; Ahmed Tewfik', display:{Lore:['[{"text": "arXiv:2204.02455", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Voice Trigger Detection with Metric Learning\\u00a7r\\n\\n\\u00a78\\u00a7oPrateeth Nayak\\nTakuya Higuchi\\nAnmol Gupta\\n+ 8 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02455\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 13 Sep 2022 18:31:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at InterSpeech 2022\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Alexander H. Liu; Cheng-I Jeff Lai; Wei-Ning Hsu; Michael Auli; Alexei Baevski; James Glass', display:{Lore:['[{"text": "arXiv:2204.02524", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSimple and Effective Unsupervised Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oAlexander H. Liu\\nCheng-I Jeff Lai\\nWei-Ning Hsu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02524\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 20 Apr 2022 17:45:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7opreprint, equal contribution from first two authors\\u00a7r"}']}
{title:'Chetouani et al. (§72022§r)', author: 'Mohamed Chetouani; Marcos Faundez-Zanuy; Bruno Gas; Jean-Luc Zarader', display:{Lore:['[{"text": "arXiv:2204.02609", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA New Nonlinear speaker parameterization algorithm for speaker identification\\u00a7r\\n\\n\\u00a78\\u00a7oMohamed Chetouani\\nMarcos Faundez-Zanuy\\nBruno Gas\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02609\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nThe speaker and Language recognition Workshop (Speaker Odyssey),\\n  Toledo (Spain), 2004\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Apr 2022 06:37:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, published in The speaker and Language recognition Workshop. ISCA tutorial and research Workshop. ISBN 84-7490-722-5, May 31 \\u2013 June 3, 2004\\u00a7r"}']}
{title:'Lei et al. (§72022§r)', author: 'Shun Lei; Yixuan Zhou; Liyang Chen; Jiankun Hu; Zhiyong Wu; Shiyin Kang; Helen Meng', display:{Lore:['[{"text": "arXiv:2204.02743", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Multi-Scale Speaking Style Modelling with Hierarchical Context Information for Mandarin Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oShun Lei\\nYixuan Zhou\\nLiyang Chen\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02743\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 5 Jul 2022 05:01:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2022\\u00a7r"}']}
{title:'Gao et al. (§72022§r)', author: 'Yan Gao; Javier Fernandez-Marques; Titouan Parcollet; Abhinav Mehrotra; Nicholas D. Lane', display:{Lore:['[{"text": "arXiv:2204.02804", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFederated Self-supervised Speech Representations: Are We There Yet?\\u00a7r\\n\\n\\u00a78\\u00a7oYan Gao\\nJavier Fernandez-Marques\\nTitouan Parcollet\\nAbhinav Mehrotra\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02804\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 19 Jul 2022 18:23:37 GMT)\\u00a7r"}']}
{title:'Kumar et al. (§72022§r)', author: 'Ritesh Kumar; Atul Kr. Ojha; Bornini Lahiri; Chingrimnng Lungleng', display:{Lore:['[{"text": "arXiv:2204.02814", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAggression in Hindi and English Speech: Acoustic Correlates and Automatic Identification\\u00a7r\\n\\n\\u00a78\\u00a7oRitesh Kumar\\nAtul Kr. Ojha\\nBornini Lahiri\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02814\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Apr 2022 13:29:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in the Proceedings of Conference on Sanskrit and Indian Languages: Technology\\u00a7r"}']}
{title:'Maniati et al. (§72022§r)', author: 'Georgia Maniati; Alexandra Vioni; Nikolaos Ellinas; Karolos Nikitaras; Konstantinos Klapsas; June Sig Sung; Gunu Jho; Aimilios Chalamandaris; Pirros Tsiakoulis', display:{Lore:['[{"text": "arXiv:2204.03040", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oGeorgia Maniati\\nAlexandra Vioni\\nNikolaos Ellinas\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03040\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10922\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 24 Aug 2022 14:24:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Shchekotov et al. (§72022§r)', author: 'Ivan Shchekotov; Pavel Andreev; Oleg Ivanov; Aibek Alanov; Dmitry Vetrov', display:{Lore:['[{"text": "arXiv:2204.03042", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFFC-SE: Fast Fourier Convolution for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oIvan Shchekotov\\nPavel Andreev\\nOleg Ivanov\\nAibek Alanov\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03042\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Apr 2022 18:52:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'You et al. (§72022§r)', author: 'Zhao You; Shulin Feng; Dan Su; Dong Yu', display:{Lore:['[{"text": "arXiv:2204.03178", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l3M: Multi-loss, Multi-path and Multi-level Neural Networks for speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhao You\\nShulin Feng\\nDan Su\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03178\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Apr 2022 12:45:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure. Submitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Ren et al. (§72022§r)', author: 'Shuo Ren; Shujie Liu; Yu Wu; Long Zhou; Furu Wei', display:{Lore:['[{"text": "arXiv:2204.03240", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Pre-training with Acoustic Piece\\u00a7r\\n\\n\\u00a78\\u00a7oShuo Ren\\nShujie Liu\\nYu Wu\\nLong Zhou\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03240\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Apr 2022 06:12:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures; submitted to Interspeech 2022\\u00a7r"}']}
{title:'Lee et al. (§72022§r)', author: 'Juheon Lee; Hyeong-Seok Choi; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2204.03249", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExpressive Singing Synthesis Using Local Style Token and Dual-path Pitch Encoder\\u00a7r\\n\\n\\u00a78\\u00a7oJuheon Lee\\nHyeong-Seok Choi\\nKyogu Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03249\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Apr 2022 06:44:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, Submitted to Interspeech 2022\\u00a7r"}']}
{title:'Masri et al. (§72022§r)', author: "Hala Al Masri; Muhy Eddin Za'ter", display:{Lore:['[{"text": "arXiv:2204.03255", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lArabic Text-To-Speech (TTS) Data Preparation\\u00a7r\\n\\n\\u00a78\\u00a7oHala Al Masri\\nMuhy Eddin Za\'ter\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03255\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Apr 2022 06:58:03 GMT)\\u00a7r"}']}
{title:'Gao et al. (§72022§r)', author: 'Xiaoxue Gao; Chitralekha Gupta; Haizhou Li', display:{Lore:['[{"text": "arXiv:2204.03307", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenre-conditioned Acoustic Models for Automatic Lyrics Transcription of Polyphonic Music\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoxue Gao\\nChitralekha Gupta\\nHaizhou Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03307\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Apr 2022 09:15:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, accepted by IEEE ICASSP 2022\\u00a7r"}']}
{title:'Shao et al. (§72022§r)', author: 'Qijie Shao; Jinghao Yan; Jian Kang; Pengcheng Guo; Xian Shi; Pengfei Hu; Lei Xie', display:{Lore:['[{"text": "arXiv:2204.03398", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLinguistic-Acoustic Similarity Based Accent Shift for Accent Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oQijie Shao\\nJinghao Yan\\nJian Kang\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03398\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 1 Jul 2022 13:32:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Klapsas et al. (§72022§r)', author: 'Konstantinos Klapsas; Nikolaos Ellinas; Karolos Nikitaras; Georgios Vamvoukakis; Panos Kakoulidis; Konstantinos Markopoulos; Spyros Raptis; June Sig Sung; Gunu Jho; Aimilios Chalamandaris; Pirros Tsiakoulis', display:{Lore:['[{"text": "arXiv:2204.03421", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervised learning for robust voice cloning\\u00a7r\\n\\n\\u00a78\\u00a7oKonstantinos Klapsas\\nNikolaos Ellinas\\nKarolos Nikitaras\\n+ 7 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03421\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 2 Nov 2022 19:27:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Tzinis et al. (§72022§r)', author: 'Efthymios Tzinis; Gordon Wichern; Aswin Subramanian; Paris Smaragdis; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2204.03594", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHeterogeneous Target Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oEfthymios Tzinis\\nGordon Wichern\\nAswin Subramanian\\nParis Smaragdis\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03594\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-46\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInterspeech 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Apr 2022 17:14:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Liang et al. (§72022§r)', author: 'Weida Liang; Lantian Li; Wenqiang Du; Dong Wang', display:{Lore:['[{"text": "arXiv:2204.03847", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhanced exemplar autoencoder with cycle consistency loss in any-to-one voice conversion\\u00a7r\\n\\n\\u00a78\\u00a7oWeida Liang\\nLantian Li\\nWenqiang Du\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03847\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 12 Apr 2022 03:14:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Pengqi Li; Lantian Li; Askar Hamdulla; Dong Wang', display:{Lore:['[{"text": "arXiv:2204.03852", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReliable Visualization for Deep Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oPengqi Li\\nLantian Li\\nAskar Hamdulla\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03852\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 12 Apr 2022 03:10:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Nick J. C. Wang; Zongfeng Quan; Shaojun Wang; Jing Xiao', display:{Lore:['[{"text": "arXiv:2204.03889", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdding Connectionist Temporal Summarization into Conformer to Improve Its Decoder Efficiency For Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oNick J. C. Wang\\nZongfeng Quan\\nShaojun Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03889\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Apr 2022 07:24:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022 (5 pages, 2 figures)\\u00a7r"}']}
{title:'Gao (§72022§r)', author: 'Jiameng Gao', display:{Lore:['[{"text": "arXiv:2204.03967", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Sillwood Technologies System for the VoiceMOS Challenge 2022\\u00a7r\\n\\n\\u00a78\\u00a7oJiameng Gao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03967\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Apr 2022 09:50:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Dissen et al. (§72022§r)', author: 'Yehoshua Dissen; Felix Kreuk; Joseph Keshet', display:{Lore:['[{"text": "arXiv:2204.04166", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervised Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oYehoshua Dissen\\nFelix Kreuk\\nJoseph Keshet\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.04166\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Apr 2022 16:27:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Quan et al. (§72022§r)', author: 'Changsheng Quan; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2204.04464", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultichannel Speech Separation with Narrow-band Conformer\\u00a7r\\n\\n\\u00a78\\u00a7oChangsheng Quan\\nXiaofei Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.04464\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 1 Jul 2022 05:34:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by INTERSPEECH 2022\\u00a7r"}']}
{title:'Ma et al. (§72022§r)', author: 'Danni Ma; Neville Ryant; Mark Liberman', display:{Lore:['[{"text": "arXiv:2204.04579", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInferring Pitch from Coarse Spectral Features\\u00a7r\\n\\n\\u00a78\\u00a7oDanni Ma\\nNeville Ryant\\nMark Liberman\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.04579\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0015792\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 26 Aug 2022 18:42:00 GMT)\\u00a7r"}']}
{title:'Kang et al. (§72022§r)', author: 'Yu Kang; Tianqiao Liu; Hang Li; Yang Hao; Wenbiao Ding', display:{Lore:['[{"text": "arXiv:2204.04645", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Audio-and-Text Pre-training with Extremely Low-Resource Parallel Data\\u00a7r\\n\\n\\u00a78\\u00a7oYu Kang\\nTianqiao Liu\\nHang Li\\nYang Hao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.04645\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 10 Apr 2022 10:25:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAAAI 2022\\u00a7r"}']}
{title:'Delgado et al. (§72022§r)', author: 'Alejandro Delgado; Emir Demirel; Vinod Subramanian; Charalampos Saitis; Mark Sandler', display:{Lore:['[{"text": "arXiv:2204.04646", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Embeddings for Robust User-Based Amateur Vocal Percussion Classification\\u00a7r\\n\\n\\u00a78\\u00a7oAlejandro Delgado\\nEmir Demirel\\nVinod Subramanian\\nCharalampos Saitis\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.04646\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 10 Apr 2022 10:26:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Sound and Music Computing (SMC) conference 2022\\u00a7r"}']}
{title:'Delgado et al. (§72022§r)', author: 'Alejandro Delgado; Charalampos Saitis; Emmanouil Benetos; Mark Sandler', display:{Lore:['[{"text": "arXiv:2204.04651", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Conditional Representation Learning for Drum Sample Retrieval by Vocalisation\\u00a7r\\n\\n\\u00a78\\u00a7oAlejandro Delgado\\nCharalampos Saitis\\nEmmanouil Benetos\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.04651\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 10 Apr 2022 10:58:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022 (under review)\\u00a7r"}']}
{title:'Kvak (§72022§r)', author: 'Daniel Kvak', display:{Lore:['[{"text": "arXiv:2204.04756", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CY\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Evaluation of Autonomously Generated Musical Compositions: A Comprehensive Survey\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Kvak\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.04756\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 10 Apr 2022 19:42:52 GMT)\\u00a7r"}']}
{title:'Shah et al. (§72022§r)', author: 'Ankit Shah; Hira Dhamyal; Yang Gao; Daniel Arancibia; Mario Arancibia; Bhiksha Raj; Rita Singh', display:{Lore:['[{"text": "arXiv:2204.04802", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the pragmatism of using binary classifiers over data intensive neural network classifiers for detection of COVID-19 from voice\\u00a7r\\n\\n\\u00a78\\u00a7oAnkit Shah\\nHira Dhamyal\\nYang Gao\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.04802\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 25 Oct 2022 22:27:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Zhengdong Yang; Wangjin Zhou; Chenhui Chu; Sheng Li; Raj Dabre; Raphael Rubino; Yi Zhao', display:{Lore:['[{"text": "arXiv:2204.04855", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFusion of Self-supervised Learned Models for MOS Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oZhengdong Yang\\nWangjin Zhou\\nChenhui Chu\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.04855\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Apr 2022 03:50:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMOS 2022 shared task system description paper\\u00a7r"}']}
{title:'Nikitaras et al. (§72022§r)', author: 'Karolos Nikitaras; Georgios Vamvoukakis; Nikolaos Ellinas; Konstantinos Klapsas; Konstantinos Markopoulos; Spyros Raptis; June Sig Sung; Gunu Jho; Aimilios Chalamandaris; Pirros Tsiakoulis', display:{Lore:['[{"text": "arXiv:2204.05070", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFine-grained Noise Control for Multispeaker Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oKarolos Nikitaras\\nGeorgios Vamvoukakis\\nNikolaos Ellinas\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.05070\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10765\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 27 Oct 2022 16:26:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Bulatovic et al. (§72022§r)', author: 'Nikola Bulatovic; Slobodan Djukanovic', display:{Lore:['[{"text": "arXiv:2204.05082", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn approach to improving sound-based vehicle speed estimation\\u00a7r\\n\\n\\u00a78\\u00a7oNikola Bulatovic\\nSlobodan Djukanovic\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.05082\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Apr 2022 12:58:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to: 2022 Zooming Innovation in Consumer Technologies Conference (ZINC)\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Ho-Hsiang Wu; Magdalena Fuentes; Prem Seetharaman; Juan Pablo Bello', display:{Lore:['[{"text": "arXiv:2204.05156", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHow to Listen? Rethinking Visual Sound Localization\\u00a7r\\n\\n\\u00a78\\u00a7oHo-Hsiang Wu\\nMagdalena Fuentes\\nPrem Seetharaman\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.05156\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Apr 2022 14:41:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Diener et al. (§72022§r)', author: 'Lorenz Diener; Sten Sootla; Solomiya Branets; Ando Saabas; Robert Aichner; Ross Cutler', display:{Lore:['[{"text": "arXiv:2204.05222", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lINTERSPEECH 2022 Audio Deep Packet Loss Concealment Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oLorenz Diener\\nSten Sootla\\nSolomiya Branets\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.05222\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Apr 2022 16:13:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages + 1 page references, 1 figure, 2 tables. Submitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Ng et al. (§72022§r)', author: 'Dianwen Ng; Jin Hui Pang; Yang Xiao; Biao Tian; Qiang Fu; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2204.05445", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSmall Footprint Multi-channel ConvMixer for Keyword Spotting with Centroid Based Awareness\\u00a7r\\n\\n\\u00a78\\u00a7oDianwen Ng\\nJin Hui Pang\\nYang Xiao\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.05445\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Apr 2022 23:41:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Zhu et al. (§72022§r)', author: 'Wenjing Zhu; Xiang Li', display:{Lore:['[{"text": "arXiv:2204.05571", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Emotion Recognition with Global-Aware Fusion on Multi-scale Feature Representation\\u00a7r\\n\\n\\u00a78\\u00a7oWenjing Zhu\\nXiang Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.05571\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Apr 2022 07:03:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 3 figures, ICASSP 2022\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'Zi Huang; Shulei Ji; Zhilan Hu; Chuangjian Cai; Jing Luo; Xinyu Yang', display:{Lore:['[{"text": "arXiv:2204.05649", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lADFF: Attention Based Deep Feature Fusion Approach for Music Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZi Huang\\nShulei Ji\\nZhilan Hu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.05649\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 30 Jun 2022 13:54:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIt has been received by Interspeech2022\\u00a7r"}']}
{title:'Ravenscroft et al. (§72022§r)', author: 'William Ravenscroft; Stefan Goetze; Thomas Hain', display:{Lore:['[{"text": "arXiv:2204.06439", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReceptive Field Analysis of Temporal Convolutional Networks for Monaural Speech Dereverberation\\u00a7r\\n\\n\\u00a78\\u00a7oWilliam Ravenscroft\\nStefan Goetze\\nThomas Hain\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.06439\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 1 Jul 2022 09:35:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at EUSIPCO 2022\\u00a7r"}']}
{title:'Faridee et al. (§72022§r)', author: 'Abu Zaher Md Faridee; Hannes Gamper', display:{Lore:['[{"text": "arXiv:2204.06616", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPredicting score distribution to improve non-intrusive speech quality estimation\\u00a7r\\n\\n\\u00a78\\u00a7oAbu Zaher Md Faridee\\nHannes Gamper\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.06616\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Apr 2022 19:16:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Esmaeilpour et al. (§72022§r)', author: 'Mohammad Esmaeilpour; Patrick Cardinal; Alessandro Lameiras Koerich', display:{Lore:['[{"text": "arXiv:2204.07018", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrom Environmental Sound Representation to Robustness of 2D CNN Models Against Adversarial Attacks\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Esmaeilpour\\nPatrick Cardinal\\nAlessandro Lameiras Koerich\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.07018\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Apr 2022 15:14:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o32 pages, Preprint Submitted to Journal of Applied Acoustics. arXivadmin note: substantial text overlap with arXiv:2007.13703\\u00a7r"}']}
{title:'Caillon et al. (§72022§r)', author: 'Antoine Caillon; Philippe Esling', display:{Lore:['[{"text": "arXiv:2204.07064", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreamable Neural Audio Synthesis With Non-Causal Convolutions\\u00a7r\\n\\n\\u00a78\\u00a7oAntoine Caillon\\nPhilippe Esling\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.07064\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Apr 2022 16:00:32 GMT)\\u00a7r"}']}
{title:'Guo et al. (§72022§r)', author: 'Li Guo; Steven Davenport; Yonghong Peng', display:{Lore:['[{"text": "arXiv:2204.07420", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep CardioSound-An Ensembled Deep Learning Model for Heart Sound MultiLabelling\\u00a7r\\n\\n\\u00a78\\u00a7oLi Guo\\nSteven Davenport\\nYonghong Peng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.07420\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Apr 2022 19:47:10 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Zhong-Qiu Wang; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2204.07566", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Frame-Online Neural Speech Enhancement with Overlapped-Frame Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oZhong-Qiu Wang\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.07566\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2022.3183473\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 16 Jun 2022 20:46:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oin IEEESignal Processing Letters\\u00a7r"}']}
{title:'Chang et al. (§72022§r)', author: 'Jiangeng Chang; Yucheng Ruan; Cui Shaoze; John Soong Tshon Yit; Mengling Feng', display:{Lore:['[{"text": "arXiv:2204.07763", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUFRC: A Unified Framework for Reliable COVID-19 Detection on Crowdsourced Cough Audio\\u00a7r\\n\\n\\u00a78\\u00a7oJiangeng Chang\\nYucheng Ruan\\nCui Shaoze\\nJohn Soong Tshon Yit\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.07763\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 30 Jun 2022 06:19:42 GMT)\\u00a7r"}']}
{title:'Fineberg et al. (§72022§r)', author: 'Eva Fineberg; Jack Walters; Joshua Reiss', display:{Lore:['[{"text": "arXiv:2204.08026", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdvances in Thunder Sound Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oEva Fineberg\\nJack Walters\\nJoshua Reiss\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.08026\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 17 Apr 2022 14:57:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 6 figures, conference paper accepted to the AES Europe Spring 2022 Audio Engineering 152nd Convention\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Chenyu Yang; Yu Wang', display:{Lore:['[{"text": "arXiv:2204.08164", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust End-to-end Speaker Diarization with Generic Neural Clustering\\u00a7r\\n\\n\\u00a78\\u00a7oChenyu Yang\\nYu Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.08164\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Apr 2022 05:02:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Muradeli et al. (§72022§r)', author: 'John Muradeli; Cyrus Vahidi; Changhong Wang; Han Han; Vincent Lostanlen; Mathieu Lagrange; George Fazekas', display:{Lore:['[{"text": "arXiv:2204.08269", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDifferentiable Time-Frequency Scattering on GPU\\u00a7r\\n\\n\\u00a78\\u00a7oJohn Muradeli\\nCyrus Vahidi\\nChanghong Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.08269\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 19 Jul 2022 19:42:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 6 figures. Submitted to the International Conference on Digital Audio Effects (DAFX) 2022\\u00a7r"}']}
{title:'Amid et al. (§72022§r)', author: 'Ehsan Amid; Om Thakkar; Arun Narayanan; Rajiv Mathews; Françoise Beaufays', display:{Lore:['[{"text": "arXiv:2204.08345", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExtracting Targeted Training Data from ASR Models, and How to Mitigate It\\u00a7r\\n\\n\\u00a78\\u00a7oEhsan Amid\\nOm Thakkar\\nArun Narayanan\\nRajiv Mathews\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.08345\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 28 Jun 2022 00:55:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to appear at Interspeech\'22\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Yiming Zhang; Hong Yu; Ruoyi Du; Zhanyu Ma; Yuan Dong', display:{Lore:['[{"text": "arXiv:2204.08409", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCaption Feature Space Regularization for Audio Captioning\\u00a7r\\n\\n\\u00a78\\u00a7oYiming Zhang\\nHong Yu\\nRuoyi Du\\nZhanyu Ma\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.08409\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Apr 2022 17:07:31 GMT)\\u00a7r"}']}
{title:'Petegrosso et al. (§72022§r)', author: 'Raphael Petegrosso; Vasistakrishna Baderdinni; Thibaud Senechal; Benjamin L. Bullough', display:{Lore:['[{"text": "arXiv:2204.08474", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAB/BA analysis: A framework for estimating keyword spotting recall improvement while maintaining audio privacy\\u00a7r\\n\\n\\u00a78\\u00a7oRaphael Petegrosso\\nVasistakrishna Baderdinni\\nThibaud Senechal\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.08474\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Apr 2022 13:52:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to NAACL 2022 Industry Track\\u00a7r"}']}
{title:'Eren et al. (§72022§r)', author: 'Ayşegül Özkaya Eren; Mustafa Sert', display:{Lore:['[{"text": "arXiv:2204.08567", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomated Audio Captioning using Audio Event Clues\\u00a7r\\n\\n\\u00a78\\u00a7oAy\\u015feg\\u00fcl \\u00d6zkaya Eren\\nMustafa Sert\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.08567\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Apr 2022 21:30:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to IEEE/ACM Transactionson Audio Speech and Language Processing\\u00a7r"}']}
{title:'Latif et al. (§72022§r)', author: 'Siddique Latif; Rajib Rana; Sara Khalifa; Raja Jurdak; Björn Schuller', display:{Lore:['[{"text": "arXiv:2204.08625", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf Supervised Adversarial Domain Adaptation for Cross-Corpus and Cross-Language Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSiddique Latif\\nRajib Rana\\nSara Khalifa\\nRaja Jurdak\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.08625\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Apr 2022 02:57:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in IEEE Transactions on Affective Computing\\u00a7r"}']}
{title:'Xu et al. (§72022§r)', author: 'Yanguang Xu; Jianwei Sun; Yang Han; Shuaijiang Zhao; Chaoyang Mei; Tingwei Guo; Shuran Zhou; Chuandong Xie; Wei Zou; Xiangang Li; Shuran Zhou; Chuandong Xie; Wei Zou; Xiangang Li', display:{Lore:['[{"text": "arXiv:2204.08686", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-Visual Wake Word Spotting System For MISP Challenge 2021\\u00a7r\\n\\n\\u00a78\\u00a7oYanguang Xu\\nJianwei Sun\\nYang Han\\n+ 10 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.08686\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Apr 2022 03:23:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022\\u00a7r"}']}
{title:'Agrawal et al. (§72022§r)', author: 'Ruchit Agrawal; Daniel Wolff; Simon Dixon', display:{Lore:['[{"text": "arXiv:2204.08822", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Convolutional-Attentional Neural Framework for Structure-Aware Performance-Score Synchronization\\u00a7r\\n\\n\\u00a78\\u00a7oRuchit Agrawal\\nDaniel Wolff\\nSimon Dixon\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.08822\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Apr 2022 11:41:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in IEEESignal Processing Letters, Volume 29, December 2021\\u00a7r"}']}
{title:'Xu et al. (§72022§r)', author: 'Jinghui Xu; Jifeng Zhu; Yong Yang', display:{Lore:['[{"text": "arXiv:2204.08977", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDisappeared Command: Spoofing Attack On Automatic Speech Recognition Systems with Sound Masking\\u00a7r\\n\\n\\u00a78\\u00a7oJinghui Xu\\nJifeng Zhu\\nYong Yang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.08977\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 8 Jun 2022 13:19:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 4 figures. arXiv admin note: text overlapwith arXiv:1903.10346 by other authors\\u00a7r"}']}
{title:'Qian et al. (§72022§r)', author: 'Kaizhi Qian; Yang Zhang; Heting Gao; Junrui Ni; Cheng-I Lai; David Cox; Mark Hasegawa-Johnson; Shiyu Chang', display:{Lore:['[{"text": "arXiv:2204.09224", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContentVec: An Improved Self-Supervised Speech Representation by Disentangling Speakers\\u00a7r\\n\\n\\u00a78\\u00a7oKaizhi Qian\\nYang Zhang\\nHeting Gao\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.09224\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 23 Jun 2022 19:15:10 GMT)\\u00a7r"}']}
{title:'van Niekerk et al. (§72022§r)', author: 'Daniel R. van Niekerk; Anqi Xu; Branislav Gerazov; Paul K. Krug; Peter Birkholz; Yi Xu', display:{Lore:['[{"text": "arXiv:2204.09381", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploration strategies for articulatory synthesis of complex syllable onsets\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel R. van Niekerk\\nAnqi Xu\\nBranislav Gerazov\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.09381\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 30 Jun 2022 15:49:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2022\\u00a7r"}']}
{title:'Lipping et al. (§72022§r)', author: 'Samuel Lipping; Parthasaarathy Sudarsanam; Konstantinos Drossos; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2204.09634", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lClotho-AQA: A Crowdsourced Dataset for Audio Question Answering\\u00a7r\\n\\n\\u00a78\\u00a7oSamuel Lipping\\nParthasaarathy Sudarsanam\\nKonstantinos Drossos\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.09634\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 17 Jun 2022 07:35:08 GMT)\\u00a7r"}']}
{title:'Gong et al. (§72022§r)', author: 'Xun Gong; Yizhou Lu; Zhikai Zhou; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2204.09883", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLayer-wise Fast Adaptation for End-to-End Multi-Accent Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXun Gong\\nYizhou Lu\\nZhikai Zhou\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.09883\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-1075\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Apr 2022 05:08:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech2021\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Zhong-Qiu Wang; Gordon Wichern; Shinji Watanabe; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2204.09911", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSTFT-Domain Neural Speech Enhancement with Very Low Algorithmic Latency\\u00a7r\\n\\n\\u00a78\\u00a7oZhong-Qiu Wang\\nGordon Wichern\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.09911\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 5 Dec 2022 19:19:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oin IEEE/ACM Transactionson Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Song et al. (§72022§r)', author: 'Qingwei Song; Qiwei Sun; Dongsheng Guo; Haiyong Zheng', display:{Lore:['[{"text": "arXiv:2204.09917", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSinTra: Learning an inspiration model from a single multi-track music segment\\u00a7r\\n\\n\\u00a78\\u00a7oQingwei Song\\nQiwei Sun\\nDongsheng Guo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.09917\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Apr 2022 07:13:30 GMT)\\u00a7r"}']}
{title:'Shim et al. (§72022§r)', author: 'Hye-jin Shim; Hemlata Tak; Xuechen Liu; Hee-Soo Heo; Jee-weon Jung; Joon Son Chung; Soo-Whan Chung; Ha-Jin Yu; Bong-Jin Lee; Massimiliano Todisco; Héctor Delgado; Kong Aik Lee; Md Sahidullah; Tomi Kinnunen; Nicholas Evans', display:{Lore:['[{"text": "arXiv:2204.09976", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBaseline Systems for the First Spoofing-Aware Speaker Verification Challenge: Score and Embedding Fusion\\u00a7r\\n\\n\\u00a78\\u00a7oHye-jin Shim\\nHemlata Tak\\nXuechen Liu\\n+ 11 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.09976\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Apr 2022 09:04:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, accepted by Odyssey 2022\\u00a7r"}']}
{title:'Parker et al. (§72022§r)', author: 'Julian D. Parker; Sebastian J. Schlecht; Rudolf Rabenstein; Maximilian Schäfer', display:{Lore:['[{"text": "arXiv:2204.10125", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.comp-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhysical Modeling using Recurrent Neural Networks with Fast Convolutional Layers\\u00a7r\\n\\n\\u00a78\\u00a7oJulian D. Parker\\nSebastian J. Schlecht\\nRudolf Rabenstein\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.10125\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 1 Jun 2022 13:40:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to DAFx2022\\u00a7r"}']}
{title:'Peng et al. (§72022§r)', author: 'Zhiyuan Peng; Xuanji He; Ke Ding; Tan Lee; Guanglu Wan', display:{Lore:['[{"text": "arXiv:2204.10523", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnifying Cosine and PLDA Back-ends for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oZhiyuan Peng\\nXuanji He\\nKe Ding\\nTan Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.10523\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Apr 2022 06:25:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to interspeech2022\\u00a7r"}']}
{title:'Xin et al. (§72022§r)', author: 'Detai Xin; Shinnosuke Takamichi; Takuma Okamoto; Hisashi Kawai; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2204.10561", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaking-Rate-Controllable HiFi-GAN Using Feature Interpolation\\u00a7r\\n\\n\\u00a78\\u00a7oDetai Xin\\nShinnosuke Takamichi\\nTakuma Okamoto\\nHisashi Kawai\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.10561\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Apr 2022 08:18:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'W. Ronny Huang; Shuo-yiin Chang; David Rybach; Rohit Prabhavalkar; Tara N. Sainath; Cyril Allauzen; Cal Peyser; Zhiyun Lu', display:{Lore:['[{"text": "arXiv:2204.10749", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lE2E Segmenter: Joint Segmenting and Decoding for Long-Form ASR\\u00a7r\\n\\n\\u00a78\\u00a7oW. Ronny Huang\\nShuo-yiin Chang\\nDavid Rybach\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.10749\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 15 Jun 2022 14:49:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2022\\u00a7r"}']}
{title:'Mijangos et al. (§72022§r)', author: 'Martín Mijangos; Alessandro Bravetti; Pablo Padilla', display:{Lore:['[{"text": "arXiv:2204.11139", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a72math.AT\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusical Stylistic Analysis: A Study of Intervallic Transition Graphs via Persistent Homology\\u00a7r\\n\\n\\u00a78\\u00a7oMart\\u00edn Mijangos\\nAlessandro Bravetti\\nPablo Padilla\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.11139\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 23 Apr 2022 20:33:49 GMT)\\u00a7r"}']}
{title:'Marras et al. (§72022§r)', author: 'Mirko Marras; Pawel Korus; Anubhav Jain; Nasir Memon', display:{Lore:['[{"text": "arXiv:2204.11304", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDictionary Attacks on Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oMirko Marras\\nPawel Korus\\nAnubhav Jain\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.11304\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 12 Dec 2022 10:51:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in IEEE Transactions on Information Forensics and Security\\u00a7r"}']}
{title:'Goel et al. (§72022§r)', author: 'Raman Goel; Seba Susan; Sachin Vashisht; Armaan Dhanda', display:{Lore:['[{"text": "arXiv:2204.11320", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotion-Aware Transformer Encoder for Empathetic Dialogue Generation\\u00a7r\\n\\n\\u00a78\\u00a7oRaman Goel\\nSeba Susan\\nSachin Vashisht\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.11320\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ACIIW52867.2021.9666315\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 24 Apr 2022 17:05:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in 20219th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Zhuo Li; Runqiu Xiao; Zihan Zhang; Zhenduo Zhao; Wenchao Wang; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2204.11403", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBack-ends Selection for Deep Speaker Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oZhuo Li\\nRunqiu Xiao\\nZihan Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.11403\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Apr 2022 02:43:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to interspeech2022\\u00a7r"}']}
{title:'Heung et al. (§72022§r)', author: 'Kwan Yee Heung; Kin Wai Cheuk; Dorien Herremans', display:{Lore:['[{"text": "arXiv:2204.11437", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnderstanding Audio Features via Trainable Basis Functions\\u00a7r\\n\\n\\u00a78\\u00a7oKwan Yee Heung\\nKin Wai Cheuk\\nDorien Herremans\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.11437\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Apr 2022 05:07:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ounder review in Interspeech 2022\\u00a7r"}']}
{title:'Gazneli et al. (§72022§r)', author: 'Avi Gazneli; Gadi Zimerman; Tal Ridnik; Gilad Sharir; Asaf Noy', display:{Lore:['[{"text": "arXiv:2204.11479", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Audio Strikes Back: Boosting Augmentations Towards An Efficient Audio Classification Network\\u00a7r\\n\\n\\u00a78\\u00a7oAvi Gazneli\\nGadi Zimerman\\nTal Ridnik\\nGilad Sharir\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.11479\\u00a7r\\n\\nVersion:\\u00a77v5 (Tue, 5 Jul 2022 06:30:51 GMT)\\u00a7r"}']}
{title:'Ye et al. (§72022§r)', author: 'Zhenhui Ye; Zhou Zhao; Yi Ren; Fei Wu', display:{Lore:['[{"text": "arXiv:2204.11792", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSyntaSpeech: Syntax-Aware Generative Adversarial Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oZhenhui Ye\\nZhou Zhao\\nYi Ren\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.11792\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Apr 2022 17:05:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IJCAI-2022. 12 pages\\u00a7r"}']}
{title:'Hsu et al. (§72022§r)', author: 'Po-chun Hsu; Da-rong Liu; Andy T. Liu; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2204.11806", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParallel Synthesis for Autoregressive Speech Generation\\u00a7r\\n\\n\\u00a78\\u00a7oPo-chun Hsu\\nDa-rong Liu\\nAndy T. Liu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.11806\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Apr 2022 17:33:22 GMT)\\u00a7r"}']}
{title:'Casebeer et al. (§72022§r)', author: 'Jonah Casebeer; Nicholas J. Bryan; Paris Smaragdis', display:{Lore:['[{"text": "arXiv:2204.11942", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMeta-AF: Meta-Learning for Adaptive Filters\\u00a7r\\n\\n\\u00a78\\u00a7oJonah Casebeer\\nNicholas J. Bryan\\nParis Smaragdis\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.11942\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 21 Nov 2022 19:12:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ACM/IEEE TASLP. Source code and audio examples: https://jmcasebeer.github.io/projects/metaaf\\u00a7r"}']}
{title:'Zheng et al. (§72022§r)', author: 'Siqi Zheng; Hongbin Suo', display:{Lore:['[{"text": "arXiv:2204.12112", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReformulating Speaker Diarization as Community Detection With Emphasis On Topological Structure\\u00a7r\\n\\n\\u00a78\\u00a7oSiqi Zheng\\nHongbin Suo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.12112\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Apr 2022 07:18:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2022\\u00a7r"}']}
{title:'Ananya et al. (§72022§r)', author: 'Ishrat Jahan Ananya; Sarah Suad; Shadab Hafiz Choudhury; Mohammad Ashrafuzzaman Khan', display:{Lore:['[{"text": "arXiv:2204.12177", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparative Study on Approaches to Acoustic Scene Classification using CNNs\\u00a7r\\n\\n\\u00a78\\u00a7oIshrat Jahan Ananya\\nSarah Suad\\nShadab Hafiz Choudhury\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.12177\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-030-89817-5_6\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nAdvances in Computational Intelligence, MICAI 2021, Lecture Notes\\n  in Artificial Intelligence vol. 13067, pp. 81-91 (2021)\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Apr 2022 09:23:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at 2021 Mexican International Conference on Artificial Intelligence. Published in Advances in Computational Intelligence, MICAI 2021, Lecture Notes in Computer Science. 12 pages, 3 figures, 5 tables\\u00a7r"}']}
{title:'Cunha et al. (§72022§r)', author: 'Barbara Cunha; Abdel-Malek Zine; Mohamed Ichchou; Christophe Droz; Stéphane Foulard', display:{Lore:['[{"text": "arXiv:2204.12290", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.med-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Machine Learning-Driven Surrogates for Sound Transmission Loss Simulations\\u00a7r\\n\\n\\u00a78\\u00a7oBarbara Cunha\\nAbdel-Malek Zine\\nMohamed Ichchou\\nChristophe Droz\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.12290\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0015797\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Apr 2022 08:04:25 GMT)\\u00a7r"}']}
{title:'Lenne et al. (§72022§r)', author: 'Lucas Lenne; Patrick Chevret; Étienne Parizet', display:{Lore:['[{"text": "arXiv:2204.12486", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMeasurement uncertainty and unicity of single number quantities describing the spatial decay of speech level in open-plan offices\\u00a7r\\n\\n\\u00a78\\u00a7oLucas Lenne\\nPatrick Chevret\\n\\u00c9tienne Parizet\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.12486\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.apacoust.2021.108269\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nApplied Acoustics, Elsevier, 2021, 182, pp.108269\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Apr 2022 12:17:20 GMT)\\u00a7r"}']}
{title:'Baril et al. (§72022§r)', author: 'Guillaume Baril; Patrick Cardinal; Alessandro Lameiras Koerich', display:{Lore:['[{"text": "arXiv:2204.12622", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNamed Entity Recognition for Audio De-Identification\\u00a7r\\n\\n\\u00a78\\u00a7oGuillaume Baril\\nPatrick Cardinal\\nAlessandro Lameiras Koerich\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.12622\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Apr 2022 22:38:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages\\u00a7r"}']}
{title:'Chong et al. (§72022§r)', author: 'Dading Chong; Helin Wang; Peilin Zhou; Qingcheng Zeng', display:{Lore:['[{"text": "arXiv:2204.12768", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMasked Spectrogram Prediction For Self-Supervised Audio Pre-Training\\u00a7r\\n\\n\\u00a78\\u00a7oDading Chong\\nHelin Wang\\nPeilin Zhou\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.12768\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Apr 2022 08:39:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmit to INTERSPEECH 2022\\u00a7r"}']}
{title:'Fuchs et al. (§72022§r)', author: 'Tzeviya Sylvia Fuchs; Yedid Hoshen; Joseph Keshet', display:{Lore:['[{"text": "arXiv:2204.13094", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Word Segmentation using K Nearest Neighbors\\u00a7r\\n\\n\\u00a78\\u00a7oTzeviya Sylvia Fuchs\\nYedid Hoshen\\nJoseph Keshet\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.13094\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Apr 2022 17:43:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to interspeech 2022\\u00a7r"}']}
{title:'Oneata et al. (§72022§r)', author: 'Dan Oneata; Horia Cucu', display:{Lore:['[{"text": "arXiv:2204.13206", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Multimodal Speech Recognition by Data Augmentation and Speech Representations\\u00a7r\\n\\n\\u00a78\\u00a7oDan Oneata\\nHoria Cucu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.13206\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Apr 2022 21:39:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the Multimodal Learning andApplications Workshop (MULA) from CVPR 2022\\u00a7r"}']}
{title:'Kandpal et al. (§72022§r)', author: 'Nikhil Kandpal; Oriol Nieto; Zeyu Jin', display:{Lore:['[{"text": "arXiv:2204.13289", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Enhancement via Image Translation and Vocoding\\u00a7r\\n\\n\\u00a78\\u00a7oNikhil Kandpal\\nOriol Nieto\\nZeyu Jin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.13289\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Apr 2022 05:00:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2022\\u00a7r"}']}
{title:'Dinkel et al. (§72022§r)', author: 'Heinrich Dinkel; Zhiyong Yan; Yongqing Wang; Junbo Zhang; Yujun Wang', display:{Lore:['[{"text": "arXiv:2204.13430", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPseudo strong labels for large scale weakly supervised audio tagging\\u00a7r\\n\\n\\u00a78\\u00a7oHeinrich Dinkel\\nZhiyong Yan\\nYongqing Wang\\nJunbo Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.13430\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Apr 2022 12:01:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Georgiou et al. (§72022§r)', author: 'Efthymios Georgiou; Kosmas Kritsis; Georgios Paraskevopoulos; Athanasios Katsamanis; Vassilis Katsouros; Alexandros Potamianos', display:{Lore:['[{"text": "arXiv:2204.13437", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRegotron: Regularizing the Tacotron2 architecture via monotonic alignment loss\\u00a7r\\n\\n\\u00a78\\u00a7oEfthymios Georgiou\\nKosmas Kritsis\\nGeorgios Paraskevopoulos\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.13437\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Jul 2022 10:29:40 GMT)\\u00a7r"}']}
{title:'Yazdani et al. (§72022§r)', author: 'Ali Yazdani; Hossein Simchi; Yasser Shekofteh', display:{Lore:['[{"text": "arXiv:2204.13601", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotion Recognition In Persian Speech Using Deep Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oAli Yazdani\\nHossein Simchi\\nYasser Shekofteh\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.13601\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICCKE54056.2021.9721504\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n11th International Conference on Computer and Knowledge\\n  Engineering (ICCKE 2021)\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 12 Nov 2022 08:16:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, 3 tables\\u00a7r"}']}
{title:'Maman et al. (§72022§r)', author: 'Ben Maman; Amit H. Bermano', display:{Lore:['[{"text": "arXiv:2204.13668", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnaligned Supervision For Automatic Music Transcription in The Wild\\u00a7r\\n\\n\\u00a78\\u00a7oBen Maman\\nAmit H. Bermano\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.13668\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Apr 2022 17:31:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages, project page available at https://benadar293.github.io\\u00a7r"}']}
{title:'Zhu et al. (§72022§r)', author: 'Boqing Zhu; Kele Xu; Changjian Wang; Zheng Qin; Tao Sun; Huaimin Wang; Yuxing Peng', display:{Lore:['[{"text": "arXiv:2204.14057", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast\\u00a7r\\n\\n\\u00a78\\u00a7oBoqing Zhu\\nKele Xu\\nChangjian Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.14057\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 27 May 2022 02:42:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 4 figures. Accepted by IJCAI-2022\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Andong Li; Shan You; Guochen Yu; Chengshi Zheng; Xiaodong Li', display:{Lore:['[{"text": "arXiv:2205.00206", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTaylor, Can You Hear Me Now? A Taylor-Unfolding Framework for Monaural Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oAndong Li\\nShan You\\nGuochen Yu\\nChengshi Zheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.00206\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 30 Apr 2022 08:42:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IJCAI2022, Long Oral\\u00a7r"}']}
{title:'Hou et al. (§72022§r)', author: 'Yuanbo Hou; Bo Kang; Wout Van Hauwermeiren; Dick Botteldooren', display:{Lore:['[{"text": "arXiv:2205.00499", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRelation-guided acoustic scene classification aided with event embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oYuanbo Hou\\nBo Kang\\nWout Van Hauwermeiren\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.00499\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 1 May 2022 16:06:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInternational Joint Conference on Neural Networks (IJCNN) 2022\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Xiaohong Li; Xiang Wang; Kai Wang; Shiguo Lian', display:{Lore:['[{"text": "arXiv:2205.00916", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Novel Speech-Driven Lip-Sync Model with CNN and LSTM\\u00a7r\\n\\n\\u00a78\\u00a7oXiaohong Li\\nXiang Wang\\nKai Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.00916\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/CISP-BMEI53629.2021.9624360\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 2 May 2022 13:57:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper has been published on CISP-BMEI2021. See https://ieeexplore.ieee.org/document/9624360\\u00a7r"}']}
{title:'Simonetta (§72022§r)', author: 'Federico Simonetta', display:{Lore:['[{"text": "arXiv:2205.00941", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Interpretation Analysis. A Multimodal Approach To Score-Informed Resynthesis of Piano Recordings\\u00a7r\\n\\n\\u00a78\\u00a7oFederico Simonetta\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.00941\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 2 May 2022 14:40:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPhD Thesis. Author: F. Simonetta; tutor: S. Ntalampiras; co-tutor: F. Avanzini; Universit\\u00e0 degli studi di Milano - Dipartimento di Informatica \\"Giovanni Degli Antoni\\", 2022 Apr 22\\u00a7r"}']}
{title:'Wei et al. (§72022§r)', author: 'Weixing Wei; Peilin Li; Yi Yu; Wei Li', display:{Lore:['[{"text": "arXiv:2205.01019", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHarmoF0: Logarithmic Scale Dilated Convolution For Pitch Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oWeixing Wei\\nPeilin Li\\nYi Yu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.01019\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 20 Jun 2022 14:22:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper is accepted by ICME2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Yu Wang; Daniel Stoller; Rachel M. Bittner; Juan Pablo Bello', display:{Lore:['[{"text": "arXiv:2205.01273", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFew-Shot Musical Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oYu Wang\\nDaniel Stoller\\nRachel M. Bittner\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.01273\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 May 2022 02:18:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Jisi Zhang; Catalin Zorila; Rama Doddipatla; Jon Barker', display:{Lore:['[{"text": "arXiv:2205.01751", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn monoaural speech enhancement for automatic recognition of real noisy speech using mixture invariant training\\u00a7r\\n\\n\\u00a78\\u00a7oJisi Zhang\\nCatalin Zorila\\nRama Doddipatla\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.01751\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-11359\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 20 Sep 2022 14:40:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Bartusiak et al. (§72022§r)', author: 'Emily R. Bartusiak; Edward J. Delp', display:{Lore:['[{"text": "arXiv:2205.01800", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynthesized Speech Detection Using Convolutional Transformer-Based Spectrogram Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oEmily R. Bartusiak\\nEdward J. Delp\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.01800\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/IEEECONF53345.2021.9723142\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Asilomar Conference on Signals, Systems, and Computers, pp.\\n  1426-1430, October 2021, Asilomar, CA\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 May 2022 22:05:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the 2021 IEEE Asilomar Conference on Signals, Systems, and Computers\\u00a7r"}']}
{title:'Bartusiak et al. (§72022§r)', author: 'Emily R. Bartusiak; Edward J. Delp', display:{Lore:['[{"text": "arXiv:2205.01806", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrequency Domain-Based Detection of Generated Audio\\u00a7r\\n\\n\\u00a78\\u00a7oEmily R. Bartusiak\\nEdward J. Delp\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.01806\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the Media Watermarking, Security, and Forensics\\n  Conference, IS&T Electronic Imaging Symposium, pp 273-1 - 273-7, January\\n  2021, Burlingame, CA\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 May 2022 22:27:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the 2021 Media Watermarking, Security,and Forensics Conference, IS T Electronic Imaging Symposium (EI)\\u00a7r"}']}
{title:'Mira et al. (§72022§r)', author: 'Rodrigo Mira; Alexandros Haliassos; Stavros Petridis; Björn W. Schuller; Maja Pantic', display:{Lore:['[{"text": "arXiv:2205.02058", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSVTS: Scalable Video-to-Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oRodrigo Mira\\nAlexandros Haliassos\\nStavros Petridis\\nBj\\u00f6rn W. Schuller\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.02058\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 15 Aug 2022 18:38:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to INTERSPEECH 2022 (Oral Presentation)\\u00a7r"}']}
{title:'Chhimwal et al. (§72022§r)', author: 'Neeraj Chhimwal; Anirudh Gupta; Rishabh Gaur; Harveen Singh Chadha; Priyanshi Shah; Ankur Dhuriya; Vivek Raghavan', display:{Lore:['[{"text": "arXiv:2205.02475", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Recognition in the Wild\\u00a7r\\n\\n\\u00a78\\u00a7oNeeraj Chhimwal\\nAnirudh Gupta\\nRishabh Gaur\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.02475\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 5 May 2022 07:17:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper was submitted to Interspeech 2022\\u00a7r"}']}
{title:'Wang (§72022§r)', author: 'Ning Wang', display:{Lore:['[{"text": "arXiv:2205.02524", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lM2R2: Missing-Modality Robust emotion Recognition framework with iterative data augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oNing Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.02524\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 5 May 2022 09:16:31 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Zui Chen; Yansen Jing; Shengcheng Yuan; Yifei Xu; Jian Wu; Hang Zhao', display:{Lore:['[{"text": "arXiv:2205.03043", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound2Synth: Interpreting Sound via FM Synthesizer Parameters Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oZui Chen\\nYansen Jing\\nShengcheng Yuan\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.03043\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.24963/ijcai.2022/682\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 28 Jul 2022 10:08:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 8 figures. v2: IJCAI2022 published, format revisions and bugfixes\\u00a7r"}']}
{title:'Lee (§72022§r)', author: 'Lin Hao Lee', display:{Lore:['[{"text": "arXiv:2205.03247", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusical Score Following and Audio Alignment\\u00a7r\\n\\n\\u00a78\\u00a7oLin Hao Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.03247\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 May 2022 14:03:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oImperial College London MEng Final YearProject Report\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Juncheng B Li; Zheng Wang; Shuhui Qu; Florian Metze', display:{Lore:['[{"text": "arXiv:2205.03268", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobustness of Neural Architectures for Audio Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJuncheng B Li\\nZheng Wang\\nShuhui Qu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.03268\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 29 Jul 2022 17:24:14 GMT)\\u00a7r"}']}
{title:'Gong et al. (§72022§r)', author: 'Yuan Gong; Ziyi Chen; Iek-Heng Chu; Peng Chang; James Glass', display:{Lore:['[{"text": "arXiv:2205.03432", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransformer-Based Multi-Aspect Multi-Granularity Non-Native English Speaker Pronunciation Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oYuan Gong\\nZiyi Chen\\nIek-Heng Chu\\nPeng Chang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.03432\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746743\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 May 2022 18:07:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2022. Code at https://github.com/YuanGongND/gopt Interactive Colab demo at https://colab.research.google.com/github/YuanGongND/gopt/blob/master/colab/GOPT_GPU.ipynb . ICASSP 2022\\u00a7r"}']}
{title:'Gong et al. (§72022§r)', author: 'Yuan Gong; Jin Yu; James Glass', display:{Lore:['[{"text": "arXiv:2205.03433", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVocalsound: A Dataset for Improving Human Vocal Sounds Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYuan Gong\\nJin Yu\\nJames Glass\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.03433\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746828\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 18 Jun 2022 03:58:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2022. Dataset and code at https://github.com/YuanGongND/vocalsound InteractiveColab demo at https://colab.research.google.com/github/YuanGongND/vocalsound/blob/main/colab/VocalSound.ipynb\\u00a7r"}']}
{title:'Shi et al. (§72022§r)', author: 'Jiatong Shi; Shuai Guo; Tao Qian; Nan Huo; Tomoki Hayashi; Yuning Wu; Frank Xu; Xuankai Chang; Huazhe Li; Peter Wu; Shinji Watanabe; Qin Jin', display:{Lore:['[{"text": "arXiv:2205.04029", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMuskits: an End-to-End Music Processing Toolkit for Singing Voice Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oJiatong Shi\\nShuai Guo\\nTao Qian\\n+ 8 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.04029\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 2 Jul 2022 15:30:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Yang Li; Cheng Yu; Guangzhi Sun; Hua Jiang; Fanglei Sun; Weiqin Zu; Ying Wen; Yang Yang; Jun Wang', display:{Lore:['[{"text": "arXiv:2205.04120", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-Utterance Conditioned VAE for Non-Autoregressive Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oYang Li\\nCheng Yu\\nGuangzhi Sun\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.04120\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 9 May 2022 08:39:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oACL 2022 camera ready\\u00a7r"}']}
{title:'Triantafyllopoulos et al. (§72022§r)', author: 'Andreas Triantafyllopoulos; Sandra Zänkert; Alice Baird; Julian Konzok; Brigitte M. Kudielka; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2205.04328", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInsights on Modelling Physiological, Appraisal, and Affective Indicators of Stress using Audio Features\\u00a7r\\n\\n\\u00a78\\u00a7oAndreas Triantafyllopoulos\\nSandra Z\\u00e4nkert\\nAlice Baird\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.04328\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 9 May 2022 14:32:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper accepted for publication at IEEE EMBC2022. Rights remain with IEEE\\u00a7r"}']}
{title:'Triantafyllopoulos et al. (§72022§r)', author: 'Andreas Triantafyllopoulos; Sandra Ottl; Alexander Gebhard; Esther Rituerto-González; Mirko Jaumann; Steffen Hüttner; Valerie Dieter; Patrick Schneeweiß; Inga Krauß; Maurice Gerczuk; Shahin Amiriparian; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2205.04343", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFatigue Prediction in Outdoor Running Conditions using Audio Data\\u00a7r\\n\\n\\u00a78\\u00a7oAndreas Triantafyllopoulos\\nSandra Ottl\\nAlexander Gebhard\\n+ 8 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.04343\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 9 May 2022 14:44:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper accepted at IEEE EMBC 2022. Rights remain with IEEE\\u00a7r"}']}
{title:'Ghimire et al. (§72022§r)', author: 'Sandip Ghimire; Tomi Kinnunen; Rosa Gonzalez Hautamäki', display:{Lore:['[{"text": "arXiv:2205.04923", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGamified Speaker Comparison by Listening\\u00a7r\\n\\n\\u00a78\\u00a7oSandip Ghimire\\nTomi Kinnunen\\nRosa Gonzalez Hautam\\u00e4ki\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.04923\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 May 2022 14:25:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Odyssey 2022 The Speaker and Language Recognition Workshop\\u00a7r"}']}
{title:'Fontaine et al. (§72022§r)', author: 'Mathieu Fontaine; Kouhei Sekiguchi; Aditya Nugraha; Yoshiaki Bando; Kazuyoshi Yoshii', display:{Lore:['[{"text": "arXiv:2205.05330", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeneralized Fast Multichannel Nonnegative Matrix Factorization Based on Gaussian Scale Mixtures for Blind Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oMathieu Fontaine\\nKouhei Sekiguchi\\nAditya Nugraha\\nYoshiaki Bando\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.05330\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2022.3172631\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech and Language Processing,\\n  Institute of Electrical and Electronics Engineers, 2022, pp.1-1\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 11 May 2022 08:09:39 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Jiafeng Liu; Yuanliang Dong; Zehua Cheng; Xinran Zhang; Xiaobing Li; Feng Yu; Maosong Sun', display:{Lore:['[{"text": "arXiv:2205.05448", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSymphony Generation with Permutation Invariant Language Model\\u00a7r\\n\\n\\u00a78\\u00a7oJiafeng Liu\\nYuanliang Dong\\nZehua Cheng\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.05448\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInternational Society for Music Information Retrieval (ISMIR) 2022\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 16 Sep 2022 08:05:37 GMT)\\u00a7r"}']}
{title:'Kalbag et al. (§72022§r)', author: 'Vedant Kalbag; Alexander Lerch', display:{Lore:['[{"text": "arXiv:2205.05580", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScream Detection in Heavy Metal Music\\u00a7r\\n\\n\\u00a78\\u00a7oVedant Kalbag\\nAlexander Lerch\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.05580\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 11 May 2022 15:48:56 GMT)\\u00a7r"}']}
{title:'Luo et al. (§72022§r)', author: 'Yin-Jyun Luo; Sebastian Ewert; Simon Dixon', display:{Lore:['[{"text": "arXiv:2205.05871", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Robust Unsupervised Disentanglement of Sequential Data \\u2013 A Case Study Using Music Audio\\u00a7r\\n\\n\\u00a78\\u00a7oYin-Jyun Luo\\nSebastian Ewert\\nSimon Dixon\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.05871\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 14 Jun 2022 21:57:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe paper is accepted to IJCAI 2022\\u00a7r"}']}
{title:'Yoneyama et al. (§72022§r)', author: 'Reo Yoneyama; Yi-Chiao Wu; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2205.06053", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnified Source-Filter GAN with Harmonic-plus-Noise Source Excitation Generation\\u00a7r\\n\\n\\u00a78\\u00a7oReo Yoneyama\\nYi-Chiao Wu\\nTomoki Toda\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.06053\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 1 Jul 2022 01:49:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Schuller et al. (§72022§r)', author: 'Björn W. Schuller; Anton Batliner; Shahin Amiriparian; Christian Bergler; Maurice Gerczuk; Natalie Holz; Pauline Larrouy-Maestri; Sebastian P. Bayerl; Korbinian Riedhammer; Adria Mallol-Ragolta; Maria Pateraki; Harry Coppock; Ivan Kiskin; Marianne Sinka; Stephen Roberts', display:{Lore:['[{"text": "arXiv:2205.06799", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe ACM Multimedia 2022 Computational Paralinguistics Challenge: Vocalisations, Stuttering, Activity,     Mosquitoes\\u00a7r\\n\\n\\u00a78\\u00a7oBj\\u00f6rn W. Schuller\\nAnton Batliner\\nShahin Amiriparian\\n+ 11 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.06799\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 13 May 2022 17:51:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, part of the ACM Multimedia 2022 Grand Challenge \\"The ACM Multimedia 2022 Computational Paralinguistics Challenge (ComParE 2022)\\"\\u00a7r"}']}
{title:'Qian et al. (§72022§r)', author: 'Tracy Qian; Jackson Kaunismaa; Tony Chung', display:{Lore:['[{"text": "arXiv:2205.07319", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lcMelGAN: An Efficient Conditional Generative Model Based on Mel Spectrograms\\u00a7r\\n\\n\\u00a78\\u00a7oTracy Qian\\nJackson Kaunismaa\\nTony Chung\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.07319\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 15 May 2022 15:53:43 GMT)\\u00a7r"}']}
{title:'Zheng et al. (§72022§r)', author: 'Siqi Zheng; Hongbin Suo; Qian Chen', display:{Lore:['[{"text": "arXiv:2205.07450", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPRISM: Pre-trained Indeterminate Speaker Representation Model for Speaker Diarization and Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oSiqi Zheng\\nHongbin Suo\\nQian Chen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.07450\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Jun 2022 12:42:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2022\\u00a7r"}']}
{title:'Campana et al. (§72022§r)', author: 'Mattia Giovanni Campana; Andrea Rovati; Franca Delmastro; Elena Pagani', display:{Lore:['[{"text": "arXiv:2205.07682", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lL3-Net Deep Audio Embeddings to Improve COVID-19 Detection from Smartphone Data\\u00a7r\\n\\n\\u00a78\\u00a7oMattia Giovanni Campana\\nAndrea Rovati\\nFranca Delmastro\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.07682\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 16 May 2022 13:50:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted for IEEE SMARTCOMP 2022\\u00a7r"}']}
{title:'Deng et al. (§72022§r)', author: 'Jiacheng Deng; Shunyi Chen; Li Dong; Diqun Yan; Rangding Wang', display:{Lore:['[{"text": "arXiv:2205.07711", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransferability of Adversarial Attacks on Synthetic Speech Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJiacheng Deng\\nShunyi Chen\\nLi Dong\\nDiqun Yan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.07711\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 16 May 2022 14:24:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, submit to Interspeech2022\\u00a7r"}']}
{title:'Ravenscroft et al. (§72022§r)', author: 'William Ravenscroft; Stefan Goetze; Thomas Hain', display:{Lore:['[{"text": "arXiv:2205.08455", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUtterance Weighted Multi-Dilation Temporal Convolutional Networks for Monaural Speech Dereverberation\\u00a7r\\n\\n\\u00a78\\u00a7oWilliam Ravenscroft\\nStefan Goetze\\nThomas Hain\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.08455\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 22 Jul 2022 21:11:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at IWAENC 2022\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Guowei Wu; Shipei Liu; Xiaoya Fan', display:{Lore:['[{"text": "arXiv:2205.08579", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Power of Fragmentation: A Hierarchical Transformer Model for Structural Segmentation in Symbolic Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oGuowei Wu\\nShipei Liu\\nXiaoya Fan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.08579\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 10 Jul 2022 07:12:43 GMT)\\u00a7r"}']}
{title:'Karimi et al. (§72022§r)', author: 'Mostafa Karimi; Changliang Liu; Kenichi Kumatani; Yao Qian; Tianyu Wu; Jian Wu', display:{Lore:['[{"text": "arXiv:2205.08598", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeploying self-supervised learning in the wild for hybrid automatic speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMostafa Karimi\\nChangliang Liu\\nKenichi Kumatani\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.08598\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 17 May 2022 19:37:40 GMT)\\u00a7r"}']}
{title:'Ratnarajah et al. (§72022§r)', author: 'Anton Ratnarajah; Zhenyu Tang; Rohith Chandrashekar Aralikatti; Dinesh Manocha', display:{Lore:['[{"text": "arXiv:2205.09248", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMESH2IR: Neural Acoustic Impulse Response Generator for Complex 3D Scenes\\u00a7r\\n\\n\\u00a78\\u00a7oAnton Ratnarajah\\nZhenyu Tang\\nRohith Chandrashekar Aralikatti\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.09248\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3503161.3548253\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 11 Jul 2022 22:23:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ACM Multimedia 2022. More results and source code is available at https://anton-jeran.github.io/M2IR/\\u00a7r"}']}
{title:'Terwilliger et al. (§72022§r)', author: 'Adam M. Terwilliger; Joshua E. Siegel', display:{Lore:['[{"text": "arXiv:2205.09667", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe AI Mechanic: Acoustic Vehicle Characterization Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oAdam M. Terwilliger\\nJoshua E. Siegel\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.09667\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 19 May 2022 16:29:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o34 pages, 12 figures, 28 tables\\u00a7r"}']}
{title:'Ferranti et al. (§72022§r)', author: 'Luca Ferranti; Kalle Åström; Magnus Oskarsson; Jani Boutellier; Juho Kannala', display:{Lore:['[{"text": "arXiv:2205.11299", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultiple Offsets Multilateration: a new paradigm for sensor network calibration with unsynchronized reference nodes\\u00a7r\\n\\n\\u00a78\\u00a7oLuca Ferranti\\nKalle \\u00c5str\\u00f6m\\nMagnus Oskarsson\\nJani Boutellier\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.11299\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 23 May 2022 13:38:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to ICASSP2022\\u00a7r"}']}
{title:'Zhao et al. (§72022§r)', author: 'Chendong Zhao; Jianzong Wang; Leilai Li; Xiaoyang Qu; Jing Xiao', display:{Lore:['[{"text": "arXiv:2205.11738", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptive Few-Shot Learning Algorithm for Rare Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oChendong Zhao\\nJianzong Wang\\nLeilai Li\\nXiaoyang Qu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.11738\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 26 May 2022 07:10:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IJCNN 2022\\u00a7r"}']}
{title:'Kuo et al. (§72022§r)', author: 'Yao-Ming Kuo; Shanq-Jang Ruan; Yu-Chin Chen; Ya-Wen Tu', display:{Lore:['[{"text": "arXiv:2205.11748", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning-based automated classification of Chinese Speech Sound Disorders\\u00a7r\\n\\n\\u00a78\\u00a7oYao-Ming Kuo\\nShanq-Jang Ruan\\nYu-Chin Chen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.11748\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3390/children9070996\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nChildren 2022, 9, 996\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 6 Jul 2022 09:24:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oChildren 2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Xulong Zhang; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2205.11817", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSinger Identification for Metaverse with Timbral and Middle-Level Perceptual Features\\u00a7r\\n\\n\\u00a78\\u00a7oXulong Zhang\\nJianzong Wang\\nNing Cheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.11817\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 24 May 2022 06:30:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IJCNN2022 (The 2022 International Joint Conference on Neural Networks). arXiv admin note: text overlap with arXiv:2002.06817 by other authors\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Xulong Zhang; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2205.11821", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMetaSID: Singer Identification with Domain Adaptation for Metaverse\\u00a7r\\n\\n\\u00a78\\u00a7oXulong Zhang\\nJianzong Wang\\nNing Cheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.11821\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 24 May 2022 06:36:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IJCNN2022 (The 2022 International Joint Conference on Neural Networks)\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Xulong Zhang; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2205.11824", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTDASS: Target Domain Adaptation Speech Synthesis Framework for Multi-speaker Low-Resource TTS\\u00a7r\\n\\n\\u00a78\\u00a7oXulong Zhang\\nJianzong Wang\\nNing Cheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.11824\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 24 May 2022 06:41:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IJCNN2022 (The 2022 International Joint Conference on Neural Networks)\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Xulong Zhang; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2205.11841", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSUSing: SU-net for Singing Voice Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oXulong Zhang\\nJianzong Wang\\nNing Cheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.11841\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 24 May 2022 07:05:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IJCNN2022 (The 2022 International Joint Conference on Neural Networks)\\u00a7r"}']}
{title:'Ansari et al. (§72022§r)', author: 'Zohreh Ansari; Farzin Pourhoseini; Fatemeh Hadaeghi', display:{Lore:['[{"text": "arXiv:2205.12594", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHeterogeneous Reservoir Computing Models for Persian Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZohreh Ansari\\nFarzin Pourhoseini\\nFatemeh Hadaeghi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.12594\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 25 May 2022 09:15:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper was accepted for oral presentation in IEEE WCCI 2022 + IJCNN 2022, special session on Reservoir Computing: algorithms, implementations and applications\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Nan Zhang; Jianzong Wang; Zhenhou Hong; Chendong Zhao; Xiaoyang Qu; Jing Xiao', display:{Lore:['[{"text": "arXiv:2205.13249", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDT-SV: A Transformer-based Time-domain Approach for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oNan Zhang\\nJianzong Wang\\nZhenhou Hong\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.13249\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 26 May 2022 09:36:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IJCNN2022 (The 2022 International Joint Conference on Neural Networks)\\u00a7r"}']}
{title:'Dohi et al. (§72022§r)', author: 'Kota Dohi; Tomoya Nishida; Harsh Purohit; Ryo Tanabe; Takashi Endo; Masaaki Yamamoto; Yuki Nikaido; Yohei Kawaguchi', display:{Lore:['[{"text": "arXiv:2205.13879", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMIMII DG: Sound Dataset for Malfunctioning Industrial Machine Investigation and Inspection for Domain Generalization Task\\u00a7r\\n\\n\\u00a78\\u00a7oKota Dohi\\nTomoya Nishida\\nHarsh Purohit\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.13879\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 22 Nov 2022 02:14:02 GMT)\\u00a7r"}']}
{title:'Luo et al. (§72022§r)', author: 'Jian Luo; Jianzong Wang; Ning Cheng; Haobin Tang; Jing Xiao', display:{Lore:['[{"text": "arXiv:2205.14329", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Augmentation Based Unsupervised Learning for Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oJian Luo\\nJianzong Wang\\nNing Cheng\\nHaobin Tang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.14329\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 28 May 2022 04:11:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by WCCI 2022\\u00a7r"}']}
{title:'Zhou et al. (§72022§r)', author: 'Liguang Zhou; Yuhongze Zhou; Xiaonan Qi; Junjie Hu; Tin Lun Lam; Yangsheng Xu', display:{Lore:['[{"text": "arXiv:2205.14411", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFeature Pyramid Attention based Residual Neural Network for Environmental Sound Classification\\u00a7r\\n\\n\\u00a78\\u00a7oLiguang Zhou\\nYuhongze Zhou\\nXiaonan Qi\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.14411\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 28 May 2022 12:06:11 GMT)\\u00a7r"}']}
{title:'Guo et al. (§72022§r)', author: 'Hanqing Guo; Qiben Yan; Nikolay Ivanov; Ying Zhu; Li Xiao; Eric J. Hunter', display:{Lore:['[{"text": "arXiv:2205.14496", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSuperVoice: Text-Independent Speaker Verification Using Ultrasound Energy in Human Speech\\u00a7r\\n\\n\\u00a78\\u00a7oHanqing Guo\\nQiben Yan\\nNikolay Ivanov\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.14496\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 28 May 2022 18:00:50 GMT)\\u00a7r"}']}
{title:'Arshad et al. (§72022§r)', author: 'Syeda Rabia Arshad; Syed Mujtaba Haider; Abdul Basit Mughal', display:{Lore:['[{"text": "arXiv:2205.14649", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Identification using Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSyeda Rabia Arshad\\nSyed Mujtaba Haider\\nAbdul Basit Mughal\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.14649\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 29 May 2022 13:03:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o3 pages\\u00a7r"}']}
{title:'Hung et al. (§72022§r)', author: 'Yun-Ning Hung; Ju-Chiang Wang; Xuchen Song; Wei-Tsung Lu; Minz Won', display:{Lore:['[{"text": "arXiv:2205.14701", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModeling Beats and Downbeats with a Time-Frequency Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oYun-Ning Hung\\nJu-Chiang Wang\\nXuchen Song\\nWei-Tsung Lu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.14701\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 29 May 2022 16:01:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper is accepted for publication at ICASSP 2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Shimin Zhang; Ziteng Wang; Yukai Ju; Yihui Fu; Yueyue Na; Qiang Fu; Lei Xie', display:{Lore:['[{"text": "arXiv:2205.15195", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPersonalized Acoustic Echo Cancellation for Full-duplex Communications\\u00a7r\\n\\n\\u00a78\\u00a7oShimin Zhang\\nZiteng Wang\\nYukai Ju\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.15195\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 30 Jun 2022 02:50:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to INTERSPEECH 22\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Sungwon Kim; Heeseung Kim; Sungroh Yoon', display:{Lore:['[{"text": "arXiv:2205.15370", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGuided-TTS 2: A Diffusion Model for High-quality Adaptive Text-to-Speech with Untranscribed Data\\u00a7r\\n\\n\\u00a78\\u00a7oSungwon Kim\\nHeeseung Kim\\nSungroh Yoon\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.15370\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 May 2022 18:30:20 GMT)\\u00a7r"}']}
{title:'Song et al. (§72022§r)', author: 'Kun Song; Heyang Xue; Xinsheng Wang; Jian Cong; Yongmao Zhang; Lei Xie; Bing Yang; Xiong Zhang; Dan Su', display:{Lore:['[{"text": "arXiv:2206.00208", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaVITS: Tiny VITS for Low Computing Resource Speaker Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oKun Song\\nHeyang Xue\\nXinsheng Wang\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.00208\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 2 Nov 2022 13:04:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ISCSLP 2022\\u00a7r"}']}
{title:'Mao et al. (§72022§r)', author: 'Shunqi Mao; Chaoyi Zhang; Heng Wang; Weidong Cai', display:{Lore:['[{"text": "arXiv:2206.00393", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Generalisable Audio Representations for Audio-Visual Navigation\\u00a7r\\n\\n\\u00a78\\u00a7oShunqi Mao\\nChaoyi Zhang\\nHeng Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.00393\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Jun 2022 11:00:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCVPR 2022 Embodied AI Workshop\\u00a7r"}']}
{title:'Agrawal (§72022§r)', author: 'Ruchit Agrawal', display:{Lore:['[{"text": "arXiv:2206.00454", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Context-Aware Neural Performance-Score Synchronisation\\u00a7r\\n\\n\\u00a78\\u00a7oRuchit Agrawal\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.00454\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 31 May 2022 16:45:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPhD Thesis, Queen Mary University of London (190 pages)\\u00a7r"}']}
{title:'Lovenia et al. (§72022§r)', author: 'Holy Lovenia; Hiroki Tanaka; Sakriani Sakti; Ayu Purwarianti; Satoshi Nakamura', display:{Lore:['[{"text": "arXiv:2206.00635", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Artifact Removal from EEG Recordings of Spoken Word Production with Tensor Decomposition\\u00a7r\\n\\n\\u00a78\\u00a7oHoly Lovenia\\nHiroki Tanaka\\nSakriani Sakti\\nAyu Purwarianti\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.00635\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8682414\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2019 IEEE International Conference on Acoustics, Speech and Signal\\n  Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Jun 2022 17:10:23 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Yijie Liu; Yanfang Yin; Qigang Zhu; Wenzhuo Cui', display:{Lore:['[{"text": "arXiv:2206.00901", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusical Instrument Recognition by XGBoost Combining Feature Fusion\\u00a7r\\n\\n\\u00a78\\u00a7oYijie Liu\\nYanfang Yin\\nQigang Zhu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.00901\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 Jun 2022 07:32:33 GMT)\\u00a7r"}']}
{title:'Cancino-Chacón et al. (§72022§r)', author: 'Carlos Cancino-Chacón; Silvan David Peter; Emmanouil Karystinaios; Francesco Foscarin; Maarten Grachten; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2206.01071", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.DL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPartitura: A Python Package for Symbolic Music Processing\\u00a7r\\n\\n\\u00a78\\u00a7oCarlos Cancino-Chac\\u00f3n\\nSilvan David Peter\\nEmmanouil Karystinaios\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.01071\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the Music Encoding Conference (MEC), 2022, Halifax,\\n  Canada\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 Jun 2022 14:39:32 GMT)\\u00a7r"}']}
{title:'Foscarin et al. (§72022§r)', author: 'Francesco Foscarin; Emmanouil Karystinaios; Silvan David Peter; Carlos Cancino-Chacón; Maarten Grachten; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2206.01104", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.DL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe match file format: Encoding Alignments between Scores and Performances\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesco Foscarin\\nEmmanouil Karystinaios\\nSilvan David Peter\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.01104\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the Music Encoding Conference (MEC), 2022, Halifax,\\n  Canada\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 Jun 2022 15:33:25 GMT)\\u00a7r"}']}
{title:'Xu (§72022§r)', author: 'Qi Xu', display:{Lore:['[{"text": "arXiv:2206.01305", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Musical Arrow of Time \\u2013 The Role of Temporal Asymmetry in Music and Its Organicist Implications\\u00a7r\\n\\n\\u00a78\\u00a7oQi Xu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.01305\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 Jun 2022 21:19:11 GMT)\\u00a7r"}']}
{title:'Cuervo et al. (§72022§r)', author: 'Santiago Cuervo; Adrian Łańcucki; Ricard Marxer; Paweł Rychlikowski; Jan Chorowski', display:{Lore:['[{"text": "arXiv:2206.02211", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVariable-rate hierarchical CPC leads to acoustic unit discovery in speech\\u00a7r\\n\\n\\u00a78\\u00a7oSantiago Cuervo\\nAdrian \\u0141a\\u0144cucki\\nRicard Marxer\\nPawe\\u0142 Rychlikowski\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.02211\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nAdvances in Neural Information Processing Systems, 2022\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 4 Dec 2022 08:24:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to 36thConference on Neural Information ProcessingSystems (NeurIPS 2022)\\u00a7r"}']}
{title:'Levkovitch et al. (§72022§r)', author: 'Alon Levkovitch; Eliya Nachmani; Lior Wolf', display:{Lore:['[{"text": "arXiv:2206.02246", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero-Shot Voice Conditioning for Denoising Diffusion TTS Models\\u00a7r\\n\\n\\u00a78\\u00a7oAlon Levkovitch\\nEliya Nachmani\\nLior Wolf\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.02246\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 22 Jun 2022 09:22:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Xiaofeng Liu; Fangxu Xing; Jerry L. Prince; Jiachen Zhuo; Maureen Stone; Georges El Fakhri; Jonghye Woo', display:{Lore:['[{"text": "arXiv:2206.02284", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTagged-MRI Sequence to Audio Synthesis via Self Residual Attention Guided Heterogeneous Translator\\u00a7r\\n\\n\\u00a78\\u00a7oXiaofeng Liu\\nFangxu Xing\\nJerry L. Prince\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.02284\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 25 Sep 2022 17:54:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMICCAI2022 (early accept, Oral Presentation 3\\u00a7r"}']}
{title:'Serrà et al. (§72022§r)', author: 'Joan Serrà; Santiago Pascual; Jordi Pons; R. Oguz Araz; Davide Scaini', display:{Lore:['[{"text": "arXiv:2206.03065", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUniversal Speech Enhancement with Score-based Diffusion\\u00a7r\\n\\n\\u00a78\\u00a7oJoan Serr\\u00e0\\nSantiago Pascual\\nJordi Pons\\nR. Oguz Araz\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.03065\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 16 Sep 2022 10:27:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o24 pages, 6 figures; includes appendix; examples in https://serrjoa.github.io/projects/universe/\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Guangke Chen; Zhe Zhao; Fu Song; Sen Chen; Lingling Fan; Yang Liu', display:{Lore:['[{"text": "arXiv:2206.03351", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAS2T: Arbitrary Source-To-Target Adversarial Attack on Speaker Recognition Systems\\u00a7r\\n\\n\\u00a78\\u00a7oGuangke Chen\\nZhe Zhao\\nFu Song\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.03351\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Jun 2022 14:38:55 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Guangke Chen; Zhe Zhao; Fu Song; Sen Chen; Lingling Fan; Feng Wang; Jiashui Wang', display:{Lore:['[{"text": "arXiv:2206.03393", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Understanding and Mitigating Audio Adversarial Examples for Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGuangke Chen\\nZhe Zhao\\nFu Song\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.03393\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Jun 2022 15:38:27 GMT)\\u00a7r"}']}
{title:'Majumder et al. (§72022§r)', author: 'Sagnik Majumder; Changan Chen; Ziad Al-Halah; Kristen Grauman', display:{Lore:['[{"text": "arXiv:2206.04006", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFew-Shot Audio-Visual Learning of Environment Acoustics\\u00a7r\\n\\n\\u00a78\\u00a7oSagnik Majumder\\nChangan Chen\\nZiad Al-Halah\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.04006\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 24 Nov 2022 16:52:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to NeurIPS 2022\\u00a7r"}']}
{title:'Elizalde et al. (§72022§r)', author: 'Benjamin Elizalde; Soham Deshmukh; Mahmoud Al Ismail; Huaming Wang', display:{Lore:['[{"text": "arXiv:2206.04769", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCLAP: Learning Audio Concepts From Natural Language Supervision\\u00a7r\\n\\n\\u00a78\\u00a7oBenjamin Elizalde\\nSoham Deshmukh\\nMahmoud Al Ismail\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.04769\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Jun 2022 21:16:10 GMT)\\u00a7r"}']}
{title:'Suzuki et al. (§72022§r)', author: 'Kohei Suzuki; Shoki Sakamoto; Tadahiro Taniguchi; Hirokazu Kameoka', display:{Lore:['[{"text": "arXiv:2206.04780", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeak Like a Dog: Human to Non-human creature Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oKohei Suzuki\\nShoki Sakamoto\\nTadahiro Taniguchi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.04780\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.23919/APSIPAASC55919.2022.9980306\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2022 Asia-Pacific Signal and Information Processing Association\\n  Annual Summit and Conference (APSIPA ASC) (pp. 1388-1393)\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Jun 2022 22:10:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures\\u00a7r"}']}
{title:'Miyaguchi et al. (§72022§r)', author: 'Anthony Miyaguchi; Jiangyue Yu; Bryan Cheungvivatpant; Dakota Dudley; Aniketh Swain', display:{Lore:['[{"text": "arXiv:2206.04805", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMotif Mining and Unsupervised Representation Learning for BirdCLEF 2022\\u00a7r\\n\\n\\u00a78\\u00a7oAnthony Miyaguchi\\nJiangyue Yu\\nBryan Cheungvivatpant\\nDakota Dudley\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.04805\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 8 Jun 2022 06:58:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to CEUR-WS under LifeCLEF for the BirdCLEF 2022 challenge as a working note\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Yi Li; ShuangLin Li; Yang Sun; Syed Mohsen Naqvi', display:{Lore:['[{"text": "arXiv:2206.04962", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFeature Learning and Ensemble Pre-Tasks Based Self-Supervised Speech Denoising and Dereverberation\\u00a7r\\n\\n\\u00a78\\u00a7oYi Li\\nShuangLin Li\\nYang Sun\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.04962\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 10 Jun 2022 09:22:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2112.11142\\u00a7r"}']}
{title:'Dogan et al. (§72022§r)', author: 'Duygu Dogan; Huang Xie; Toni Heittola; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2206.04984", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero-Shot Audio Classification using Image Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oDuygu Dogan\\nHuang Xie\\nToni Heittola\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.04984\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 10 Jun 2022 10:36:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the European Signal Processing Conference (EUSIPCO) 2022\\u00a7r"}']}
{title:'Braun et al. (§72022§r)', author: 'Franziska Braun; Andreas Erzigkeit; Hartmut Lehfeld; Thomas Hillemacher; Korbinian Riedhammer; Sebastian P. Bayerl', display:{Lore:['[{"text": "arXiv:2206.05018", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGoing Beyond the Cookie Theft Picture Test: Detecting Cognitive Impairments using Acoustic Features\\u00a7r\\n\\n\\u00a78\\u00a7oFranziska Braun\\nAndreas Erzigkeit\\nHartmut Lehfeld\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.05018\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-031-16270-1_36\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 10 Jun 2022 12:04:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the 25th International Conference on Text, Speech and Dialogue (TSD 2022)\\u00a7r"}']}
{title:'Ali et al. (§72022§r)', author: 'Asfand Ali; Danial Nasir; Mohammad Hassan Jawad', display:{Lore:['[{"text": "arXiv:2206.05286", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAHD ConvNet for Speech Emotion Classification\\u00a7r\\n\\n\\u00a78\\u00a7oAsfand Ali\\nDanial Nasir\\nMohammad Hassan Jawad\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.05286\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 21 Jun 2022 12:25:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWrong authors quoted\\u00a7r"}']}
{title:'Hawthorne et al. (§72022§r)', author: 'Curtis Hawthorne; Ian Simon; Adam Roberts; Neil Zeghidour; Josh Gardner; Ethan Manilow; Jesse Engel', display:{Lore:['[{"text": "arXiv:2206.05408", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-instrument Music Synthesis with Spectrogram Diffusion\\u00a7r\\n\\n\\u00a78\\u00a7oCurtis Hawthorne\\nIan Simon\\nAdam Roberts\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.05408\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 12 Dec 2022 19:27:37 GMT)\\u00a7r"}']}
{title:'Dohi et al. (§72022§r)', author: 'Kota Dohi; Keisuke Imoto; Noboru Harada; Daisuke Niizumi; Yuma Koizumi; Tomoya Nishida; Harsh Purohit; Takashi Endo; Masaaki Yamamoto; Yohei Kawaguchi', display:{Lore:['[{"text": "arXiv:2206.05876", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDescription and Discussion on DCASE 2022 Challenge Task 2: Unsupervised Anomalous Sound Detection for Machine Condition Monitoring Applying Domain Generalization Techniques\\u00a7r\\n\\n\\u00a78\\u00a7oKota Dohi\\nKeisuke Imoto\\nNoboru Harada\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.05876\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 22 Nov 2022 02:27:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2106.04492\\u00a7r"}']}
{title:'Kuroyanagi et al. (§72022§r)', author: 'Ibuki Kuroyanagi; Tomoki Hayashi; Kazuya Takeda; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2206.05929", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImprovement of Serial Approach to Anomalous Sound Detection by Incorporating Two Binary Cross-Entropies for Outlier Exposure\\u00a7r\\n\\n\\u00a78\\u00a7oIbuki Kuroyanagi\\nTomoki Hayashi\\nKazuya Takeda\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.05929\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Jun 2022 06:56:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 3 tables, EUSIPCO 2022\\u00a7r"}']}
{title:'Pham et al. (§72022§r)', author: 'Lam Pham; Dat Ngo; Anahid Jalali; Alexander Schindler', display:{Lore:['[{"text": "arXiv:2206.06057", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-complexity deep learning frameworks for acoustic scene classification\\u00a7r\\n\\n\\u00a78\\u00a7oLam Pham\\nDat Ngo\\nAnahid Jalali\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.06057\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Jun 2022 11:41:39 GMT)\\u00a7r"}']}
{title:'A (§72022§r)', author: 'Steve Mathew D A', display:{Lore:['[{"text": "arXiv:2206.06117", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimizing musical chord inversions using the cartesian coordinate system\\u00a7r\\n\\n\\u00a78\\u00a7oSteve Mathew D A\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.06117\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 10 Jun 2022 14:48:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 5 tables\\u00a7r"}']}
{title:'Frusque et al. (§72022§r)', author: 'Gaetan Frusque; Olga Fink', display:{Lore:['[{"text": "arXiv:2206.06126", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Time Series Denoising with Learnable Wavelet Packet Transform\\u00a7r\\n\\n\\u00a78\\u00a7oGaetan Frusque\\nOlga Fink\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.06126\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 15 Nov 2022 17:06:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 13 figures, 8 tables\\u00a7r"}']}
{title:'Triantafyllopoulos et al. (§72022§r)', author: 'Andreas Triantafyllopoulos; Meishu Song; Zijiang Yang; Xin Jing; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2206.06680", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring speaker enrolment for few-shot personalisation in emotional vocalisation prediction\\u00a7r\\n\\n\\u00a78\\u00a7oAndreas Triantafyllopoulos\\nMeishu Song\\nZijiang Yang\\nXin Jing\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.06680\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 20 Jun 2022 15:29:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of the ICML Expressive Vocalizations Workshopand Competition held in conjunction with the 39^th International Conference on MachineLearning, Copyright 2022 by the author(s)\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Yang Liu; Na Tang; Xiaoli Chu; Yang Yang; Jun Wang', display:{Lore:['[{"text": "arXiv:2206.06908", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLPCSE: Neural Speech Enhancement through Linear Predictive Coding\\u00a7r\\n\\n\\u00a78\\u00a7oYang Liu\\nNa Tang\\nXiaoli Chu\\nYang Yang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.06908\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 22 Jun 2022 13:51:36 GMT)\\u00a7r"}']}
{title:'Berjon et al. (§72022§r)', author: 'Pierre Berjon; Rajib Sharma; Avishek Nag; Soumyabrata Dev', display:{Lore:['[{"text": "arXiv:2206.07176", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrequency-centroid features for word recognition of non-native English speakers\\u00a7r\\n\\n\\u00a78\\u00a7oPierre Berjon\\nRajib Sharma\\nAvishek Nag\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.07176\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Jun 2022 21:19:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in IEEEIrish Signals Systems Conference (ISSC), 2022\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Rui Liu; Berrak Sisman; Björn Schuller; Guanglai Gao; Haizhou Li', display:{Lore:['[{"text": "arXiv:2206.07229", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAccurate Emotion Strength Assessment for Seen and Unseen Speech Based on Data-Driven Deep Learning\\u00a7r\\n\\n\\u00a78\\u00a7oRui Liu\\nBerrak Sisman\\nBj\\u00f6rn Schuller\\nGuanglai Gao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.07229\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Jun 2022 01:25:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in INTERSPEECH 2022. 5 pages, 4 figures. Substantial text overlap with arXiv:2110.03156\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Ziyi Chen; Haoran Miao; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2206.07288", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreaming non-autoregressive model for any-to-many voice conversion\\u00a7r\\n\\n\\u00a78\\u00a7oZiyi Chen\\nHaoran Miao\\nPengyuan Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.07288\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Jun 2022 04:04:14 GMT)\\u00a7r"}']}
{title:'Peng et al. (§72022§r)', author: 'Linkai Peng; Yingming Gao; Binghuai Lin; Dengfeng Ke; Yanlu Xie; Jinsong Zhang', display:{Lore:['[{"text": "arXiv:2206.07289", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lText-Aware End-to-end Mispronunciation Detection and Diagnosis\\u00a7r\\n\\n\\u00a78\\u00a7oLinkai Peng\\nYingming Gao\\nBinghuai Lin\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.07289\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Jun 2022 04:08:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oRejected by Interspeech2022\\u00a7r"}']}
{title:'Zhao et al. (§72022§r)', author: 'Shengkui Zhao; Bin Ma; Karn N. Watcharasupat; Woon-Seng Gan', display:{Lore:['[{"text": "arXiv:2206.07293", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFRCRN: Boosting Feature Representation using Frequency Recurrence for Monaural Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oShengkui Zhao\\nBin Ma\\nKarn N. Watcharasupat\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.07293\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 24 Nov 2022 09:19:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe paper has been accepted by ICASSP 2022. 5 pages, 2 figures, 5 tables\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Kai Li; Xiaolin Hu; Yi Luo', display:{Lore:['[{"text": "arXiv:2206.07347", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Use of Deep Mask Estimation Module for Neural Source Separation Systems\\u00a7r\\n\\n\\u00a78\\u00a7oKai Li\\nXiaolin Hu\\nYi Luo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.07347\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Jun 2022 07:57:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Turab et al. (§72022§r)', author: 'Muhammad Turab; Teerath Kumar; Malika Bendechache; Takfarinas Saber', display:{Lore:['[{"text": "arXiv:2206.07511", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating Multi-Feature Selection and Ensembling for Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oMuhammad Turab\\nTeerath Kumar\\nMalika Bendechache\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.07511\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Jun 2022 13:11:08 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Li-Chin Chen; Po-Hsun Chen; Richard Tzong-Han Tsai; Yu Tsao', display:{Lore:['[{"text": "arXiv:2206.07860", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEPG2S: Speech Generation and Speech Enhancement based on Electropalatography and Audio Signals using Multimodal Learning\\u00a7r\\n\\n\\u00a78\\u00a7oLi-Chin Chen\\nPo-Hsun Chen\\nRichard Tzong-Han Tsai\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.07860\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2022.3184636\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Signal Processing Letters, vol. 29, p. 2582-2586, 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Jun 2022 00:33:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted By IEEE Signal Processing Letter\\u00a7r"}']}
{title:'Dai et al. (§72022§r)', author: 'Ziqian Dai; Jianwei Yu; Yan Wang; Nuo Chen; Yanyao Bian; Guangzhi Li; Deng Cai; Dong Yu', display:{Lore:['[{"text": "arXiv:2206.07956", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Prosody Annotation with Pre-Trained Text-Speech Model\\u00a7r\\n\\n\\u00a78\\u00a7oZiqian Dai\\nJianwei Yu\\nYan Wang\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.07956\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Jun 2022 06:54:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by INTERSPEECH2022\\u00a7r"}']}
{title:'Zaragoza-Paredes et al. (§72022§r)', author: 'Josep Zaragoza-Paredes; Javier Naranjo-Alcazar; Valery Naranjo; Pedro Zuccarello', display:{Lore:['[{"text": "arXiv:2206.08007", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDCASE 2022: Comparative Analysis Of CNNs For Acoustic Scene Classification Under Low-Complexity Considerations\\u00a7r\\n\\n\\u00a78\\u00a7oJosep Zaragoza-Paredes\\nJavier Naranjo-Alcazar\\nValery Naranjo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.08007\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Jun 2022 09:03:56 GMT)\\u00a7r"}']}
{title:'Nishimura et al. (§72022§r)', author: 'Yuto Nishimura; Yuki Saito; Shinnosuke Takamichi; Kentaro Tachibana; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2206.08039", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic Modeling for End-to-End Empathetic Dialogue Speech Synthesis Using Linguistic and Prosodic Contexts of Dialogue History\\u00a7r\\n\\n\\u00a78\\u00a7oYuto Nishimura\\nYuki Saito\\nShinnosuke Takamichi\\nKentaro Tachibana\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.08039\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Jun 2022 09:47:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, Accepted for INTERSPEECH2022\\u00a7r"}']}
{title:'Dong et al. (§72022§r)', author: 'Mingyu Dong; Diqun Yan; Rangding Wang', display:{Lore:['[{"text": "arXiv:2206.08170", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Privacy Protection on Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oMingyu Dong\\nDiqun Yan\\nRangding Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.08170\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Jun 2022 13:38:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 6 figures\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Bowen Zhang; Songjun Cao; Xiaoming Zhang; Yike Zhang; Long Ma; Takahiro Shinozaki', display:{Lore:['[{"text": "arXiv:2206.08189", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCenser: Curriculum Semi-supervised Learning for Speech Recognition Based on Self-supervised Pre-training\\u00a7r\\n\\n\\u00a78\\u00a7oBowen Zhang\\nSongjun Cao\\nXiaoming Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.08189\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Jun 2022 09:30:43 GMT)\\u00a7r"}']}
{title:'Hou et al. (§72022§r)', author: 'Yuanbo Hou; Dick Botteldooren', display:{Lore:['[{"text": "arXiv:2206.08233", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvent-related data conditioning for acoustic event classification\\u00a7r\\n\\n\\u00a78\\u00a7oYuanbo Hou\\nDick Botteldooren\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.08233\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Jun 2022 14:57:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2022\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Haibin Wu; Jiawen Kang; Lingwei Meng; Yang Zhang; Xixin Wu; Zhiyong Wu; Hung-yi Lee; Helen Meng', display:{Lore:['[{"text": "arXiv:2206.09131", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTackling Spoofing-Aware Speaker Verification with Multi-Model Fusion\\u00a7r\\n\\n\\u00a78\\u00a7oHaibin Wu\\nJiawen Kang\\nLingwei Meng\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.09131\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 18 Jun 2022 06:41:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Odyssey 2022\\u00a7r"}']}
{title:'Jing et al. (§72022§r)', author: 'Xin Jing; Meishu Song; Andreas Triantafyllopoulos; Zijiang Yang; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2206.09142", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRedundancy Reduction Twins Network: A Training framework for Multi-output Emotion Regression\\u00a7r\\n\\n\\u00a78\\u00a7oXin Jing\\nMeishu Song\\nAndreas Triantafyllopoulos\\nZijiang Yang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.09142\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 28 Jun 2022 09:58:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted by ICML Exvo workshop\\u00a7r"}']}
{title:'Manamperi et al. (§72022§r)', author: 'Wageesha Manamperi; Prasanga N. Samarasinghe; Thushara D. Abhayapala; Jihui Zhang', display:{Lore:['[{"text": "arXiv:2206.09298", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGMM based multi-stage Wiener filtering for low SNR speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oWageesha Manamperi\\nPrasanga N. Samarasinghe\\nThushara D. Abhayapala\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.09298\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Jul 2022 22:34:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, submitted to a conference\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Yi Wang; Yi Si', display:{Lore:['[{"text": "arXiv:2206.09920", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWOLONet: Wave Outlooker for Efficient and High Fidelity Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYi Wang\\nYi Si\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.09920\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Jun 2022 17:58:52 GMT)\\u00a7r"}']}
{title:'Hu et al. (§72022§r)', author: 'Ying Hu; Xiujuan Zhu; Yunlong Li; Hao Huang; Liang He', display:{Lore:['[{"text": "arXiv:2206.10175", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Multi-grained based Attention Network for Semi-supervised Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oYing Hu\\nXiujuan Zhu\\nYunlong Li\\nHao Huang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.10175\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-767\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nINTERSPEECH 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Jun 2022 08:15:27 GMT)\\u00a7r"}']}
{title:'Udagawa et al. (§72022§r)', author: 'Kenta Udagawa; Yuki Saito; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2206.10256", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHuman-in-the-loop Speaker Adaptation for DNN-based Multi-speaker TTS\\u00a7r\\n\\n\\u00a78\\u00a7oKenta Udagawa\\nYuki Saito\\nHiroshi Saruwatari\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.10256\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Jun 2022 11:08:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, Accepted for INTERSPEECH2022\\u00a7r"}']}
{title:'Nada et al. (§72022§r)', author: 'Kayo Nada; Keisuke Imoto; Takao Tsuchiya', display:{Lore:['[{"text": "arXiv:2206.10349", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Analysis of Acoustic Scenes and Sound Events Based on Multitask Learning with Dynamic Weight Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oKayo Nada\\nKeisuke Imoto\\nTakao Tsuchiya\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.10349\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Jun 2022 13:05:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Acoustical Science and Technology\\u00a7r"}']}
{title:'Wuerkaixi et al. (§72022§r)', author: 'Abudukelimu Wuerkaixi; You Zhang; Zhiyao Duan; Changshui Zhang', display:{Lore:['[{"text": "arXiv:2206.10421", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRethinking Audio-visual Synchronization for Active Speaker Detection\\u00a7r\\n\\n\\u00a78\\u00a7oAbudukelimu Wuerkaixi\\nYou Zhang\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.10421\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 10 Jul 2022 05:52:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE International Workshop on Machine Learning for Signal Processing (MLSP 2022)\\u00a7r"}']}
{title:'Xin et al. (§72022§r)', author: 'Detai Xin; Shinnosuke Takamichi; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2206.10695", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring the Effectiveness of Self-supervised Learning and Classifier Chains in Emotion Recognition of Nonverbal Vocalizations\\u00a7r\\n\\n\\u00a78\\u00a7oDetai Xin\\nShinnosuke Takamichi\\nHiroshi Saruwatari\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.10695\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Jun 2022 19:29:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the ICML Expressive Vocalizations Workshopand Competition 2022\\u00a7r"}']}
{title:'Cheuk et al. (§72022§r)', author: 'Kin Wai Cheuk; Keunwoo Choi; Qiuqiang Kong; Bochen Li; Minz Won; Amy Hung; Ju-Chiang Wang; Dorien Herremans', display:{Lore:['[{"text": "arXiv:2206.10805", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJointist: Joint Learning for Multi-instrument Transcription and Its Applications\\u00a7r\\n\\n\\u00a78\\u00a7oKin Wai Cheuk\\nKeunwoo Choi\\nQiuqiang Kong\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.10805\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 28 Jun 2022 22:36:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ISMIR\\u00a7r"}']}
{title:'Song et al. (§72022§r)', author: 'Meishu Song; Zijiang Yang; Andreas Triantafyllopoulos; Xin Jing; Vincent Karas; Xie Jiangjian; Zixing Zhang; Yamamoto Yoshiharu; Bjoern W. Schuller', display:{Lore:['[{"text": "arXiv:2206.11049", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDynamic Restrained Uncertainty Weighting Loss for Multitask Learning of Vocal Expression\\u00a7r\\n\\n\\u00a78\\u00a7oMeishu Song\\nZijiang Yang\\nAndreas Triantafyllopoulos\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.11049\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Jun 2022 19:48:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Zhao et al. (§72022§r)', author: 'Running Zhao; Jiangtao Yu; Tingle Li; Hang Zhao; Edith C. H. Ngai', display:{Lore:['[{"text": "arXiv:2206.11066", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRadio2Speech: High Quality Speech Recovery from Radio Frequency Signals\\u00a7r\\n\\n\\u00a78\\u00a7oRunning Zhao\\nJiangtao Yu\\nTingle Li\\nHang Zhao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.11066\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 Jun 2022 13:29:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Conde et al. (§72022§r)', author: 'Marcos V. Conde; Ui-Jin Choi', display:{Lore:['[{"text": "arXiv:2206.11260", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFew-shot Long-Tailed Bird Audio Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos V. Conde\\nUi-Jin Choi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.11260\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 4 Jul 2022 11:23:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oLifeCLEF2022 (best paper award)\\u00a7r"}']}
{title:'Diehl et al. (§72022§r)', author: 'Peter Udo Diehl; Yosef Singer; Hannes Zilly; Uwe Schönfeld; Paul Meyer-Rachner; Mark Berry; Henning Sprekeler; Elias Sprengel; Annett Pudszuhn; Veit M. Hofmann', display:{Lore:['[{"text": "arXiv:2206.11567", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRestoring speech intelligibility for hearing aid users with deep learning\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Udo Diehl\\nYosef Singer\\nHannes Zilly\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.11567\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Jun 2022 09:26:45 GMT)\\u00a7r"}']}
{title:'Shrem et al. (§72022§r)', author: 'Yosi Shrem; Felix Kreuk; Joseph Keshet', display:{Lore:['[{"text": "arXiv:2206.11632", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFormant Estimation and Tracking using Probabilistic Heat-Maps\\u00a7r\\n\\n\\u00a78\\u00a7oYosi Shrem\\nFelix Kreuk\\nJoseph Keshet\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.11632\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Jun 2022 11:39:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ointerspeech 2022\\u00a7r"}']}
{title:'Xu et al. (§72022§r)', author: 'Junhao Xu; Shoukang Hu; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2206.11643", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Green ASR: Lossless 4-bit Quantization of a Hybrid TDNN System on the 300-hr Switchboard Corpus\\u00a7r\\n\\n\\u00a78\\u00a7oJunhao Xu\\nShoukang Hu\\nXunying Liu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.11643\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Jun 2022 12:02:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2022 Accepted. arXiv admin note: text overlapwith arXiv:2111.14479\\u00a7r"}']}
{title:'Purohit et al. (§72022§r)', author: 'Tilak Purohit; Imen Ben Mahmoud; Bogdan Vlasenko; Mathew Magimai. -Doss', display:{Lore:['[{"text": "arXiv:2206.11968", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparing supervised and self-supervised embedding for ExVo Multi-Task learning track\\u00a7r\\n\\n\\u00a78\\u00a7oTilak Purohit\\nImen Ben Mahmoud\\nBogdan Vlasenko\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.11968\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the ICML 2022 Expressive Vocalizations Workshop and\\n  Competition: Recognizing, Generating, and Personalizing Vocal Bursts\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Jun 2022 20:32:09 GMT)\\u00a7r"}']}
{title:'Elbanna et al. (§72022§r)', author: 'Gasser Elbanna; Neil Scheidwasser-Clow; Mikolaj Kegler; Pierre Beckmann; Karl El Hajal; Milos Cernak', display:{Lore:['[{"text": "arXiv:2206.12038", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBYOL-S: Learning Self-supervised Speech Representations by Bootstrapping\\u00a7r\\n\\n\\u00a78\\u00a7oGasser Elbanna\\nNeil Scheidwasser-Clow\\nMikolaj Kegler\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12038\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 25 Oct 2022 14:22:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to HEAR-PMLR 2021\\u00a7r"}']}
{title:'Lux et al. (§72022§r)', author: 'Florian Lux; Julia Koch; Ngoc Thang Vu', display:{Lore:['[{"text": "arXiv:2206.12229", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExact Prosody Cloning in Zero-Shot Multispeaker Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oFlorian Lux\\nJulia Koch\\nNgoc Thang Vu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12229\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 21 Oct 2022 20:07:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEESLT 2022\\u00a7r"}']}
{title:'Yamamoto et al. (§72022§r)', author: 'Yuya Yamamoto; Juhan Nam; Hiroko Terasawa', display:{Lore:['[{"text": "arXiv:2206.12230", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeformable CNN and Imbalance-Aware Feature Learning for Singing Technique Classification\\u00a7r\\n\\n\\u00a78\\u00a7oYuya Yamamoto\\nJuhan Nam\\nHiroko Terasawa\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12230\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Jun 2022 11:56:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH2022\\u00a7r"}']}
{title:'Demir et al. (§72022§r)', author: 'Kubilay Can Demir; Matthias May; Axel Schmid; Michael Uder; Katharina Breininger; Tobias Weise; Andreas Maier; Seung Hee Yang', display:{Lore:['[{"text": "arXiv:2206.12320", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPoCaP Corpus: A Multimodal Dataset for Smart Operating Room Speech Assistant using Interventional Radiology Workflow Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oKubilay Can Demir\\nMatthias May\\nAxel Schmid\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12320\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Jun 2022 14:39:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 4 figures, Text, Speech and Dialogue 2022 Conference\\u00a7r"}']}
{title:'Anuchitanukul et al. (§72022§r)', author: 'Atijit Anuchitanukul; Lucia Specia', display:{Lore:['[{"text": "arXiv:2206.12469", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBurst2Vec: An Adversarial Multi-Task Approach for Predicting Emotion, Age, and Origin from Vocal Bursts\\u00a7r\\n\\n\\u00a78\\u00a7oAtijit Anuchitanukul\\nLucia Specia\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12469\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 18 Oct 2022 05:48:23 GMT)\\u00a7r"}']}
{title:'Belanich et al. (§72022§r)', author: 'Josh Belanich; Krishna Somandepalli; Brian Eoff; Brendan Jou', display:{Lore:['[{"text": "arXiv:2206.12494", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers\\u00a7r\\n\\n\\u00a78\\u00a7oJosh Belanich\\nKrishna Somandepalli\\nBrian Eoff\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12494\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Jun 2022 21:42:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in the ICML Expressive Vocalizations WorkshopCompetition 2022 (https://www.competitions.hume.ai/exvo2022)\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Byeonggeun Kim; Seunghan Yang; Jangho Kim; Hyunsin Park; Juntae Lee; Simyung Chang', display:{Lore:['[{"text": "arXiv:2206.12513", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDomain Generalization with Relaxed Instance Frequency-wise Normalization for Multi-device Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oByeonggeun Kim\\nSeunghan Yang\\nJangho Kim\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12513\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Jun 2022 23:45:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH 2022\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Yihan Wu; Xi Wang; Shaofei Zhang; Lei He; Ruihua Song; Jian-Yun Nie', display:{Lore:['[{"text": "arXiv:2206.12559", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervised Context-aware Style Representation for Expressive Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYihan Wu\\nXi Wang\\nShaofei Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12559\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 25 Jun 2022 05:29:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Jiralerspong et al. (§72022§r)', author: 'Marco Jiralerspong; Gauthier Gidel', display:{Lore:['[{"text": "arXiv:2206.12563", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerating Diverse Vocal Bursts with StyleGAN2 and MEL-Spectrograms\\u00a7r\\n\\n\\u00a78\\u00a7oMarco Jiralerspong\\nGauthier Gidel\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12563\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 25 Jun 2022 05:39:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published at the ICML Expressive Vocalizations Workshopand Competition (ExVo Generate) held in conjunction with the 39th International Conference on MachineLearning\\u00a7r"}']}
{title:'Sharma et al. (§72022§r)', author: 'Roshan Sharma; Tyler Vuong; Mark Lindsey; Hira Dhamyal; Rita Singh; Bhiksha Raj', display:{Lore:['[{"text": "arXiv:2206.12568", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervision and Learnable STRFs for Age, Emotion, and Country Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oRoshan Sharma\\nTyler Vuong\\nMark Lindsey\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12568\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 39th International Conference on Machine\\n  Learning, Baltimore, Maryland, USA, PMLR 162, 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 25 Jun 2022 06:09:10 GMT)\\u00a7r"}']}
{title:'Hsu (§72022§r)', author: 'Chin-Cheng Hsu', display:{Lore:['[{"text": "arXiv:2206.12662", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynthesizing Personalized Non-speech Vocalization from Discrete Speech Representations\\u00a7r\\n\\n\\u00a78\\u00a7oChin-Cheng Hsu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12662\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 25 Jun 2022 14:27:10 GMT)\\u00a7r"}']}
{title:'Joshi et al. (§72022§r)', author: 'Raviraj Joshi; Subodh Kumar', display:{Lore:['[{"text": "arXiv:2206.12829", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Comparison of Encoders for Attention based End to End Speech Recognition in Standalone and Rescoring Mode\\u00a7r\\n\\n\\u00a78\\u00a7oRaviraj Joshi\\nSubodh Kumar\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12829\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/SPCOM55316.2022.9840823\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 26 Jun 2022 09:12:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at SPCOM 2022\\u00a7r"}']}
{title:'Ho et al. (§72022§r)', author: 'Tuan Vu Ho; Maori Kobayashi; Masato Akagi', display:{Lore:['[{"text": "arXiv:2206.13021", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeak Like a Professional: Increasing Speech Intelligibility by Mimicking Professional Announcer Voice with Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oTuan Vu Ho\\nMaori Kobayashi\\nMasato Akagi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13021\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Jun 2022 02:39:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2022\\u00a7r"}']}
{title:'Ye et al. (§72022§r)', author: 'Tong Ye; Shijing Si; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2206.13071", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUncertainty Calibration for Deep Audio Classifiers\\u00a7r\\n\\n\\u00a78\\u00a7oTong Ye\\nShijing Si\\nJianzong Wang\\nNing Cheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13071\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Jun 2022 06:33:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by InterSpeech 2022, the first two authors contributed equally\\u00a7r"}']}
{title:'Wyse et al. (§72022§r)', author: 'Lonce Wyse; Purnima Kamath; Chitralekha Gupta', display:{Lore:['[{"text": "arXiv:2206.13085", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Model Factory: An Integrated System Architecture for Generative Audio Modelling\\u00a7r\\n\\n\\u00a78\\u00a7oLonce Wyse\\nPurnima Kamath\\nChitralekha Gupta\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13085\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInternational Conference on Computational Intelligence in Music,\\n  Sound, Art and Design (Part of EvoStar) (pp. 308-322). Springer, Cham. 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Jun 2022 07:10:22 GMT)\\u00a7r"}']}
{title:'Kang et al. (§72022§r)', author: 'Zuheng Kang; Junqing Peng; Jianzong Wang; Jing Xiao', display:{Lore:['[{"text": "arXiv:2206.13101", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeechEQ: Speech Emotion Recognition based on Multi-scale Unified Datasets and Multitask Learning\\u00a7r\\n\\n\\u00a78\\u00a7oZuheng Kang\\nJunqing Peng\\nJianzong Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13101\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 28 Jul 2022 01:43:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper is accepted by Interspeech 2022\\u00a7r"}']}
{title:'Fan et al. (§72022§r)', author: 'Zhiyun Fan; Linhao Dong; Meng Cai; Zejun Ma; Bo Xu', display:{Lore:['[{"text": "arXiv:2206.13110", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSequence-level Speaker Change Detection with Difference-based Continuous Integrate-and-fire\\u00a7r\\n\\n\\u00a78\\u00a7oZhiyun Fan\\nLinhao Dong\\nMeng Cai\\nZejun Ma\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13110\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2022.3185955\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Jun 2022 08:29:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSignal Processing Letters 2022\\u00a7r"}']}
{title:'Hou et al. (§72022§r)', author: 'Zhongshu Hou; Qinwen Hu; Kai Chen; Jing Lu', display:{Lore:['[{"text": "arXiv:2206.13136", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA two-stage full-band speech enhancement model with effective spectral compression mapping\\u00a7r\\n\\n\\u00a78\\u00a7oZhongshu Hou\\nQinwen Hu\\nKai Chen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13136\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Jun 2022 09:30:53 GMT)\\u00a7r"}']}
{title:'Parikh et al. (§72022§r)', author: 'Rahil Parikh; Harshavardhan Sundar; Ming Sun; Chao Wang; Spyros Matsoukas', display:{Lore:['[{"text": "arXiv:2206.13476", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImpact of Acoustic Event Tagging on Scene Classification in a Multi-Task Learning Framework\\u00a7r\\n\\n\\u00a78\\u00a7oRahil Parikh\\nHarshavardhan Sundar\\nMing Sun\\nChao Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13476\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Jun 2022 17:38:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ISCAInterspeech 2022\\u00a7r"}']}
{title:'Chatterjee et al. (§72022§r)', author: 'Ishan Chatterjee; Maruchi Kim; Vivek Jayaram; Shyamnath Gollakota; Ira Kemelmacher-Shlizerman; Shwetak Patel; Steven M. Seitz', display:{Lore:['[{"text": "arXiv:2206.13611", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lClearBuds: Wireless Binaural Earbuds for Learning-Based Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oIshan Chatterjee\\nMaruchi Kim\\nVivek Jayaram\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13611\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3498361.3538933\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Jun 2022 20:09:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, Published in Mobisys 2022\\u00a7r"}']}
{title:'Luo et al. (§72022§r)', author: 'Jian Luo; Jianzong Wang; Ning Cheng; Edward Xiao; Xulong Zhang; Jing Xiao', display:{Lore:['[{"text": "arXiv:2206.13689", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTiny-Sepformer: A Tiny Time-Domain Transformer Network for Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oJian Luo\\nJianzong Wang\\nNing Cheng\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13689\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 30 Jun 2022 08:33:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Byeonggeun Kim; Seunghan Yang; Inseop Chung; Simyung Chang', display:{Lore:['[{"text": "arXiv:2206.13691", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDummy Prototypical Networks for Few-Shot Open-Set Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oByeonggeun Kim\\nSeunghan Yang\\nInseop Chung\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13691\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Jun 2022 01:56:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH 2022\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Seunghan Yang; Debasmit Das; Janghoon Cho; Hyoungwoo Park; Sungrack Yun', display:{Lore:['[{"text": "arXiv:2206.13700", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDomain Agnostic Few-shot Learning for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oSeunghan Yang\\nDebasmit Das\\nJanghoon Cho\\nHyoungwoo Park\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13700\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Jun 2022 02:22:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH 2022\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Seunghan Yang; Byeonggeun Kim; Inseop Chung; Simyung Chang', display:{Lore:['[{"text": "arXiv:2206.13708", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPersonalized Keyword Spotting through Multi-task Learning\\u00a7r\\n\\n\\u00a78\\u00a7oSeunghan Yang\\nByeonggeun Kim\\nInseop Chung\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13708\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Jun 2022 02:48:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH 2022\\u00a7r"}']}
{title:'Kunikoshi et al. (§72022§r)', author: 'Aki Kunikoshi; Jaebok Kim; Wonsuk Jun; Kåre Sjölander', display:{Lore:['[{"text": "arXiv:2206.13817", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparison of Speech Representations for the MOS Prediction System\\u00a7r\\n\\n\\u00a78\\u00a7oAki Kunikoshi\\nJaebok Kim\\nWonsuk Jun\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13817\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Jun 2022 08:18:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Byeonggeun Kim; Seunghan Yang; Jangho Kim; Simyung Chang', display:{Lore:['[{"text": "arXiv:2206.13909", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQTI Submission to DCASE 2021: residual normalization for device-imbalanced acoustic scene classification with efficient design\\u00a7r\\n\\n\\u00a78\\u00a7oByeonggeun Kim\\nSeunghan Yang\\nJangho Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13909\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 25 Oct 2022 05:24:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7otech report; won 1st place inDCASE2021 challenge. arXiv admin note: substantial text overlap with arXiv:2111.06531\\u00a7r"}']}
{title:'Kawa et al. (§72022§r)', author: 'Piotr Kawa; Marcin Plata; Piotr Syga', display:{Lore:['[{"text": "arXiv:2206.13979", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttack Agnostic Dataset: Towards Generalization and Stabilization of Audio DeepFake Detection\\u00a7r\\n\\n\\u00a78\\u00a7oPiotr Kawa\\nMarcin Plata\\nPiotr Syga\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13979\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10078\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 21 Jul 2022 15:11:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH 2022 (Updated version: corrected ASVspoof dataset description)\\u00a7r"}']}
{title:'Koh et al. (§72022§r)', author: 'Andrew Koh; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2206.14659", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLanguage-Based Audio Retrieval with Converging Tied Layers and Contrastive Loss\\u00a7r\\n\\n\\u00a78\\u00a7oAndrew Koh\\nEng Siong Chng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.14659\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 29 Jun 2022 13:59:19 GMT)\\u00a7r"}']}
{title:'Nistal et al. (§72022§r)', author: 'Javier Nistal; Cyran Aouameur; Ithan Velarde; Stefan Lattner', display:{Lore:['[{"text": "arXiv:2206.14723", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDrumGAN VST: A Plugin for Drum Sound Analysis/Synthesis With Autoencoding Generative Adversarial Networks\\u00a7r\\n\\n\\u00a78\\u00a7oJavier Nistal\\nCyran Aouameur\\nIthan Velarde\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.14723\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 29 Jun 2022 15:44:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 2 figures, 3 tables, ICML2022 Machine Learning for Audio Synthesis (MLAS) Workshop, for sound examples visit https://cslmusicteam.sony.fr/drumgan-vst/\\u00a7r"}']}
{title:'Duan et al. (§72022§r)', author: 'Wei Duan; Zhe Zhang; Yi Yu; Keizo Oyama', display:{Lore:['[{"text": "arXiv:2206.15027", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInterpretable Melody Generation from Lyrics with Discrete-Valued Adversarial Training\\u00a7r\\n\\n\\u00a78\\u00a7oWei Duan\\nZhe Zhang\\nYi Yu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.15027\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Jun 2022 05:45:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o3 pages, 3 figures\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Szu-Jui Chen; Jiamin Xie; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:2206.15056", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFeaRLESS: Feature Refinement Loss for Ensembling Self-Supervised Learning Features in Robust End-to-end Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSzu-Jui Chen\\nJiamin Xie\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.15056\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Jun 2022 06:39:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for Interspeech 2022\\u00a7r"}']}
{title:'Yoon et al. (§72022§r)', author: 'Hyun-Wook Yoon; Ohsung Kwon; Hoyeon Lee; Ryuichi Yamamoto; Eunwoo Song; Jae-Min Kim; Min-Jae Hwang', display:{Lore:['[{"text": "arXiv:2206.15067", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLanguage Model-Based Emotion Prediction Methods for Emotional Speech Synthesis Systems\\u00a7r\\n\\n\\u00a78\\u00a7oHyun-Wook Yoon\\nOhsung Kwon\\nHoyeon Lee\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.15067\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 1 Jul 2022 01:13:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH2022\\u00a7r"}']}
{title:'Choi et al. (§72022§r)', author: 'Yeonjong Choi; Chao Xie; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2206.15155", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Evaluation of Three-Stage Voice Conversion Framework for Noisy and Reverberant Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oYeonjong Choi\\nChao Xie\\nTomoki Toda\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.15155\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Jun 2022 09:39:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Lerch (§72022§r)', author: 'Alexander Lerch', display:{Lore:['[{"text": "arXiv:2206.15219", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7llibACA, pyACA, and ACA-Code: Audio Content Analysis in 3 Languages\\u00a7r\\n\\n\\u00a78\\u00a7oAlexander Lerch\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.15219\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Jun 2022 12:09:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint submitted to \\"Software Impacts\\"\\u00a7r"}']}
{title:'Kastner et al. (§72022§r)', author: 'Kyle Kastner; Aaron Courville', display:{Lore:['[{"text": "arXiv:2206.15276", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lR-MelNet: Reduced Mel-Spectral Modeling for Neural TTS\\u00a7r\\n\\n\\u00a78\\u00a7oKyle Kastner\\nAaron Courville\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.15276\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Jun 2022 13:29:31 GMT)\\u00a7r"}']}
{title:'Matinfar et al. (§72022§r)', author: 'Sasan Matinfar; Mehrdad Salehi; Daniel Suter; Matthias Seibold; Navid Navab; Shervin Dehghani; Florian Wanivenhaus; Philipp Fürnstahl; Mazda Farshad; Nassir Navab', display:{Lore:['[{"text": "arXiv:2206.15291", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSonification as a Reliable Alternative to Conventional Visual Surgical Navigation\\u00a7r\\n\\n\\u00a78\\u00a7oSasan Matinfar\\nMehrdad Salehi\\nDaniel Suter\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.15291\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Jun 2022 13:49:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o19 pages, 7 figures\\u00a7r"}']}
{title:'Markovic et al. (§72022§r)', author: 'Dejan Markovic; Alexandre Defossez; Alexander Richard', display:{Lore:['[{"text": "arXiv:2206.15423", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImplicit Neural Spatial Filtering for Multichannel Source Separation in the Waveform Domain\\u00a7r\\n\\n\\u00a78\\u00a7oDejan Markovic\\nAlexandre Defossez\\nAlexander Richard\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.15423\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Jun 2022 17:13:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2022\\u00a7r"}']}
{title:'Lee (§72022§r)', author: 'Anthony Lee', display:{Lore:['[{"text": "arXiv:2206.15426", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVolume-Independent Music Matching by Frequency Spectrum Comparison\\u00a7r\\n\\n\\u00a78\\u00a7oAnthony Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.15426\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Jun 2022 16:58:13 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Muqiao Yang; Joseph Konan; David Bick; Anurag Kumar; Shinji Watanabe; Bhiksha Raj', display:{Lore:['[{"text": "arXiv:2207.00237", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Speech Enhancement through Fine-Grained Speech Characteristics\\u00a7r\\n\\n\\u00a78\\u00a7oMuqiao Yang\\nJoseph Konan\\nDavid Bick\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.00237\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 11 Jul 2022 17:58:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at InterSpeech 2022\\u00a7r"}']}
{title:'Kamil et al. (§72022§r)', author: 'Deja Kamil; Sanchez Ariadna; Roth Julian; Cotescu Marius', display:{Lore:['[{"text": "arXiv:2207.00344", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Evaluation of Speaker Similarity\\u00a7r\\n\\n\\u00a78\\u00a7oDeja Kamil\\nSanchez Ariadna\\nRoth Julian\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.00344\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Jul 2022 11:23:16 GMT)\\u00a7r"}']}
{title:'Patterson et al. (§72022§r)', author: 'Katharine Patterson; Kevin Wilson; Scott Wisdom; John R. Hershey', display:{Lore:['[{"text": "arXiv:2207.00562", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDistance-Based Sound Separation\\u00a7r\\n\\n\\u00a78\\u00a7oKatharine Patterson\\nKevin Wilson\\nScott Wisdom\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.00562\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Jul 2022 17:27:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at Interspeech 2022\\u00a7r"}']}
{title:'Xue et al. (§72022§r)', author: 'Liumeng Xue; Shan Yang; Na Hu; Dan Su; Lei Xie', display:{Lore:['[{"text": "arXiv:2207.00756", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Noise-independent Speech Representation for High-quality Voice Conversion for Noisy Target Speakers\\u00a7r\\n\\n\\u00a78\\u00a7oLiumeng Xue\\nShan Yang\\nNa Hu\\nDan Su\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.00756\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Jul 2022 06:51:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2022\\u00a7r"}']}
{title:'Bassan et al. (§72022§r)', author: 'Shahaf Bassan; Yossi Adi; Jeffrey S. Rosenschein', display:{Lore:['[{"text": "arXiv:2207.00760", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Symbolic Music Segmentation using Ensemble Temporal Prediction Errors\\u00a7r\\n\\n\\u00a78\\u00a7oShahaf Bassan\\nYossi Adi\\nJeffrey S. Rosenschein\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.00760\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Jul 2022 07:13:54 GMT)\\u00a7r"}']}
{title:'Sun et al. (§72022§r)', author: 'Guangzhi Sun; Chao Zhang; Philip C. Woodland', display:{Lore:['[{"text": "arXiv:2207.00857", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTree-constrained Pointer Generator with Graph Neural Network Encodings for Contextual Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGuangzhi Sun\\nChao Zhang\\nPhilip C. Woodland\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.00857\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Jul 2022 15:12:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Interspeech 2022. arXiv admin note: text overlap with arXiv:2205.09058\\u00a7r"}']}
{title:'Wei et al. (§72022§r)', author: 'Kun Wei; Pengcheng Guo; Ning Jiang', display:{Lore:['[{"text": "arXiv:2207.00883", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Transformer-based Conversational ASR by Inter-Sentential Attention Mechanism\\u00a7r\\n\\n\\u00a78\\u00a7oKun Wei\\nPengcheng Guo\\nNing Jiang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.00883\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Jul 2022 17:17:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech2022\\u00a7r"}']}
{title:'Xue et al. (§72022§r)', author: 'Huaying Xue; Xiulian Peng; Xue Jiang; Yan Lu', display:{Lore:['[{"text": "arXiv:2207.00993", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Error-Resilient Neural Speech Coding\\u00a7r\\n\\n\\u00a78\\u00a7oHuaying Xue\\nXiulian Peng\\nXue Jiang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.00993\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 3 Jul 2022 09:38:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, Interspeech 2022(Accepted)\\u00a7r"}']}
{title:'Stoidis et al. (§72022§r)', author: 'Dimitrios Stoidis; Andrea Cavallaro', display:{Lore:['[{"text": "arXiv:2207.01052", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerating gender-ambiguous voices for privacy-preserving speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDimitrios Stoidis\\nAndrea Cavallaro\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.01052\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 3 Jul 2022 14:23:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, submitted to INTERSPEECH\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Xiaoyu Wang; Xiangyu Kong; Xiulian Peng; Yan Lu', display:{Lore:['[{"text": "arXiv:2207.01197", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Modal Multi-Correlation Learning for Audio-Visual Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoyu Wang\\nXiangyu Kong\\nXiulian Peng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.01197\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Jul 2022 04:53:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted by interspeech2022\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Tao Li; Xinsheng Wang; Qicong Xie; Zhichao Wang; Mingqi Jiang; Lei Xie', display:{Lore:['[{"text": "arXiv:2207.01198", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-speaker Emotion Transfer Based On Prosody Compensation for End-to-End Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oTao Li\\nXinsheng Wang\\nQicong Xie\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.01198\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Jul 2022 04:56:10 GMT)\\u00a7r"}']}
{title:'Guan et al. (§72022§r)', author: 'Yuansheng Guan; Guochen Yu; Andong Li; Chengshi Zheng; Jie Wang', display:{Lore:['[{"text": "arXiv:2207.01255", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTMGAN-PLC: Audio Packet Loss Concealment using Temporal Memory Generative Adversarial Network\\u00a7r\\n\\n\\u00a78\\u00a7oYuansheng Guan\\nGuochen Yu\\nAndong Li\\nChengshi Zheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.01255\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Jul 2022 08:27:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by INTERSPEECH 2022\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Zhanheng Yang; Hang Lv; Xiong Wang; Ao Zhang; Lei Xie', display:{Lore:['[{"text": "arXiv:2207.01261", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMinimizing Sequential Confusion Error in Speech Command Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhanheng Yang\\nHang Lv\\nXiong Wang\\nAo Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.01261\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Jul 2022 08:50:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Zhanheng Yang; Sining Sun; Jin Li; Xiaoming Zhang; Xiong Wang; Long Ma; Lei Xie', display:{Lore:['[{"text": "arXiv:2207.01267", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCaTT-KWS: A Multi-stage Customized Keyword Spotting Framework based on Cascaded Transducer-Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oZhanheng Yang\\nSining Sun\\nJin Li\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.01267\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Jul 2022 09:01:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Lattner et al. (§72022§r)', author: 'Stefan Lattner; Javier Nistal', display:{Lore:['[{"text": "arXiv:2207.01667", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStochastic Restoration of Heavily Compressed Musical Audio using Generative Adversarial Networks\\u00a7r\\n\\n\\u00a78\\u00a7oStefan Lattner\\nJavier Nistal\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.01667\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3390/electronics10111349\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nMDPI Electronics 2021, 10, 1349\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Jul 2022 18:33:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o21 pages, 5 figures, published in MDPI Electronics Special Issue \\"Machine Learning Applied to Music/Audio Signal Processing\\"\\u00a7r"}']}
{title:'Santos et al. (§72022§r)', author: 'Gustavo Amaral Costa dos Santos; Augusto Baffa; Jean-Pierre Briot; Bruno Feijó; Antonio Luz Furtado', display:{Lore:['[{"text": "arXiv:2207.01698", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn adaptive music generation architecture for games based on the deep learning Transformer mode\\u00a7r\\n\\n\\u00a78\\u00a7oGustavo Amaral Costa dos Santos\\nAugusto Baffa\\nJean-Pierre Briot\\nBruno Feij\\u00f3\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.01698\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 10 Sep 2022 15:46:27 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Li Zhang; Yue Li; Huan Zhao; Qing Wang; Lei Xie', display:{Lore:['[{"text": "arXiv:2207.01802", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBackend Ensemble for Speaker Verification and Spoofing Countermeasure\\u00a7r\\n\\n\\u00a78\\u00a7oLi Zhang\\nYue Li\\nHuan Zhao\\nQing Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.01802\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 23 Sep 2022 09:14:27 GMT)\\u00a7r"}']}
{title:'Lei et al. (§72022§r)', author: 'Yi Lei; Shan Yang; Jian Cong; Lei Xie; Dan Su', display:{Lore:['[{"text": "arXiv:2207.01832", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGlow-WaveGAN 2: High-quality Zero-shot Text-to-speech Synthesis and Any-to-any Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oYi Lei\\nShan Yang\\nJian Cong\\nLei Xie\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.01832\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Jul 2022 06:31:55 GMT)\\u00a7r"}']}
{title:'Siahkoohi et al. (§72022§r)', author: 'Ali Siahkoohi; Michael Chinen; Tom Denton; W. Bastiaan Kleijn; Jan Skoglund', display:{Lore:['[{"text": "arXiv:2207.02262", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUltra-Low-Bitrate Speech Coding with Pretrained Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oAli Siahkoohi\\nMichael Chinen\\nTom Denton\\nW. Bastiaan Kleijn\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.02262\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Jul 2022 18:52:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH 2022\\u00a7r"}']}
{title:'Jiang et al. (§72022§r)', author: 'Xue Jiang; Xiulian Peng; Huaying Xue; Yuan Zhang; Yan Lu', display:{Lore:['[{"text": "arXiv:2207.03067", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-Scale Vector Quantization for Scalable Neural Speech Coding\\u00a7r\\n\\n\\u00a78\\u00a7oXue Jiang\\nXiulian Peng\\nHuaying Xue\\nYuan Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.03067\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Jul 2022 03:23:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2022(Accepted)\\u00a7r"}']}
{title:'Sun et al. (§72022§r)', author: 'Wei Sun; Lili Qiu', display:{Lore:['[{"text": "arXiv:2207.03074", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisual-Assisted Sound Source Depth Estimation in the Wild\\u00a7r\\n\\n\\u00a78\\u00a7oWei Sun\\nLili Qiu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.03074\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Jul 2022 23:47:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages;in submission\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'Wen Chin Huang; Dejan Markovic; Alexander Richard; Israel Dejene Gebru; Anjali Menon', display:{Lore:['[{"text": "arXiv:2207.03697", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Binaural Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oWen Chin Huang\\nDejan Markovic\\nAlexander Richard\\nIsrael Dejene Gebru\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.03697\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Jul 2022 05:18:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022. Demo link: https://unilight.github.io/Publication-Demos/publications/e2e-binaural-synthesis\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Yongqi Wang; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2207.03800", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFastLTS: Non-Autoregressive End-to-End Unconstrained Lip-to-Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYongqi Wang\\nZhou Zhao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.03800\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3503161.3548194\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 13 Jul 2022 09:15:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 5 figures, accepted by ACMMM 2022\\u00a7r"}']}
{title:'Kuang et al. (§72022§r)', author: 'Sheng Kuang; Kiki van der Heijden; Siamak Mehrkanoon', display:{Lore:['[{"text": "arXiv:2207.03927", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBAST: Binaural Audio Spectrogram Transformer for Binaural Sound Localization\\u00a7r\\n\\n\\u00a78\\u00a7oSheng Kuang\\nKiki van der Heijden\\nSiamak Mehrkanoon\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.03927\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Jul 2022 14:27:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Yu Li; Anisha Parsan; Bill Wang; Penghao Dong; Shanshan Yao; Ruwen Qin', display:{Lore:['[{"text": "arXiv:2207.04027", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Multi-tasking Model of Speaker-Keyword Classification for Keeping Human in the Loop of Drone-assisted Inspection\\u00a7r\\n\\n\\u00a78\\u00a7oYu Li\\nAnisha Parsan\\nBill Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.04027\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 31 Oct 2022 19:49:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Engineering Applications of Artificial Intelligence journal on Oct 31th. Upload the accepted clean version\\u00a7r"}']}
{title:'Xu et al. (§72022§r)', author: 'Zhongweiyang Xu; Romit Roy Choudhury', display:{Lore:['[{"text": "arXiv:2207.04203", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning to Separate Voices by Spatial Regions\\u00a7r\\n\\n\\u00a78\\u00a7oZhongweiyang Xu\\nRomit Roy Choudhury\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.04203\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 15 Jul 2022 03:01:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICML2022. For associated audio samples, see https://uiuc-earable-computing.github.io/binaural\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'Wen-Chin Huang; Shu-Wen Yang; Tomoki Hayashi; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2207.04356", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparative Study of Self-supervised Speech Representation Based Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oWen-Chin Huang\\nShu-Wen Yang\\nTomoki Hayashi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.04356\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JSTSP.2022.3193761\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 10 Jul 2022 01:02:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEJournal of Selected Topics in Signal Processing. arXiv adminnote: substantial text overlap with arXiv:2110.06280\\u00a7r"}']}
{title:'Tsubaki et al. (§72022§r)', author: 'Shunsuke Tsubaki; Keisuke Imoto; Nobutaka Ono', display:{Lore:['[{"text": "arXiv:2207.04357", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Analysis of Acoustic Scenes and Sound Events with Weakly labeled Data\\u00a7r\\n\\n\\u00a78\\u00a7oShunsuke Tsubaki\\nKeisuke Imoto\\nNobutaka Ono\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.04357\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 10 Jul 2022 01:20:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IWAENC2022\\u00a7r"}']}
{title:'Choi et al. (§72022§r)', author: 'Jeong Choi; Seongwon Jang; Hyunsouk Cho; Sehee Chung', display:{Lore:['[{"text": "arXiv:2207.04471", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Proper Contrastive Self-supervised Learning Strategies For Music Audio Representation\\u00a7r\\n\\n\\u00a78\\u00a7oJeong Choi\\nSeongwon Jang\\nHyunsouk Cho\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.04471\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 10 Jul 2022 14:25:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2022 IEEE International Conference on Multimedia and Expo (ICME)\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Yanqing Liu; Ruiqing Xue; Lei He; Xu Tan; Sheng Zhao', display:{Lore:['[{"text": "arXiv:2207.04646", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDelightfulTTS 2: End-to-End Speech Synthesis with Adversarial Vector-Quantized Auto-Encoders\\u00a7r\\n\\n\\u00a78\\u00a7oYanqing Liu\\nRuiqing Xue\\nLei He\\nXu Tan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.04646\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Jul 2022 06:15:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Interspeech 2022\\u00a7r"}']}
{title:'Makishima et al. (§72022§r)', author: 'Naoki Makishima; Satoshi Suzuki; Atsushi Ando; Ryo Masumura', display:{Lore:['[{"text": "arXiv:2207.04659", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker consistency loss and step-wise optimization for semi-supervised joint training of TTS and ASR using unpaired text data\\u00a7r\\n\\n\\u00a78\\u00a7oNaoki Makishima\\nSatoshi Suzuki\\nAtsushi Ando\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.04659\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Jul 2022 06:47:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Zhuo Li; Runqiu Xiao; Hangting Chen; Zhenduo Zhao; Zihan Zhang; Wenchao Wang', display:{Lore:['[{"text": "arXiv:2207.04676", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe HCCL System for the NIST SRE21\\u00a7r\\n\\n\\u00a78\\u00a7oZhuo Li\\nRunqiu Xiao\\nHangting Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.04676\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Jul 2022 07:42:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by interspeech 2022\\u00a7r"}']}
{title:'Meyer et al. (§72022§r)', author: 'Sarina Meyer; Florian Lux; Pavel Denisov; Julia Koch; Pascal Tilli; Ngoc Thang Vu', display:{Lore:['[{"text": "arXiv:2207.04834", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Anonymization with Phonetic Intermediate Representations\\u00a7r\\n\\n\\u00a78\\u00a7oSarina Meyer\\nFlorian Lux\\nPavel Denisov\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.04834\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Jul 2022 13:02:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2022\\u00a7r"}']}
{title:'Hao et al. (§72022§r)', author: 'Haiqing Hao; Zhongwang Pang; Guan Wang; Bo Wang', display:{Lore:['[{"text": "arXiv:2207.05267", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.ins-det\\u00a7r, \\u00a75physics.optics\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIndoor optical fiber eavesdropping approach and its avoidance\\u00a7r\\n\\n\\u00a78\\u00a7oHaiqing Hao\\nZhongwang Pang\\nGuan Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.05267\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1364/OE.470529\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 3 Aug 2022 13:58:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 4 figures, submitted to Optics Express\\u00a7r"}']}
{title:'Latif et al. (§72022§r)', author: 'Siddique Latif; Rajib Rana; Sara Khalifa; Raja Jurdak; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2207.05298", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultitask Learning from Augmented Auxiliary Data for Improving Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSiddique Latif\\nRajib Rana\\nSara Khalifa\\nRaja Jurdak\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.05298\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Jul 2022 04:12:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review IEEE Transactions on Affective Computing\\u00a7r"}']}
{title:'Gómez-Gómez et al. (§72022§r)', author: 'Juan Gómez-Gómez; Ester Vidaña-Vila; Xavier Sevillano', display:{Lore:['[{"text": "arXiv:2207.05393", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWestern Mediterranean wetlands bird species classification: evaluating small-footprint deep learning approaches on a new annotated dataset\\u00a7r\\n\\n\\u00a78\\u00a7oJuan G\\u00f3mez-G\\u00f3mez\\nEster Vida\\u00f1a-Vila\\nXavier Sevillano\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.05393\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Jul 2022 08:48:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o17 pages, 8 figures, 3 tables\\u00a7r"}']}
{title:'Ahmed et al. (§72022§r)', author: 'W. W. Ahmed; M. Farhat; P. -Y. Chen; X. Zhang; Y. Wu', display:{Lore:['[{"text": "arXiv:2207.05433", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.app-ph\\u00a7r, \\u00a75physics.comp-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Generative deep learning approach for shape recognition of arbitrary objects from phaseless acoustic scattering data\\u00a7r\\n\\n\\u00a78\\u00a7oW. W. Ahmed\\nM. Farhat\\nP. -Y. Chen\\nX. Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.05433\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Jul 2022 09:56:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o43 pages, 19 figures\\u00a7r"}']}
{title:'Schlüter et al. (§72022§r)', author: 'Jan Schlüter; Gerald Gutenbrunner', display:{Lore:['[{"text": "arXiv:2207.05508", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficientLEAF: A Faster LEarnable Audio Frontend of Questionable Use\\u00a7r\\n\\n\\u00a78\\u00a7oJan Schl\\u00fcter\\nGerald Gutenbrunner\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.05508\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Jul 2022 13:04:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at EUSIPCO 2022. Code at https://github.com/CPJKU/EfficientLEAF\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Chen Zhang; Luchin Chang; Songruoyao Wu; Xu Tan; Tao Qin; Tie-Yan Liu; Kejun Zhang', display:{Lore:['[{"text": "arXiv:2207.05688", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReLyMe: Improving Lyric-to-Melody Generation by Incorporating Lyric-Melody Relationships\\u00a7r\\n\\n\\u00a78\\u00a7oChen Zhang\\nLuchin Chang\\nSongruoyao Wu\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.05688\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3503161.3548357\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Jul 2022 17:09:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ACMMM 2022, oral\\u00a7r"}']}
{title:'Guo et al. (§72022§r)', author: 'Hanqing Guo; Chenning Li; Lingkun Li; Zhichao Cao; Qiben Yan; Li Xiao', display:{Lore:['[{"text": "arXiv:2207.05848", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNEC: Speaker Selective Cancellation via Neural Enhanced Ultrasound Shadowing\\u00a7r\\n\\n\\u00a78\\u00a7oHanqing Guo\\nChenning Li\\nLingkun Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.05848\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Jul 2022 21:26:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages\\u00a7r"}']}
{title:'Hong et al. (§72022§r)', author: 'Joanna Hong; Minsu Kim; Daehun Yoo; Yong Man Ro', display:{Lore:['[{"text": "arXiv:2207.06020", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisual Context-driven Audio Feature Enhancement for Robust End-to-End Audio-Visual Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJoanna Hong\\nMinsu Kim\\nDaehun Yoo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.06020\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Jul 2022 08:07:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2022\\u00a7r"}']}
{title:'Ma et al. (§72022§r)', author: 'Jian Ma; Zhedong Zheng; Hao Fei; Feng Zheng; Tat-seng Chua; Yi Yang', display:{Lore:['[{"text": "arXiv:2207.06057", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSubband-based Generative Adversarial Network for Non-parallel Many-to-many Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oJian Ma\\nZhedong Zheng\\nHao Fei\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.06057\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 27 Jul 2022 07:31:57 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Zhengxi Liu; Qiao Tian; Chenxu Hu; Xudong Liu; Menglin Wu; Yuping Wang; Hang Zhao; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2207.06088", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lControllable and Lossless Non-Autoregressive End-to-End Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oZhengxi Liu\\nQiao Tian\\nChenxu Hu\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.06088\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Jul 2022 09:57:06 GMT)\\u00a7r"}']}
{title:'Parrilla et al. (§72022§r)', author: 'Alberto García Arroba Parrilla; Dan Stowell', display:{Lore:['[{"text": "arXiv:2207.06349", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPolyphonic sound event detection for highly dense birdsong scenes\\u00a7r\\n\\n\\u00a78\\u00a7oAlberto Garc\\u00eda Arroba Parrilla\\nDan Stowell\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.06349\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Jul 2022 17:02:29 GMT)\\u00a7r"}']}
{title:'Parthasarathi et al. (§72022§r)', author: 'Sree Hari Krishnan Parthasarathi; Lu Zeng; Christin Jose; Joseph Wang', display:{Lore:['[{"text": "arXiv:2207.06423", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWakeword Detection under Distribution Shifts\\u00a7r\\n\\n\\u00a78\\u00a7oSree Hari Krishnan Parthasarathi\\nLu Zeng\\nChristin Jose\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.06423\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Jul 2022 17:35:08 GMT)\\u00a7r"}']}
{title:'Esmaeilpour et al. (§72022§r)', author: 'Mohammad Esmaeilpour; Nourhene Chaalia; Patrick Cardinal', display:{Lore:['[{"text": "arXiv:2207.06858", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRSD-GAN: Regularized Sobolev Defense GAN Against Speech-to-Text Adversarial Attacks\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Esmaeilpour\\nNourhene Chaalia\\nPatrick Cardinal\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.06858\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2022.3208528\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 24 Sep 2022 17:14:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper ACCEPTED FOR PUBLICATION IEEE Signal Processing Letters Journal\\u00a7r"}']}
{title:'Zevallos et al. (§72022§r)', author: 'Rodolfo Zevallos; Nuria Bel; Guillermo Cámbara; Mireia Farrús; Jordi Luque', display:{Lore:['[{"text": "arXiv:2207.06872", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData Augmentation for Low-Resource Quechua ASR Improvement\\u00a7r\\n\\n\\u00a78\\u00a7oRodolfo Zevallos\\nNuria Bel\\nGuillermo C\\u00e1mbara\\nMireia Farr\\u00fas\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.06872\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Jul 2022 12:49:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022. arXiv admin note: substantial text overlap with arXiv:2204.00291\\u00a7r"}']}
{title:'Zeng et al. (§72022§r)', author: 'Lu Zeng; Sree Hari Krishnan Parthasarathi; Yuzong Liu; Alex Escott; Santosh Kumar Cheekatmalla; Nikko Strom; Shiv Vitaladevuni', display:{Lore:['[{"text": "arXiv:2207.06920", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSub 8-Bit Quantization of Streaming Keyword Spotting Models for Embedded Chipsets\\u00a7r\\n\\n\\u00a78\\u00a7oLu Zeng\\nSree Hari Krishnan Parthasarathi\\nYuzong Liu\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.06920\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 8 Sep 2022 09:15:46 GMT)\\u00a7r"}']}
{title:'Baird et al. (§72022§r)', author: 'Alice Baird; Panagiotis Tzirakis; Gauthier Gidel; Marco Jiralerspong; Eilif B. Muller; Kory Mathewson; Björn Schuller; Erik Cambria; Dacher Keltner; Alan Cowen', display:{Lore:['[{"text": "arXiv:2207.06958", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProceedings of the ICML 2022 Expressive Vocalizations Workshop and Competition: Recognizing, Generating, and Personalizing Vocal Bursts\\u00a7r\\n\\n\\u00a78\\u00a7oAlice Baird\\nPanagiotis Tzirakis\\nGauthier Gidel\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.06958\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 Aug 2022 14:35:03 GMT)\\u00a7r"}']}
{title:'Marien et al. (§72022§r)', author: 'James Marien; Sam Leroux; Bart Dhoedt; Cedric De Boom', display:{Lore:['[{"text": "arXiv:2207.07162", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-guided Album Cover Art Generation with Genetic Algorithms\\u00a7r\\n\\n\\u00a78\\u00a7oJames Marien\\nSam Leroux\\nBart Dhoedt\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.07162\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Jul 2022 18:41:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 6 figures, 4 tables\\u00a7r"}']}
{title:'Schmidt et al. (§72022§r)', author: 'Nicolás Schmidt; Jordi Pons; Marius Miron', display:{Lore:['[{"text": "arXiv:2207.07403", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.DB\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPodcastMix: A dataset for separating music and speech in podcasts\\u00a7r\\n\\n\\u00a78\\u00a7oNicol\\u00e1s Schmidt\\nJordi Pons\\nMarius Miron\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.07403\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Jul 2022 11:12:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn proceedings of INTERSPEECH2022. Project webpage: http://www.jordipons.me/apps/podcastmix/\\u00a7r"}']}
{title:'Xiao et al. (§72022§r)', author: 'Yang Xiao; Xubo Liu; James King; Arshdeep Singh; Eng Siong Chng; Mark D. Plumbley; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2207.07429", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContinual Learning For On-Device Environmental Sound Classification\\u00a7r\\n\\n\\u00a78\\u00a7oYang Xiao\\nXubo Liu\\nJames King\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.07429\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 18 Jul 2022 07:54:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe first two authors contributed equally, 5 pages one figure, submitted to DCASE2022Workshop\\u00a7r"}']}
{title:'Avila et al. (§72022§r)', author: 'Anderson R. Avila; Khalil Bibi; Rui Heng Yang; Xinlin Li; Chao Xing; Xiao Chen', display:{Lore:['[{"text": "arXiv:2207.07497", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-bit Shift Network for End-to-End Spoken Language Understanding\\u00a7r\\n\\n\\u00a78\\u00a7oAnderson R. Avila\\nKhalil Bibi\\nRui Heng Yang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.07497\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Jul 2022 14:34:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2022\\u00a7r"}']}
{title:'Nolasco et al. (§72022§r)', author: 'I. Nolasco; S. Singh; E. Vidana-Villa; E. Grout; J. Morford; M. Emmerson; F. Jensens; H. Whitehead; I. Kiskin; A. Strandburg-Peshkin; L. Gill; H. Pamula; V. Lostanlen; V. Morfi; D. Stowell', display:{Lore:['[{"text": "arXiv:2207.07911", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFew-shot bioacoustic event detection at the DCASE 2022 challenge\\u00a7r\\n\\n\\u00a78\\u00a7oI. Nolasco\\nS. Singh\\nE. Vidana-Villa\\n+ 11 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.07911\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Jul 2022 09:33:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to DCASE2022 workshop\\u00a7r"}']}
{title:'Shirian et al. (§72022§r)', author: 'Amir Shirian; Krishna Somandepalli; Victor Sanchez; Tanaya Guha', display:{Lore:['[{"text": "arXiv:2207.07935", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisually-aware Acoustic Event Detection using Heterogeneous Graphs\\u00a7r\\n\\n\\u00a78\\u00a7oAmir Shirian\\nKrishna Somandepalli\\nVictor Sanchez\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.07935\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 16 Jul 2022 13:09:25 GMT)\\u00a7r"}']}
{title:'Alon-Ronen et al. (§72022§r)', author: 'Or Alon-Ronen; Yosi Shrem; Yossi Keshet; Eva Gilboa-Schechtman', display:{Lore:['[{"text": "arXiv:2207.08534", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Vocal Signature of Social Anxiety: Exploration using Hypothesis-Testing and Machine-Learning Approaches\\u00a7r\\n\\n\\u00a78\\u00a7oOr Alon-Ronen\\nYosi Shrem\\nYossi Keshet\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.08534\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Jul 2022 11:55:38 GMT)\\u00a7r"}']}
{title:'Steinmetz et al. (§72022§r)', author: 'Christian J. Steinmetz; Nicholas J. Bryan; Joshua D. Reiss', display:{Lore:['[{"text": "arXiv:2207.08759", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStyle Transfer of Audio Effects with Differentiable Signal Processing\\u00a7r\\n\\n\\u00a78\\u00a7oChristian J. Steinmetz\\nNicholas J. Bryan\\nJoshua D. Reiss\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.08759\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Jul 2022 17:06:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint. To appear in the Journal of the Audio Engineering Society\\u00a7r"}']}
{title:'Zhang (§72022§r)', author: 'Hanhaodi Zhang', display:{Lore:['[{"text": "arXiv:2207.08813", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Input Generates Continuous Frames to Synthesize Facial Video Using Generative Adiversarial Networks\\u00a7r\\n\\n\\u00a78\\u00a7oHanhaodi Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.08813\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Jul 2022 03:25:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures\\u00a7r"}']}
{title:'Ochieng et al. (§72022§r)', author: 'Peter Ochieng; Dennis Kaburu', display:{Lore:['[{"text": "arXiv:2207.08825", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContrastive Environmental Sound Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Ochieng\\nDennis Kaburu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.08825\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Jul 2022 16:56:30 GMT)\\u00a7r"}']}
{title:'Srivastava et al. (§72022§r)', author: 'Prerak Srivastava; Antoine Deleforge; Emmanuel Vincent', display:{Lore:['[{"text": "arXiv:2207.09133", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRealistic sources, receivers and walls improve the generalisability of virtually-supervised blind acoustic parameter estimators\\u00a7r\\n\\n\\u00a78\\u00a7oPrerak Srivastava\\nAntoine Deleforge\\nEmmanuel Vincent\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.09133\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Jul 2022 09:09:38 GMT)\\u00a7r"}']}
{title:'Kraxberger et al. (§72022§r)', author: 'Florian Kraxberger; Andreas Wurzinger; Stefan Schoder', display:{Lore:['[{"text": "arXiv:2207.09265", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.bio-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMachine-learning applied to classify flow-induced sound parameters from simulated human voice\\u00a7r\\n\\n\\u00a78\\u00a7oFlorian Kraxberger\\nAndreas Wurzinger\\nStefan Schoder\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.09265\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Jul 2022 13:25:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o17 pages, 11 figures, v0.1, work in progress, working paper\\u00a7r"}']}
{title:'Hajavi et al. (§72022§r)', author: 'Amirhossein Hajavi; Ali Etemad', display:{Lore:['[{"text": "arXiv:2207.10006", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFine-grained Early Frequency Attention for Deep Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAmirhossein Hajavi\\nAli Etemad\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.10006\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Jul 2022 16:09:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted In IJCNN 2022\\u00a7r"}']}
{title:'Tzinis et al. (§72022§r)', author: 'Efthymios Tzinis; Scott Wisdom; Tal Remez; John R. Hershey', display:{Lore:['[{"text": "arXiv:2207.10141", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudioScopeV2: Audio-Visual Attention Architectures for Calibrated Open-Domain On-Screen Sound Separation\\u00a7r\\n\\n\\u00a78\\u00a7oEfthymios Tzinis\\nScott Wisdom\\nTal Remez\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.10141\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Jul 2022 18:44:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oECCV 2022\\u00a7r"}']}
{title:'Sun et al. (§72022§r)', author: 'Wei Sun; Mei Wang; Lili Qiu', display:{Lore:['[{"text": "arXiv:2207.10229", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatial Aware Multi-Task Learning Based Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oWei Sun\\nMei Wang\\nLili Qiu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.10229\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Jul 2022 23:41:12 GMT)\\u00a7r"}']}
{title:'Turetzky et al. (§72022§r)', author: 'Arnon Turetzky; Tzvi Michelson; Yossi Adi; Shmuel Peleg', display:{Lore:['[{"text": "arXiv:2207.10441", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Audio Waveform Prior\\u00a7r\\n\\n\\u00a78\\u00a7oArnon Turetzky\\nTzvi Michelson\\nYossi Adi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.10441\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10735\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Jul 2022 12:25:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2022\\u00a7r"}']}
{title:'Gao et al. (§72022§r)', author: 'Shan Gao; Xihong Wu; Tianshu Qu', display:{Lore:['[{"text": "arXiv:2207.10478", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRoom geometry blind inference based on the localization of real sound source and first order reflections\\u00a7r\\n\\n\\u00a78\\u00a7oShan Gao\\nXihong Wu\\nTianshu Qu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.10478\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 22 Jul 2022 09:34:10 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Haohe Liu; Xubo Liu; Xinhao Mei; Qiuqiang Kong; Wenwu Wang; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:2207.10547", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSurrey System for DCASE 2022 Task 5: Few-shot Bioacoustic Event Detection with Segment-level Metric Learning\\u00a7r\\n\\n\\u00a78\\u00a7oHaohe Liu\\nXubo Liu\\nXinhao Mei\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.10547\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Jul 2022 15:43:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTechnical Reportof the system that ranks 2nd in the DCASE Challenge Task 5. arXiv admin note: text overlapwith arXiv:2207.07773\\u00a7r"}']}
{title:'Gong et al. (§72022§r)', author: 'Xun Gong; Zhikai Zhou; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2207.10600", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKnowledge Transfer and Distillation from Autoregressive to Non-Autoregressive Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXun Gong\\nZhikai Zhou\\nYanmin Qian\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.10600\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Jul 2022 13:38:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Choi et al. (§72022§r)', author: 'Keunwoo Choi; Sangshin Oh; Minsung Kang; Brian McFee', display:{Lore:['[{"text": "arXiv:2207.10760", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Proposal for Foley Sound Synthesis Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oKeunwoo Choi\\nSangshin Oh\\nMinsung Kang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.10760\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Jul 2022 21:19:07 GMT)\\u00a7r"}']}
{title:'Sheikh et al. (§72022§r)', author: 'Shakeel Ahmad Sheikh; Md Sahidullah; Fabrice Hirsch; Slim Ouni', display:{Lore:['[{"text": "arXiv:2207.10817", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End and Self-Supervised Learning for ComParE 2022 Stuttering Sub-Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oShakeel Ahmad Sheikh\\nMd Sahidullah\\nFabrice Hirsch\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.10817\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Jul 2022 11:57:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ACM MM 2022 Conference : Grand Challenges, \\"o\\u0327p\\u0327y\\u0327\\u0157i\\u0327\\u0123\\u1e29\\u0163 Owner/Author | ACM 2022. This is the author\'s version of thework. It is posted here for your personal use. Not for redistribution\\u00a7r"}']}
{title:'Shigemi et al. (§72022§r)', author: 'Kazuhide Shigemi; Shoichi Koyama; Tomohiko Nakamura; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2207.10937", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhysics-informed convolutional neural network with bicubic spline interpolation for sound field estimation\\u00a7r\\n\\n\\u00a78\\u00a7oKazuhide Shigemi\\nShoichi Koyama\\nTomohiko Nakamura\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.10937\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Jul 2022 08:27:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to International Workshopon Acoustic Signal Enhancement (IWAENC) 2022\\u00a7r"}']}
{title:'Ito et al. (§72022§r)', author: 'Yuki Ito; Tomohiko Nakamura; Shoichi Koyama; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2207.10967", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHead-Related Transfer Function Interpolation from Spatially Sparse Measurements Using Autoencoder with Source Position Conditioning\\u00a7r\\n\\n\\u00a78\\u00a7oYuki Ito\\nTomohiko Nakamura\\nShoichi Koyama\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.10967\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Jul 2022 09:24:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to International Workshopon Acoustic Signal Enhancement (IWAENC) 2022\\u00a7r"}']}
{title:'Le et al. (§72022§r)', author: 'Xiaohuai Le; Tong Lei; Kai Chen; Jing Lu', display:{Lore:['[{"text": "arXiv:2207.11108", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInference skipping for more efficient real-time speech enhancement with parallel RNNs\\u00a7r\\n\\n\\u00a78\\u00a7oXiaohuai Le\\nTong Lei\\nKai Chen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.11108\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2022.3190738\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Jul 2022 14:34:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 8 figures, accepted by IEEE/ACM TASLP\\u00a7r"}']}
{title:'Afchar et al. (§72022§r)', author: 'Darius Afchar; Romain Hennequin; Vincent Guigue', display:{Lore:['[{"text": "arXiv:2207.11231", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Unsupervised Hierarchies of Audio Concepts\\u00a7r\\n\\n\\u00a78\\u00a7oDarius Afchar\\nRomain Hennequin\\nVincent Guigue\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.11231\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Jul 2022 16:34:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oISMIR 2022\\u00a7r"}']}
{title:'Li (§72022§r)', author: 'Xinyu Li', display:{Lore:['[{"text": "arXiv:2207.11690", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHouseX: A Fine-grained House Music Dataset and its Potential in the Music Industry\\u00a7r\\n\\n\\u00a78\\u00a7oXinyu Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.11690\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 12 Oct 2022 00:34:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages. Accepted by APSIPA ASC 2022 to be held during Nov. 2022\\u00a7r"}']}
{title:'Chung et al. (§72022§r)', author: 'HaeChun Chung; JooYong Shim; Jong-Kook Kim', display:{Lore:['[{"text": "arXiv:2207.12121", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-Modal Contrastive Representation Learning for Audio-to-Image Generation\\u00a7r\\n\\n\\u00a78\\u00a7oHaeChun Chung\\nJooYong Shim\\nJong-Kook Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.12121\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Jul 2022 10:00:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 3 figures, Accepted to MUE 2022\\u00a7r"}']}
{title:'Rajapakshe et al. (§72022§r)', author: 'Thejan Rajapakshe; Rajib Rana; Sara Khalifa; Bjorn W. Schuller', display:{Lore:['[{"text": "arXiv:2207.12248", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDomain Adapting Deep Reinforcement Learning for Real-world Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oThejan Rajapakshe\\nRajib Rana\\nSara Khalifa\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.12248\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 23 Sep 2022 11:53:10 GMT)\\u00a7r"}']}
{title:'Millán-Castillo et al. (§72022§r)', author: 'R. San Millán-Castillo; L. Martino; E. Morgado; F. Llorente', display:{Lore:['[{"text": "arXiv:2207.12743", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a7cstat.CO\\u00a7r, \\u00a7cstat.ME\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn exhaustive variable selection study for linear models of soundscape emotions: rankings and Gibbs analysis\\u00a7r\\n\\n\\u00a78\\u00a7oR. San Mill\\u00e1n-Castillo\\nL. Martino\\nE. Morgado\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.12743\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2022.3192664\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE-ACM Transactions on Audio, Speech and Language Processing,\\n  2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Jul 2022 08:49:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7opublished in IEEE-ACM Transactionson Audio, Speech and Language Processing\\u00a7r"}']}
{title:'Triantafyllopoulos et al. (§72022§r)', author: 'Andreas Triantafyllopoulos; Markus Fendler; Anton Batliner; Maurice Gerczuk; Shahin Amiriparian; Thomas M. Berghaus; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2207.12784", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDistinguishing between pre- and post-treatment in the speech of patients with chronic obstructive pulmonary disease\\u00a7r\\n\\n\\u00a78\\u00a7oAndreas Triantafyllopoulos\\nMarkus Fendler\\nAnton Batliner\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.12784\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10333\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2022, 3623-3627\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Jul 2022 10:01:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in INTERSPEECH 2022\\u00a7r"}']}
{title:'Duan et al. (§72022§r)', author: 'Rui Duan; Zhe Qu; Shangqing Zhao; Leah Ding; Yao Liu; Zhuo Lu', display:{Lore:['[{"text": "arXiv:2207.13192", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerception-Aware Attack: Creating Adversarial Music via Reverse-Engineering Human Perception\\u00a7r\\n\\n\\u00a78\\u00a7oRui Duan\\nZhe Qu\\nShangqing Zhao\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.13192\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Jul 2022 21:40:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oACM CCS 2022\\u00a7r"}']}
{title:'Tesema et al. (§72022§r)', author: 'Fiseha B. Tesema; Zheyuan Lin; Shiqiang Zhu; Wei Song; Jason Gu; Hong Wu', display:{Lore:['[{"text": "arXiv:2207.13434", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-To-End Audiovisual Feature Fusion for Active Speaker Detection\\u00a7r\\n\\n\\u00a78\\u00a7oFiseha B. Tesema\\nZheyuan Lin\\nShiqiang Zhu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.13434\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1117/12.2643881\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings Volume 12342, Fourteenth International Conference on\\n  Digital Image Processing (ICDIP 2022); 123422A (2022)\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Jul 2022 10:25:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear on the proceeding of the Fourteenth International Conference on Digital Image Processing (ICDIP 2022), May 20-23, Wuhan, China, 8 pages, 3 figures\\u00a7r"}']}
{title:'Ploujnikov et al. (§72022§r)', author: 'Artem Ploujnikov; Mirco Ravanelli', display:{Lore:['[{"text": "arXiv:2207.13703", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSoundChoice: Grapheme-to-Phoneme Models with Semantic Disambiguation\\u00a7r\\n\\n\\u00a78\\u00a7oArtem Ploujnikov\\nMirco Ravanelli\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.13703\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Jul 2022 01:14:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, submitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Khandelwal et al. (§72022§r)', author: 'Devesh Khandelwal; Sean Campos; Shwetha Nagaraj; Fred Nugen; Alberto Todeschini', display:{Lore:['[{"text": "arXiv:2207.13843", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning-Based Acoustic Mosquito Detection in Noisy Conditions Using Trainable Kernels and Augmentations\\u00a7r\\n\\n\\u00a78\\u00a7oDevesh Khandelwal\\nSean Campos\\nShwetha Nagaraj\\nFred Nugen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.13843\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3503161.3551586\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 18 Aug 2022 20:24:47 GMT)\\u00a7r"}']}
{title:'Ramirez-Aristizabal et al… (§72022§r)', author: 'Adolfo G. Ramirez-Aristizabal; Chris Kello', display:{Lore:['[{"text": "arXiv:2207.13845", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEEG2Mel: Reconstructing Sound from Brain Responses to Music\\u00a7r\\n\\n\\u00a78\\u00a7oAdolfo G. Ramirez-Aristizabal\\nChris Kello\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.13845\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Jul 2022 01:06:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 figures, 2 tables, listening examples and code provided\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Da-rong Liu; Po-chun Hsu; Yi-chen Chen; Sung-feng Huang; Shun-po Chuang; Da-yi Wu; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2207.14568", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Phone Recognition from Unpaired Audio and Phone Sequences Based on Generative Adversarial Network\\u00a7r\\n\\n\\u00a78\\u00a7oDa-rong Liu\\nPo-chun Hsu\\nYi-chen Chen\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.14568\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Jul 2022 09:29:28 GMT)\\u00a7r"}']}
{title:'Bunks et al. (§72022§r)', author: 'C. Bunks; T. Weyde', display:{Lore:['[{"text": "arXiv:2208.00792", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJazz Contrafact Detection\\u00a7r\\n\\n\\u00a78\\u00a7oC. Bunks\\nT. Weyde\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.00792\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Aug 2022 12:07:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 6 figures, 4 tables\\u00a7r"}']}
{title:'Lattner (§72022§r)', author: 'Stefan Lattner', display:{Lore:['[{"text": "arXiv:2208.01141", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSampleMatch: Drum Sample Retrieval by Musical Context\\u00a7r\\n\\n\\u00a78\\u00a7oStefan Lattner\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.01141\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Aug 2022 21:10:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 3 figures, 1 table; Accepted at the ISMIR conference, Bengaluru, India, 2022\\u00a7r"}']}
{title:'Xue et al. (§72022§r)', author: 'Jun Xue; Cunhang Fan; Zhao Lv; Jianhua Tao; Jiangyan Yi; Chengshi Zheng; Zhengqi Wen; Minmin Yuan; Shegang Shao', display:{Lore:['[{"text": "arXiv:2208.01214", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Deepfake Detection Based on a Combination of F0 Information and Real Plus Imaginary Spectrogram Features\\u00a7r\\n\\n\\u00a78\\u00a7oJun Xue\\nCunhang Fan\\nZhao Lv\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.01214\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3552466.3556526\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Aug 2022 02:46:16 GMT)\\u00a7r"}']}
{title:'Shi et al. (§72022§r)', author: 'Jiatong Shi; George Saon; David Haws; Shinji Watanabe; Brian Kingsbury', display:{Lore:['[{"text": "arXiv:2208.01818", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVQ-T: RNN Transducers using Vector-Quantized Prediction Network States\\u00a7r\\n\\n\\u00a78\\u00a7oJiatong Shi\\nGeorge Saon\\nDavid Haws\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.01818\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Aug 2022 02:45:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2022 accepted paper\\u00a7r"}']}
{title:'Fares et al. (§72022§r)', author: 'Mireille Fares; Michele Grimaldi; Catherine Pelachaud; Nicolas Obin', display:{Lore:['[{"text": "arXiv:2208.01917", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero-Shot Style Transfer for Gesture Animation driven by Text and Speech using Adversarial Disentanglement of Multimodal Style Encoding\\u00a7r\\n\\n\\u00a78\\u00a7oMireille Fares\\nMichele Grimaldi\\nCatherine Pelachaud\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.01917\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Aug 2022 08:49:55 GMT)\\u00a7r"}']}
{title:'Han et al. (§72022§r)', author: 'Bing Han; Zhengyang Chen; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2208.01928", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Speaker Verification Using Dynamic Loss-Gate and Label Correction\\u00a7r\\n\\n\\u00a78\\u00a7oBing Han\\nZhengyang Chen\\nYanmin Qian\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.01928\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Aug 2022 09:10:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Han et al. (§72022§r)', author: 'Bing Han; Zhengyang Chen; Zhikai Zhou; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2208.01933", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe SJTU System for Short-duration Speaker Verification Challenge 2021\\u00a7r\\n\\n\\u00a78\\u00a7oBing Han\\nZhengyang Chen\\nZhikai Zhou\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.01933\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Aug 2022 09:19:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished by Interspeech 2021\\u00a7r"}']}
{title:'Hou et al. (§72022§r)', author: 'Yuanbo Hou; Bo Kang; Dick Botteldooren', display:{Lore:['[{"text": "arXiv:2208.02086", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-visual scene classification via contrastive event-object alignment and semantic-based fusion\\u00a7r\\n\\n\\u00a78\\u00a7oYuanbo Hou\\nBo Kang\\nDick Botteldooren\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.02086\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Aug 2022 14:11:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE MMSP 2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Xiao Zhang; Hao Tan; Xuan Huang; Denghui Zhang; Keke Tang; Zhaoquan Gu', display:{Lore:['[{"text": "arXiv:2208.02250", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Attacks on ASR Systems: An Overview\\u00a7r\\n\\n\\u00a78\\u00a7oXiao Zhang\\nHao Tan\\nXuan Huang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.02250\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Aug 2022 06:46:42 GMT)\\u00a7r"}']}
{title:'Kalonaris (§72022§r)', author: 'Stefano Kalonaris', display:{Lore:['[{"text": "arXiv:2208.02494", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTokyo Kion-On: Query-Based Generative Sonification of Atmospheric Data\\u00a7r\\n\\n\\u00a78\\u00a7oStefano Kalonaris\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.02494\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21785/icad2022.039\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Aug 2022 06:56:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in: Proceedings of the 27th International Conference on Auditory Display (ICAD 2022)\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Jingyi Wang; Shengchen Li', display:{Lore:['[{"text": "arXiv:2208.02765", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.AR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKeyword Spotting System and Evaluation of Pruning and Quantization Methods on Low-power Edge Microcontrollers\\u00a7r\\n\\n\\u00a78\\u00a7oJingyi Wang\\nShengchen Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.02765\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Aug 2022 16:49:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to DCASE2022 Workshop. Code available at: https://github.com/RoboBachelor/Keyword-Spotting-STM32\\u00a7r"}']}
{title:'Poirè et al. (§72022§r)', author: 'Alessandro Maria Poirè; Federico Simonetta; Stavros Ntalampiras', display:{Lore:['[{"text": "arXiv:2208.03084", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Feature Learning for Medical Acoustics\\u00a7r\\n\\n\\u00a78\\u00a7oAlessandro Maria Poir\\u00e8\\nFederico Simonetta\\nStavros Ntalampiras\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.03084\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Aug 2022 10:39:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at ICANN 2022\\u00a7r"}']}
{title:'Kumar et al. (§72022§r)', author: 'A Kishore Kumar; Shefali Waldekar; Md Sahidullah; Goutam Saha', display:{Lore:['[{"text": "arXiv:2208.03162", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Acoustic Domain Identification with its Application to Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oA Kishore Kumar\\nShefali Waldekar\\nMd Sahidullah\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.03162\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s10772-022-09990-9\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Aug 2022 05:35:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in International Journal of Speech Technology (Springer Nature)\\u00a7r"}']}
{title:'Loiseau et al. (§72022§r)', author: 'Romain Loiseau; Baptiste Bouvier; Yann Teytaut; Elliot Vincent; Mathieu Aubry; Loic Landrieu', display:{Lore:['[{"text": "arXiv:2208.03311", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Model You Can Hear: Audio Identification with Playable Prototypes\\u00a7r\\n\\n\\u00a78\\u00a7oRomain Loiseau\\nBaptiste Bouvier\\nYann Teytaut\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.03311\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Aug 2022 17:57:20 GMT)\\u00a7r"}']}
{title:'Padfield et al. (§72022§r)', author: 'Dirk Padfield; Daniel J. Liebling', display:{Lore:['[{"text": "arXiv:2208.03393", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChronological Self-Training for Real-Time Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oDirk Padfield\\nDaniel J. Liebling\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.03393\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-822\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech (2021) 4613-4617\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Aug 2022 21:45:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, ICASSP 2021\\u00a7r"}']}
{title:'Tang et al. (§72022§r)', author: 'Huaizhen Tang; Xulong Zhang; Jianzong Wang; Ning Cheng; Zhen Zeng; Edward Xiao; Jing Xiao', display:{Lore:['[{"text": "arXiv:2208.04035", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTGAVC: Improving Autoencoder Voice Conversion with Text-Guided and Adversarial Training\\u00a7r\\n\\n\\u00a78\\u00a7oHuaizhen Tang\\nXulong Zhang\\nJianzong Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.04035\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ASRU51503.2021.9688088\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2021 IEEE Automatic Speech Recognition and Understanding Workshop\\n  (ASRU), 2021, pp. 938-945\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Aug 2022 10:33:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oASRU 6pages\\u00a7r"}']}
{title:'Tran et al. (§72022§r)', author: 'Thanh Tran; Sebastian Bader; Jan Lundgren', display:{Lore:['[{"text": "arXiv:2208.04462", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDenoising Induction Motor Sounds Using an Autoencoder\\u00a7r\\n\\n\\u00a78\\u00a7oThanh Tran\\nSebastian Bader\\nJan Lundgren\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.04462\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Aug 2022 23:14:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 10 figures, conference\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Da-Yi Wu; Wen-Yi Hsiao; Fu-Rong Yang; Oscar Friedman; Warren Jackson; Scott Bruzenak; Yi-Wen Liu; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2208.04756", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDDSP-based Singing Vocoders: A New Subtractive-based Synthesizer and A Comprehensive Evaluation\\u00a7r\\n\\n\\u00a78\\u00a7oDa-Yi Wu\\nWen-Yi Hsiao\\nFu-Rong Yang\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.04756\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInternational Society for Music Information Retrieval (ISMIR) 2022\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 19 Aug 2022 03:19:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ISMIR 2022\\u00a7r"}']}
{title:'Calatayud (§72022§r)', author: 'Patricio F. Calatayud', display:{Lore:['[{"text": "arXiv:2208.04877", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPure Data and INScore: Animated notation for new music\\u00a7r\\n\\n\\u00a78\\u00a7oPatricio F. Calatayud\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.04877\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Aug 2022 16:11:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 10 figures\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Shijun Wang; Hamed Hemati; Jón Guðnason; Damian Borth', display:{Lore:['[{"text": "arXiv:2208.04994", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerative Data Augmentation Guided by Triplet Loss for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oShijun Wang\\nHamed Hemati\\nJ\\u00f3n Gu\\u00f0nason\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.04994\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Aug 2022 18:39:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in INTERSPEECH 2022\\u00a7r"}']}
{title:'Naithani et al. (§72022§r)', author: 'Gaurav Naithani; Kirsi Pietilä; Riitta Niemistö; Erkki Paajanen; Tero Takala; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2208.05057", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSubjective Evaluation of Deep Neural Network Based Speech Enhancement Systems in Real-World Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oGaurav Naithani\\nKirsi Pietil\\u00e4\\nRiitta Niemist\\u00f6\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.05057\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 14 Aug 2022 18:02:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in IEEE MMSP2022\\u00a7r"}']}
{title:'Ferreira et al. (§72022§r)', author: 'Lucas N. Ferreira; Lili Mou; Jim Whitehead; Levi H. S. Lelis', display:{Lore:['[{"text": "arXiv:2208.05162", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lControlling Perceived Emotion in Symbolic Music Generation with Monte Carlo Tree Search\\u00a7r\\n\\n\\u00a78\\u00a7oLucas N. Ferreira\\nLili Mou\\nJim Whitehead\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.05162\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 1 Sep 2022 14:55:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at the 18th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE-22)\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Xiang Li; Changhe Song; Xianhao Wei; Zhiyong Wu; Jia Jia; Helen Meng', display:{Lore:['[{"text": "arXiv:2208.05359", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Cross-speaker Reading Style Transfer on Audiobook Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oXiang Li\\nChanghe Song\\nXianhao Wei\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.05359\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 19 Aug 2022 10:31:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted to INTERSPEECH 2022, demo page at https://thuhcsi.github.io/is2022-cross-speaker-reading-style-transfer\\u00a7r"}']}
{title:'Han et al. (§72022§r)', author: 'Sangjun Han; Hyeongrae Ihm; Moontae Lee; Woohyung Lim', display:{Lore:['[{"text": "arXiv:2208.05605", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSymbolic Music Loop Generation with Neural Discrete Representations\\u00a7r\\n\\n\\u00a78\\u00a7oSangjun Han\\nHyeongrae Ihm\\nMoontae Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.05605\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 29 Oct 2022 16:26:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ISMIR 2022\\u00a7r"}']}
{title:'Yan et al. (§72022§r)', author: 'Peiran Yan; Shengchen Li', display:{Lore:['[{"text": "arXiv:2208.06127", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn investigation on selecting audio pre-trained models for audio captioning\\u00a7r\\n\\n\\u00a78\\u00a7oPeiran Yan\\nShengchen Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.06127\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Aug 2022 06:14:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 7 figures\\u00a7r"}']}
{title:'Caspe et al. (§72022§r)', author: 'Franco Caspe; Andrew McPherson; Mark Sandler', display:{Lore:['[{"text": "arXiv:2208.06169", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDDX7: Differentiable FM Synthesis of Musical Instrument Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oFranco Caspe\\nAndrew McPherson\\nMark Sandler\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.06169\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Aug 2022 08:39:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ISMIR 2022. See online supplement at https://fcaspe.github.io/ddx7/\\u00a7r"}']}
{title:'Sethia et al. (§72022§r)', author: 'Abhimanyu Sethia; Aayush', display:{Lore:['[{"text": "arXiv:2208.06878", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModels of Music Cognition and Composition\\u00a7r\\n\\n\\u00a78\\u00a7oAbhimanyu Sethia\\nAayush\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.06878\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 14 Aug 2022 16:27:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTLDR: literature review of models of music cognition and composition\\u00a7r"}']}
{title:'Švec et al. (§72022§r)', author: 'Ján Švec; Kateřina Žmolíková; Martin Kocour; Marc Delcroix; Tsubasa Ochiai; Ladislav Mošner; Jan Černocký', display:{Lore:['[{"text": "arXiv:2208.07091", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalysis of impact of emotions on target speech extraction and speech separation\\u00a7r\\n\\n\\u00a78\\u00a7oJ\\u00e1n \\u0160vec\\nKate\\u0159ina \\u017dmol\\u00edkov\\u00e1\\nMartin Kocour\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.07091\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Aug 2022 09:47:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IWAENC 2022\\u00a7r"}']}
{title:'Al-Radhi et al. (§72022§r)', author: 'Mohammed Salah Al-Radhi; Tamás Gábor Csapó; Csaba Zainkó; Géza Németh', display:{Lore:['[{"text": "arXiv:2208.07122", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Parametric Speech Synthesis Using Gaussian-Markov Model of Spectral Envelope and Wavelet-Based Decomposition of F0\\u00a7r\\n\\n\\u00a78\\u00a7oMohammed Salah Al-Radhi\\nTam\\u00e1s G\\u00e1bor Csap\\u00f3\\nCsaba Zaink\\u00f3\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.07122\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Aug 2022 11:24:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at EUSIPCO2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Chenggang Zhang; Jinjiang Liu; Xueliang Zhang', display:{Lore:['[{"text": "arXiv:2208.07277", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLCSM: A Lightweight Complex Spectral Mapping Framework for Stereophonic Acoustic Echo Cancellation\\u00a7r\\n\\n\\u00a78\\u00a7oChenggang Zhang\\nJinjiang Liu\\nXueliang Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.07277\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Aug 2022 15:38:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Okamoto et al. (§72022§r)', author: 'Yuki Okamoto; Keisuke Imoto; Shinnosuke Takamichi; Takahiro Fukumori; Yoichi Yamashita', display:{Lore:['[{"text": "arXiv:2208.07679", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHow Should We Evaluate Synthesized Environmental Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oYuki Okamoto\\nKeisuke Imoto\\nShinnosuke Takamichi\\nTakahiro Fukumori\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.07679\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Aug 2022 11:32:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted APSIPAASC 2022\\u00a7r"}']}
{title:'Verma et al. (§72022§r)', author: 'Prateek Verma; Jonathan Berger', display:{Lore:['[{"text": "arXiv:2208.07994", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Audio Perception of Music By AI Picked Room Acoustics\\u00a7r\\n\\n\\u00a78\\u00a7oPrateek Verma\\nJonathan Berger\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.07994\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Aug 2022 23:47:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o24th International Congresson Acoustics, Gyeongju, South Korea\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Fang-Ching Chen; Kuan-Dar Chen; Yi-Wen Liu', display:{Lore:['[{"text": "arXiv:2208.08131", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDomestic sound event detection by shift consistency mean-teacher training and adversarial domain adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oFang-Ching Chen\\nKuan-Dar Chen\\nYi-Wen Liu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.08131\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Aug 2022 07:57:12 GMT)\\u00a7r"}']}
{title:'Xing et al. (§72022§r)', author: 'Ruowei Xing; Shengchen Li', display:{Lore:['[{"text": "arXiv:2208.08354", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExtract fundamental frequency based on CNN combined with PYIN\\u00a7r\\n\\n\\u00a78\\u00a7oRuowei Xing\\nShengchen Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.08354\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Aug 2022 15:34:54 GMT)\\u00a7r"}']}
{title:'Pasini et al. (§72022§r)', author: 'Marco Pasini; Jan Schlüter', display:{Lore:['[{"text": "arXiv:2208.08706", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusika! Fast Infinite Waveform Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oMarco Pasini\\nJan Schl\\u00fcter\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.08706\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Aug 2022 08:31:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ISMIR 2022\\u00a7r"}']}
{title:'Bidner et al. (§72022§r)', author: 'Annika Bidner; Julia Lindberg; Olof Lindman; Kinga Skorupska', display:{Lore:['[{"text": "arXiv:2208.08960", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeploying Enhanced Speech Feature Decreased Audio Complaints at SVT Play VOD Service\\u00a7r\\n\\n\\u00a78\\u00a7oAnnika Bidner\\nJulia Lindberg\\nOlof Lindman\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.08960\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Aug 2022 17:05:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, study based on a practical implementation at SVT\\u00a7r"}']}
{title:'Ma et al. (§72022§r)', author: 'Alison B. Ma; Alexander Lerch', display:{Lore:['[{"text": "arXiv:2208.09096", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRepresentation Learning for the Automatic Indexing of Sound Effects Libraries\\u00a7r\\n\\n\\u00a78\\u00a7oAlison B. Ma\\nAlexander Lerch\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.09096\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Aug 2022 23:46:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the 23rd International Society for Music Information Retrieval Conference (ISMIR 2022), 10 pages, 7 figures\\u00a7r"}']}
{title:'Chao et al. (§72022§r)', author: 'Fu-An Chao; Tien-Hong Lo; Tzu-I Wu; Yao-Ting Sung; Berlin Chen', display:{Lore:['[{"text": "arXiv:2208.09110", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l3M: An Effective Multi-view, Multi-granularity, and Multi-aspect Modeling Approach to English Pronunciation Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oFu-An Chao\\nTien-Hong Lo\\nTzu-I Wu\\nYao-Ting Sung\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.09110\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 12 Sep 2022 03:16:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to APSIPA ASC 2022\\u00a7r"}']}
{title:'Giannakopoulos et al. (§72022§r)', author: 'Petros Giannakopoulos; Aggelos Pikrakis; Yannis Cotronis', display:{Lore:['[{"text": "arXiv:2208.09201", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Post-Processing of Audio Event Detectors Using Reinforcement Learning\\u00a7r\\n\\n\\u00a78\\u00a7oPetros Giannakopoulos\\nAggelos Pikrakis\\nYannis Cotronis\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.09201\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ACCESS.2022.3197907\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Aug 2022 08:00:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished on IEEE Access journal, Volume 10, 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Chenglong Wang; Jiangyan Yi; Jianhua Tao; Haiyang Sun; Xun Chen; Zhengkun Tian; Haoxin Ma; Cunhang Fan; Ruibo Fu', display:{Lore:['[{"text": "arXiv:2208.09618", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFully Automated End-to-End Fake Audio Detection\\u00a7r\\n\\n\\u00a78\\u00a7oChenglong Wang\\nJiangyan Yi\\nJianhua Tao\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.09618\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 20 Aug 2022 06:46:55 GMT)\\u00a7r"}']}
{title:'Yan et al. (§72022§r)', author: 'Xinrui Yan; Jiangyan Yi; Jianhua Tao; Chenglong Wang; Haoxin Ma; Tao Wang; Shiming Wang; Ruibo Fu', display:{Lore:['[{"text": "arXiv:2208.09646", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Initial Investigation for Detecting Vocoder Fingerprints of Fake Audio\\u00a7r\\n\\n\\u00a78\\u00a7oXinrui Yan\\nJiangyan Yi\\nJianhua Tao\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.09646\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3552466.3556525\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 20 Aug 2022 09:23:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ACM Multimedia 2022 Workshop: First International Workshopon Deepfake Detection for Audio Multimedia\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Junghun Kim; Jihie Kim', display:{Lore:['[{"text": "arXiv:2208.09830", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRepresentation Learning with Graph Neural Networks for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJunghun Kim\\nJihie Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.09830\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 21 Aug 2022 07:37:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAAAI 2022 Workshop on Graphs and More Complex Structures for Learning and Reasoning (GCLR)\\u00a7r"}']}
{title:'Shin et al. (§72022§r)', author: 'Wooseok Shin; Hyun Joon Park; Jin Sob Kim; Byung Hoon Lee; Sung Won Han', display:{Lore:['[{"text": "arXiv:2208.10367", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-View Attention Transfer for Efficient Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oWooseok Shin\\nHyun Joon Park\\nJin Sob Kim\\nByung Hoon Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.10367\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10251\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 31 Oct 2022 01:10:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of Interspeech 2022\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Junghun Kim; Yoojin An; Jihie Kim', display:{Lore:['[{"text": "arXiv:2208.10491", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Speech Emotion Recognition Through Focus and Calibration Attention Mechanisms\\u00a7r\\n\\n\\u00a78\\u00a7oJunghun Kim\\nYoojin An\\nJihie Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.10491\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 21 Aug 2022 08:04:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2022\\u00a7r"}']}
{title:'Naeini et al. (§72022§r)', author: 'Saeid Alavi Naeini; Leif Simmatis; Yana Yunusova; Babak Taati', display:{Lore:['[{"text": "arXiv:2208.10597", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConcurrent Validity of Automatic Speech and Pause Measures During Passage Reading in ALS\\u00a7r\\n\\n\\u00a78\\u00a7oSaeid Alavi Naeini\\nLeif Simmatis\\nYana Yunusova\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.10597\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Aug 2022 21:01:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2022 IEEE EMBS InternationalConference on Biomedical Health Informatics (BHI)\\u00a7r"}']}
{title:'Kaur et al. (§72022§r)', author: 'Prabhjot Kaur; Qifan Wang; Weisong Shi', display:{Lore:['[{"text": "arXiv:2208.10659", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFall Detection from Audios with Audio Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oPrabhjot Kaur\\nQifan Wang\\nWeisong Shi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.10659\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Aug 2022 00:24:19 GMT)\\u00a7r"}']}
{title:'Primus et al. (§72022§r)', author: 'Paul Primus; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2208.11402", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Zero-Shot Audio Tagging     Classification with Patchout Spectrogram Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oPaul Primus\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.11402\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Aug 2022 09:48:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7opublished in EUSIPCO 2022\\u00a7r"}']}
{title:'Primus et al. (§72022§r)', author: 'Paul Primus; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2208.11460", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Natural-Language-based Audio Retrieval with Transfer Learning and Audio     Text Augmentations\\u00a7r\\n\\n\\u00a78\\u00a7oPaul Primus\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.11460\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 29 Oct 2022 19:55:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at DCASE Workshop 2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Yixiao Zhang; Junyan Jiang; Gus Xia; Simon Dixon', display:{Lore:['[{"text": "arXiv:2208.11671", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInterpreting Song Lyrics with an Audio-Informed Pre-trained Language Model\\u00a7r\\n\\n\\u00a78\\u00a7oYixiao Zhang\\nJunyan Jiang\\nGus Xia\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.11671\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Aug 2022 17:07:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ISMIR 2022\\u00a7r"}']}
{title:'Zeng et al. (§72022§r)', author: 'Chunyan Zeng; Shuai Kong; Zhifeng Wang; Xiangkui Wan; Yunfan Chen', display:{Lore:['[{"text": "arXiv:2208.11920", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDigital Audio Tampering Detection Based on ENF Spatio-temporal Features Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oChunyan Zeng\\nShuai Kong\\nZhifeng Wang\\nXiangkui Wan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.11920\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 Aug 2022 08:01:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o19 pages, 6 figures\\u00a7r"}']}
{title:'Heakl et al. (§72022§r)', author: 'Ahmed Heakl; Abdelrahman Abdelgawad; Victor Parque', display:{Lore:['[{"text": "arXiv:2208.12086", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Study on Broadcast Networks for Music Genre Classification\\u00a7r\\n\\n\\u00a78\\u00a7oAhmed Heakl\\nAbdelrahman Abdelgawad\\nVictor Parque\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.12086\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 Aug 2022 13:36:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted for oral presentation at the World Congress on Computational Intelligence (WCCI 2022) - International Joint Conference on Neural Networks (IJCNN2022)\\u00a7r"}']}
{title:'Manco et al. (§72022§r)', author: 'Ilaria Manco; Emmanouil Benetos; Elio Quinton; György Fazekas', display:{Lore:['[{"text": "arXiv:2208.12208", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContrastive Audio-Language Learning for Music\\u00a7r\\n\\n\\u00a78\\u00a7oIlaria Manco\\nEmmanouil Benetos\\nElio Quinton\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.12208\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 Aug 2022 16:55:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ISMIR 2022\\u00a7r"}']}
{title:'Schaffer et al. (§72022§r)', author: 'Noah Schaffer; Boaz Cogan; Ethan Manilow; Max Morrison; Prem Seetharaman; Bryan Pardo', display:{Lore:['[{"text": "arXiv:2208.12387", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Separation Enhancement with Generative Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oNoah Schaffer\\nBoaz Cogan\\nEthan Manilow\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.12387\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Aug 2022 00:44:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ISMIR 2022\\u00a7r"}']}
{title:'Agarwal et al. (§72022§r)', author: 'Shrutina Agarwal; Sriram Ganapathy; Naoya Takahashi', display:{Lore:['[{"text": "arXiv:2208.12410", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Symmetrical Convolutional Transformer Networks for Speech to Singing Voice Style Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oShrutina Agarwal\\nSriram Ganapathy\\nNaoya Takahashi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.12410\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Aug 2022 02:54:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Foscarin et al. (§72022§r)', author: 'Francesco Foscarin; Katharina Hoedt; Verena Praher; Arthur Flexer; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2208.12485", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConcept-Based Techniques for \\"Musicologist-friendly\\" Explanations in a Deep Music Classifier\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesco Foscarin\\nKatharina Hoedt\\nVerena Praher\\nArthur Flexer\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.12485\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 29 Aug 2022 09:43:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of the 23rd International Society for Music Information Retrieval Conference (ISMIR 2022), Bengaluru, India\\u00a7r"}']}
{title:'Zeng et al. (§72022§r)', author: 'Chunyan Zeng; Shixiong Feng; Zhifeng Wang; Xiangkui Wan; Yunfan Chen; Nan Zhao', display:{Lore:['[{"text": "arXiv:2208.12753", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatio-Temporal Representation Learning Enhanced Source Cell-phone Recognition from Speech Recordings\\u00a7r\\n\\n\\u00a78\\u00a7oChunyan Zeng\\nShixiong Feng\\nZhifeng Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.12753\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 Aug 2022 07:47:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o29 pages, 4 figures\\u00a7r"}']}
{title:'Di Giorgi et al. (§72022§r)', author: 'Bruno Di Giorgi; Mark Levy; Richard Sharp', display:{Lore:['[{"text": "arXiv:2208.12782", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMel Spectrogram Inversion with Stable Pitch\\u00a7r\\n\\n\\u00a78\\u00a7oBruno Di Giorgi\\nMark Levy\\nRichard Sharp\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.12782\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Aug 2022 17:01:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 5 figures, Proceedings of the 23st International Society for Music Information Retrieval Conference, ISMIR 2022\\u00a7r"}']}
{title:'JiaCheng et al. (§72022§r)', author: 'Deng JiaCheng; Dong Li; Yan Diqun; Wang Rangding; Zeng Jiaming', display:{Lore:['[{"text": "arXiv:2208.13066", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSA: Sliding attack for synthetic speech detection with resistance to clipping and self-splicing\\u00a7r\\n\\n\\u00a78\\u00a7oDeng JiaCheng\\nDong Li\\nYan Diqun\\nWang Rangding\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.13066\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 12 Oct 2022 10:01:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUpdated description and formula\\u00a7r"}']}
{title:'Finkelstein et al. (§72022§r)', author: 'Lev Finkelstein; Heiga Zen; Norman Casagrande; Chun-an Chan; Ye Jia; Tom Kenter; Alexey Petelin; Jonathan Shen; Vincent Wan; Yu Zhang; Yonghui Wu; Rob Clark', display:{Lore:['[{"text": "arXiv:2208.13183", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTraining Text-To-Speech Systems From Synthetic Data: A Practical Approach For Accent Transfer Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oLev Finkelstein\\nHeiga Zen\\nNorman Casagrande\\n+ 8 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.13183\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 28 Aug 2022 09:24:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in Interspeech 2022\\u00a7r"}']}
{title:'Peyser et al. (§72022§r)', author: 'Cal Peyser; Ronny Huang Andrew Rosenberg Tara N. Sainath; Michael Picheny; Kyunghyun Cho', display:{Lore:['[{"text": "arXiv:2208.13191", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Disentangled Speech Representations\\u00a7r\\n\\n\\u00a78\\u00a7oCal Peyser\\nRonny Huang Andrew Rosenberg Tara N. Sainath\\nMichael Picheny\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.13191\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 28 Aug 2022 10:03:55 GMT)\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'Ping-Chen Huang; Denis Kleyko; Jan M. Rabaey; Bruno A. Olshausen; Pentti Kanerva', display:{Lore:['[{"text": "arXiv:2208.13285", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComputing with Hypervectors for Efficient Speaker Identification\\u00a7r\\n\\n\\u00a78\\u00a7oPing-Chen Huang\\nDenis Kleyko\\nJan M. Rabaey\\nBruno A. Olshausen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.13285\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 28 Aug 2022 20:32:24 GMT)\\u00a7r"}']}
{title:'Sprunck et al. (§72022§r)', author: 'Tom Sprunck; Yannick Privat; Cédric Foy; Antoine Deleforge', display:{Lore:['[{"text": "arXiv:2208.14017", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a75physics.class-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGridless 3D Recovery of Image Sources from Room Impulse Responses\\u00a7r\\n\\n\\u00a78\\u00a7oTom Sprunck\\nYannick Privat\\nC\\u00e9dric Foy\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.14017\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2022.3224682\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 7 Dec 2022 14:37:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE Signal Processing Letters, 2022\\u00a7r"}']}
{title:'Wei et al. (§72022§r)', author: 'Weixing Wei; Peilin Li; Yi Yu; Wei Li', display:{Lore:['[{"text": "arXiv:2208.14339", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHPPNet: Modeling the Harmonic Structure and Pitch Invariance in Piano Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oWeixing Wei\\nPeilin Li\\nYi Yu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.14339\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 31 Aug 2022 01:22:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ISMIR 2022\\u00a7r"}']}
{title:'Lu et al. (§72022§r)', author: 'Peiling Lu; Xu Tan; Botao Yu; Tao Qin; Sheng Zhao; Tie-Yan Liu', display:{Lore:['[{"text": "arXiv:2208.14345", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMeloForm: Generating Melody with Musical Form based on Expert Systems and Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oPeiling Lu\\nXu Tan\\nBotao Yu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.14345\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Aug 2022 15:44:15 GMT)\\u00a7r"}']}
{title:'Jeon et al. (§72022§r)', author: 'Chang-Bin Jeon; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2208.14355", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards robust music source separation on loud commercial music\\u00a7r\\n\\n\\u00a78\\u00a7oChang-Bin Jeon\\nKyogu Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.14355\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Aug 2022 16:00:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ISMIR 2022\\u00a7r"}']}
{title:'Carnovalini et al. (§72022§r)', author: 'Filippo Carnovalini; Antonio Rodà', display:{Lore:['[{"text": "arXiv:2208.14717", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Real-Time Tempo and Meter Tracking System for Rhythmic Improvis\\u00a7r\\n\\n\\u00a78\\u00a7oFilippo Carnovalini\\nAntonio Rod\\u00e0\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.14717\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3356590.3356596\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIn Audio Mostly (AM\'19), September 18-20, 2019, Nottingham, UK.\\n  ACM, New York, NY, USA, 8 pages\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 Aug 2022 09:25:39 GMT)\\u00a7r"}']}
{title:'Carnovalini (§72022§r)', author: 'Filippo Carnovalini', display:{Lore:['[{"text": "arXiv:2208.14734", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOpen Challenges in Musical Metacreation\\u00a7r\\n\\n\\u00a78\\u00a7oFilippo Carnovalini\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.14734\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3342428.3342678\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIn EAI International Conference on Smart Objects and Technologies\\n  for Social Good (GoodTechs \'19), September 25-27, 2019, Valencia, Spain. ACM,\\n  New York, NY, USA, 2 pages\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 Aug 2022 09:34:27 GMT)\\u00a7r"}']}
{title:'Carnovalini et al. (§72022§r)', author: 'Filippo Carnovalini; Antonio Rodà; Nicholas Harley; Steven T. Homer; Geraint A. Wiggins', display:{Lore:['[{"text": "arXiv:2208.14747", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA New Corpus for Computational Music Research and A Novel Method for Musical Structure Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oFilippo Carnovalini\\nAntonio Rod\\u00e0\\nNicholas Harley\\nSteven T. Homer\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.14747\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3478384.3478402\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIn Audio Mostly 2021 (AM \'21), September 1-3, 2021,\\n  virtual/Trento, Italy. ACM, New York, NY, USA, 4 pages\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 Aug 2022 09:49:10 GMT)\\u00a7r"}']}
{title:'Carnovalini et al. (§72022§r)', author: 'Filippo Carnovalini; Alessandro Pelizzo; Antonio Rodà; Sergio Canazza', display:{Lore:['[{"text": "arXiv:2208.14750", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHarmonization and Evaluation; Tweaking the Parameters on Human Listeners\\u00a7r\\n\\n\\u00a78\\u00a7oFilippo Carnovalini\\nAlessandro Pelizzo\\nAntonio Rod\\u00e0\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.14750\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 Aug 2022 09:54:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in 9th International Conference on Kansei Engineering and Emotion Research 2022\\u00a7r"}']}
{title:'Yan et al. (§72022§r)', author: 'Jing-ke Yan; Xin Wang; Qin Wang; Qin Qin; Huang-he Li; Peng-fei Ye; Yue-ping He; Jing Zeng', display:{Lore:['[{"text": "arXiv:2208.14812", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDomain Shift-oriented Machine Anomalous Sound Detection Model Based on Self-Supervised Learning\\u00a7r\\n\\n\\u00a78\\u00a7oJing-ke Yan\\nXin Wang\\nQin Wang\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.14812\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 7 Sep 2022 07:49:26 GMT)\\u00a7r"}']}
{title:'Karystinaios et al. (§72022§r)', author: 'Emmanouil Karystinaios; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2208.14819", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCadence Detection in Symbolic Classical Music using Graph Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oEmmanouil Karystinaios\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.14819\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 Aug 2022 12:39:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn proceedings of the International Society for Music Information Retrieval Conference 2022 (ISMIR)\\u00a7r"}']}
{title:'Rhyu et al. (§72022§r)', author: 'Seungyeon Rhyu; Sarah Kim; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2208.14867", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSketching the Expression: Flexible Rendering of Expressive Piano Performance with Self-Supervised Learning\\u00a7r\\n\\n\\u00a78\\u00a7oSeungyeon Rhyu\\nSarah Kim\\nKyogu Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.14867\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 3 Sep 2022 08:58:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 4 figures, the 23rd International Society for Music Information Retrieval Conference, Bengaluru, India, 2022\\u00a7r"}']}
{title:'Vinay et al. (§72022§r)', author: 'Ashvala Vinay; Alexander Lerch', display:{Lore:['[{"text": "arXiv:2209.00130", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluating generative audio systems and their metrics\\u00a7r\\n\\n\\u00a78\\u00a7oAshvala Vinay\\nAlexander Lerch\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.00130\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 Aug 2022 21:48:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ISMIR 2022\\u00a7r"}']}
{title:'Dai et al. (§72022§r)', author: 'Shuqi Dai; Huiran Yu; Roger B. Dannenberg', display:{Lore:['[{"text": "arXiv:2209.00182", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhat is missing in deep music generation? A study of repetition and structure in popular music\\u00a7r\\n\\n\\u00a78\\u00a7oShuqi Dai\\nHuiran Yu\\nRoger B. Dannenberg\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.00182\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Sep 2022 02:22:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of the 23rd Int. Society for Music Information Retrieval (ISMIR) 2022\\u00a7r"}']}
{title:'Dahale et al. (§72022§r)', author: 'Rishabh Dahale; Vaibhav Talwadker; Preeti Rao; Prateek Verma', display:{Lore:['[{"text": "arXiv:2209.00291", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerating Coherent Drum Accompaniment With Fills And Improvisations\\u00a7r\\n\\n\\u00a78\\u00a7oRishabh Dahale\\nVaibhav Talwadker\\nPreeti Rao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.00291\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Sep 2022 08:31:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 7 figures, 23rd International Society for Music Information Retrieval Conference (ISMIR 2022), Bengaluru, India\\u00a7r"}']}
{title:'Yi et al. (§72022§r)', author: 'Li Yi; Haochen Hu; Jingwei Zhao; Gus Xia', display:{Lore:['[{"text": "arXiv:2209.00353", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAccoMontage2: A Complete Harmonization and Accompaniment Arrangement System\\u00a7r\\n\\n\\u00a78\\u00a7oLi Yi\\nHaochen Hu\\nJingwei Zhao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.00353\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Sep 2022 10:42:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ISMIR 2022\\u00a7r"}']}
{title:'Quaderi et al. (§72022§r)', author: 'Shah Jafor Sadeek Quaderi; Sadia Afrin Labonno; Sadia Mostafa; Shamim Akhter', display:{Lore:['[{"text": "arXiv:2209.01374", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIdentify The Beehive Sound Using Deep Learning\\u00a7r\\n\\n\\u00a78\\u00a7oShah Jafor Sadeek Quaderi\\nSadia Afrin Labonno\\nSadia Mostafa\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.01374\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 3 Sep 2022 09:07:39 GMT)\\u00a7r"}']}
{title:'Quinton (§72022§r)', author: 'Elio Quinton', display:{Lore:['[{"text": "arXiv:2209.01478", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEquivariant Self-Supervision for Musical Tempo Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oElio Quinton\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.01478\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 3 Sep 2022 18:43:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ISMIR 2022\\u00a7r"}']}
{title:'Yeh et al. (§72022§r)', author: 'Yen-Tung Yeh; Bo-Yu Chen; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2209.01751", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting Pre-trained Feature Networks for Generative Adversarial Networks in Audio-domain Loop Generation\\u00a7r\\n\\n\\u00a78\\u00a7oYen-Tung Yeh\\nBo-Yu Chen\\nYi-Hsuan Yang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.01751\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Sep 2022 04:06:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ISMIR 2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Peining Zhang; Junliang Guo; Linli Xu; Mu You; Junming Yin', display:{Lore:['[{"text": "arXiv:2209.01996", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBridging Music and Text with Crowdsourced Music Comments: A Sequence-to-Sequence Framework for Thematic Music Comments Generation\\u00a7r\\n\\n\\u00a78\\u00a7oPeining Zhang\\nJunliang Guo\\nLinli Xu\\nMu You\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.01996\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Sep 2022 14:51:51 GMT)\\u00a7r"}']}
{title:'Han et al. (§72022§r)', author: 'Sangjun Han; Hyeongrae Ihm; DaeHan Ahn; Woohyung Lim', display:{Lore:['[{"text": "arXiv:2209.02696", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInstrument Separation of Symbolic Music by Explicitly Guided Diffusion Model\\u00a7r\\n\\n\\u00a78\\u00a7oSangjun Han\\nHyeongrae Ihm\\nDaeHan Ahn\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.02696\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Sep 2022 04:50:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to NeurIPS 2022 Workshopon Machine Learning for Creativity and Design\\u00a7r"}']}
{title:'Bansal (§72022§r)', author: 'Rishibha Bansal', display:{Lore:['[{"text": "arXiv:2209.02785", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRead it to me: An emotionally aware Speech Narration Application\\u00a7r\\n\\n\\u00a78\\u00a7oRishibha Bansal\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.02785\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Sep 2022 19:13:09 GMT)\\u00a7r"}']}
{title:'Noufi et al. (§72022§r)', author: 'Camille Noufi; Lloyd May; Jonathan Berger', display:{Lore:['[{"text": "arXiv:2209.02855", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.SY\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SY\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Role of Vocal Persona in Natural and Synthesized Speech\\u00a7r\\n\\n\\u00a78\\u00a7oCamille Noufi\\nLloyd May\\nJonathan Berger\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.02855\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 31 Oct 2022 17:30:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in the proceedings of the 17th IEEE InternationalConference on Automatic Face and Gesture Recognition as part of the Workshop on Socially Interactive Human-like Virtual Agents (SIVA \'23)\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Ke Chen; Hao-Wen Dong; Yi Luo; Julian McAuley; Taylor Berg-Kirkpatrick; Miller Puckette; Shlomo Dubnov', display:{Lore:['[{"text": "arXiv:2209.02871", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Choral Music Separation through Expressive Synthesized Data from Sampled Instruments\\u00a7r\\n\\n\\u00a78\\u00a7oKe Chen\\nHao-Wen Dong\\nYi Luo\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.02871\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nThe 23rd International Society for Music Information Retrieval\\n  Conference, 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Sep 2022 01:13:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCamera Ready for Proceedings of the 23rd International Society for Music Information Retrieval Conference, ISMIR 2022\\u00a7r"}']}
{title:'Lovenia et al. (§72022§r)', author: 'Holy Lovenia; Dessi Puji Lestari; Rita Frieske', display:{Lore:['[{"text": "arXiv:2209.03711", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhat Did I Just Hear? Detecting Pornographic Sounds in Adult Videos Using Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oHoly Lovenia\\nDessi Puji Lestari\\nRita Frieske\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.03711\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3561212.3561244\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Sep 2022 11:02:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in AudioMostly 2022, ACM\\u00a7r"}']}
{title:'Aljbawi et al. (§72022§r)', author: 'Wafaa Aljbawi; Sami O. Simmons; Visara Urovi', display:{Lore:['[{"text": "arXiv:2209.03727", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeveloping a multi-variate prediction model for the detection of COVID-19 from Crowd-sourced Respiratory Voice Data\\u00a7r\\n\\n\\u00a78\\u00a7oWafaa Aljbawi\\nSami O. Simmons\\nVisara Urovi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.03727\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Sep 2022 11:46:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 6 figures, poster presented at the European Respiratory Society, Barcelona, September 2022\\u00a7r"}']}
{title:'Gerum et al. (§72022§r)', author: 'Christoph Gerum; Adrian Frischknecht; Tobias Hald; Paul Palomero Bernardo; Konstantin Lübeck; Oliver Bringmann', display:{Lore:['[{"text": "arXiv:2209.03807", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHardware Accelerator and Neural Network Co-Optimization for Ultra-Low-Power Audio Processing Devices\\u00a7r\\n\\n\\u00a78\\u00a7oChristoph Gerum\\nAdrian Frischknecht\\nTobias Hald\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.03807\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 29 Sep 2022 13:26:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted Version for: EUROMICRODSD 2022\\u00a7r"}']}
{title:'Lamichhane et al. (§72022§r)', author: 'Bishal Lamichhane; Nidal Moukaddam; Ankit B. Patel; Ashutosh Sabharwal', display:{Lore:['[{"text": "arXiv:2209.03901", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDyadic Interaction Assessment from Free-living Audio for Depression Severity Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oBishal Lamichhane\\nNidal Moukaddam\\nAnkit B. Patel\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.03901\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Sep 2022 16:16:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Walden et al. (§72022§r)', author: 'Finley Walden; Sagar Dasgupta; Mizanur Rahman; Mhafuzul Islam', display:{Lore:['[{"text": "arXiv:2209.04075", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving the Environmental Perception of Autonomous Vehicles using Deep Learning-based Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oFinley Walden\\nSagar Dasgupta\\nMizanur Rahman\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.04075\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Sep 2022 01:23:13 GMT)\\u00a7r"}']}
{title:'Ono et al. (§72022§r)', author: 'Yusuke Ono; Sunao Hara; Masanobu Abe', display:{Lore:['[{"text": "arXiv:2209.04077", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrediction method of Soundscape Impressions using Environmental Sounds and Aerial Photographs\\u00a7r\\n\\n\\u00a78\\u00a7oYusuke Ono\\nSunao Hara\\nMasanobu Abe\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.04077\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Sep 2022 01:41:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted APSIPAASC 2022\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Xiaokai Liu; Menghua Zhang', display:{Lore:['[{"text": "arXiv:2209.04109", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMATT: A Multiple-instance Attention Mechanism for Long-tail Music Genre Classification\\u00a7r\\n\\n\\u00a78\\u00a7oXiaokai Liu\\nMenghua Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.04109\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Sep 2022 03:52:44 GMT)\\u00a7r"}']}
{title:'Lebourdais et al. (§72022§r)', author: 'Martin Lebourdais; Marie Tahon; Antoine Laurent; Sylvain Meignier', display:{Lore:['[{"text": "arXiv:2209.04167", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOverlapped speech and gender detection with WavLM pre-trained features\\u00a7r\\n\\n\\u00a78\\u00a7oMartin Lebourdais\\nMarie Tahon\\nAntoine Laurent\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.04167\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Sep 2022 08:00:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted and accepted to Interspeech 2022\\u00a7r"}']}
{title:'Orlandic et al. (§72022§r)', author: 'Lara Orlandic; Tomas Teijeiro; David Atienza', display:{Lore:['[{"text": "arXiv:2209.04360", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Semi-Supervised Algorithm for Improving the Consistency of Crowdsourced Datasets: The COVID-19 Case Study on Respiratory Disorder Classification\\u00a7r\\n\\n\\u00a78\\u00a7oLara Orlandic\\nTomas Teijeiro\\nDavid Atienza\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.04360\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.cmpb.2023.107743\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Sep 2022 15:44:26 GMT)\\u00a7r"}']}
{title:'Yuan et al. (§72022§r)', author: 'Ruibin Yuan; Yuxuan Wu; Jacob Li; Jaxter Kim', display:{Lore:['[{"text": "arXiv:2209.04530", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeID-VC: Speaker De-identification via Zero-shot Pseudo Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oRuibin Yuan\\nYuxuan Wu\\nJacob Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.04530\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Sep 2022 21:13:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Lantian Li; Di Wang; Dong Wang', display:{Lore:['[{"text": "arXiv:2209.04687", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPay Attention to Hard Trials\\u00a7r\\n\\n\\u00a78\\u00a7oLantian Li\\nDi Wang\\nDong Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.04687\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 10 Sep 2022 15:16:05 GMT)\\u00a7r"}']}
{title:'Krause et al. (§72022§r)', author: 'Daniel Aleksander Krause; Annamaria Mesaros', display:{Lore:['[{"text": "arXiv:2209.05900", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBinaural Signal Representations for Joint Sound Event Detection and Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Aleksander Krause\\nAnnamaria Mesaros\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.05900\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Sep 2022 11:29:00 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Zihao Wang; Qihao Liang; Kejun Zhang; Yuxing Wang; Chen Zhang; Pengfei Yu; Yongsheng Feng; Wenbo Liu; Yikai Wang; Yuntai Bao; Yiheng Yang', display:{Lore:['[{"text": "arXiv:2209.06054", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSongDriver: Real-time Music Accompaniment Generation without Logical Latency nor Exposure Bias\\u00a7r\\n\\n\\u00a78\\u00a7oZihao Wang\\nQihao Liang\\nKejun Zhang\\n+ 7 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.06054\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3503161.3548368\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 13 Oct 2022 10:03:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o*Both Zihao Wang and Qihao Liang contribute equally to the paper and share the co-first authorship. This paper has been accepted by ACM Multimedia 2022, oral session, full paper(main track)\\u00a7r"}']}
{title:'Chinen et al. (§72022§r)', author: 'Michael Chinen; Jan Skoglund; Chandan K A Reddy; Alessandro Ragano; Andrew Hines', display:{Lore:['[{"text": "arXiv:2209.06358", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing Rater and System Metadata to Explain Variance in the VoiceMOS Challenge 2022 Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oMichael Chinen\\nJan Skoglund\\nChandan K A Reddy\\nAlessandro Ragano\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.06358\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Sep 2022 00:45:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint; accepted for Interspeech 2022\\u00a7r"}']}
{title:'Ng et al. (§72022§r)', author: 'Dianwen Ng; Jia Qi Yip; Tanmay Surana; Zhao Yang; Chong Zhang; Yukun Ma; Chongjia Ni; Eng Siong Chng; Bin Ma', display:{Lore:['[{"text": "arXiv:2209.06360", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lI2CR: Improving Noise Robustness on Keyword Spotting Using Inter-Intra Contrastive Regularization\\u00a7r\\n\\n\\u00a78\\u00a7oDianwen Ng\\nJia Qi Yip\\nTanmay Surana\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.06360\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Sep 2022 00:52:19 GMT)\\u00a7r"}']}
{title:'Ma et al. (§72022§r)', author: 'Qiaowei Ma; Jinghui Zhong; Yitao Yang; Weiheng Liu; Ying Gao; Wing W. Y. Ng', display:{Lore:['[{"text": "arXiv:2209.06434", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConvNeXt Based Neural Network for Audio Anti-Spoofing\\u00a7r\\n\\n\\u00a78\\u00a7oQiaowei Ma\\nJinghui Zhong\\nYitao Yang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.06434\\u00a7r\\n\\nVersion:\\u00a77v5 (Thu, 22 Dec 2022 03:11:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages\\u00a7r"}']}
{title:'Xue et al. (§72022§r)', author: 'Liumeng Xue; Frank K. Soong; Shaofei Zhang; Lei Xie', display:{Lore:['[{"text": "arXiv:2209.06484", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParaTTS: Learning Linguistic and Prosodic Cross-sentence Information in Paragraph-based TTS\\u00a7r\\n\\n\\u00a78\\u00a7oLiumeng Xue\\nFrank K. Soong\\nShaofei Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.06484\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Sep 2022 08:34:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in IEEE/ACM Transactionson Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Gary Wang; Andrew Rosenberg; Bhuvana Ramabhadran; Fadi Biadsy; Yinghui Huang; Jesse Emond; Pedro Moreno Mengibar', display:{Lore:['[{"text": "arXiv:2209.06987", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-Parallel Voice Conversion for ASR Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oGary Wang\\nAndrew Rosenberg\\nBhuvana Ramabhadran\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.06987\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Sep 2022 00:40:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Zhao et al. (§72022§r)', author: 'Jingwei Zhao; Gus Xia; Ye Wang', display:{Lore:['[{"text": "arXiv:2209.07140", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBeat Transformer: Demixed Beat and Downbeat Tracking with Dilated Self-Attention\\u00a7r\\n\\n\\u00a78\\u00a7oJingwei Zhao\\nGus Xia\\nYe Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.07140\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Sep 2022 08:38:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ISMIR 2022\\u00a7r"}']}
{title:'Zhao et al. (§72022§r)', author: 'Jingwei Zhao; Gus Xia; Ye Wang', display:{Lore:['[{"text": "arXiv:2209.07144", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDomain Adversarial Training on Conditional Variational Auto-Encoder for Controllable Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oJingwei Zhao\\nGus Xia\\nYe Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.07144\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Sep 2022 08:45:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ISMIR 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Jianrong Wang; Xiaomin Li; Xuewei Li; Mei Yu; Qiang Fang; Li Liu', display:{Lore:['[{"text": "arXiv:2209.07302", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMVNet: Memory Assistance and Vocal Reinforcement Network for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oJianrong Wang\\nXiaomin Li\\nXuewei Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.07302\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Sep 2022 13:57:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICONIP2022\\u00a7r"}']}
{title:'Karas et al. (§72022§r)', author: 'Vincent Karas; Andreas Triantafyllopoulos; Meishu Song; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2209.07384", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Attention Networks and Uncertainty Loss Weighting for Multi-Task Emotion Recognition on Vocal Bursts\\u00a7r\\n\\n\\u00a78\\u00a7oVincent Karas\\nAndreas Triantafyllopoulos\\nMeishu Song\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.07384\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 27 Sep 2022 16:54:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 1 figure, accepted at The 2022 ACII Affective Vocal Burst Workshop Challenge (A-VB)\\u00a7r"}']}
{title:'Rahman et al. (§72022§r)', author: 'Md Hafizur Rahman; Martin Graciarena; Diego Castan; Chris Cobo-Kroenke; Mitchell McLaren; Aaron Lawson', display:{Lore:['[{"text": "arXiv:2209.07498", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetecting Synthetic Speech Manipulation in Real Audio Recordings\\u00a7r\\n\\n\\u00a78\\u00a7oMd Hafizur Rahman\\nMartin Graciarena\\nDiego Castan\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.07498\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Sep 2022 17:40:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEEInternational Workshopon Information Forensics and Security(WIFS)\\u00a7r"}']}
{title:'Trinh et al. (§72022§r)', author: 'Dang-Linh Trinh; Minh-Cong Vo; Guee-Sang Lee', display:{Lore:['[{"text": "arXiv:2209.07629", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Relation Attention and Temporal Awareness for Emotion Recognition via Vocal Burst\\u00a7r\\n\\n\\u00a78\\u00a7oDang-Linh Trinh\\nMinh-Cong Vo\\nGuee-Sang Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.07629\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 26 Sep 2022 06:45:27 GMT)\\u00a7r"}']}
{title:'Hernandez-Olivan et al. (§72022§r)', author: 'Carlos Hernandez-Olivan; Jose R. Beltran', display:{Lore:['[{"text": "arXiv:2209.07974", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lmusicaiz: A Python Library for Symbolic Music Generation, Analysis and Visualization\\u00a7r\\n\\n\\u00a78\\u00a7oCarlos Hernandez-Olivan\\nJose R. Beltran\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.07974\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 16 Sep 2022 14:42:47 GMT)\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Dichucheng Li; Yulun Wu; Qinyu Li; Jiahao Zhao; Yi Yu; Fan Xia; Wei Li', display:{Lore:['[{"text": "arXiv:2209.08774", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPlaying Technique Detection by Fusing Note Onset Information in Guzheng Performance\\u00a7r\\n\\n\\u00a78\\u00a7oDichucheng Li\\nYulun Wu\\nQinyu Li\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.08774\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Sep 2022 06:02:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ISMIR 2022\\u00a7r"}']}
{title:'Tian et al. (§72022§r)', author: 'Jingguang Tian; Xinhui Hu; Xinkang Xu', display:{Lore:['[{"text": "arXiv:2209.09010", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Royalflush System for VoxCeleb Speaker Recognition Challenge 2022\\u00a7r\\n\\n\\u00a78\\u00a7oJingguang Tian\\nXinhui Hu\\nXinkang Xu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.09010\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 20 Sep 2022 12:46:18 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Zhengyang Chen; Bing Han; Xu Xiang; Houjun Huang; Bei Liu; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2209.09076", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSJTU-AISPEECH System for VoxCeleb Speaker Recognition Challenge 2022\\u00a7r\\n\\n\\u00a78\\u00a7oZhengyang Chen\\nBing Han\\nXu Xiang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.09076\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 20 Sep 2022 15:33:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSystem description of VoxSRC 2022\\u00a7r"}']}
{title:'Mo et al. (§72022§r)', author: 'Shentong Mo; Pedro Morgado', display:{Lore:['[{"text": "arXiv:2209.09634", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Closer Look at Weakly-Supervised Audio-Visual Source Localization\\u00a7r\\n\\n\\u00a78\\u00a7oShentong Mo\\nPedro Morgado\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.09634\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Aug 2022 14:17:46 GMT)\\u00a7r"}']}
{title:'Zhou et al. (§72022§r)', author: 'Ruohua Zhou; Yuxuan Du; Chenlei Hu', display:{Lore:['[{"text": "arXiv:2209.09635", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe BUCEA Speaker Diarization System for the VoxCeleb Speaker Recognition Challenge 2022\\u00a7r\\n\\n\\u00a78\\u00a7oRuohua Zhou\\nYuxuan Du\\nChenlei Hu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.09635\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Sep 2022 11:33:58 GMT)\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Junkai Wu; Jonah Casebeer; Nicholas J. Bryan; Paris Smaragdis', display:{Lore:['[{"text": "arXiv:2209.09955", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMeta-Learning for Adaptive Filters with Higher-Order Frequency Dependencies\\u00a7r\\n\\n\\u00a78\\u00a7oJunkai Wu\\nJonah Casebeer\\nNicholas J. Bryan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.09955\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Sep 2022 19:22:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSource code andaudio examples: https://jmcasebeer.github.io/metaaf/higher-order\\u00a7r"}']}
{title:'Tripodi (§72022§r)', author: 'Ignacio J. Tripodi', display:{Lore:['[{"text": "arXiv:2209.10016", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSetting the rhythm scene: deep learning-based drum loop generation from arbitrary language cues\\u00a7r\\n\\n\\u00a78\\u00a7oIgnacio J. Tripodi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.10016\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Sep 2022 21:53:35 GMT)\\u00a7r"}']}
{title:'Alisamir et al. (§72022§r)', author: 'Sina Alisamir; Fabien Ringeval; Francois Portet', display:{Lore:['[{"text": "arXiv:2209.10223", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDynamic Time-Alignment of Dimensional Annotations of Emotion using Recurrent Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oSina Alisamir\\nFabien Ringeval\\nFrancois Portet\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.10223\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Sep 2022 09:38:57 GMT)\\u00a7r"}']}
{title:'Jiang et al. (§72022§r)', author: 'Junyan Jiang; Daniel Chin; Yixiao Zhang; Gus Xia', display:{Lore:['[{"text": "arXiv:2209.10259", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Hierarchical Metrical Structure Beyond Measures\\u00a7r\\n\\n\\u00a78\\u00a7oJunyan Jiang\\nDaniel Chin\\nYixiao Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.10259\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Sep 2022 11:08:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the International Society for Music Information Retrieval (ISMIR), 2022\\u00a7r"}']}
{title:'Qu et al. (§72022§r)', author: 'Yang Qu; Yutian Qin; Lecheng Chao; Hangkai Qian; Ziyu Wang; Gus Xia', display:{Lore:['[{"text": "arXiv:2209.10674", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModeling Perceptual Loudness of Piano Tone: Theory and Applications\\u00a7r\\n\\n\\u00a78\\u00a7oYang Qu\\nYutian Qin\\nLecheng Chao\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.10674\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 30 Oct 2022 22:59:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ISMIR 2022\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Rui Liu; Berrak Sisman; Guanglai Gao; Haizhou Li', display:{Lore:['[{"text": "arXiv:2209.10804", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lControllable Accented Text-to-Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oRui Liu\\nBerrak Sisman\\nGuanglai Gao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.10804\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Sep 2022 06:13:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be submitted for possible journal publication\\u00a7r"}']}
{title:'Zheng et al. (§72022§r)', author: 'Yu Zheng; Yihao Chen; Jinghan Peng; Yajun Zhang; Min Liu; Minqiang Xu', display:{Lore:['[{"text": "arXiv:2209.10846", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe SpeakIn System Description for CNSRC2022\\u00a7r\\n\\n\\u00a78\\u00a7oYu Zheng\\nYihao Chen\\nJinghan Peng\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.10846\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Sep 2022 08:17:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages\\u00a7r"}']}
{title:'Hu et al. (§72022§r)', author: 'Yifan Hu; Pengkai Yin; Rui Liu; Feilong Bao; Guanglai Gao', display:{Lore:['[{"text": "arXiv:2209.10848", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMnTTS: An Open-Source Mongolian Text-to-Speech Synthesis Dataset and Accompanied Baseline\\u00a7r\\n\\n\\u00a78\\u00a7oYifan Hu\\nPengkai Yin\\nRui Liu\\nFeilong Bao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.10848\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Sep 2022 08:24:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the 2022 International Conference on Asian Language Processing (IALP2022)\\u00a7r"}']}
{title:'Guo et al. (§72022§r)', author: 'Haohan Guo; Fenglong Xie; Frank K. Soong; Xixin Wu; Helen Meng', display:{Lore:['[{"text": "arXiv:2209.10887", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Multi-Stage Multi-Codebook VQ-VAE Approach to High-Performance Neural TTS\\u00a7r\\n\\n\\u00a78\\u00a7oHaohan Guo\\nFenglong Xie\\nFrank K. Soong\\nXixin Wu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.10887\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Sep 2022 09:43:17 GMT)\\u00a7r"}']}
{title:'Díaz-Báñez et al. (§72022§r)', author: 'José-Miguel Díaz-Báñez; Nadine Kroher', display:{Lore:['[{"text": "arXiv:2209.10970", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMaths, Computation and Flamenco: overview and challenges\\u00a7r\\n\\n\\u00a78\\u00a7oJos\\u00e9-Miguel D\\u00edaz-B\\u00e1\\u00f1ez\\nNadine Kroher\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.10970\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Sep 2022 12:43:17 GMT)\\u00a7r"}']}
{title:'Valentini-Botinhao et al. (§72022§r)', author: 'Cassia Valentini-Botinhao; Manuel Sam Ribeiro; Oliver Watts; Korin Richmond; Gustav Eje Henter', display:{Lore:['[{"text": "arXiv:2209.11003", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPredicting pairwise preferences between TTS audio stimuli using parallel ratings data and anti-symmetric twin neural networks\\u00a7r\\n\\n\\u00a78\\u00a7oCassia Valentini-Botinhao\\nManuel Sam Ribeiro\\nOliver Watts\\nKorin Richmond\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.11003\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10132\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of INTERSPEECH 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Sep 2022 13:34:22 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Gang Liu; Tianyan Zhou; Yong Zhao; Yu Wu; Zhuo Chen; Yao Qian; Jian Wu', display:{Lore:['[{"text": "arXiv:2209.11266", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Microsoft System for VoxCeleb Speaker Recognition Challenge 2022\\u00a7r\\n\\n\\u00a78\\u00a7oGang Liu\\nTianyan Zhou\\nYong Zhao\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.11266\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Sep 2022 18:36:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o3 pages, 3 tables, VoxSRC2022\\u00a7r"}']}
{title:'Dinkel et al. (§72022§r)', author: 'Heinrich Dinkel; Yongqing Wang; Zhiyong Yan; Junbo Zhang; Yujun Wang', display:{Lore:['[{"text": "arXiv:2209.11377", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUniKW-AT: Unified Keyword Spotting and Audio Tagging\\u00a7r\\n\\n\\u00a78\\u00a7oHeinrich Dinkel\\nYongqing Wang\\nZhiyong Yan\\nJunbo Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.11377\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-607\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Sep 2022 02:39:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in Interspeech2022\\u00a7r"}']}
{title:'Tran et al. (§72022§r)', author: 'Thanh Tran; Sebastian Bader; Jan Lundgren', display:{Lore:['[{"text": "arXiv:2209.11527", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn artificial neural network-based system for detecting machine failures using tiny sound data: A case study\\u00a7r\\n\\n\\u00a78\\u00a7oThanh Tran\\nSebastian Bader\\nJan Lundgren\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.11527\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Sep 2022 11:13:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 9 figures, conference\\u00a7r"}']}
{title:'Hu et al. (§72022§r)', author: 'Chenlei Hu; Ruohua Zhou', display:{Lore:['[{"text": "arXiv:2209.11585", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynthetic Voice Spoofing Detection Based On Online Hard Example Mining\\u00a7r\\n\\n\\u00a78\\u00a7oChenlei Hu\\nRuohua Zhou\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.11585\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 26 Sep 2022 13:16:22 GMT)\\u00a7r"}']}
{title:'Zheng et al. (§72022§r)', author: 'Yu Zheng; Jinghan Peng; Yihao Chen; Yajun Zhang; Jialong Wang; Min Liu; Minqiang Xu', display:{Lore:['[{"text": "arXiv:2209.11625", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe SpeakIn Speaker Verification System for Far-Field Speaker Verification Challenge 2022\\u00a7r\\n\\n\\u00a78\\u00a7oYu Zheng\\nJinghan Peng\\nYihao Chen\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.11625\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Sep 2022 14:51:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages. arXiv admin note:text overlap with arXiv:2209.10846\\u00a7r"}']}
{title:'Wan et al. (§72022§r)', author: 'Xucheng Wan; Kai Liu; Ziqing Du; Huan Zhou', display:{Lore:['[{"text": "arXiv:2209.11905", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Enhancement with Perceptually-motivated Optimization and Dual Transformations\\u00a7r\\n\\n\\u00a78\\u00a7oXucheng Wan\\nKai Liu\\nZiqing Du\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.11905\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 24 Sep 2022 02:33:40 GMT)\\u00a7r"}']}
{title:'Du et al. (§72022§r)', author: 'Ziqing Du; Kai Liu; Xucheng Wan; Huan Zhou', display:{Lore:['[{"text": "arXiv:2209.11906", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Speech Activity and Overlap Detection with Multi-Exit Architecture\\u00a7r\\n\\n\\u00a78\\u00a7oZiqing Du\\nKai Liu\\nXucheng Wan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.11906\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 24 Sep 2022 02:34:11 GMT)\\u00a7r"}']}
{title:'Mai et al. (§72022§r)', author: 'Long Mai; Julie Carson-Berndsen', display:{Lore:['[{"text": "arXiv:2209.12043", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised domain adaptation for speech recognition with unsupervised error correction\\u00a7r\\n\\n\\u00a78\\u00a7oLong Mai\\nJulie Carson-Berndsen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.12043\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 24 Sep 2022 16:05:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2022\\u00a7r"}']}
{title:'Rosero et al. (§72022§r)', author: 'Karen Rosero; Arthur Nicholas dos Santos; Pedro Benevenuto Valadares; Bruno Sanches Masiero', display:{Lore:['[{"text": "arXiv:2209.12045", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSong Emotion Recognition: a Performance Comparison Between Audio Features and Artificial Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oKaren Rosero\\nArthur Nicholas dos Santos\\nPedro Benevenuto Valadares\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.12045\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 24 Sep 2022 16:13:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages,\\u00a7r"}']}
{title:'Nakai et al. (§72022§r)', author: 'Yusuke Nakai; Yuki Saito; Kenta Udagawa; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2209.12549", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Task Adversarial Training Algorithm for Multi-Speaker Neural Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oYusuke Nakai\\nYuki Saito\\nKenta Udagawa\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.12549\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Sep 2022 10:10:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 1 figure, Accepted for APSIPA ASC 2022\\u00a7r"}']}
{title:'Sztahó et al. (§72022§r)', author: 'Dávid Sztahó; Attila Fejes', display:{Lore:['[{"text": "arXiv:2209.12602", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEffects of language mismatch in automatic forensic voice comparison using deep learning embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oD\\u00e1vid Sztah\\u00f3\\nAttila Fejes\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.12602\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1111/1556-4029.15250\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Sep 2022 11:49:37 GMT)\\u00a7r"}']}
{title:'Bereg et al. (§72022§r)', author: 'Sergey Bereg; José-Miguel Díaz-Báñez; Nadine Kroher; Inmaculada Ventura', display:{Lore:['[{"text": "arXiv:2209.13598", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComputing Melodic Templates in Oral Music Traditions\\u00a7r\\n\\n\\u00a78\\u00a7oSergey Bereg\\nJos\\u00e9-Miguel D\\u00edaz-B\\u00e1\\u00f1ez\\nNadine Kroher\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.13598\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Sep 2022 08:40:55 GMT)\\u00a7r"}']}
{title:'Hallmen et al. (§72022§r)', author: 'Tobias Hallmen; Silvan Mertes; Dominik Schiller; Elisabeth André', display:{Lore:['[{"text": "arXiv:2209.13914", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Efficient Multitask Learning Architecture for Affective Vocal Burst Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oTobias Hallmen\\nSilvan Mertes\\nDominik Schiller\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.13914\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Sep 2022 08:32:08 GMT)\\u00a7r"}']}
{title:'Pianese et al. (§72022§r)', author: 'Alessandro Pianese; Davide Cozzolino; Giovanni Poggi; Luisa Verdoliva', display:{Lore:['[{"text": "arXiv:2209.14098", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeepfake audio detection by speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oAlessandro Pianese\\nDavide Cozzolino\\nGiovanni Poggi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.14098\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Sep 2022 13:46:29 GMT)\\u00a7r"}']}
{title:'Anton et al. (§72022§r)', author: 'Jonah Anton; Harry Coppock; Pancham Shukla; Bjorn W. Schuller', display:{Lore:['[{"text": "arXiv:2209.14345", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Barlow Twins: Self-Supervised Audio Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oJonah Anton\\nHarry Coppock\\nPancham Shukla\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.14345\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Sep 2022 18:17:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages (4 main text, rest references + appendices)\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Yusong Wu; Josh Gardner; Ethan Manilow; Ian Simon; Curtis Hawthorne; Jesse Engel', display:{Lore:['[{"text": "arXiv:2209.14458", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Chamber Ensemble Generator: Limitless High-Quality MIR Data via Generative Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oYusong Wu\\nJosh Gardner\\nEthan Manilow\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.14458\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Sep 2022 22:55:15 GMT)\\u00a7r"}']}
{title:'Syed et al. (§72022§r)', author: 'Muhammad Shehram Shah Syed; Zafi Sherhan Syed; Abbas Syed', display:{Lore:['[{"text": "arXiv:2209.14842", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lClassification of Vocal Bursts for ACII 2022 A-VB-Type Competition using Convolutional Neural Networks and Deep Acoustic Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oMuhammad Shehram Shah Syed\\nZafi Sherhan Syed\\nAbbas Syed\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.14842\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 13 Oct 2022 04:55:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oReport for our submission to the ACII 2022 Affective Vocal Bursts (A-VB) Competition\\u00a7r"}']}
{title:'Radfar et al. (§72022§r)', author: 'Martin Radfar; Rohit Barnwal; Rupak Vignesh Swaminathan; Feng-Ju Chang; Grant P. Strimel; Nathan Susanj; Athanasios Mouchtaris', display:{Lore:['[{"text": "arXiv:2209.14868", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConvRNN-T: Convolutional Augmented Recurrent Neural Network Transducers for Streaming Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMartin Radfar\\nRohit Barnwal\\nRupak Vignesh Swaminathan\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.14868\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 29 Sep 2022 15:33:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper was presented inInterspeech 2022\\u00a7r"}']}
{title:'Dinkel et al. (§72022§r)', author: 'Heinrich Dinkel; Zhiyong Yan; Yongqing Wang; Junbo Zhang; Yujun Wang', display:{Lore:['[{"text": "arXiv:2209.15167", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn empirical study of weakly supervised audio tagging embeddings for general audio representations\\u00a7r\\n\\n\\u00a78\\u00a7oHeinrich Dinkel\\nZhiyong Yan\\nYongqing Wang\\nJunbo Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.15167\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Odyssey.2022-54\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Sep 2022 01:35:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oOdyssey 2022\\u00a7r"}']}
{title:'Yu et al. (§72022§r)', author: 'Qiuchen Yu; Ruohua Zhou', display:{Lore:['[{"text": "arXiv:2209.15296", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWake Word Detection Based on Res2Net\\u00a7r\\n\\n\\u00a78\\u00a7oQiuchen Yu\\nRuohua Zhou\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.15296\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Sep 2022 08:10:16 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Weiguo Wang; Jinming Li; Yuan He; Yunhao Liu', display:{Lore:['[{"text": "arXiv:2209.15325", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.NI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSymphony: Localizing Multiple Acoustic Sources with a Single Microphone Array\\u00a7r\\n\\n\\u00a78\\u00a7oWeiguo Wang\\nJinming Li\\nYuan He\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.15325\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Sep 2022 09:08:35 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Weiguo Wang; Jinming Li; Meng Jin; Yuan He', display:{Lore:['[{"text": "arXiv:2209.15334", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.NI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChordMics: Acoustic Signal Purification with Distributed Microphones\\u00a7r\\n\\n\\u00a78\\u00a7oWeiguo Wang\\nJinming Li\\nMeng Jin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.15334\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Sep 2022 09:18:42 GMT)\\u00a7r"}']}
{title:'Gao et al. (§72022§r)', author: 'Yan Gao; Javier Fernandez-Marques; Titouan Parcollet; Pedro P. B. de Gusmao; Nicholas D. Lane', display:{Lore:['[{"text": "arXiv:2209.15575", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMatch to Win: Analysing Sequences Lengths for Efficient Self-supervised Learning in Speech and Audio\\u00a7r\\n\\n\\u00a78\\u00a7oYan Gao\\nJavier Fernandez-Marques\\nTitouan Parcollet\\nPedro P. B. de Gusmao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.15575\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 22 Nov 2022 12:05:42 GMT)\\u00a7r"}']}
{title:'Fu et al. (§72022§r)', author: 'Xiao Fu; Xin Yuan; Jinglu Hu', display:{Lore:['[{"text": "arXiv:2209.15640", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHSD: A hierarchical singing annotation dataset\\u00a7r\\n\\n\\u00a78\\u00a7oXiao Fu\\nXin Yuan\\nJinglu Hu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.15640\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Sep 2022 17:00:07 GMT)\\u00a7r"}']}
{title:'Rathod et al. (§72022§r)', author: 'Jash Rathod; Nauman Dawalatabad; Shatrughan Singh; Dhananjaya Gowda', display:{Lore:['[{"text": "arXiv:2210.00169", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-stage Progressive Compression of Conformer Transducer for On-device Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJash Rathod\\nNauman Dawalatabad\\nShatrughan Singh\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.00169\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10582\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2022, 1691-1695\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 1 Oct 2022 02:23:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in INTERSPEECH 2022\\u00a7r"}']}
{title:'Heymans et al. (§72022§r)', author: 'Walter Heymans; Marelie H. Davel; Charl van Heerden', display:{Lore:['[{"text": "arXiv:2210.00721", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient acoustic feature transformation in mismatched environments using a Guided-GAN\\u00a7r\\n\\n\\u00a78\\u00a7oWalter Heymans\\nMarelie H. Davel\\nCharl van Heerden\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.00721\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.specom.2022.07.002\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nSpeech Communication, 143, pp.10-20 (2022)\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 6 Oct 2022 06:33:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oFinal published version available at: Efficient acoustic feature transformation in mismatched environments using a Guided-GAN. Speech Communication, 143, pp.10-20\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Xuanjun Chen; Haibin Wu; Helen Meng; Hung-yi Lee; Jyh-Shing Roger Jang', display:{Lore:['[{"text": "arXiv:2210.00753", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPush-Pull: Characterizing the Adversarial Robustness for Audio-Visual Active Speaker Detection\\u00a7r\\n\\n\\u00a78\\u00a7oXuanjun Chen\\nHaibin Wu\\nHelen Meng\\nHung-yi Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.00753\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 3 Oct 2022 08:10:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by SLT 2022\\u00a7r"}']}
{title:'Abrassart et al. (§72022§r)', author: 'Mathilde Abrassart; Guillaume Doras', display:{Lore:['[{"text": "arXiv:2210.01256", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnd what if two musical versions don\'t share melody, harmony, rhythm, or lyrics ?\\u00a7r\\n\\n\\u00a78\\u00a7oMathilde Abrassart\\nGuillaume Doras\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.01256\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 3 Oct 2022 22:33:14 GMT)\\u00a7r"}']}
{title:'Yu et al. (§72022§r)', author: 'Yinfeng Yu; Lele Cao; Fuchun Sun; Xiaohong Liu; Liejun Wang', display:{Lore:['[{"text": "arXiv:2210.01353", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPay Self-Attention to Audio-Visual Navigation\\u00a7r\\n\\n\\u00a78\\u00a7oYinfeng Yu\\nLele Cao\\nFuchun Sun\\nXiaohong Liu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.01353\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 5 Oct 2022 06:23:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMain paper (10 pages and 7 figures) and appendix (21 figures and 4 tables). Accepted for publication by BMVC 2022. For data and code, see https://yyf17.github.io/FSAAVN/index.html\\u00a7r"}']}
{title:'Xie et al. (§72022§r)', author: 'Luyuan Xie; Yan Zhong; Lin Yang; Zhaoyu Yan; Zhonghai Wu; Junjie Wang', display:{Lore:['[{"text": "arXiv:2210.02287", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTC-SKNet with GridMask for Low-complexity Classification of Acoustic scene\\u00a7r\\n\\n\\u00a78\\u00a7oLuyuan Xie\\nYan Zhong\\nLin Yang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.02287\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 5 Oct 2022 14:24:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to APSIPA ASC 2022\\u00a7r"}']}
{title:'Morgan et al. (§72022§r)', author: 'Osian Morgan; Hakan Kayan; Charith Perera', display:{Lore:['[{"text": "arXiv:2210.02642", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFeasibility on Detecting Door Slamming towards Monitoring Early Signs of Domestic Violence\\u00a7r\\n\\n\\u00a78\\u00a7oOsian Morgan\\nHakan Kayan\\nCharith Perera\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.02642\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 6 Oct 2022 02:25:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of the 2022 IEEE/ACM SeventhInternational Conference on Internet-of-Things Design and Implementation (IoTDI) 2022\\u00a7r"}']}
{title:'Mari et al. (§72022§r)', author: 'Daniele Mari; Federica Latora; Simone Milani', display:{Lore:['[{"text": "arXiv:2210.02746", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Sound of Silence: Efficiency of First Digit Features in Synthetic Audio Detection\\u00a7r\\n\\n\\u00a78\\u00a7oDaniele Mari\\nFederica Latora\\nSimone Milani\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.02746\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 6 Oct 2022 08:31:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at WIFS2022\\u00a7r"}']}
{title:'Tan et al. (§72022§r)', author: 'Chih-Pin Tan; Alvin W. Y. Su; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2210.02829", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMelody Infilling with User-Provided Structural Context\\u00a7r\\n\\n\\u00a78\\u00a7oChih-Pin Tan\\nAlvin W. Y. Su\\nYi-Hsuan Yang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.02829\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 6 Oct 2022 11:37:04 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Zixing Zhang; Thorin Farnsworth; Senling Lin; Salah Karout', display:{Lore:['[{"text": "arXiv:2210.02904", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWakeUpNet: A Mobile-Transformer based Framework for End-to-End Streaming Voice Trigger\\u00a7r\\n\\n\\u00a78\\u00a7oZixing Zhang\\nThorin Farnsworth\\nSenling Lin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.02904\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 6 Oct 2022 13:18:48 GMT)\\u00a7r"}']}
{title:'Zhou et al. (§72022§r)', author: 'Yuecheng Zhou; Yaolong Ju; Lingyun Xie', display:{Lore:['[{"text": "arXiv:2210.03027", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnimeTAB: A new guitar tablature dataset of anime and game music\\u00a7r\\n\\n\\u00a78\\u00a7oYuecheng Zhou\\nYaolong Ju\\nLingyun Xie\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.03027\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 6 Oct 2022 16:21:26 GMT)\\u00a7r"}']}
{title:'Majumdar et al. (§72022§r)', author: 'Somshubra Majumdar; Shantanu Acharya; Vitaly Lavrukhin; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2210.03255", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDamage Control During Domain Adaptation for Transducer Based Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSomshubra Majumdar\\nShantanu Acharya\\nVitaly Lavrukhin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.03255\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 6 Oct 2022 23:38:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proc. SLT 2022, Jan 09-12, 2023, Doha, Qatar\\u00a7r"}']}
{title:'Müller et al. (§72022§r)', author: 'Kaspar Müller; Franz Zotter', display:{Lore:['[{"text": "arXiv:2210.03360", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe PerspectiveLiberator \\u2013 an upmixing 6DoF rendering plugin for single-perspective Ambisonic room impulse responses\\u00a7r\\n\\n\\u00a78\\u00a7oKaspar M\\u00fcller\\nFranz Zotter\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.03360\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nFortschritte der Akustik - DAGA 2021, Vienna, Austria, 2021, vol.\\n  47, pp. 306-309\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 10 Oct 2022 07:24:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, submitted to conference: DAGA 2021,Vienna, Austria, 2021\\u00a7r"}']}
{title:'Müller et al. (§72022§r)', author: 'Kaspar Müller; Simon Doclo; Jan Østergaard; Tobias Wolff', display:{Lore:['[{"text": "arXiv:2210.03363", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModel-based estimation of in-car-communication feedback applied to speech zone detection\\u00a7r\\n\\n\\u00a78\\u00a7oKaspar M\\u00fcller\\nSimon Doclo\\nJan \\u00d8stergaard\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.03363\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/iwaenc53105.2022.9914770\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 7 Oct 2022 07:13:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, submitted to International Workshopon Acoustic Signal Enhancement (IWAENC), Bamberg, Germany, 2022\\u00a7r"}']}
{title:'Triantafyllopoulos et al. (§72022§r)', author: 'Andreas Triantafyllopoulos; Björn W. Schuller; Gökçe İymen; Metin Sezgin; Xiangheng He; Zijiang Yang; Panagiotis Tzirakis; Shuo Liu; Silvan Mertes; Elisabeth André; Ruibo Fu; Jianhua Tao', display:{Lore:['[{"text": "arXiv:2210.03538", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Overview of Affective Speech Synthesis and Conversion in the Deep Learning Era\\u00a7r\\n\\n\\u00a78\\u00a7oAndreas Triantafyllopoulos\\nBj\\u00f6rn W. Schuller\\nG\\u00f6k\\u00e7e \\u0130ymen\\n+ 8 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.03538\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JPROC.2023.3250266\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 6 Oct 2022 13:55:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to the Proceedings of IEEE\\u00a7r"}']}
{title:'McCallum et al. (§72022§r)', author: 'Matthew C. McCallum; Filip Korzeniowski; Sergio Oramas; Fabien Gouyon; Andreas F. Ehmann', display:{Lore:['[{"text": "arXiv:2210.03799", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSupervised and Unsupervised Learning of Audio Representations for Music Understanding\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew C. McCallum\\nFilip Korzeniowski\\nSergio Oramas\\nFabien Gouyon\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.03799\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 7 Oct 2022 20:07:35 GMT)\\u00a7r"}']}
{title:'Sun et al. (§72022§r)', author: 'Jianyuan Sun; Xubo Liu; Xinhao Mei; Mark D. Plumbley; Volkan Kilic; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2210.05037", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomated Audio Captioning via Fusion of Low- and High- Dimensional Features\\u00a7r\\n\\n\\u00a78\\u00a7oJianyuan Sun\\nXubo Liu\\nXinhao Mei\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.05037\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 Oct 2022 22:39:41 GMT)\\u00a7r"}']}
{title:'Fan et al. (§72022§r)', author: 'Wanpeng Fan; Yuanzhi Su; Yuxin Huang', display:{Lore:['[{"text": "arXiv:2210.05076", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConchShell: A Generative Adversarial Networks that Turns Pictures into Piano Music\\u00a7r\\n\\n\\u00a78\\u00a7oWanpeng Fan\\nYuanzhi Su\\nYuxin Huang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.05076\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Oct 2022 01:04:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Qin et al. (§72022§r)', author: 'Xiaoyi Qin; Na Li; Yuke Lin; Yiwei Ding; Chao Weng; Dan Su; Ming Li', display:{Lore:['[{"text": "arXiv:2210.05092", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe DKU-Tencent System for the VoxCeleb Speaker Recognition Challenge 2022\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoyi Qin\\nNa Li\\nYuke Lin\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.05092\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Oct 2022 02:09:40 GMT)\\u00a7r"}']}
{title:'Cheuk et al. (§72022§r)', author: 'Kin Wai Cheuk; Ryosuke Sawata; Toshimitsu Uesaka; Naoki Murata; Naoya Takahashi; Shusuke Takahashi; Dorien Herremans; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2210.05148", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffRoll: Diffusion-based Generative Music Transcription with Unsupervised Pretraining Capability\\u00a7r\\n\\n\\u00a78\\u00a7oKin Wai Cheuk\\nRyosuke Sawata\\nToshimitsu Uesaka\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.05148\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 20 Oct 2022 05:47:43 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Xiaohui Liu; Meng Liu; Lin Zhang; Linjuan Zhang; Chang Zeng; Kai Li; Nan Li; Kong Aik Lee; Longbiao Wang; Jianwu Dang', display:{Lore:['[{"text": "arXiv:2210.05254", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Spectro-temporal Artifacts for Detecting Synthesized Speech\\u00a7r\\n\\n\\u00a78\\u00a7oXiaohui Liu\\nMeng Liu\\nLin Zhang\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.05254\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3552466.3556527\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Oct 2022 08:31:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 1 figures, Accecpted by Proceedings of the 1st International Workshopon Deepfake Detection for Audio Multimedia\\u00a7r"}']}
{title:'Yu et al. (§72022§r)', author: 'Fan Yu; Shiliang Zhang; Pengcheng Guo; Yuhao Liang; Zhihao Du; Yuxiao Lin; Lei Xie', display:{Lore:['[{"text": "arXiv:2210.05265", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMFCCA:Multi-Frame Cross-Channel attention for multi-speaker ASR in Multi-party meeting scenario\\u00a7r\\n\\n\\u00a78\\u00a7oFan Yu\\nShiliang Zhang\\nPengcheng Guo\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.05265\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Oct 2022 08:54:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by SLT 2022\\u00a7r"}']}
{title:'Baas et al. (§72022§r)', author: 'Matthew Baas; Herman Kamper', display:{Lore:['[{"text": "arXiv:2210.05271", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGAN You Hear Me? Reclaiming Unconditional Speech Synthesis from Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew Baas\\nHerman Kamper\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.05271\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Oct 2022 09:12:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 2 figures, 2 tables. Accepted at IEEE SLT 2022\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Chao-Han Huck Yang; I-Fan Chen; Andreas Stolcke; Sabato Marco Siniscalchi; Chin-Hui Lee', display:{Lore:['[{"text": "arXiv:2210.05614", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Experimental Study on Private Aggregation of Teacher Ensemble Learning for End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oChao-Han Huck Yang\\nI-Fan Chen\\nAndreas Stolcke\\nSabato Marco Siniscalchi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.05614\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 13 Oct 2022 19:46:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages. Accepted to IEEE SLT 2022. A first version draft was finished in Aug 2021\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Yueh-Kao Wu; Ching-Yu Chiu; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2210.06007", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJukeDrummer: Conditional Beat-aware Audio-domain Drum Accompaniment Generation via Transformer VQ-VAE\\u00a7r\\n\\n\\u00a78\\u00a7oYueh-Kao Wu\\nChing-Yu Chiu\\nYi-Hsuan Yang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.06007\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 31 Oct 2022 08:54:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ISMIR 2022\\u00a7r"}']}
{title:'Kawa et al. (§72022§r)', author: 'Piotr Kawa; Marcin Plata; Piotr Syga', display:{Lore:['[{"text": "arXiv:2210.06105", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpecRNet: Towards Faster and More Accessible Audio DeepFake Detection\\u00a7r\\n\\n\\u00a78\\u00a7oPiotr Kawa\\nMarcin Plata\\nPiotr Syga\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.06105\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 12 Oct 2022 11:36:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by TrustCom 2022: The 21st IEEE InternationalConference on Trust, Security and Privacy in Computing and Communications\\u00a7r"}']}
{title:'Zheng et al. (§72022§r)', author: 'Yu Zheng; Jinghan Peng; Miao Zhao; Yufeng Ma; Min Liu; Xinyue Ma; Tianyu Liang; Tianlong Kong; Liang He; Minqiang Xu', display:{Lore:['[{"text": "arXiv:2210.06111", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTHUEE system description for NIST 2020 SRE CTS challenge\\u00a7r\\n\\n\\u00a78\\u00a7oYu Zheng\\nJinghan Peng\\nMiao Zhao\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.06111\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 12 Oct 2022 12:01:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o3 pages, 1 table; System desciption of NIST 2020SRE CTS challenge\\u00a7r"}']}
{title:'Sun et al. (§72022§r)', author: 'Tao Sun; Nidal Abuhajar; Shuyu Gong; Zhewei Wang; Charles D. Smith; Xianhui Wang; Li Xu; Jundong Liu', display:{Lore:['[{"text": "arXiv:2210.06368", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIndividualized Conditioning and Negative Distances for Speaker Separation\\u00a7r\\n\\n\\u00a78\\u00a7oTao Sun\\nNidal Abuhajar\\nShuyu Gong\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.06368\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 12 Oct 2022 16:18:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICMLA 2022\\u00a7r"}']}
{title:'Meyer et al. (§72022§r)', author: 'Sarina Meyer; Pascal Tilli; Pavel Denisov; Florian Lux; Julia Koch; Ngoc Thang Vu', display:{Lore:['[{"text": "arXiv:2210.07002", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnonymizing Speech with Generative Adversarial Networks to Preserve Speaker Privacy\\u00a7r\\n\\n\\u00a78\\u00a7oSarina Meyer\\nPascal Tilli\\nPavel Denisov\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.07002\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 20 Oct 2022 09:26:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE Spoken Language Technology Workshop 2022\\u00a7r"}']}
{title:'Takahashi et al. (§72022§r)', author: 'Naoya Takahashi; Mayank Kumar; Singh; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2210.07508", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierarchical Diffusion Models for Singing Voice Neural Vocoder\\u00a7r\\n\\n\\u00a78\\u00a7oNaoya Takahashi\\nMayank Kumar\\nSingh\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.07508\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 18 Oct 2022 00:59:12 GMT)\\u00a7r"}']}
{title:'Bartusiak et al. (§72022§r)', author: 'Emily R. Bartusiak; Edward J. Delp', display:{Lore:['[{"text": "arXiv:2210.07546", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransformer-Based Speech Synthesizer Attribution in an Open Set Scenario\\u00a7r\\n\\n\\u00a78\\u00a7oEmily R. Bartusiak\\nEdward J. Delp\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.07546\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE International Conference on Machine Learning and\\n  Applications, pp. 1-8, December 2022, Nassau, The Bahamas\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 Oct 2022 05:55:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the 2022 IEEE InternationalConference on MachineLearning and Applications\\u00a7r"}']}
{title:'Shamsi et al. (§72022§r)', author: 'Meysam Shamsi; Marie Tahon', display:{Lore:['[{"text": "arXiv:2210.07642", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTraining speech emotion classifier without categorical annotations\\u00a7r\\n\\n\\u00a78\\u00a7oMeysam Shamsi\\nMarie Tahon\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.07642\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 Oct 2022 08:47:41 GMT)\\u00a7r"}']}
{title:'Stowell et al. (§72022§r)', author: 'Dan Stowell; Caitlin Black; Florencia Noriega; Sarab S. Sethi', display:{Lore:['[{"text": "arXiv:2210.07685", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFull-Stack Bioacoustics: Field Kit to AI to Action (Workshop report)\\u00a7r\\n\\n\\u00a78\\u00a7oDan Stowell\\nCaitlin Black\\nFlorencia Noriega\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.07685\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 Oct 2022 10:21:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWorkshop report: Lorentz Center, Leiden, the Netherlands, 1-5 August 2022\\u00a7r"}']}
{title:'Rusci et al. (§72022§r)', author: 'Manuele Rusci; Marco Fariselli; Martin Croome; Francesco Paci; Eric Flamand', display:{Lore:['[{"text": "arXiv:2210.07692", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SY\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SY\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAccelerating RNN-based Speech Enhancement on a Multi-Core MCU with Mixed FP16-INT8 Post-Training Quantization\\u00a7r\\n\\n\\u00a78\\u00a7oManuele Rusci\\nMarco Fariselli\\nMartin Croome\\nFrancesco Paci\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.07692\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 Oct 2022 10:32:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the ITEM Workshop 2022 (located at ECML-PKDD2022)\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'Kuan-Po Huang; Yu-Kuan Fu; Tsu-Yuan Hsu; Fabian Ritter Gutierrez; Fan-Lin Wang; Liang-Hsuan Tseng; Yu Zhang; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2210.07978", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving generalizability of distilled self-supervised speech processing models under distorted settings\\u00a7r\\n\\n\\u00a78\\u00a7oKuan-Po Huang\\nYu-Kuan Fu\\nTsu-Yuan Hsu\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.07978\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 20 Oct 2022 05:10:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE SLT2022\\u00a7r"}']}
{title:'Zhao et al. (§72022§r)', author: 'Chendong Zhao; Jianzong Wang; Xiaoyang Qu; Haoqian Wang; Jing Xiao', display:{Lore:['[{"text": "arXiv:2210.08182", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Invariant Representation and Risk Minimized for Unsupervised Accent Domain Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oChendong Zhao\\nJianzong Wang\\nXiaoyang Qu\\nHaoqian Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.08182\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 29 Oct 2022 07:57:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to 2022 IEEE Spoken Language Technology Workshop (SLT 2022)\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Rui Li; Guodong Ma; Dexin Zhao; Ranran Zeng; Xiaoyu Li; Hao Huang', display:{Lore:['[{"text": "arXiv:2210.08520", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Policy-based Approach to the SpecAugment Method for Low Resource E2E ASR\\u00a7r\\n\\n\\u00a78\\u00a7oRui Li\\nGuodong Ma\\nDexin Zhao\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.08520\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 16 Oct 2022 12:39:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to APSIPA ASC 2022\\u00a7r"}']}
{title:'Pham et al. (§72022§r)', author: 'Lam Pham; Dusan Salovic; Anahid Jalali; Alexander Schindler; Khoa Tran; Canh Vu; Phu X. Nguyen', display:{Lore:['[{"text": "arXiv:2210.08610", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust, General, and Low Complexity Acoustic Scene Classification Systems and An Effective Visualization for Presenting a Sound Scene Context\\u00a7r\\n\\n\\u00a78\\u00a7oLam Pham\\nDusan Salovic\\nAnahid Jalali\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.08610\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 16 Oct 2022 19:07:21 GMT)\\u00a7r"}']}
{title:'Dowerah et al. (§72022§r)', author: 'Sandipana Dowerah; Romain Serizel; Denis Jouvet; Mohammad Mohammadamini; Driss Matrouf', display:{Lore:['[{"text": "arXiv:2210.08834", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHow to Leverage DNN-based speech enhancement for multi-channel speaker verification?\\u00a7r\\n\\n\\u00a78\\u00a7oSandipana Dowerah\\nRomain Serizel\\nDenis Jouvet\\nMohammad Mohammadamini\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.08834\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n4th International Conference on Advances in Signal Processing and\\n  Artificial Intelligence (ASPAI\' 2022), Oct 2022, Corfu, Greece\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Oct 2022 08:22:08 GMT)\\u00a7r"}']}
{title:'Ohnaka et al. (§72022§r)', author: 'Hien Ohnaka; Shinnosuke Takamichi; Keisuke Imoto; Yuki Okamoto; Kazuki Fujii; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2210.09173", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisual onoma-to-wave: environmental sound synthesis from visual onomatopoeias and sound-source images\\u00a7r\\n\\n\\u00a78\\u00a7oHien Ohnaka\\nShinnosuke Takamichi\\nKeisuke Imoto\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.09173\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Oct 2022 15:19:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Zhen et al. (§72022§r)', author: 'Kai Zhen; Martin Radfar; Hieu Duy Nguyen; Grant P. Strimel; Nathan Susanj; Athanasios Mouchtaris', display:{Lore:['[{"text": "arXiv:2210.09188", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSub-8-bit quantization for on-device speech recognition: a regularization-free approach\\u00a7r\\n\\n\\u00a78\\u00a7oKai Zhen\\nMartin Radfar\\nHieu Duy Nguyen\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.09188\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 1 Nov 2022 16:15:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at IEEE SLT\'22\\u00a7r"}']}
{title:'Kang et al. (§72022§r)', author: 'Zuheng Kang; Jianzong Wang; Junqing Peng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2210.09524", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSVLDL: Improved Speaker Age Estimation Using Selective Variance Label Distribution Learning\\u00a7r\\n\\n\\u00a78\\u00a7oZuheng Kang\\nJianzong Wang\\nJunqing Peng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.09524\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 16 Nov 2022 06:18:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by SLT 2022. The 2022 IEEE Spoken Language Technology Workshop (SLT 2022)\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Yiming Li; Zhifang Guo; Zhirong Ye; Xiangdong Wang; Hong Liu; Yueliang Qian; Rui Tao; Long Yan; Kazushige Ouchi', display:{Lore:['[{"text": "arXiv:2210.09529", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Hybrid System of Sound Event Detection Transformer and Frame-wise Model for DCASE 2022 Task 4\\u00a7r\\n\\n\\u00a78\\u00a7oYiming Li\\nZhifang Guo\\nZhirong Ye\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.09529\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 Oct 2022 01:47:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, accepted for publication in DCASE2022 Workshop\\u00a7r"}']}
{title:'Watanabe et al. (§72022§r)', author: 'Aya Watanabe; Shinnosuke Takamichi; Yuki Saito; Detai Xin; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2210.09916", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMid-attribute speaker generation using optimal-transport-based interpolation of Gaussian mixture models\\u00a7r\\n\\n\\u00a78\\u00a7oAya Watanabe\\nShinnosuke Takamichi\\nYuki Saito\\nDetai Xin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.09916\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 Oct 2022 14:59:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023. Demo: https://sarulab-speech.github.io/demo_mid-attribute-speaker-generation\\u00a7r"}']}
{title:'Raissi et al. (§72022§r)', author: 'Tina Raissi; Wei Zhou; Simon Berger; Ralf Schlüter; Hermann Ney', display:{Lore:['[{"text": "arXiv:2210.09951", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHMM vs. CTC for Automatic Speech Recognition: Comparison Based on Full-Sum Training from Scratch\\u00a7r\\n\\n\\u00a78\\u00a7oTina Raissi\\nWei Zhou\\nSimon Berger\\nRalf Schl\\u00fcter\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.09951\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 Oct 2022 16:03:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for Presentation at IEEE SLT 2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Youshan Zhang; Jialu Li', display:{Lore:['[{"text": "arXiv:2210.10196", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBirdSoundsDenoising: Deep Visual Audio Denoising for Bird Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oYoushan Zhang\\nJialu Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.10196\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 Oct 2022 22:37:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWACV 2023\\u00a7r"}']}
{title:'Shahin et al. (§72022§r)', author: 'Mostafa Shahin; Beena Ahmed; Julien Epps', display:{Lore:['[{"text": "arXiv:2210.10231", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker- and Age-Invariant Training for Child Acoustic Modeling Using Adversarial Multi-Task Learning\\u00a7r\\n\\n\\u00a78\\u00a7oMostafa Shahin\\nBeena Ahmed\\nJulien Epps\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.10231\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 7 Nov 2022 02:02:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP2023\\u00a7r"}']}
{title:'Ma et al. (§72022§r)', author: 'Ding Ma; Lester Phillip Violeta; Kazuhiro Kobayashi; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2210.10314", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTwo-stage training method for Japanese electrolaryngeal speech enhancement based on sequence-to-sequence voice conversion\\u00a7r\\n\\n\\u00a78\\u00a7oDing Ma\\nLester Phillip Violeta\\nKazuhiro Kobayashi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.10314\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 19 Oct 2022 06:08:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to SLT 2022\\u00a7r"}']}
{title:'Yu et al. (§72022§r)', author: 'Botao Yu; Peiling Lu; Rui Wang; Wei Hu; Xu Tan; Wei Ye; Shikun Zhang; Tao Qin; Tie-Yan Liu', display:{Lore:['[{"text": "arXiv:2210.10349", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMuseformer: Transformer with Fine- and Coarse-Grained Attention for Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oBotao Yu\\nPeiling Lu\\nRui Wang\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.10349\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 31 Oct 2022 03:50:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the Thirty-sixth Conference on Neural Information ProcessingSystems (NeurIPS 2022)\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Zhifeng Wang; Yao Yang; Chunyan Zeng; Shuai Kong; Shixiong Feng; Nan Zhao', display:{Lore:['[{"text": "arXiv:2210.10506", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Tampering Detection Based on Shallow and Deep Feature Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oZhifeng Wang\\nYao Yang\\nChunyan Zeng\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.10506\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 19 Oct 2022 12:22:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAudio tampering detection, 21 pages, 4 figures\\u00a7r"}']}
{title:'Masuyama et al. (§72022§r)', author: 'Yoshiki Masuyama; Xuankai Chang; Samuele Cornell; Shinji Watanabe; Nobutaka Ono', display:{Lore:['[{"text": "arXiv:2210.10742", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Integration of Speech Recognition, Dereverberation, Beamforming, and Self-Supervised Learning Representation\\u00a7r\\n\\n\\u00a78\\u00a7oYoshiki Masuyama\\nXuankai Chang\\nSamuele Cornell\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.10742\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 19 Oct 2022 17:33:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEESLT 2022\\u00a7r"}']}
{title:'Hagiwara et al. (§72022§r)', author: 'Masato Hagiwara; Maddie Cusimano; Jen-Yu Liu', display:{Lore:['[{"text": "arXiv:2210.10857", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModeling Animal Vocalizations through Synthesizers\\u00a7r\\n\\n\\u00a78\\u00a7oMasato Hagiwara\\nMaddie Cusimano\\nJen-Yu Liu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.10857\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 19 Oct 2022 19:40:22 GMT)\\u00a7r"}']}
{title:'Jung et al. (§72022§r)', author: 'Jee-weon Jung; Hee-Soo Heo; Bong-Jin Lee; Jaesong Lee; Hye-jin Shim; Youngki Kwon; Joon Son Chung; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2210.10985", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLarge-scale learning of generalised representations for speaker recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJee-weon Jung\\nHee-Soo Heo\\nBong-Jin Lee\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.10985\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 27 Oct 2022 05:11:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5pages, 5 tables,submitted to ICASSP\\u00a7r"}']}
{title:'Qiang et al. (§72022§r)', author: 'Chunyu Qiang; Jianhua Tao; Ruibo Fu; Zhengqi Wen; Jiangyan Yi; Tao Wang; Shiming Wang', display:{Lore:['[{"text": "arXiv:2210.11429", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lText Enhancement for Paragraph Processing in End-to-End Code-switching TTS\\u00a7r\\n\\n\\u00a78\\u00a7oChunyu Qiang\\nJianhua Tao\\nRuibo Fu\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.11429\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 20 Oct 2022 17:19:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted in ISCSLP 2021\\u00a7r"}']}
{title:'Dongre et al. (§72022§r)', author: 'Vardhan Dongre; Abhinav Thimma Reddy; Nikhitha Reddeddy', display:{Lore:['[{"text": "arXiv:2210.11722", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptive re-calibration of channel-wise features for Adversarial Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oVardhan Dongre\\nAbhinav Thimma Reddy\\nNikhitha Reddeddy\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.11722\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 Oct 2022 04:21:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 8 figures, 4 tables\\u00a7r"}']}
{title:'Nguyen et al. (§72022§r)', author: 'Thien Nguyen; Nathalie Tran; Liuhui Deng; Thiago Fraga da Silva; Matthew Radzihovsky; Roger Hsiao; Henry Mason; Stefan Braun; Erik McDermott; Dogan Can; Pawel Swietojanski; Lyan Verwimp; Sibel Oyman; Tresi Arvizo; Honza Silovsky; Arnab Ghoshal; Mathieu Martel; Bharat Ram Ambati; Mohamed Ali', display:{Lore:['[{"text": "arXiv:2210.12214", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimizing Bilingual Neural Transducer with Synthetic Code-switching Text Generation\\u00a7r\\n\\n\\u00a78\\u00a7oThien Nguyen\\nNathalie Tran\\nLiuhui Deng\\n+ 15 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.12214\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 Oct 2022 19:42:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, submitted to ICASSP 2023, *: equal contributions\\u00a7r"}']}
{title:'Hagiwara et al. (§72022§r)', author: 'Masato Hagiwara; Benjamin Hoffman; Jen-Yu Liu; Maddie Cusimano; Felix Effenberger; Katie Zacarian', display:{Lore:['[{"text": "arXiv:2210.12300", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBEANS: The Benchmark of Animal Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oMasato Hagiwara\\nBenjamin Hoffman\\nJen-Yu Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.12300\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 Oct 2022 23:34:06 GMT)\\u00a7r"}']}
{title:'Kong et al. (§72022§r)', author: 'Qiuqiang Kong; Shilei Liu; Junjie Shi; Xuzhou Ye; Yin Cao; Qiaoxi Zhu; Yong Xu; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2210.12345", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Sound Field Decomposition with Super-resolution of Sound Direction\\u00a7r\\n\\n\\u00a78\\u00a7oQiuqiang Kong\\nShilei Liu\\nJunjie Shi\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.12345\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 22 Oct 2022 04:20:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages\\u00a7r"}']}
{title:'Lu et al. (§72022§r)', author: 'Cheng Lu; Wenming Zheng; Hailun Lian; Yuan Zong; Chuangao Tang; Sunan Li; Yan Zhao', display:{Lore:['[{"text": "arXiv:2210.12430", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Emotion Recognition via an Attentive Time-Frequency Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oCheng Lu\\nWenming Zheng\\nHailun Lian\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.12430\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 22 Oct 2022 12:18:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper has been accepted as a regular paper on IEEE Transactions on Computational Social Systems\\u00a7r"}']}
{title:'Hou et al. (§72022§r)', author: 'Yuanbo Hou; Yun Wang; Wenwu Wang; Dick Botteldooren', display:{Lore:['[{"text": "arXiv:2210.12541", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGCT: Gated Contextual Transformer for Sequential Audio Tagging\\u00a7r\\n\\n\\u00a78\\u00a7oYuanbo Hou\\nYun Wang\\nWenwu Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.12541\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 22 Oct 2022 20:07:57 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Xiaoyu Liu; Xu Li; Joan Serrà', display:{Lore:['[{"text": "arXiv:2210.12635", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQuantitative Evidence on Overlooked Aspects of Enrollment Speaker Embeddings for Target Speaker Separation\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoyu Liu\\nXu Li\\nJoan Serr\\u00e0\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.12635\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 26 Oct 2022 04:48:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted version to ICASSP 2023\\u00a7r"}']}
{title:'Min et al. (§72022§r)', author: 'Zeping Min; Qian Ge; Zhong Li', display:{Lore:['[{"text": "arXiv:2210.13067", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l10 hours data is all you need\\u00a7r\\n\\n\\u00a78\\u00a7oZeping Min\\nQian Ge\\nZhong Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.13067\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Oct 2022 09:34:05 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Xulong Zhang; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2210.13803", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdapitch: Adaption Multi-Speaker Text-to-Speech Conditioned on Pitch Disentangling with Untranscribed Data\\u00a7r\\n\\n\\u00a78\\u00a7oXulong Zhang\\nJianzong Wang\\nNing Cheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.13803\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Oct 2022 07:21:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by MSN2022, The 18th International Conference on Mobility, Sensing and Networking\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Xulong Zhang; Jianzong Wang; Ning Cheng; Kexin Zhu; Jing Xiao', display:{Lore:['[{"text": "arXiv:2210.13805", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Speech Representation Learning via Speech-level and Phoneme-level Masking Approach\\u00a7r\\n\\n\\u00a78\\u00a7oXulong Zhang\\nJianzong Wang\\nNing Cheng\\nKexin Zhu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.13805\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Oct 2022 07:26:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by MSN2022, The 18th International Conference on Mobility, Sensing and Networking\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Xulong Zhang; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2210.13811", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMetaSpeech: Speech Effects Switch Along with Environment for Metaverse\\u00a7r\\n\\n\\u00a78\\u00a7oXulong Zhang\\nJianzong Wang\\nNing Cheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.13811\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Oct 2022 07:37:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by AI2OT2022, The Third International Workshopon Artificial Intelligence Applications in Internetof Things\\u00a7r"}']}
{title:'Kapka et al. (§72022§r)', author: 'Sławomir Kapka; Jakub Tkaczuk', display:{Lore:['[{"text": "arXiv:2210.13932", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCoLoC: Conditioned Localizer and Classifier for Sound Event Localization and Detection\\u00a7r\\n\\n\\u00a78\\u00a7oS\\u0142awomir Kapka\\nJakub Tkaczuk\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.13932\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Oct 2022 11:37:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, conference\\u00a7r"}']}
{title:'Gauy et al. (§72022§r)', author: 'Marcelo Matheus Gauy; Marcelo Finger', display:{Lore:['[{"text": "arXiv:2210.14085", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio MFCC-gram Transformers for respiratory insufficiency detection in COVID-19\\u00a7r\\n\\n\\u00a78\\u00a7oMarcelo Matheus Gauy\\nMarcelo Finger\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14085\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5753/stil.2021.17793\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nSIMP\\\\\'OSIO BRASILEIRO DE TECNOLOGIA DA INFORMA\\\\c{C}\\\\~AO E DA\\n  LINGUAGEM HUMANA (STIL), 13. , 2021, Evento Online. Anais [...]. Porto\\n  Alegre: Sociedade Brasileira de Computa\\\\c{c}\\\\~ao, 2021 . p. 143-152\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Oct 2022 15:11:40 GMT)\\u00a7r"}']}
{title:'Liang et al. (§72022§r)', author: 'Dawei Liang; Hang Su; Tarun Singh; Jay Mahadeokar; Shanil Puri; Jiedan Zhu; Edison Thomaz; Mike Seltzer', display:{Lore:['[{"text": "arXiv:2210.14252", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDynamic Speech Endpoint Detection with Regression Targets\\u00a7r\\n\\n\\u00a78\\u00a7oDawei Liang\\nHang Su\\nTarun Singh\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14252\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Oct 2022 18:09:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oManuscript submitted to ICASSP 2023\\u00a7r"}']}
{title:'Liang et al. (§72022§r)', author: 'Yuhao Liang; Peikun Chen; Fan Yu; Xinfa Zhu; Tianyi Xu; Lei Xie', display:{Lore:['[{"text": "arXiv:2210.14448", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe NPU-ASLP System for The ISCSLP 2022 Magichub Code-Swiching ASR Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oYuhao Liang\\nPeikun Chen\\nFan Yu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14448\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Oct 2022 03:42:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ISCSLP 2022\\u00a7r"}']}
{title:'Zadorozhnyy et al. (§72022§r)', author: 'Vasily Zadorozhnyy; Qiang Ye; Kazuhito Koishida', display:{Lore:['[{"text": "arXiv:2210.14474", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSCP-GAN: Self-Correcting Discriminator Optimization for Training Consistency Preserving Metric GAN on Speech Enhancement Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oVasily Zadorozhnyy\\nQiang Ye\\nKazuhito Koishida\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14474\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Oct 2022 04:48:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages (4 - manuscript and 1 - references), 2 figures, 2 tables\\u00a7r"}']}
{title:'Hagiwara (§72022§r)', author: 'Masato Hagiwara', display:{Lore:['[{"text": "arXiv:2210.14493", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAVES: Animal Vocalization Encoder based on Self-Supervision\\u00a7r\\n\\n\\u00a78\\u00a7oMasato Hagiwara\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14493\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Oct 2022 05:38:50 GMT)\\u00a7r"}']}
{title:'Atmaja et al. (§72022§r)', author: 'Bagus Tris Atmaja; Masato Akagi', display:{Lore:['[{"text": "arXiv:2210.14495", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTwo-stage dimensional emotion recognition by fusing predictions of acoustic and text networks using SVM\\u00a7r\\n\\n\\u00a78\\u00a7oBagus Tris Atmaja\\nMasato Akagi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14495\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.specom.2020.11.003\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nSpeech Commun., vol. 126, pp. 9-21, Feb. 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Oct 2022 05:49:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in Speech Communications\\u00a7r"}']}
{title:'Cui et al. (§72022§r)', author: 'Jianqiao Cui; Stefan Bleeck', display:{Lore:['[{"text": "arXiv:2210.14509", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParallel Gated Neural Network With Attention Mechanism For Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oJianqiao Cui\\nStefan Bleeck\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14509\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 27 Oct 2022 04:47:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 6 figures, references added\\u00a7r"}']}
{title:'Ren et al. (§72022§r)', author: 'Zhao Ren; Thanh Tam Nguyen; Yi Chang; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2210.14636", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast Yet Effective Speech Emotion Recognition with Self-distillation\\u00a7r\\n\\n\\u00a78\\u00a7oZhao Ren\\nThanh Tam Nguyen\\nYi Chang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14636\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Oct 2022 11:28:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Du et al. (§72022§r)', author: 'Yuxuan Du; Ruohua Zhou', display:{Lore:['[{"text": "arXiv:2210.14644", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Diarization Based on Multi-channel Microphone Array in Small-scale Meeting\\u00a7r\\n\\n\\u00a78\\u00a7oYuxuan Du\\nRuohua Zhou\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14644\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Oct 2022 11:44:31 GMT)\\u00a7r"}']}
{title:'Pang et al. (§72022§r)', author: 'Bowen Pang; Huan Zhao; Gaosheng Zhang; Xiaoyue Yang; Yang Sun; Li Zhang; Qing Wang; Lei Xie', display:{Lore:['[{"text": "arXiv:2210.14653", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTSUP Speaker Diarization System for Conversational Short-phrase Speaker Diarization Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oBowen Pang\\nHuan Zhao\\nGaosheng Zhang\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14653\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ISCSLP57327.2022.10037846\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Oct 2022 12:01:24 GMT)\\u00a7r"}']}
{title:'Pascual et al. (§72022§r)', author: 'Santiago Pascual; Gautam Bhattacharya; Chunghsin Yeh; Jordi Pons; Joan Serrà', display:{Lore:['[{"text": "arXiv:2210.14661", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFull-band General Audio Synthesis with Score-based Diffusion\\u00a7r\\n\\n\\u00a78\\u00a7oSantiago Pascual\\nGautam Bhattacharya\\nChunghsin Yeh\\nJordi Pons\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14661\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Oct 2022 12:25:57 GMT)\\u00a7r"}']}
{title:'Jung et al. (§72022§r)', author: 'Jee-weon Jung; Hee-Soo Heo; Bong-Jin Lee; Jaesung Huh; Andrew Brown; Youngki Kwon; Shinji Watanabe; Joon Son Chung', display:{Lore:['[{"text": "arXiv:2210.14682", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIn search of strong embedding extractors for speaker diarisation\\u00a7r\\n\\n\\u00a78\\u00a7oJee-weon Jung\\nHee-Soo Heo\\nBong-Jin Lee\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14682\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Oct 2022 13:00:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5pages, 1 figure,2 tables, submitted to ICASSP\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Wei Wang; Chao Zhang; Xiaopei Wu', display:{Lore:['[{"text": "arXiv:2210.14691", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPronunciation Generation for Foreign Language Words in Intra-Sentential Code-Switching Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oWei Wang\\nChao Zhang\\nXiaopei Wu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14691\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Oct 2022 13:19:35 GMT)\\u00a7r"}']}
{title:'Gauy et al. (§72022§r)', author: 'Marcelo Matheus Gauy; Marcelo Finger', display:{Lore:['[{"text": "arXiv:2210.14716", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPretrained audio neural networks for Speech emotion recognition in Portuguese\\u00a7r\\n\\n\\u00a78\\u00a7oMarcelo Matheus Gauy\\nMarcelo Finger\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14716\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nFirst Workshop on Automatic Speech Recognition for Spontaneous and\\n  Prepared Speech Speech emotion recognition in Portuguese (SER 2022)\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Oct 2022 13:48:51 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Xulong Zhang; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2210.14723", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-Supervised Learning Based on Reference Model for Low-resource TTS\\u00a7r\\n\\n\\u00a78\\u00a7oXulong Zhang\\nJianzong Wang\\nNing Cheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14723\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Oct 2022 07:48:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by NMIC2022, The Fourth International Workshopon Network Meets Intelligent Computations\\u00a7r"}']}
{title:'Seki et al. (§72022§r)', author: 'Kentaro Seki; Shinnosuke Takamichi; Takaaki Saeki; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2210.14850", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lText-to-speech synthesis from dark data with evaluation-in-the-loop data selection\\u00a7r\\n\\n\\u00a78\\u00a7oKentaro Seki\\nShinnosuke Takamichi\\nTakaaki Saeki\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14850\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Oct 2022 16:49:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Guo et al. (§72022§r)', author: 'Haohan Guo; Fenglong Xie; Xixin Wu; Hui Lu; Helen Meng', display:{Lore:['[{"text": "arXiv:2210.15131", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards High-Quality Neural TTS for Low-Resource Languages by Learning Compact Speech Representations\\u00a7r\\n\\n\\u00a78\\u00a7oHaohan Guo\\nFenglong Xie\\nXixin Wu\\nHui Lu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15131\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Oct 2022 02:32:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Deng et al. (§72022§r)', author: 'Jiangyi Deng; Fei Teng; Yanjiao Chen; Xiaofu Chen; Zhaohui Wang; Wenyuan Xu', display:{Lore:['[{"text": "arXiv:2210.15140", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lV-Cloak: Intelligibility-, Naturalness-     Timbre-Preserving Real-Time Voice Anonymization\\u00a7r\\n\\n\\u00a78\\u00a7oJiangyi Deng\\nFei Teng\\nYanjiao Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15140\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Oct 2022 02:58:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by USENIX Security Symposium 2023\\u00a7r"}']}
{title:'Feng et al. (§72022§r)', author: 'Kexin Feng; Theodora Chaspari', display:{Lore:['[{"text": "arXiv:2210.15261", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA knowledge-driven vowel-based approach of depression classification from speech using data augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oKexin Feng\\nTheodora Chaspari\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15261\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Oct 2022 08:34:08 GMT)\\u00a7r"}']}
{title:'Min et al. (§72022§r)', author: 'Zeping Min; Qian Ge; Guanhua Huang', display:{Lore:['[{"text": "arXiv:2210.15285", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSAN: a robust end-to-end ASR model architecture\\u00a7r\\n\\n\\u00a78\\u00a7oZeping Min\\nQian Ge\\nGuanhua Huang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15285\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Oct 2022 09:36:25 GMT)\\u00a7r"}']}
{title:'Diaz et al. (§72022§r)', author: 'Rodrigo Diaz; Ben Hayes; Charalampos Saitis; György Fazekas; Mark Sandler', display:{Lore:['[{"text": "arXiv:2210.15306", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRigid-Body Sound Synthesis with Differentiable Modal Resonators\\u00a7r\\n\\n\\u00a78\\u00a7oRodrigo Diaz\\nBen Hayes\\nCharalampos Saitis\\nGy\\u00f6rgy Fazekas\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15306\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 28 Oct 2022 11:47:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Rui Liu; Haolin Zuo; De Hu; Guanglai Gao; Haizhou Li', display:{Lore:['[{"text": "arXiv:2210.15364", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExplicit Intensity Control for Accented Text-to-speech\\u00a7r\\n\\n\\u00a78\\u00a7oRui Liu\\nHaolin Zuo\\nDe Hu\\nGuanglai Gao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15364\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Oct 2022 12:23:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures. Submitted to ICASSP 2023. arXiv admin note: text overlap with arXiv:2209.10804\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Fan-Lin Wang; Yao-Fei Cheng; Hung-Shin Lee; Yu Tsao; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:2210.15370", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCasNet: Investigating Channel Robustness for Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oFan-Lin Wang\\nYao-Fei Cheng\\nHung-Shin Lee\\nYu Tsao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15370\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Oct 2022 12:28:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Choi et al. (§72022§r)', author: 'Kwanghee Choi; Eun Jung Yeo', display:{Lore:['[{"text": "arXiv:2210.15386", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOpening the Black Box of wav2vec Feature Encoder\\u00a7r\\n\\n\\u00a78\\u00a7oKwanghee Choi\\nEun Jung Yeo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15386\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Oct 2022 12:47:35 GMT)\\u00a7r"}']}
{title:'li et al. (§72022§r)', author: 'Jingyi li; Weiping tu; Li xiao', display:{Lore:['[{"text": "arXiv:2210.15418", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFreeVC: Towards High-Quality Text-Free One-Shot Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oJingyi li\\nWeiping tu\\nLi xiao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15418\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Oct 2022 13:32:38 GMT)\\u00a7r"}']}
{title:'Silnova et al. (§72022§r)', author: 'Anna Silnova; Niko Brümmer; Albert Swart; Lukáš Burget', display:{Lore:['[{"text": "arXiv:2210.15441", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lToroidal Probabilistic Spherical Discriminant Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oAnna Silnova\\nNiko Br\\u00fcmmer\\nAlbert Swart\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15441\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Oct 2022 14:05:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Vechtomova et al. (§72022§r)', author: 'Olga Vechtomova; Gaurav Sahu', display:{Lore:['[{"text": "arXiv:2210.15638", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLyricJam Sonic: A Generative System for Real-Time Composition and Musical Improvisation\\u00a7r\\n\\n\\u00a78\\u00a7oOlga Vechtomova\\nGaurav Sahu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15638\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Oct 2022 17:27:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 9 figures, 2 tables\\u00a7r"}']}
{title:'Verma et al. (§72022§r)', author: 'Prateek Verma; Chris Chafe; Jonathan Berger', display:{Lore:['[{"text": "arXiv:2210.15750", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOne-Shot Acoustic Matching Of Audio Signals \\u2013 Learning to Hear Music In Any Room/ Concert Hall\\u00a7r\\n\\n\\u00a78\\u00a7oPrateek Verma\\nChris Chafe\\nJonathan Berger\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15750\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 31 Oct 2022 23:55:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure; fixed up broken url; added acknowledgments\\u00a7r"}']}
{title:'Avramidis et al. (§72022§r)', author: 'Kleanthis Avramidis; Shanti Stewart; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2210.15828", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Role of Visual Context in Enriching Music Representations\\u00a7r\\n\\n\\u00a78\\u00a7oKleanthis Avramidis\\nShanti Stewart\\nShrikanth Narayanan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15828\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Oct 2022 01:45:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, 1 table\\u00a7r"}']}
{title:'Ye et al. (§72022§r)', author: 'Jia-Xin Ye; Xin-Cheng Wen; Xuan-Ze Wang; Yong Xu; Yan Luo; Chang-Li Wu; Li-Yan Chen; Kun-Hong Liu', display:{Lore:['[{"text": "arXiv:2210.15834", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGM-TCNet: Gated Multi-scale Temporal Convolutional Network using Emotion Causality for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJia-Xin Ye\\nXin-Cheng Wen\\nXuan-Ze Wang\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15834\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.specom.2022.07.005\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nspeech communication, 145, November 2022, 21-35\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Oct 2022 02:00:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe source code is available at: https://github.com/Jiaxin-Ye/GM-TCNet\\u00a7r"}']}
{title:'He et al. (§72022§r)', author: 'Shulin He; Wei Rao; Jinjiang Liu; Jun Chen; Yukai Ju; Xueliang Zhang; Yannan Wang; Shidong Shang', display:{Lore:['[{"text": "arXiv:2210.15853", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Enhancement with Intelligent Neural Homomorphic Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oShulin He\\nWei Rao\\nJinjiang Liu\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15853\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Oct 2022 02:49:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Morioka et al. (§72022§r)', author: 'Nobuyuki Morioka; Heiga Zen; Nanxin Chen; Yu Zhang; Yifan Ding', display:{Lore:['[{"text": "arXiv:2210.15868", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lResidual Adapters for Few-Shot Text-to-Speech Speaker Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oNobuyuki Morioka\\nHeiga Zen\\nNanxin Chen\\nYu Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15868\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Oct 2022 03:33:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Zhengyang Chen; Yao Qian; Bing Han; Yanmin Qian; Michael Zeng', display:{Lore:['[{"text": "arXiv:2210.15936", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA comprehensive study on self-supervised distillation for speaker representation learning\\u00a7r\\n\\n\\u00a78\\u00a7oZhengyang Chen\\nYao Qian\\nBing Han\\nYanmin Qian\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15936\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 25 Nov 2022 12:21:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by SLT2022\\u00a7r"}']}
{title:'Zhao et al. (§72022§r)', author: 'Leyi Zhao; Yi Li', display:{Lore:['[{"text": "arXiv:2210.15988", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpectrograms Are Sequences of Patches\\u00a7r\\n\\n\\u00a78\\u00a7oLeyi Zhao\\nYi Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15988\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Oct 2022 08:39:36 GMT)\\u00a7r"}']}
{title:'Fong et al. (§72022§r)', author: 'Jason Fong; Yun Wang; Prabhav Agrawal; Vimal Manohar; Jilong Wu; Thilo Köhler; Qing He', display:{Lore:['[{"text": "arXiv:2210.16045", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards zero-shot Text-based voice editing using acoustic context conditioning, utterance embeddings, and reference encoders\\u00a7r\\n\\n\\u00a78\\u00a7oJason Fong\\nYun Wang\\nPrabhav Agrawal\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16045\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Oct 2022 10:31:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Wood et al. (§72022§r)', author: 'Luke Wood; Kevin Anderson; Peter Gerstoft; Richard Bell; Raghab Subbaraman; Dinesh Bharadia', display:{Lore:['[{"text": "arXiv:2210.16173", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning Object Detection Approaches to Signal Identification\\u00a7r\\n\\n\\u00a78\\u00a7oLuke Wood\\nKevin Anderson\\nPeter Gerstoft\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16173\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 1 Nov 2022 18:07:46 GMT)\\u00a7r"}']}
{title:'Novoselov et al. (§72022§r)', author: 'Sergey Novoselov; Vladimir Volokhov; Galina Lavrentyeva', display:{Lore:['[{"text": "arXiv:2210.16231", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUniversal speaker recognition encoders for different speech segments duration\\u00a7r\\n\\n\\u00a78\\u00a7oSergey Novoselov\\nVladimir Volokhov\\nGalina Lavrentyeva\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16231\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Oct 2022 16:06:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP\'23\\u00a7r"}']}
{title:'Jin et al. (§72022§r)', author: 'Zezhong Jin; Dading Zhong; Xiao Song; Zhaoyi Liu; Naipeng Ye; Qingcheng Zeng', display:{Lore:['[{"text": "arXiv:2210.16318", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFilter and evolve: progressive pseudo label refining for semi-supervised automatic speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZezhong Jin\\nDading Zhong\\nXiao Song\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16318\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Oct 2022 16:15:58 GMT)\\u00a7r"}']}
{title:'Mashhoor et al. (§72022§r)', author: 'Reza Yousefi Mashhoor; Ahmad Ayatollahi', display:{Lore:['[{"text": "arXiv:2210.16394", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHeartSiam: A Domain Invariant Model for Heart Sound Classification\\u00a7r\\n\\n\\u00a78\\u00a7oReza Yousefi Mashhoor\\nAhmad Ayatollahi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16394\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICSPIS56952.2022.10044047\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 8 Dec 2022 10:07:07 GMT)\\u00a7r"}']}
{title:'Chatterjee et al. (§72022§r)', author: 'Moitreya Chatterjee; Narendra Ahuja; Anoop Cherian', display:{Lore:['[{"text": "arXiv:2210.16472", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Audio-Visual Dynamics Using Scene Graphs for Audio Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oMoitreya Chatterjee\\nNarendra Ahuja\\nAnoop Cherian\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16472\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 29 Oct 2022 02:55:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at NeurIPS 2022\\u00a7r"}']}
{title:'McNeal et al. (§72022§r)', author: 'Nikolas McNeal; Jennifer Huang; Aniekan Umoren; Shuqi Dai; Roger Dannenberg; Richard Randall; Tai Sing Lee', display:{Lore:['[{"text": "arXiv:2210.16587", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRelating Human Perception of Musicality to Prediction in a Predictive Coding Model\\u00a7r\\n\\n\\u00a78\\u00a7oNikolas McNeal\\nJennifer Huang\\nAniekan Umoren\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16587\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 29 Oct 2022 12:20:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, currently in peer review\\u00a7r"}']}
{title:'Sharma et al. (§72022§r)', author: 'Roshan Sharma; Hira Dhamyal; Bhiksha Raj; Rita Singh', display:{Lore:['[{"text": "arXiv:2210.16642", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnifying the Discrete and Continuous Emotion labels for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oRoshan Sharma\\nHira Dhamyal\\nBhiksha Raj\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16642\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 29 Oct 2022 16:12:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder Review at ICASSP 2023\\u00a7r"}']}
{title:'Yao et al. (§72022§r)', author: 'Jiadi Yao; Xing Chen; Xiao-Lei Zhang; Wei-Qiang Zhang; Kunde Yang', display:{Lore:['[{"text": "arXiv:2210.16777", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSymmetric Saliency-based Adversarial Attack To Speaker Identification\\u00a7r\\n\\n\\u00a78\\u00a7oJiadi Yao\\nXing Chen\\nXiao-Lei Zhang\\nWei-Qiang Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16777\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2023.3236509\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 30 Oct 2022 08:54:02 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Bozhong Liu; Xiaoxi Yu; Hantao Huang', display:{Lore:['[{"text": "arXiv:2210.16791", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptive Speech Quality Aware Complex Neural Network for Acoustic Echo Cancellation with Supervised Contrastive Learning\\u00a7r\\n\\n\\u00a78\\u00a7oBozhong Liu\\nXiaoxi Yu\\nHantao Huang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16791\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 9 Nov 2022 05:48:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2023. Under review\\u00a7r"}']}
{title:'Qiu et al. (§72022§r)', author: 'Zhibin Qiu; Mengfan Fu; Yinfeng Yu; LiLi Yin; Fuchun Sun; Hao Huang', display:{Lore:['[{"text": "arXiv:2210.16805", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSRTNet: Time Domain Speech Enhancement Via Stochastic Refinement\\u00a7r\\n\\n\\u00a78\\u00a7oZhibin Qiu\\nMengfan Fu\\nYinfeng Yu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16805\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 30 Oct 2022 10:36:11 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Yiwen Wang; Zijian Lan; Xihong Wu; Tianshu Qu', display:{Lore:['[{"text": "arXiv:2210.16849", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTT-Net: Dual-path transformer based sound field translation in the spherical harmonic domain\\u00a7r\\n\\n\\u00a78\\u00a7oYiwen Wang\\nZijian Lan\\nXihong Wu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16849\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 30 Oct 2022 14:16:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Hongji Wang; Chengdong Liang; Shuai Wang; Zhengyang Chen; Binbin Zhang; Xu Xiang; Yanlei Deng; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2210.17016", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWespeaker: A Research and Production oriented Speaker Embedding Learning Toolkit\\u00a7r\\n\\n\\u00a78\\u00a7oHongji Wang\\nChengdong Liang\\nShuai Wang\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.17016\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 1 Nov 2022 08:18:35 GMT)\\u00a7r"}']}
{title:'Wei et al. (§72022§r)', author: 'Kun Wei; Long Zhou; Ziqiang Zhang; Liping Chen; Shujie Liu; Lei He; Jinyu Li; Furu Wei', display:{Lore:['[{"text": "arXiv:2210.17027", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Pre-Training with Speech and Bilingual Text for Direct Speech to Speech Translation\\u00a7r\\n\\n\\u00a78\\u00a7oKun Wei\\nLong Zhou\\nZiqiang Zhang\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.17027\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Oct 2022 02:55:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Song et al. (§72022§r)', author: 'Xingchen Song; Di Wu; Binbin Zhang; Zhiyong Wu; Wenpeng Li; Dongfang Li; Pengshen Zhang; Zhendong Peng; Fuping Pan; Changbao Zhu; Zhongqin Wu', display:{Lore:['[{"text": "arXiv:2210.17079", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFusionFormer: Fusing Operations in Transformer for Efficient Streaming Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXingchen Song\\nDi Wu\\nBinbin Zhang\\n+ 7 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.17079\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Oct 2022 06:01:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, plus 3 appendix\\u00a7r"}']}
{title:'Miyazaki et al. (§72022§r)', author: 'Koichi Miyazaki; Masato Murata; Tomoki Koriyama', display:{Lore:['[{"text": "arXiv:2210.17098", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStructured State Space Decoder for Speech Recognition and Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oKoichi Miyazaki\\nMasato Murata\\nTomoki Koriyama\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.17098\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Oct 2022 06:54:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Attorresi et al. (§72022§r)', author: 'Luigi Attorresi; Davide Salvi; Clara Borrelli; Paolo Bestagini; Stefano Tubaro', display:{Lore:['[{"text": "arXiv:2210.17222", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCombining Automatic Speaker Verification and Prosody Analysis for Synthetic Speech Detection\\u00a7r\\n\\n\\u00a78\\u00a7oLuigi Attorresi\\nDavide Salvi\\nClara Borrelli\\nPaolo Bestagini\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.17222\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Oct 2022 11:03:03 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Yongmao Zhang; Zhichao Wang; Peiji Yang; Hongshen Sun; Zhisheng Wang; Lei Xie', display:{Lore:['[{"text": "arXiv:2210.17305", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAccentSpeech: Learning Accent from Crowd-sourced Data for Target Speaker TTS with Accents\\u00a7r\\n\\n\\u00a78\\u00a7oYongmao Zhang\\nZhichao Wang\\nPeiji Yang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.17305\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Oct 2022 13:31:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ISCSLP2022\\u00a7r"}']}
{title:'Song et al. (§72022§r)', author: 'Kun Song; Jian Cong; Xinsheng Wang; Yongmao Zhang; Lei Xie; Ning Jiang; Haiying Wu', display:{Lore:['[{"text": "arXiv:2210.17349", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust MelGAN: A robust universal neural vocoder for high-fidelity TTS\\u00a7r\\n\\n\\u00a78\\u00a7oKun Song\\nJian Cong\\nXinsheng Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.17349\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 2 Nov 2022 13:05:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ISCSLP 2022\\u00a7r"}']}
{title:'Yamamoto et al. (§72022§r)', author: 'Yuya Yamamoto; Juhan Nam; Hiroko Terasawa', display:{Lore:['[{"text": "arXiv:2210.17367", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.DL\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalysis and Detection of Singing Techniques in Repertoires of J-POP Solo Singers\\u00a7r\\n\\n\\u00a78\\u00a7oYuya Yamamoto\\nJuhan Nam\\nHiroko Terasawa\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.17367\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 15 Nov 2022 19:31:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ISMIR 2022, appendix website: https://yamathcy.github.io/ISMIR2022J-POP/\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Chen Zhang; Yi Ren; Kejun Zhang; Shuicheng Yan', display:{Lore:['[{"text": "arXiv:2211.00222", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSDMuse: Stochastic Differential Music Editing and Generation via Hybrid Representation\\u00a7r\\n\\n\\u00a78\\u00a7oChen Zhang\\nYi Ren\\nKejun Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00222\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 2 Nov 2022 04:00:54 GMT)\\u00a7r"}']}
{title:'Nikitaras et al. (§72022§r)', author: 'Karolos Nikitaras; Konstantinos Klapsas; Nikolaos Ellinas; Georgia Maniati; June Sig Sung; Inchul Hwang; Spyros Raptis; Aimilios Chalamandaris; Pirros Tsiakoulis', display:{Lore:['[{"text": "arXiv:2211.00523", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning utterance-level representations through token-level acoustic latents prediction for Expressive Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oKarolos Nikitaras\\nKonstantinos Klapsas\\nNikolaos Ellinas\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00523\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Nov 2022 15:17:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Schwartz et al. (§72022§r)', author: 'Ayal Schwartz; Sharon Gannot; Shlomo E. Chazan', display:{Lore:['[{"text": "arXiv:2211.00607", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMagnitude or Phase? A Two Stage Algorithm for Dereverberation\\u00a7r\\n\\n\\u00a78\\u00a7oAyal Schwartz\\nSharon Gannot\\nShlomo E. Chazan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00607\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Oct 2022 07:56:48 GMT)\\u00a7r"}']}
{title:'Bijwadia et al. (§72022§r)', author: 'Shaan Bijwadia; Shuo-yiin Chang; Bo Li; Tara Sainath; Chao Zhang; Yanzhang He', display:{Lore:['[{"text": "arXiv:2211.00786", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnified End-to-End Speech Recognition and Endpointing for Fast and Efficient Speech Systems\\u00a7r\\n\\n\\u00a78\\u00a7oShaan Bijwadia\\nShuo-yiin Chang\\nBo Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00786\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/SLT54892.2023.10022338\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Nov 2022 23:43:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in Spoken Language Technology Workshop (SLT) 2022\\u00a7r"}']}
{title:'Vargas-Quiros et al. (§72022§r)', author: 'Jose Vargas-Quiros; Laura Cabrera-Quiros; Catharine Oertel; Hayley Hung', display:{Lore:['[{"text": "arXiv:2211.00794", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.CY\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImpact of annotation modality on label quality and model performance in the automatic assessment of laughter in-the-wild\\u00a7r\\n\\n\\u00a78\\u00a7oJose Vargas-Quiros\\nLaura Cabrera-Quiros\\nCatharine Oertel\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00794\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Nov 2022 00:18:08 GMT)\\u00a7r"}']}
{title:'Zhao et al. (§72022§r)', author: 'Huaibo Zhao; Shinya Fujie; Tetsuji Ogawa; Jin Sakuma; Yusuke Kida; Tetsunori Kobayashi', display:{Lore:['[{"text": "arXiv:2211.00858", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConversation-oriented ASR with multi-look-ahead CBS architecture\\u00a7r\\n\\n\\u00a78\\u00a7oHuaibo Zhao\\nShinya Fujie\\nTetsuji Ogawa\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00858\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Nov 2022 03:58:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP2023\\u00a7r"}']}
{title:'Kanagawa et al. (§72022§r)', author: 'Hiroki Kanagawa; Yusuke Ijima', display:{Lore:['[{"text": "arXiv:2211.00898", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSIMD-size aware weight regularization for fast neural vocoding on CPU\\u00a7r\\n\\n\\u00a78\\u00a7oHiroki Kanagawa\\nYusuke Ijima\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00898\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Nov 2022 05:43:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to SLT 2022\\u00a7r"}']}
{title:'Liang et al. (§72022§r)', author: 'Chengdong Liang; Xiao-Lei Zhang; BinBin Zhang; Di Wu; Shengqiang Li; Xingchen Song; Zhendong Peng; Fuping Pan', display:{Lore:['[{"text": "arXiv:2211.00941", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast-U2++: Fast and Accurate End-to-End Speech Recognition in Joint CTC/Attention Frames\\u00a7r\\n\\n\\u00a78\\u00a7oChengdong Liang\\nXiao-Lei Zhang\\nBinBin Zhang\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00941\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Nov 2022 08:01:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Song et al. (§72022§r)', author: 'Wei Song; Yanghao Yue; Ya-jie Zhang; Zhengchen Zhang; Youzheng Wu; Xiaodong He', display:{Lore:['[{"text": "arXiv:2211.00967", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Speaker Multi-Style Speech Synthesis with Timbre and Style Disentanglement\\u00a7r\\n\\n\\u00a78\\u00a7oWei Song\\nYanghao Yue\\nYa-jie Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00967\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 22 Nov 2022 08:23:47 GMT)\\u00a7r"}']}
{title:'Song et al. (§72022§r)', author: 'Yingjie Song; Wei Song; Wei Zhang; Zhengchen Zhang; Dan Zeng; Zhi Liu; Yang Yu', display:{Lore:['[{"text": "arXiv:2211.00996", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSinging Voice Synthesis with Vibrato Modeling and Latent Energy Representation\\u00a7r\\n\\n\\u00a78\\u00a7oYingjie Song\\nWei Song\\nWei Zhang\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00996\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Nov 2022 09:58:25 GMT)\\u00a7r"}']}
{title:'Kreuk et al. (§72022§r)', author: 'Felix Kreuk; Yaniv Taigman; Adam Polyak; Jade Copet; Gabriel Synnaeve; Alexandre Défossez; Yossi Adi', display:{Lore:['[{"text": "arXiv:2211.01223", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Language Modeling using Perceptually-Guided Discrete Representations\\u00a7r\\n\\n\\u00a78\\u00a7oFelix Kreuk\\nYaniv Taigman\\nAdam Polyak\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01223\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 4 Nov 2022 10:50:00 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Chao-Han Huck Yang; Bo Li; Yu Zhang; Nanxin Chen; Tara N. Sainath; Sabato Marco Siniscalchi; Chin-Hui Lee', display:{Lore:['[{"text": "arXiv:2211.01263", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75quant-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Quantum Kernel Learning Approach to Acoustic Modeling for Spoken Command Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oChao-Han Huck Yang\\nBo Li\\nYu Zhang\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01263\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10095142\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Nov 2022 16:46:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Klapsas et al. (§72022§r)', author: 'Konstantinos Klapsas; Karolos Nikitaras; Nikolaos Ellinas; June Sig Sung; Inchul Hwang; Spyros Raptis; Aimilios Chalamandaris; Pirros Tsiakoulis', display:{Lore:['[{"text": "arXiv:2211.01327", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPredicting phoneme-level prosody latents using AR and flow-based Prior Networks for expressive speech synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oKonstantinos Klapsas\\nKarolos Nikitaras\\nNikolaos Ellinas\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01327\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Nov 2022 17:45:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Ao Zhang; Fan Yu; Kaixun Huang; Lei Xie; Longbiao Wang; Eng Siong Chng; Hui Bu; Binbin Zhang; Wei Chen; Xin Xu', display:{Lore:['[{"text": "arXiv:2211.01585", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe ISCSLP 2022 Intelligent Cockpit Speech Recognition Challenge (ICSRC): Dataset, Tracks, Baseline and Results\\u00a7r\\n\\n\\u00a78\\u00a7oAo Zhang\\nFan Yu\\nKaixun Huang\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01585\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 3 Nov 2022 04:45:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ISCSLP2022\\u00a7r"}']}
{title:'Xiao et al. (§72022§r)', author: 'Zixuan Xiao; Shengshi Yao; Jincheng Dai; Sixian Wang; Kai Niu; Ping Zhang', display:{Lore:['[{"text": "arXiv:2211.02283", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IT\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a72math.IT\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWireless Deep Speech Semantic Transmission\\u00a7r\\n\\n\\u00a78\\u00a7oZixuan Xiao\\nShengshi Yao\\nJincheng Dai\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02283\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Nov 2022 06:49:42 GMT)\\u00a7r"}']}
{title:'Zhu et al. (§72022§r)', author: 'Yin Zhu; Qiuqiang Kong; Junjie Shi; Shilei Liu; Xuzhou Ye; Ju-chiang Wang; Junping Zhang', display:{Lore:['[{"text": "arXiv:2211.02301", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBinaural Rendering of Ambisonic Signals by Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oYin Zhu\\nQiuqiang Kong\\nJunjie Shi\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02301\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Nov 2022 07:57:37 GMT)\\u00a7r"}']}
{title:'Xin et al. (§72022§r)', author: 'Detai Xin; Sharath Adavanne; Federico Ang; Ashish Kulkarni; Shinnosuke Takamichi; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2211.02336", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Speech Prosody of Audiobook Text-to-Speech Synthesis with Acoustic and Textual Contexts\\u00a7r\\n\\n\\u00a78\\u00a7oDetai Xin\\nSharath Adavanne\\nFederico Ang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02336\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Nov 2022 09:23:02 GMT)\\u00a7r"}']}
{title:'Arezzo et al. (§72022§r)', author: 'A. Arezzo; S. Berretti', display:{Lore:['[{"text": "arXiv:2211.02366", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSPEAKER VGG CCT: Cross-corpus Speech Emotion Recognition with Speaker Embedding and Vision Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oA. Arezzo\\nS. Berretti\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02366\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Nov 2022 10:49:44 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Dongchao Yang; Songxiang Liu; Jianwei Yu; Helin Wang; Chao Weng; Yuexian Zou', display:{Lore:['[{"text": "arXiv:2211.02448", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoreSpeech: Knowledge Distillation based Conditional Diffusion Model for Noise-robust Expressive TTS\\u00a7r\\n\\n\\u00a78\\u00a7oDongchao Yang\\nSongxiang Liu\\nJianwei Yu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02448\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Nov 2022 13:32:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP2023\\u00a7r"}']}
{title:'Sadeghi et al. (§72022§r)', author: 'Mostafa Sadeghi; Romain Serizel', display:{Lore:['[{"text": "arXiv:2211.02728", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast and efficient speech enhancement with variational autoencoders\\u00a7r\\n\\n\\u00a78\\u00a7oMostafa Sadeghi\\nRomain Serizel\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02728\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Nov 2022 09:52:13 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Yongmao Zhang; Heyang Xue; Hanzhao Li; Lei Xie; Tingwei Guo; Ruixiong Zhang; Caixia Gong', display:{Lore:['[{"text": "arXiv:2211.02903", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVISinger 2: High-Fidelity End-to-End Singing Voice Synthesis Enhanced by Digital Signal Processing Synthesizer\\u00a7r\\n\\n\\u00a78\\u00a7oYongmao Zhang\\nHeyang Xue\\nHanzhao Li\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02903\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 5 Nov 2022 13:35:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Christonasis et al. (§72022§r)', author: 'Antonios Marios Christonasis; Stef van Eijndhoven; Peter Duin', display:{Lore:['[{"text": "arXiv:2211.03202", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.AP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l\\"Seeing Sound\\": Audio Classification with the Wigner-Wille Distribution and Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oAntonios Marios Christonasis\\nStef van Eijndhoven\\nPeter Duin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.03202\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 6 Nov 2022 19:01:02 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Taesu Kim; SeungHeon Doh; Gyunpyo Lee; Hyungseok Jeon; Juhan Nam; Hyeon-Jeong Suk', display:{Lore:['[{"text": "arXiv:2211.03371", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHi,KIA: A Speech Emotion Recognition Dataset for Wake-Up Words\\u00a7r\\n\\n\\u00a78\\u00a7oTaesu Kim\\nSeungHeon Doh\\nGyunpyo Lee\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.03371\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Nov 2022 08:57:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAsia Pacific Signal and Information ProcessingAssociation Annual Summit and Conference (APSIPA), 2022\\u00a7r"}']}
{title:'Heo et al. (§72022§r)', author: 'Hee-Soo Heo; Youngki Kwon; Bong-Jin Lee; You Jin Kim; Jee-weon Jung', display:{Lore:['[{"text": "arXiv:2211.04060", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHigh-resolution embedding extractor for speaker diarisation\\u00a7r\\n\\n\\u00a78\\u00a7oHee-Soo Heo\\nYoungki Kwon\\nBong-Jin Lee\\nYou Jin Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.04060\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Nov 2022 07:41:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5pages, 2 figure,3 tables, submitted to ICASSP\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Tao Liu; Kai Yu', display:{Lore:['[{"text": "arXiv:2211.04304", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBER: Balanced Error Rate For Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oTao Liu\\nKai Yu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.04304\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Nov 2022 15:17:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Jun Chen; Wei Rao; Zilin Wang; Zhiyong Wu; Yannan Wang; Tao Yu; Shidong Shang; Helen Meng', display:{Lore:['[{"text": "arXiv:2211.05432", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Enhancement with Fullband-Subband Cross-Attention Network\\u00a7r\\n\\n\\u00a78\\u00a7oJun Chen\\nWei Rao\\nZilin Wang\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.05432\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Nov 2022 09:17:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by InterSpeech 2022. arXiv admin note: text overlap with arXiv:2203.12188\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Meng Chen; Li Lu; Jiadi Yu; Yingying Chen; Zhongjie Ba; Feng Lin; Kui Ren', display:{Lore:['[{"text": "arXiv:2211.05446", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrivacy-Utility Balanced Voice De-Identification Using Adversarial Examples\\u00a7r\\n\\n\\u00a78\\u00a7oMeng Chen\\nLi Lu\\nJiadi Yu\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.05446\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Nov 2022 09:35:58 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Runbang Zhang; Yixiao Zhang; Kai Shao; Ying Shan; Gus Xia', display:{Lore:['[{"text": "arXiv:2211.05543", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVis2Mus: Exploring Multimodal Representation Mapping for Controllable Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oRunbang Zhang\\nYixiao Zhang\\nKai Shao\\nYing Shan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.05543\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Nov 2022 13:01:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023. GitHub repo: https://github.com/ldzhangyx/vis2mus\\u00a7r"}']}
{title:'Tzinis et al. (§72022§r)', author: 'Efthymios Tzinis; Gordon Wichern; Paris Smaragdis; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2211.05927", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimal Condition Training for Target Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oEfthymios Tzinis\\nGordon Wichern\\nParis Smaragdis\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.05927\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Nov 2022 00:04:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Summoogum et al. (§72022§r)', author: 'Kelvin Summoogum; Debayan Das; Parvati Jayakumar', display:{Lore:['[{"text": "arXiv:2211.05944", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Gait Triaging Toolkit for Overlapping Acoustic Events in Indoor Environments\\u00a7r\\n\\n\\u00a78\\u00a7oKelvin Summoogum\\nDebayan Das\\nParvati Jayakumar\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.05944\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Nov 2022 01:33:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Zhou et al. (§72022§r)', author: 'Lifeng Zhou; Kaifeng Wei; Yuke Li; Yiya Hao; Weiqiang Yang; Haoqi Zhu', display:{Lore:['[{"text": "arXiv:2211.05983", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic Pornography Recognition Using Convolutional Neural Networks and Bag of Refinements\\u00a7r\\n\\n\\u00a78\\u00a7oLifeng Zhou\\nKaifeng Wei\\nYuke Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.05983\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Nov 2022 03:21:32 GMT)\\u00a7r"}']}
{title:'Lin et al. (§72022§r)', author: 'Hsin-Yi Lin; Huan-Hsin Tseng; Yu Tsao', display:{Lore:['[{"text": "arXiv:2211.06508", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the robustness of non-intrusive speech quality model by adversarial examples\\u00a7r\\n\\n\\u00a78\\u00a7oHsin-Yi Lin\\nHuan-Hsin Tseng\\nYu Tsao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.06508\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Nov 2022 23:06:24 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Yikang Wang; Xingming Wang; Hiromitsu Nishizaki; Ming Li', display:{Lore:['[{"text": "arXiv:2211.06546", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow Pass Filtering and Bandwidth Extension for Robust Anti-spoofing Countermeasure Against Codec Variabilities\\u00a7r\\n\\n\\u00a78\\u00a7oYikang Wang\\nXingming Wang\\nHiromitsu Nishizaki\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.06546\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 12 Nov 2022 02:04:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted by ISCSLP 2022\\u00a7r"}']}
{title:'Guimarães et al. (§72022§r)', author: 'Heitor R. Guimarães; Arthur Pimentel; Anderson R. Avila; Mehdi Rezagholizadeh; Tiago H. Falk', display:{Lore:['[{"text": "arXiv:2211.06562", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving the Robustness of DistilHuBERT to Unseen Noisy Conditions via Data Augmentation, Curriculum Learning, and Multi-Task Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oHeitor R. Guimar\\u00e3es\\nArthur Pimentel\\nAnderson R. Avila\\nMehdi Rezagholizadeh\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.06562\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 12 Nov 2022 03:50:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oENLSP-II NeurIPS Workshop 2022, 6 pages\\u00a7r"}']}
{title:'Choi et al. (§72022§r)', author: 'Eunjin Choi; Yoonjin Chung; Seolhee Lee; JongIk Jeon; Taegyun Kwon; Juhan Nam', display:{Lore:['[{"text": "arXiv:2211.07131", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lYM2413-MDB: A Multi-Instrumental FM Video Game Music Dataset with Emotion Annotations\\u00a7r\\n\\n\\u00a78\\u00a7oEunjin Choi\\nYoonjin Chung\\nSeolhee Lee\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.07131\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Nov 2022 06:18:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe paper has been accepted for publication at ISMIR 2022\\u00a7r"}']}
{title:'Ibrahim et al. (§72022§r)', author: 'Karim M. Ibrahim; Elena V. Epure; Geoffroy Peeters; Gaël Richard', display:{Lore:['[{"text": "arXiv:2211.07250", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting Device and Audio Data to Tag Music with User-Aware Listening Contexts\\u00a7r\\n\\n\\u00a78\\u00a7oKarim M. Ibrahim\\nElena V. Epure\\nGeoffroy Peeters\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.07250\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Nov 2022 10:08:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in ISMIR\\u00a7r"}']}
{title:'Dhamyal et al. (§72022§r)', author: 'Hira Dhamyal; Benjamin Elizalde; Soham Deshmukh; Huaming Wang; Bhiksha Raj; Rita Singh', display:{Lore:['[{"text": "arXiv:2211.07737", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDescribing emotions with acoustic property prompts for speech emotion recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHira Dhamyal\\nBenjamin Elizalde\\nSoham Deshmukh\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.07737\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Nov 2022 20:29:37 GMT)\\u00a7r"}']}
{title:'Hashizume et al. (§72022§r)', author: 'Yuka Hashizume; Li Li; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2211.07863", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Similarity Calculation of Individual Instrumental Sounds Using Metric Learning\\u00a7r\\n\\n\\u00a78\\u00a7oYuka Hashizume\\nLi Li\\nTomoki Toda\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.07863\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Nov 2022 03:03:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAPSIPA ASC 2022 (pp.33\\u201338)\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Kyungsu Kim; Minju Park; Haesun Joung; Yunkee Chae; Yeongbeom Hong; Seonghyeon Go; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2211.07951", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lShow Me the Instruments: Musical Instrument Retrieval from Mixture Audio\\u00a7r\\n\\n\\u00a78\\u00a7oKyungsu Kim\\nMinju Park\\nHaesun Joung\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.07951\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Nov 2022 07:32:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, submitted to ICASSP 2023\\u00a7r"}']}
{title:'Peeters et al. (§72022§r)', author: 'Geoffroy Peeters; Florian Angulo', display:{Lore:['[{"text": "arXiv:2211.08141", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSSM-Net: feature learning for Music Structure Analysis using a Self-Similarity-Matrix based loss\\u00a7r\\n\\n\\u00a78\\u00a7oGeoffroy Peeters\\nFlorian Angulo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08141\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Nov 2022 13:48:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oExtended Abstracts for the Late-Breaking Demo Session of the 23rd Int.Society for Music Information Retrieval Conf., Bengaluru, India, 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Zihan Wang; Qi Meng; HaiFeng Lan; XinRui Zhang; KeHao Guo; Akshat Gupta', display:{Lore:['[{"text": "arXiv:2211.08237", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultilingual Speech Emotion Recognition With Multi-Gating Mechanism and Neural Architecture Search\\u00a7r\\n\\n\\u00a78\\u00a7oZihan Wang\\nQi Meng\\nHaiFeng Lan\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08237\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 16 Nov 2022 01:47:34 GMT)\\u00a7r"}']}
{title:'Masuyama et al. (§72022§r)', author: 'Yoshiki Masuyama; Kohei Yatabe; Kento Nagatomo; Yasuhiro Oikawa', display:{Lore:['[{"text": "arXiv:2211.08246", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOnline Phase Reconstruction via DNN-based Phase Differences Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oYoshiki Masuyama\\nKohei Yatabe\\nKento Nagatomo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08246\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2022.3221041\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 12 Nov 2022 20:45:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEE/ACM Trans. Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Hsin-Hung Chen; Alexander Lerch', display:{Lore:['[{"text": "arXiv:2211.08379", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Instrument Classification Reprogrammed\\u00a7r\\n\\n\\u00a78\\u00a7oHsin-Hung Chen\\nAlexander Lerch\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08379\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Nov 2022 18:26:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at 29thInternational Conference on Multimedia Modeling (MMM23)\\u00a7r"}']}
{title:'Cai et al. (§72022§r)', author: 'Hanbo Cai; Pengcheng Zhang; Hai Dong; Yan Xiao; Shunhui Ji', display:{Lore:['[{"text": "arXiv:2211.08697", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPBSM: Backdoor attack against Keyword spotting based on pitch boosting and sound masking\\u00a7r\\n\\n\\u00a78\\u00a7oHanbo Cai\\nPengcheng Zhang\\nHai Dong\\nYan Xiao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08697\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Nov 2022 06:20:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Jiahao Wang; Guo Chen; Yin-Dong Zheng; Tong Lu', display:{Lore:['[{"text": "arXiv:2211.08708", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Detection-based Method For Speaker Diarization @ Ego4D Audio-only Diarization Challenge 2022\\u00a7r\\n\\n\\u00a78\\u00a7oJiahao Wang\\nGuo Chen\\nYin-Dong Zheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08708\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Nov 2022 06:48:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2 pages\\u00a7r"}']}
{title:'Lee et al. (§72022§r)', author: 'Seokjin Lee; Minhan Kim; Seunghyeon Shin; Daeho Lee; Inseon Jang; Wootaek Lim', display:{Lore:['[{"text": "arXiv:2211.08715", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConditional variational autoencoder to improve neural audio synthesis for polyphonic music sound\\u00a7r\\n\\n\\u00a78\\u00a7oSeokjin Lee\\nMinhan Kim\\nSeunghyeon Shin\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08715\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Nov 2022 07:11:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 6 figures\\u00a7r"}']}
{title:'Witbrock et al. (§72022§r)', author: 'Michael Witbrock; Patrick Haffner', display:{Lore:['[{"text": "arXiv:2211.08978", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRapid Connectionist Speaker Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oMichael Witbrock\\nPatrick Haffner\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08978\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.1992.225874\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP-92: 1992 IEEE International Conference on Acoustics,\\n  Speech, and Signal Processing, 1992, pp. 453-456 vol.1\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Nov 2022 00:15:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 Figures, Two Tables, ICASSP-92\\u00a7r"}']}
{title:'Labbé et al. (§72022§r)', author: 'Etienne Labbé; Thomas Pellegrini; Julien Pinquier', display:{Lore:['[{"text": "arXiv:2211.08983", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIs my automatic audio captioning system so bad? spider-max: a metric to consider several caption candidates\\u00a7r\\n\\n\\u00a78\\u00a7oEtienne Labb\\u00e9\\nThomas Pellegrini\\nJulien Pinquier\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08983\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Nov 2022 19:16:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWorkshop on Detection and Classification of Acoustic Scenes and Events (DCASE 2022), Nov 2022, Nancy, France\\u00a7r"}']}
{title:'Xiao et al. (§72022§r)', author: 'Yi Xiao; Harshit Sharma; Victoria Tumanova; Asif Salekin', display:{Lore:['[{"text": "arXiv:2211.09089", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPsychophysiology-aided Perceptually Fluent Speech Analysis of Children Who Stutter\\u00a7r\\n\\n\\u00a78\\u00a7oYi Xiao\\nHarshit Sharma\\nVictoria Tumanova\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.09089\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Nov 2022 18:12:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o20 pages, 5 figures\\u00a7r"}']}
{title:'Kumar et al. (§72022§r)', author: 'Sumit Kumar; B. Anshuman; Linus Ruettimann; Richard H. R. Hahnloser; Vipul Arora', display:{Lore:['[{"text": "arXiv:2211.09376", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBalanced Deep CCA for Bird Vocalization Detection\\u00a7r\\n\\n\\u00a78\\u00a7oSumit Kumar\\nB. Anshuman\\nLinus Ruettimann\\nRichard H. R. Hahnloser\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.09376\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Nov 2022 07:09:07 GMT)\\u00a7r"}']}
{title:'Fan et al. (§72022§r)', author: 'Zhiyun Fan; Zhenlin Liang; Linhao Dong; Yi Liu; Shiyu Zhou; Meng Cai; Jun Zhang; Zejun Ma; Bo Xu', display:{Lore:['[{"text": "arXiv:2211.09381", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lToken-level Speaker Change Detection Using Speaker Difference and Speech Content via Continuous Integrate-and-fire\\u00a7r\\n\\n\\u00a78\\u00a7oZhiyun Fan\\nZhenlin Liang\\nLinhao Dong\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.09381\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Nov 2022 07:16:17 GMT)\\u00a7r"}']}
{title:'Hyun et al. (§72022§r)', author: 'Lee Hyun; Taehyun Kim; Hyolim Kang; Minjoo Ki; Hyeonchan Hwang; Kwanho Park; Sharang Han; Seon Joo Kim', display:{Lore:['[{"text": "arXiv:2211.09385", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComMU: Dataset for Combinatorial Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oLee Hyun\\nTaehyun Kim\\nHyolim Kang\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.09385\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Nov 2022 07:25:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o19 pages, 12 figures\\u00a7r"}']}
{title:'Choi et al. (§72022§r)', author: 'Hyeong-Seok Choi; Jinhyeok Yang; Juheon Lee; Hyeongju Kim', display:{Lore:['[{"text": "arXiv:2211.09407", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNANSY++: Unified Voice Synthesis with Neural Analysis and Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oHyeong-Seok Choi\\nJinhyeok Yang\\nJuheon Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.09407\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Nov 2022 08:29:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICLR2023\\u00a7r"}']}
{title:'Gong et al. (§72022§r)', author: 'Xun Gong; Yu Wu; Jinyu Li; Shujie Liu; Rui Zhao; Xie Chen; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2211.09412", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLongFNT: Long-form Speech Recognition with Factorized Neural Transducer\\u00a7r\\n\\n\\u00a78\\u00a7oXun Gong\\nYu Wu\\nJinyu Li\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.09412\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Nov 2022 08:48:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP2023\\u00a7r"}']}
{title:'Qiang et al. (§72022§r)', author: 'Chunyu Qiang; Peng Yang; Hao Che; Jinba Xiao; Xiaorui Wang; Zhongyuan Wang', display:{Lore:['[{"text": "arXiv:2211.09495", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBack-Translation-Style Data Augmentation for Mandarin Chinese Polyphone Disambiguation\\u00a7r\\n\\n\\u00a78\\u00a7oChunyu Qiang\\nPeng Yang\\nHao Che\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.09495\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Nov 2022 12:37:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished to APSIPA ASC 2022\\u00a7r"}']}
{title:'Faiß (§72022§r)', author: 'Marius Faiß', display:{Lore:['[{"text": "arXiv:2211.09503", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptive Representations of Sound for Automatic Insect Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMarius Fai\\u00df\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.09503\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Nov 2022 12:52:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o30 pages, 9 figures Dataset: https://doi.org/10.5281/zenodo.7072196\\u00a7r"}']}
{title:'Rashid et al. (§72022§r)', author: 'Nayeeb Rashid; Swapnil Saha; Mohseu Rashid Subah; Rizwan Ahmed Robin; Syed Mortuza Hasan Fahim; Shahed Ahmed; Talha Ibn Mahmud', display:{Lore:['[{"text": "arXiv:2211.09751", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.med-ph\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHeart Abnormality Detection from Heart Sound Signals using MFCC Feature and Dual Stream Attention Based Network\\u00a7r\\n\\n\\u00a78\\u00a7oNayeeb Rashid\\nSwapnil Saha\\nMohseu Rashid Subah\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.09751\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Nov 2022 18:20:46 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Zhenyu Wang; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:2211.09898", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Anti-spoofing Using a Simple Attention Module and Joint Optimization Based on Additive Angular Margin Loss and Meta-learning\\u00a7r\\n\\n\\u00a78\\u00a7oZhenyu Wang\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.09898\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-904\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Nov 2022 21:25:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Zhenyu Wang; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:2211.09913", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-source Domain Adaptation for Text-independent Forensic Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhenyu Wang\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.09913\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3130975\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Nov 2022 22:11:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING\\u00a7r"}']}
{title:'Du et al. (§72022§r)', author: 'Zhihao Du; Shiliang Zhang; Siqi Zheng; Zhijie Yan', display:{Lore:['[{"text": "arXiv:2211.10243", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Overlap-aware Neural Diarization for Multi-party Meeting Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oZhihao Du\\nShiliang Zhang\\nSiqi Zheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.10243\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Nov 2022 14:03:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by EMNLP 2022\\u00a7r"}']}
{title:'Tseng et al. (§72022§r)', author: 'Jonathan Tseng; Rodrigo Castellon; C. Karen Liu', display:{Lore:['[{"text": "arXiv:2211.10658", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEDGE: Editable Dance Generation From Music\\u00a7r\\n\\n\\u00a78\\u00a7oJonathan Tseng\\nRodrigo Castellon\\nC. Karen Liu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.10658\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 27 Nov 2022 06:27:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProject website: https://edge-dance.github.io\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Jiakai Wang; Zhendong Chen; Zixin Yin; Qinghong Yang; Xianglong Liu', display:{Lore:['[{"text": "arXiv:2211.10661", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhonemic Adversarial Attack against Audio Recognition in Real World\\u00a7r\\n\\n\\u00a78\\u00a7oJiakai Wang\\nZhendong Chen\\nZixin Yin\\nQinghong Yang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.10661\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 19 Nov 2022 11:01:21 GMT)\\u00a7r"}']}
{title:'Qian et al. (§72022§r)', author: 'Fan Qian; Jiqing Han', display:{Lore:['[{"text": "arXiv:2211.10885", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContrastive Regularization for Multimodal Emotion Recognition Using Audio and Text\\u00a7r\\n\\n\\u00a78\\u00a7oFan Qian\\nJiqing Han\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.10885\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 20 Nov 2022 06:56:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCompleted in October 2020 and submitted to ICASSP2021\\u00a7r"}']}
{title:'Jonason et al. (§72022§r)', author: 'Nicolas Jonason; Bob L. T. Sturm', display:{Lore:['[{"text": "arXiv:2211.11225", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTimbreCLIP: Connecting Timbre to Text and Images\\u00a7r\\n\\n\\u00a78\\u00a7oNicolas Jonason\\nBob L. T. Sturm\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.11225\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Nov 2022 07:40:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to AAAIworkshop on creative AI across modalities\\u00a7r"}']}
{title:'Papaioannou et al. (§72022§r)', author: 'Charilaos Papaioannou; Ioannis Valiantzas; Theodoros Giannakopoulos; Maximos Kaliakatsos-Papakostas; Alexandros Potamianos', display:{Lore:['[{"text": "arXiv:2211.11479", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Dataset for Greek Traditional and Folk Music: Lyra\\u00a7r\\n\\n\\u00a78\\u00a7oCharilaos Papaioannou\\nIoannis Valiantzas\\nTheodoros Giannakopoulos\\nMaximos Kaliakatsos-Papakostas\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.11479\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Nov 2022 14:15:43 GMT)\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Andong Li; Guochen Yu; Wenzhe Liu; Xiaodong Li; Chengshi Zheng', display:{Lore:['[{"text": "arXiv:2211.12024", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTaylorBeamixer: Learning Taylor-Inspired All-Neural Multi-Channel Speech Enhancement from Beam-Space Dictionary Perspective\\u00a7r\\n\\n\\u00a78\\u00a7oAndong Li\\nGuochen Yu\\nWenzhe Liu\\nXiaodong Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.12024\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 30 Nov 2022 15:21:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn submission to ICASSP 2023, 5 pages\\u00a7r"}']}
{title:'Gopinath et al. (§72022§r)', author: 'Deepa P Gopinath; Thennal D K; Vrinda V Nair; Swaraj K S; Sachin G', display:{Lore:['[{"text": "arXiv:2211.12796", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIMaSC \\u2013 ICFOSS Malayalam Speech Corpus\\u00a7r\\n\\n\\u00a78\\u00a7oDeepa P Gopinath\\nThennal D K\\nVrinda V Nair\\nSwaraj K S\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.12796\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 23 Nov 2022 09:21:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages, 8 figures\\u00a7r"}']}
{title:'Bjare et al. (§72022§r)', author: 'Mathias Rose Bjare; Stefan Lattner', display:{Lore:['[{"text": "arXiv:2211.13016", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Typicality of Musical Sequences\\u00a7r\\n\\n\\u00a78\\u00a7oMathias Rose Bjare\\nStefan Lattner\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.13016\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 23 Nov 2022 16:05:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2 pages, 1 figure, Accepted at the Extended Abstracts for the Late-Breaking Demo Session of the 23rd Int. Society for Music Information Retrieval Conf., Bengaluru, India, 2022\\u00a7r"}']}
{title:'Yao et al. (§72022§r)', author: 'Zhuoyuan Yao; Shuo Ren; Sanyuan Chen; Ziyang Ma; Pengcheng Guo; Lei Xie', display:{Lore:['[{"text": "arXiv:2211.13443", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTESSP: Text-Enhanced Self-Supervised Speech Pre-training\\u00a7r\\n\\n\\u00a78\\u00a7oZhuoyuan Yao\\nShuo Ren\\nSanyuan Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.13443\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Nov 2022 07:08:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 4 figures\\u00a7r"}']}
{title:'Du et al. (§72022§r)', author: 'Muyang Du; Chuan Liu; Jiaxing Qi; Junjie Lai', display:{Lore:['[{"text": "arXiv:2211.13939", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Incremental Text-to-Speech on GPUs\\u00a7r\\n\\n\\u00a78\\u00a7oMuyang Du\\nChuan Liu\\nJiaxing Qi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.13939\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 5 Dec 2022 08:03:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures\\u00a7r"}']}
{title:'Korkmaz et al. (§72022§r)', author: 'Burla Nur Korkmaz; Roee Diamant; Gil Danino; Alberto Testolin', display:{Lore:['[{"text": "arXiv:2211.15406", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomated Detection of Dolphin Whistles with Convolutional Networks and Transfer Learning\\u00a7r\\n\\n\\u00a78\\u00a7oBurla Nur Korkmaz\\nRoee Diamant\\nGil Danino\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.15406\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3389/frai.2023.1099022\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nFrontiers in Artificial Intelligence, 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Nov 2022 15:06:46 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Ju-Chiang Wang; Jordan B. L. Smith; Yun-Ning Hung', display:{Lore:['[{"text": "arXiv:2211.15787", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMuSFA: Improving Music Structural Function Analysis with Partially Labeled Data\\u00a7r\\n\\n\\u00a78\\u00a7oJu-Chiang Wang\\nJordan B. L. Smith\\nYun-Ning Hung\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.15787\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Nov 2022 21:48:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oISMIR2022, LBD paper\\u00a7r"}']}
{title:'Collins (§72022§r)', author: 'Nick Collins', display:{Lore:['[{"text": "arXiv:2211.15834", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOK Computer Analysis: An Audio Corpus Study of Radiohead\\u00a7r\\n\\n\\u00a78\\u00a7oNick Collins\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.15834\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Nov 2022 00:08:31 GMT)\\u00a7r"}']}
{title:'Im et al. (§72022§r)', author: 'Jaekwon Im; Soonbeom Choi; Sangeon Yong; Juhan Nam', display:{Lore:['[{"text": "arXiv:2211.15948", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Vocoder Feature Estimation for Dry Singing Voice Separation\\u00a7r\\n\\n\\u00a78\\u00a7oJaekwon Im\\nSoonbeom Choi\\nSangeon Yong\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.15948\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n14th Asia Pacific Signal and Information Processing Association\\n  Annual Summit and Conference (APSIPA), 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Nov 2022 06:16:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 4 figures\\u00a7r"}']}
{title:'Ellinas et al. (§72022§r)', author: 'Nikolaos Ellinas; Myrsini Christidou; Alexandra Vioni; June Sig Sung; Aimilios Chalamandaris; Pirros Tsiakoulis; Paris Mastorocostas', display:{Lore:['[{"text": "arXiv:2211.16307", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lControllable speech synthesis by learning discrete phoneme-level prosodic representations\\u00a7r\\n\\n\\u00a78\\u00a7oNikolaos Ellinas\\nMyrsini Christidou\\nAlexandra Vioni\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.16307\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.specom.2022.11.006\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Nov 2022 15:43:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oFinal published version available at: Speech Communication. arXiv admin note: substantial text overlap with arXiv:2111.10168\\u00a7r"}']}
{title:'Guo et al. (§72022§r)', author: 'Z. Guo; J. Kang; D. Herremans', display:{Lore:['[{"text": "arXiv:2212.00973", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Domain-Knowledge-Inspired Music Embedding Space and a Novel Attention Mechanism for Symbolic Music Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oZ. Guo\\nJ. Kang\\nD. Herremans\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.00973\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Dec 2022 05:04:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper is accepted at AAAI 2023\\u00a7r"}']}
{title:'Shriram et al. (§72022§r)', author: 'Jaidev Shriram; Makarand Tapaswi; Vinoo Alluri', display:{Lore:['[{"text": "arXiv:2212.01033", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSonus Texere! Automated Dense Soundtrack Construction for Books using Movie Adaptations\\u00a7r\\n\\n\\u00a78\\u00a7oJaidev Shriram\\nMakarand Tapaswi\\nVinoo Alluri\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.01033\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Dec 2022 08:57:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ISMIR 2022. Project page: https://auto-book-soundtrack.github.io/\\u00a7r"}']}
{title:'Hu et al. (§72022§r)', author: 'Pengfei Hu; Hui Zhuang; Panneer Selvam Santhalingamy; Riccardo Spolaor; Parth Pathaky; Guoming Zhang; Xiuzhen Cheng', display:{Lore:['[{"text": "arXiv:2212.01042", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAccEar: Accelerometer Acoustic Eavesdropping with Unconstrained Vocabulary\\u00a7r\\n\\n\\u00a78\\u00a7oPengfei Hu\\nHui Zhuang\\nPanneer Selvam Santhalingamy\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.01042\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/SP46214.2022.9833716\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2022 IEEE Symposium on Security and Privacy (SP)\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Dec 2022 09:13:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2022 IEEE Symposium on Security and Privacy (SP)\\u00a7r"}']}
{title:'Gibbons et al. (§72022§r)', author: 'Anthony Gibbons; Ian Donohue; Courtney E. Gorman; Emma King; Andrew Parnell', display:{Lore:['[{"text": "arXiv:2212.01457", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNEAL: An open-source tool for audio annotation\\u00a7r\\n\\n\\u00a78\\u00a7oAnthony Gibbons\\nIan Donohue\\nCourtney E. Gorman\\nEmma King\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.01457\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 8 Dec 2022 16:53:51 GMT)\\u00a7r"}']}
{title:'Lei et al. (§72022§r)', author: 'Yi Lei; Shan Yang; Xinsheng Wang; Qicong Xie; Jixun Yao; Lei Xie; Dan Su', display:{Lore:['[{"text": "arXiv:2212.01546", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUniSyn: An End-to-End Unified Model for Text-to-Speech and Singing Voice Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYi Lei\\nShan Yang\\nXinsheng Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.01546\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 6 Dec 2022 11:28:30 GMT)\\u00a7r"}']}
{title:'Donahue et al. (§72022§r)', author: 'Chris Donahue; John Thickstun; Percy Liang', display:{Lore:['[{"text": "arXiv:2212.01884", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMelody transcription via generative pre-training\\u00a7r\\n\\n\\u00a78\\u00a7oChris Donahue\\nJohn Thickstun\\nPercy Liang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.01884\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 4 Dec 2022 18:09:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished as a conference paper at ISMIR 2022\\u00a7r"}']}
{title:'Akrami et al. (§72022§r)', author: 'Haleh Akrami; Hannes Gamper', display:{Lore:['[{"text": "arXiv:2212.01911", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech MOS multi-task learning and rater bias correction\\u00a7r\\n\\n\\u00a78\\u00a7oHaleh Akrami\\nHannes Gamper\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.01911\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 4 Dec 2022 20:06:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Quan et al. (§72022§r)', author: 'Changsheng Quan; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2212.02076", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNBC2: Multichannel Speech Separation with Revised Narrow-band Conformer\\u00a7r\\n\\n\\u00a78\\u00a7oChangsheng Quan\\nXiaofei Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.02076\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Dec 2022 07:44:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to TASLP\\u00a7r"}']}
{title:'Zeng et al. (§72022§r)', author: 'Chunyan Zeng; Dongliang Zhu; Zhifeng Wang; Minghu Wu; Wei Xiong; Nan Zhao', display:{Lore:['[{"text": "arXiv:2212.02084", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end Recording Device Identification Based on Deep Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oChunyan Zeng\\nDongliang Zhu\\nZhifeng Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.02084\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Dec 2022 07:56:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o20 pages, 5 figures, recording device identification\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Yizhi Li; Ruibin Yuan; Ge Zhang; Yinghao Ma; Chenghua Lin; Xingran Chen; Anton Ragni; Hanzhi Yin; Zhijie Hu; Haoyu He; Emmanouil Benetos; Norbert Gyenge; Ruibo Liu; Jie Fu', display:{Lore:['[{"text": "arXiv:2212.02508", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMAP-Music2Vec: A Simple and Effective Baseline for Self-Supervised Music Audio Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oYizhi Li\\nRuibin Yuan\\nGe Zhang\\n+ 10 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.02508\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Dec 2022 16:04:26 GMT)\\u00a7r"}']}
{title:'Jonason et al. (§72022§r)', author: 'Nicolas Jonason; Bob L. T. Sturm', display:{Lore:['[{"text": "arXiv:2212.02610", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Latent Space Cartography\\u00a7r\\n\\n\\u00a78\\u00a7oNicolas Jonason\\nBob L. T. Sturm\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.02610\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 7 Dec 2022 09:46:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oLate Breaking / Demo, ISMIR 2022 (https://ismir2022program.ismir.net/lbd_413.html)\\u00a7r"}']}
{title:'Peng et al. (§72022§r)', author: 'Zhiyuan Peng; Mingjie Shao; Xuanji He; Xu Li; Tan Lee; Ke Ding; Guanglu Wan', display:{Lore:['[{"text": "arXiv:2212.03039", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCovariance Regularization for Probabilistic Linear Discriminant Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oZhiyuan Peng\\nMingjie Shao\\nXuanji He\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.03039\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Dec 2022 15:12:58 GMT)\\u00a7r"}']}
{title:'Peng et al. (§72022§r)', author: 'Zhiyuan Peng; Xuanji He; Ke Ding; Tan Lee; Guanglu Wan', display:{Lore:['[{"text": "arXiv:2212.03090", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLabel-free Knowledge Distillation with Contrastive Loss for Light-weight Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhiyuan Peng\\nXuanji He\\nKe Ding\\nTan Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.03090\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Dec 2022 16:01:59 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Fengyu Yang; Jian Luan; Yujun Wang', display:{Lore:['[{"text": "arXiv:2212.03435", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImprove Bilingual TTS Using Dynamic Language and Phonology Embedding\\u00a7r\\n\\n\\u00a78\\u00a7oFengyu Yang\\nJian Luan\\nYujun Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.03435\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Dec 2022 03:46:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP2023\\u00a7r"}']}
{title:'Yao et al. (§72022§r)', author: 'Shengshi Yao; Zixuan Xiao; Sixian Wang; Jincheng Dai; Kai Niu; Ping Zhang', display:{Lore:['[{"text": "arXiv:2212.05294", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IT\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a72math.IT\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVariational Speech Waveform Compression to Catalyze Semantic Communications\\u00a7r\\n\\n\\u00a78\\u00a7oShengshi Yao\\nZixuan Xiao\\nSixian Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.05294\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 13 Dec 2022 13:20:57 GMT)\\u00a7r"}']}
{title:'Hebbar et al. (§72022§r)', author: 'Devayani Hebbar; Vandana Jagtap', display:{Lore:['[{"text": "arXiv:2212.05335", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparison of Audio Preprocessing Techniques and Deep Learning Algorithms for Raga Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDevayani Hebbar\\nVandana Jagtap\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.05335\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 10 Dec 2022 16:51:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 6 figures, 7 tables\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Hyeongju Kim; Hyeong-Seok Choi', display:{Lore:['[{"text": "arXiv:2212.06387", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards trustworthy phoneme boundary detection with autoregressive model and improved evaluation metric\\u00a7r\\n\\n\\u00a78\\u00a7oHyeongju Kim\\nHyeong-Seok Choi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.06387\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Dec 2022 05:56:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, submitted to ICASSP 2023\\u00a7r"}']}
{title:'Qiang et al. (§72022§r)', author: 'Chunyu Qiang; Peng Yang; Hao Che; Xiaorui Wang; Zhongyuan Wang', display:{Lore:['[{"text": "arXiv:2212.06397", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStyle-Label-Free: Cross-Speaker Style Transfer by Quantized VAE and Speaker-wise Normalization in Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oChunyu Qiang\\nPeng Yang\\nHao Che\\nXiaorui Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.06397\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Dec 2022 06:26:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished to ISCSLP 2022\\u00a7r"}']}
{title:'Xu et al. (§72022§r)', author: 'Yinhao Xu; Jian Zhou; Liang Tao; Hon Keung Kwan', display:{Lore:['[{"text": "arXiv:2212.07163", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Scale Feature Fusion Transformer Network for End-to-End Single Channel Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oYinhao Xu\\nJian Zhou\\nLiang Tao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.07163\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Dec 2022 11:32:28 GMT)\\u00a7r"}']}
{title:'Gu et al. (§72022§r)', author: 'Rongzhi Gu; Shi-Xiong Zhang; Yuexian Zou; Dong Yu', display:{Lore:['[{"text": "arXiv:2212.08348", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Unified All-Neural Beamforming for Time and Frequency Domain Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oRongzhi Gu\\nShi-Xiong Zhang\\nYuexian Zou\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.08348\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 24 Dec 2022 03:05:18 GMT)\\u00a7r"}']}
{title:'Zhu et al. (§72022§r)', author: 'Tinglong Zhu; Xingming Wang; Xiaoyi Qin; Ming Li', display:{Lore:['[{"text": "arXiv:2212.08601", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSource Tracing: Detecting Voice Spoofing\\u00a7r\\n\\n\\u00a78\\u00a7oTinglong Zhu\\nXingming Wang\\nXiaoyi Qin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.08601\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 16 Dec 2022 17:29:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by APSIPA ASC\\u00a7r"}']}
{title:'Liang et al. (§72022§r)', author: 'Jinhua Liang; Huy Phan; Emmanouil Benetos', display:{Lore:['[{"text": "arXiv:2212.08952", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning from Taxonomy: Multi-label Few-Shot Classification for Everyday Sound Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJinhua Liang\\nHuy Phan\\nEmmanouil Benetos\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.08952\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 17 Dec 2022 20:56:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP2023\\u00a7r"}']}
{title:'Feng et al. (§72022§r)', author: 'Tiantian Feng; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2212.09090", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Workplace Behaviors through Speaking Patterns using Large-scale Multimodal Wearable Recordings: A Study of Healthcare Providers\\u00a7r\\n\\n\\u00a78\\u00a7oTiantian Feng\\nShrikanth Narayanan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.09090\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 18 Dec 2022 14:01:35 GMT)\\u00a7r"}']}
{title:'Tang et al. (§72022§r)', author: 'Changli Tang; Yujin Wang; Xie Chen; Wei-Qiang Zhang', display:{Lore:['[{"text": "arXiv:2212.10092", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Effective Fusion Algorithms for Speech Based Self-Supervised Learning Models\\u00a7r\\n\\n\\u00a78\\u00a7oChangli Tang\\nYujin Wang\\nXie Chen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.10092\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Dec 2022 09:09:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by NCMMSC2022\\u00a7r"}']}
{title:'Illium et al. (§72022§r)', author: 'Steffen Illium; Robert Müller; Andreas Sedlmeier; Claudia-Linnhoff Popien', display:{Lore:['[{"text": "arXiv:2212.10093", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisual Transformers for Primates Classification and Covid Detection\\u00a7r\\n\\n\\u00a78\\u00a7oSteffen Illium\\nRobert M\\u00fcller\\nAndreas Sedlmeier\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.10093\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-273\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Dec 2022 09:10:25 GMT)\\u00a7r"}']}
{title:'Cai et al. (§72022§r)', author: 'Hanbo Cai; Pengcheng Zhang; Hai Dong; Yan Xiao; Shunhui Ji', display:{Lore:['[{"text": "arXiv:2212.10103", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVSVC: Backdoor attack against Keyword Spotting based on Voiceprint Selection and Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oHanbo Cai\\nPengcheng Zhang\\nHai Dong\\nYan Xiao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.10103\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Dec 2022 09:24:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages,5 figures\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Tao Wang; Jiangyan Yi; Ruibo Fu; Jianhua Tao; Zhengqi Wen; Chu Yuan Zhang', display:{Lore:['[{"text": "arXiv:2212.10191", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotion Selectable End-to-End Text-based Speech Editing\\u00a7r\\n\\n\\u00a78\\u00a7oTao Wang\\nJiangyan Yi\\nRuibo Fu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.10191\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Dec 2022 12:02:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review, 12 pages, 11 figures, demo page is available at https://hairuo55.github.io/Emo-CampNet/\\u00a7r"}']}
{title:'Marmoret et al. (§72022§r)', author: 'Axel Marmoret; Jérémy E. Cohen; Frédéric Bimbot', display:{Lore:['[{"text": "arXiv:2212.11054", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPolytopic Analysis of Music\\u00a7r\\n\\n\\u00a78\\u00a7oAxel Marmoret\\nJ\\u00e9r\\u00e9my E. Cohen\\nFr\\u00e9d\\u00e9ric Bimbot\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.11054\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 22 Dec 2022 13:19:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWork document\\u00a7r"}']}
{title:'Neves et al. (§72022§r)', author: 'Pedro Neves; Jose Fornari; João Florindo', display:{Lore:['[{"text": "arXiv:2212.11134", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerating music with sentiment using Transformer-GANs\\u00a7r\\n\\n\\u00a78\\u00a7oPedro Neves\\nJose Fornari\\nJo\\u00e3o Florindo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.11134\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Dec 2022 15:59:35 GMT)\\u00a7r"}']}
{title:'Akesbi (§72022§r)', author: 'Kamil Akesbi', display:{Lore:['[{"text": "arXiv:2212.11277", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Denoising for Robust Audio Fingerprinting\\u00a7r\\n\\n\\u00a78\\u00a7oKamil Akesbi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.11277\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Dec 2022 09:46:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o63 pages, master thesis\\u00a7r"}']}
{title:'Mahdad et al. (§72022§r)', author: 'Ahmed Tanvir Mahdad; Cong Shi; Zhengkun Ye; Tianming Zhao; Yan Wang; Yingying Chen; Nitesh Saxena', display:{Lore:['[{"text": "arXiv:2212.12151", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEarSpy: Spying Caller Speech and Identity through Tiny Vibrations of Smartphone Ear Speakers\\u00a7r\\n\\n\\u00a78\\u00a7oAhmed Tanvir Mahdad\\nCong Shi\\nZhengkun Ye\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.12151\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Dec 2022 05:05:09 GMT)\\u00a7r"}']}
{title:'Cai et al. (§72022§r)', author: 'Le Cai; Sam Ferguson; Haiyan Lu; Gengfa Fang', display:{Lore:['[{"text": "arXiv:2212.13369", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFeature Selection Approaches for Optimising Music Emotion Recognition Methods\\u00a7r\\n\\n\\u00a78\\u00a7oLe Cai\\nSam Ferguson\\nHaiyan Lu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.13369\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5121/csit.2022.122302\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Dec 2022 05:55:34 GMT)\\u00a7r"}']}
{title:'Slizovskaia et al. (§72022§r)', author: 'Olga Slizovskaia; Jordi Janer; Pritish Chandna; Oscar Mayor', display:{Lore:['[{"text": "arXiv:2212.13581", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice conversion with limited data and limitless data augmentations\\u00a7r\\n\\n\\u00a78\\u00a7oOlga Slizovskaia\\nJordi Janer\\nPritish Chandna\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.13581\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Dec 2022 18:37:21 GMT)\\u00a7r"}']}
{title:'Diep et al. (§72022§r)', author: 'Brian Diep; Marija Stanojevic; Jekaterina Novikova', display:{Lore:['[{"text": "arXiv:2212.14490", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-modal deep learning system for depression and anxiety detection\\u00a7r\\n\\n\\u00a78\\u00a7oBrian Diep\\nMarija Stanojevic\\nJekaterina Novikova\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.14490\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Dec 2022 00:02:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to the PAI4MH workshop at NeurIPS 2022\\u00a7r"}']}
