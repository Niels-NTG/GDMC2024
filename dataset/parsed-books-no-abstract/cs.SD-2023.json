{title:'Elizalde et al. (§72023§r)', author: 'Benjamin Elizalde; Rohan Badlani; Ankit Shah; Anurag Kumar; Bhiksha Raj', display:{Lore:['[{"text": "arXiv:1801.05544", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNELS \\u2013 Never-Ending Learner of Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oBenjamin Elizalde\\nRohan Badlani\\nAnkit Shah\\nAnurag Kumar\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1801.05544\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 29 Mar 2023 19:52:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Machine Learning for Audio Signal Processing(ML4Audio), 31st Conference on Neural Information ProcessingSystems (NIPS 2017), Long Beach, CA, USA\\u00a7r"}']}
{title:'Becker et al. (§72023§r)', author: 'Sören Becker; Johanna Vielhaben; Marcel Ackermann; Klaus-Robert Müller; Sebastian Lapuschkin; Wojciech Samek', display:{Lore:['[{"text": "arXiv:1807.03418", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudioMNIST: Exploring Explainable Artificial Intelligence for Audio Analysis on a Simple Benchmark\\u00a7r\\n\\n\\u00a78\\u00a7oS\\u00f6ren Becker\\nJohanna Vielhaben\\nMarcel Ackermann\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1807.03418\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 27 Nov 2023 18:26:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 5 figures, 1 table\\u00a7r"}']}
{title:'Huang et al. (§72023§r)', author: 'Sicong Huang; Qiyang Li; Cem Anil; Xuchan Bao; Sageev Oore; Roger B. Grosse', display:{Lore:['[{"text": "arXiv:1811.09620", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oSicong Huang\\nQiyang Li\\nCem Anil\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.09620\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICLR 2019\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 22 Oct 2023 04:43:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o17 pages, published as a conference paper at ICLR 2019\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Jaeyoung Kim; Mostafa El-Khamy; Jungwon Lee', display:{Lore:['[{"text": "arXiv:1901.09146", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Multi-Task Denoising for joint SDR and PESQ Optimization\\u00a7r\\n\\n\\u00a78\\u00a7oJaeyoung Kim\\nMostafa El-Khamy\\nJungwon Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.09146\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 8 Mar 2023 23:46:09 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Liwen Zhang; Ziqiang Shi; Jiqing Han; Anyan Shi; Ding Ma', display:{Lore:['[{"text": "arXiv:1902.04891", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFurcaNeXt: End-to-end monaural speech separation with dynamic gated dilated temporal convolutional networks\\u00a7r\\n\\n\\u00a78\\u00a7oLiwen Zhang\\nZiqiang Shi\\nJiqing Han\\nAnyan Shi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.04891\\u00a7r\\n\\nVersion:\\u00a77v6 (Mon, 26 Jun 2023 06:00:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oArxiv only allows figures with a small resolution. If you need to see large-resolution figures, please contactus. arXiv admin note: substantial text overlap with arXiv:1902.00651\\u00a7r"}']}
{title:'Parmer et al. (§72023§r)', author: 'Thomas Parmer; Yong-Yeol Ahn', display:{Lore:['[{"text": "arXiv:1907.04292", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvolution of the Informational Complexity of Contemporary Western Music\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Parmer\\nYong-Yeol Ahn\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.04292\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 17 Apr 2023 23:50:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 6 figures; addedsupplementary materials\\u00a7r"}']}
{title:'Aghakhani et al. (§72023§r)', author: 'Hojjat Aghakhani; Lea Schönherr; Thorsten Eisenhofer; Dorothea Kolossa; Thorsten Holz; Christopher Kruegel; Giovanni Vigna', display:{Lore:['[{"text": "arXiv:2010.10682", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVenoMave: Targeted Poisoning Against Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHojjat Aghakhani\\nLea Sch\\u00f6nherr\\nThorsten Eisenhofer\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.10682\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 20 Apr 2023 21:21:04 GMT)\\u00a7r"}']}
{title:'Lu et al. (§72023§r)', author: 'Yen-Ju Lu; Chia-Yu Chang; Cheng Yu; Ching-Feng Liu; Jeih-weih Hung; Shinji Watanabe; Yu Tsao', display:{Lore:['[{"text": "arXiv:2011.07442", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Speech Enhancement Performance by Leveraging Contextual Broad Phonetic Class Information\\u00a7r\\n\\n\\u00a78\\u00a7oYen-Ju Lu\\nChia-Yu Chang\\nCheng Yu\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.07442\\u00a7r\\n\\nVersion:\\u00a77v5 (Sun, 18 Jun 2023 11:52:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in IEEE Transactions on Audio, Speech and Language Processing (TASLP)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Qing Wang; Jun Du; Hua-Xin Wu; Jia Pan; Feng Ma; Chin-Hui Lee', display:{Lore:['[{"text": "arXiv:2101.02919", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Four-Stage Data Augmentation Approach to ResNet-Conformer Based Acoustic Modeling for Sound Event Localization and Detection\\u00a7r\\n\\n\\u00a78\\u00a7oQing Wang\\nJun Du\\nHua-Xin Wu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.02919\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 7 Mar 2023 07:00:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 8 figures, Accepted by Transactions on Audio, Speech and Language Processing\\u00a7r"}']}
{title:'Du et al. (§72023§r)', author: 'Chenpeng Du; Kai Yu', display:{Lore:['[{"text": "arXiv:2102.00851", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRich Prosody Diversity Modelling with Phone-level Mixture Density Network\\u00a7r\\n\\n\\u00a78\\u00a7oChenpeng Du\\nKai Yu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.00851\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-802\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 1 Oct 2023 11:32:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021\\u00a7r"}']}
{title:'Cances et al. (§72023§r)', author: 'Léo Cances; Etienne Labbé; Thomas Pellegrini', display:{Lore:['[{"text": "arXiv:2102.08183", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparison of semi-supervised deep learning algorithms for audio classification\\u00a7r\\n\\n\\u00a78\\u00a7oL\\u00e9o Cances\\nEtienne Labb\\u00e9\\nThomas Pellegrini\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.08183\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nEURASIP Journal on Audio, Speech, and Music Processing, 2022\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 8 Mar 2023 13:54:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 5 figures, 5 tables. This is the version 3 of the paper. Contains minor fixes compared to the EURASIP one (which is the version 2 of the paper)\\u00a7r"}']}
{title:'Yi et al. (§72023§r)', author: 'Jiangyan Yi; Ye Bai; Jianhua Tao; Haoxin Ma; Zhengkun Tian; Chenglong Wang; Tao Wang; Ruibo Fu', display:{Lore:['[{"text": "arXiv:2104.03617", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHalf-Truth: A Partially Fake Audio Detection Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oJiangyan Yi\\nYe Bai\\nJianhua Tao\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.03617\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 16 Dec 2023 02:17:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by Interspeech 2021\\u00a7r"}']}
{title:'Jaiswal et al. (§72023§r)', author: 'Mimansa Jaiswal; Emily Mower Provost', display:{Lore:['[{"text": "arXiv:2104.08806", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBest Practices for Noise-Based Augmentation to Improve the Performance of Deployable Speech-Based Emotion Recognition Systems\\u00a7r\\n\\n\\u00a78\\u00a7oMimansa Jaiswal\\nEmily Mower Provost\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.08806\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 31 Aug 2023 18:26:56 GMT)\\u00a7r"}']}
{title:'Lluís et al. (§72023§r)', author: 'Francesc Lluís; Vasileios Chatziioannou; Alex Hofmann', display:{Lore:['[{"text": "arXiv:2104.12462", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPoints2Sound: From mono to binaural audio using 3D point cloud scenes\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesc Llu\\u00eds\\nVasileios Chatziioannou\\nAlex Hofmann\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.12462\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1186/s13636-022-00265-4\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nEURASIP Journal on Audio, Speech, and Music Processing 2022 (1),\\n  1-15\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 19 May 2023 12:54:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCode, data, and listening examples: https://github.com/francesclluis/points2sound\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Byeonggeun Kim; Simyung Chang; Jinkyu Lee; Dooyong Sung', display:{Lore:['[{"text": "arXiv:2106.04140", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBroadcasted Residual Learning for Efficient Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oByeonggeun Kim\\nSimyung Chang\\nJinkyu Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.04140\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 5 Jul 2023 15:18:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH 2021\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Changan Chen; Wei Sun; David Harwath; Kristen Grauman', display:{Lore:['[{"text": "arXiv:2106.07732", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Audio-Visual Dereverberation\\u00a7r\\n\\n\\u00a78\\u00a7oChangan Chen\\nWei Sun\\nDavid Harwath\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.07732\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 13 Mar 2023 21:34:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2023. This is the longer version of the five-page camera-ready paper. Project page: https://vision.cs.utexas.edu/projects/learning-audio-visual-dereverberation\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Weidong Chen; Xiaofeng Xing; Xiangmin Xu; Jichen Yang; Jianxin Pang', display:{Lore:['[{"text": "arXiv:2106.11532", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKey-Sparse Transformer for Multimodal Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oWeidong Chen\\nXiaofeng Xing\\nXiangmin Xu\\nJichen Yang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.11532\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746598\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 27 Feb 2023 13:10:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper was accepted by ICASSP 2022\\u00a7r"}']}
{title:'Tang et al. (§72023§r)', author: 'Haoyu Tang; Yunxiao Wang; Jihua Zhu; Shuaike Zhang; Mingzhu Xu; Qinghai Zheng; Yupeng Hu', display:{Lore:['[{"text": "arXiv:2106.14136", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lListen As You Wish: Audio based Event Detection via Text-to-Audio Grounding in Smart Cities\\u00a7r\\n\\n\\u00a78\\u00a7oHaoyu Tang\\nYunxiao Wang\\nJihua Zhu\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.14136\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 23 Dec 2023 15:06:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages\\u00a7r"}']}
{title:'Hsu et al. (§72023§r)', author: 'Fu-Shun Hsu; Shang-Ran Huang; Chang-Fu Su; Chien-Wen Huang; Yuan-Ren Cheng; Chun-Chieh Chen; Chun-Yu Wu; Chung-Wei Chen; Yen-Chun Lai; Tang-Wei Cheng; Nian-Jhen Lin; Wan-Ling Tsai; Ching-Shiang Lu; Chuan Chen; Feipei Lai', display:{Lore:['[{"text": "arXiv:2107.04229", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Dual-Purpose Deep Learning Model for Auscultated Lung and Tracheal Sound Analysis Based on Mixed Set Training\\u00a7r\\n\\n\\u00a78\\u00a7oFu-Shun Hsu\\nShang-Ran Huang\\nChang-Fu Su\\n+ 11 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.04229\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.bspc.2023.105222\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nBiomed. Signal Process. Control 86 (2023) 105222\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 4 Jan 2023 06:31:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be submitted, 37 pages, 6 figures, 5 tables, 1 summplementary table\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Wenxuan Liu; Tianyao He; Chen Gong; Ning Zhang; Hua Yang; Junchi Yan', display:{Lore:['[{"text": "arXiv:2107.09889", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFine-Grained Music Plagiarism Detection: Revealing Plagiarists through Bipartite Graph Matching and a Comprehensive Large-Scale Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oWenxuan Liu\\nTianyao He\\nChen Gong\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.09889\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 2 Jul 2023 08:28:07 GMT)\\u00a7r"}']}
{title:'Borrel-Jensen et al. (§72023§r)', author: 'Nikolas Borrel-Jensen; Allan P. Engsig-Karup; Cheol-Ho Jeong', display:{Lore:['[{"text": "arXiv:2109.11313", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.comp-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhysics-informed neural networks for one-dimensional sound field predictions with parameterized sources and impedance boundaries\\u00a7r\\n\\n\\u00a78\\u00a7oNikolas Borrel-Jensen\\nAllan P. Engsig-Karup\\nCheol-Ho Jeong\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.11313\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0009057\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJasa Express Letters 2021, Volume 1, Issue 12, pp. 122402\\u00a7r\\n\\nVersion:\\u00a77v5 (Thu, 10 Aug 2023 11:51:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 5 figures, 3 tables\\u00a7r"}']}
{title:'Ghosh et al. (§72023§r)', author: 'Sreyan Ghosh; Sandesh V Katta; Ashish Seth; S. Umesh', display:{Lore:['[{"text": "arXiv:2110.08895", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDECAR: Deep Clustering for learning general-purpose Audio Representations\\u00a7r\\n\\n\\u00a78\\u00a7oSreyan Ghosh\\nSandesh V Katta\\nAshish Seth\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.08895\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JSTSP.2022.3202093\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 14 Mar 2023 14:29:58 GMT)\\u00a7r"}']}
{title:'Strong et al. (§72023§r)', author: 'Marek Strong; Jonas Rohnke; Antonio Bonafonte; Mateusz Łajszczak; Trevor Wood', display:{Lore:['[{"text": "arXiv:2110.12539", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiscrete Acoustic Space for an Efficient Sampling in Neural Text-To-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oMarek Strong\\nJonas Rohnke\\nAntonio Bonafonte\\nMateusz \\u0141ajszczak\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.12539\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/IberSPEECH.2022-1\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 14 Sep 2023 12:34:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, accepted at IberSPEECH 2022\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Haozhe Zhang; Zexin Cai; Xiaoyi Qin; Ming Li', display:{Lore:['[{"text": "arXiv:2111.03811", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSIG-VC: A Speaker Information Guided Zero-shot Voice Conversion System for Both Human Beings and Machines\\u00a7r\\n\\n\\u00a78\\u00a7oHaozhe Zhang\\nZexin Cai\\nXiaoyi Qin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.03811\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 2 Apr 2023 04:21:40 GMT)\\u00a7r"}']}
{title:'Younes et al. (§72023§r)', author: 'Abdelrahman Younes; Daniel Honerkamp; Tim Welschehold; Abhinav Valada', display:{Lore:['[{"text": "arXiv:2111.14843", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCatch Me If You Hear Me: Audio-Visual Navigation in Complex Unmapped Environments with Moving Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oAbdelrahman Younes\\nDaniel Honerkamp\\nTim Welschehold\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.14843\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 3 Jan 2023 11:07:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper has been accepted for publication at IEEE ROBOTICS AND AUTOMATION LETTERS\\u00a7r"}']}
{title:'Casanova et al. (§72023§r)', author: 'Edresson Casanova; Julian Weber; Christopher Shulby; Arnaldo Candido Junior; Eren Gölge; Moacir Antonelli Ponti', display:{Lore:['[{"text": "arXiv:2112.02418", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lYourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone\\u00a7r\\n\\n\\u00a78\\u00a7oEdresson Casanova\\nJulian Weber\\nChristopher Shulby\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.02418\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 39th International Conference on Machine\\n  Learning, PMLR 162:2709-2720, 2022\\u00a7r\\n\\nVersion:\\u00a77v4 (Sun, 30 Apr 2023 17:46:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAn Erratum was added on the last page of this paper\\u00a7r"}']}
{title:'Takashima et al. (§72023§r)', author: 'Naoki Takashima; Frédéric Li; Marcin Grzegorzek; Kimiaki Shirahama', display:{Lore:['[{"text": "arXiv:2112.07192", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmbedding-based Music Emotion Recognition Using Composite Loss\\u00a7r\\n\\n\\u00a78\\u00a7oNaoki Takashima\\nFr\\u00e9d\\u00e9ric Li\\nMarcin Grzegorzek\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.07192\\u00a7r\\n\\nVersion:\\u00a77v5 (Sat, 8 Apr 2023 06:26:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o27 pages, 14 figures, This paper has been accepted to IEEE Access\\u00a7r"}']}
{title:'Lin et al. (§72023§r)', author: 'David Chuan-En Lin; Anastasis Germanidis; Cristóbal Valenzuela; Yining Shi; Nikolas Martelaro', display:{Lore:['[{"text": "arXiv:2112.09726", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSoundify: Matching Sound Effects to Video\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Chuan-En Lin\\nAnastasis Germanidis\\nCrist\\u00f3bal Valenzuela\\nYining Shi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.09726\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 13 Oct 2023 08:10:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oFull paper in UIST 2023; Short paper in NeurIPS 2021 ML4CD Workshop; Online demo: http://soundify.cc\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Shangda Wu; Yue Yang; Zhaowen Wang; Xiaobing Li; Maosong Sun', display:{Lore:['[{"text": "arXiv:2112.11122", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerating Chord Progression from Melody with Flexible Harmonic Rhythm and Controllable Harmonic Density\\u00a7r\\n\\n\\u00a78\\u00a7oShangda Wu\\nYue Yang\\nZhaowen Wang\\nXiaobing Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.11122\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 2 Dec 2023 06:21:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 6 figures, 1 table, accepted by EURASIP JASMP\\u00a7r"}']}
{title:'Desblancs (§72023§r)', author: 'Dorian Desblancs', display:{Lore:['[{"text": "arXiv:2201.01771", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Beat Tracking in Musical Signals with Polyphonic Contrastive Learning\\u00a7r\\n\\n\\u00a78\\u00a7oDorian Desblancs\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.01771\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 16 Jul 2023 01:12:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o59 pages, 20 figures, masters thesis, degree granted\\u00a7r"}']}
{title:'Hoang et al. (§72023§r)', author: 'Truong V. Hoang; Quang H. Nguyen; Cuong Q. Nguyen; Phong X. Nguyen; Hoang D. Nguyen', display:{Lore:['[{"text": "arXiv:2201.04581", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound-Dr: Reliable Sound Dataset and Baseline Artificial Intelligence System for Respiratory Illnesses\\u00a7r\\n\\n\\u00a78\\u00a7oTruong V. Hoang\\nQuang H. Nguyen\\nCuong Q. Nguyen\\nPhong X. Nguyen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.04581\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIJPHM (2023)\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 4 Aug 2023 15:28:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, PHMAP2023, PHM\\u00a7r"}']}
{title:'Schulze-Forster et al. (§72023§r)', author: 'Kilian Schulze-Forster; Gaël Richard; Liam Kelley; Clement S. J. Doire; Roland Badeau', display:{Lore:['[{"text": "arXiv:2201.09592", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Music Source Separation Using Differentiable Parametric Source Models\\u00a7r\\n\\n\\u00a78\\u00a7oKilian Schulze-Forster\\nGa\\u00ebl Richard\\nLiam Kelley\\nClement S. J. Doire\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.09592\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 31 Jan 2023 18:12:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oRevised version of the submission\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Shangda Wu; Xiaobing Li; Maosong Sun', display:{Lore:['[{"text": "arXiv:2202.08423", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChord-Conditioned Melody Harmonization with Controllable Harmonicity\\u00a7r\\n\\n\\u00a78\\u00a7oShangda Wu\\nXiaobing Li\\nMaosong Sun\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.08423\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 22 Feb 2023 14:43:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 7 figures, accepted by ICASSP 2023\\u00a7r"}']}
{title:'Ferdaoussi et al. (§72023§r)', author: 'Ahmad El Ferdaoussi; Éric Plourde; Jean Rouat', display:{Lore:['[{"text": "arXiv:2202.09619", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluation of Neuromorphic Spike Encoding of Sound Using Information Theory\\u00a7r\\n\\n\\u00a78\\u00a7oAhmad El Ferdaoussi\\n\\u00c9ric Plourde\\nJean Rouat\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.09619\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 15 Feb 2023 18:06:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 7 figures, internal report\\u00a7r"}']}
{title:'Sha et al. (§72023§r)', author: 'Yu Sha; Johannes Faber; Shuiping Gou; Bo Liu; Wei Li; Stefan Schramm; Horst Stoecker; Thomas Steckenreiter; Domagoj Vnucec; Nadine Wetzstein; Andreas Widl; Kai Zhou', display:{Lore:['[{"text": "arXiv:2203.01429", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSMTNet: Hierarchical cavitation intensity recognition based on sub-main transfer network\\u00a7r\\n\\n\\u00a78\\u00a7oYu Sha\\nJohannes Faber\\nShuiping Gou\\n+ 8 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.01429\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 12 Jul 2023 12:58:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7owe need update this paper\\u00a7r"}']}
{title:'Pelecanos et al. (§72023§r)', author: 'Jason Pelecanos; Quan Wang; Yiling Huang; Ignacio Lopez Moreno', display:{Lore:['[{"text": "arXiv:2203.05642", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParameter-Free Attentive Scoring for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oJason Pelecanos\\nQuan Wang\\nYiling Huang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.05642\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 6 Mar 2023 17:57:00 GMT)\\u00a7r"}']}
{title:'Yuan et al. (§72023§r)', author: 'Xin Yuan; Yongbing Feng; Mingming Ye; Cheng Tuo; Minghang Zhang', display:{Lore:['[{"text": "arXiv:2203.09825", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaVocoder: Adaptive Vocoder for Custom Voice\\u00a7r\\n\\n\\u00a78\\u00a7oXin Yuan\\nYongbing Feng\\nMingming Ye\\nCheng Tuo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.09825\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-288\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 5 Jan 2023 08:58:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2022\\u00a7r"}']}
{title:'Nguyen et al. (§72023§r)', author: 'Bac Nguyen; Fabien Cardinaux; Stefan Uhlich', display:{Lore:['[{"text": "arXiv:2203.11049", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutoTTS: End-to-End Text-to-Speech Synthesis through Differentiable Duration Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oBac Nguyen\\nFabien Cardinaux\\nStefan Uhlich\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.11049\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 7 Mar 2023 07:44:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Andreev et al. (§72023§r)', author: 'Pavel Andreev; Aibek Alanov; Oleg Ivanov; Dmitry Vetrov', display:{Lore:['[{"text": "arXiv:2203.13086", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHiFi++: a Unified Framework for Bandwidth Extension and Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oPavel Andreev\\nAibek Alanov\\nOleg Ivanov\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.13086\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10097255\\u00a7r\\n\\nVersion:\\u00a77v4 (Sun, 10 Dec 2023 13:52:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Vaessen et al. (§72023§r)', author: 'Nik Vaessen; David A. van Leeuwen', display:{Lore:['[{"text": "arXiv:2203.14688", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTraining speaker recognition systems with limited data\\u00a7r\\n\\n\\u00a78\\u00a7oNik Vaessen\\nDavid A. van Leeuwen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14688\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-135\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2022, 4760-4764\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Feb 2023 06:31:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to Interspeech 2022\\u00a7r"}']}
{title:'Omran et al. (§72023§r)', author: 'Ahmed Omran; Neil Zeghidour; Zalán Borsos; Félix de Chaumont Quitry; Malcolm Slaney; Marco Tagliasacchi', display:{Lore:['[{"text": "arXiv:2203.15578", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDisentangling speech from surroundings with neural embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oAhmed Omran\\nNeil Zeghidour\\nZal\\u00e1n Borsos\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15578\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 4 Jun 2023 18:08:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2023\\u00a7r"}']}
{title:'Cheng et al. (§72023§r)', author: 'Chin-Yi Cheng; Hung-Shin Lee; Yu Tsao; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:2203.16007", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-target Extractor and Detector for Unknown-number Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oChin-Yi Cheng\\nHung-Shin Lee\\nYu Tsao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16007\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2023.3279781\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Signal Processing Letters, vol. 30, pp. 638-642, 2023\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 22 May 2023 14:07:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE Signal Processing Letters\\u00a7r"}']}
{title:'Luo et al. (§72023§r)', author: 'Andrew Luo; Yilun Du; Michael J. Tarr; Joshua B. Tenenbaum; Antonio Torralba; Chuang Gan', display:{Lore:['[{"text": "arXiv:2204.00628", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Neural Acoustic Fields\\u00a7r\\n\\n\\u00a78\\u00a7oAndrew Luo\\nYilun Du\\nMichael J. Tarr\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00628\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 15 Jan 2023 02:41:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oNeurIPS 2022. Project page: https://www.andrew.cmu.edu/user/afluo/Neural_Acoustic_Fields/\\u00a7r"}']}
{title:'Yoon et al. (§72023§r)', author: 'Hyungchan Yoon; Seyun Um; Changwhan Kim; Hong-Goo Kang', display:{Lore:['[{"text": "arXiv:2204.02172", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Learning of Intermediate Acoustic Feature for End-to-End Lightweight Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oHyungchan Yoon\\nSeyun Um\\nChangwhan Kim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02172\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1571\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 28 Aug 2023 13:45:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023\\u00a7r"}']}
{title:'Adolfi et al. (§72023§r)', author: 'Federico Adolfi; Jeffrey S. Bowers; David Poeppel', display:{Lore:['[{"text": "arXiv:2204.03740", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSuccesses and critical failures of neural networks in capturing human-like speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oFederico Adolfi\\nJeffrey S. Bowers\\nDavid Poeppel\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03740\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.neunet.2023.02.032\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nNeural Networks, 162, 199-211 (2023)\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 19 Apr 2023 12:12:17 GMT)\\u00a7r"}']}
{title:'Tonami et al. (§72023§r)', author: 'Noriyuki Tonami; Keisuke Imoto', display:{Lore:['[{"text": "arXiv:2204.06402", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Event Triage: Detecting Sound Events Considering Priority of Classes\\u00a7r\\n\\n\\u00a78\\u00a7oNoriyuki Tonami\\nKeisuke Imoto\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.06402\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 11 Jan 2023 14:20:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to EURASIP Journal on Audio, Speech, and Music Processing\\u00a7r"}']}
{title:'Arasteh et al. (§72023§r)', author: 'Soroosh Tayebi Arasteh; Tobias Weise; Maria Schuster; Elmar Noeth; Andreas Maier; Seung Hee Yang', display:{Lore:['[{"text": "arXiv:2204.06450", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe effect of speech pathology on automatic speaker verification \\u2013 a large-scale study\\u00a7r\\n\\n\\u00a78\\u00a7oSoroosh Tayebi Arasteh\\nTobias Weise\\nMaria Schuster\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.06450\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1038/s41598-023-47711-7\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nSci Rep 13, 20476 (2023)\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 22 Nov 2023 14:10:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in Scientific Reports\\u00a7r"}']}
{title:'Sadok et al. (§72023§r)', author: 'Samir Sadok; Simon Leglaive; Laurent Girin; Xavier Alameda-Pineda; Renaud Séguier', display:{Lore:['[{"text": "arXiv:2204.07075", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning and controlling the source-filter representation of speech with a variational autoencoder\\u00a7r\\n\\n\\u00a78\\u00a7oSamir Sadok\\nSimon Leglaive\\nLaurent Girin\\nXavier Alameda-Pineda\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.07075\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.specom.2023.02.005\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nSpeech Communication, vol. 148, 2023\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 21 Mar 2023 10:41:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o23 pages, 7 figures, companion website: https://samsad35.github.io/site-sfvae/\\u00a7r"}']}
{title:'Truong et al. (§72023§r)', author: 'Tuan Truong; Matthias Lenga; Antoine Serrurier; Sadegh Mohammadi', display:{Lore:['[{"text": "arXiv:2204.10581", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFused Audio Instance and Representation for Respiratory Disease Detection\\u00a7r\\n\\n\\u00a78\\u00a7oTuan Truong\\nMatthias Lenga\\nAntoine Serrurier\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.10581\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 23 Nov 2023 09:15:48 GMT)\\u00a7r"}']}
{title:'Rehman et al. (§72023§r)', author: 'Abdul Rehman; Zhen-Tao Liu; Min Wu; Wei-Hua Cao; Cheng-Shan Jiang', display:{Lore:['[{"text": "arXiv:2204.11382", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-time Speech Emotion Recognition Based on Syllable-Level Feature Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oAbdul Rehman\\nZhen-Tao Liu\\nMin Wu\\nWei-Hua Cao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.11382\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 22 Feb 2023 05:50:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSignificant revisions\\u00a7r"}']}
{title:'Xu et al. (§72023§r)', author: 'Xuenan Xu; Zeyu Xie; Mengyue Wu; Kai Yu', display:{Lore:['[{"text": "arXiv:2205.05357", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBeyond the Status Quo: A Contemporary Survey of Advances and Challenges in Audio Captioning\\u00a7r\\n\\n\\u00a78\\u00a7oXuenan Xu\\nZeyu Xie\\nMengyue Wu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.05357\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 16 Nov 2023 00:04:01 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Kexin Li; Mandar Chitre', display:{Lore:['[{"text": "arXiv:2205.06066", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData-aided Underwater Acoustic Ray Propagation Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oKexin Li\\nMandar Chitre\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.06066\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JOE.2023.3292417\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 24 Aug 2023 03:22:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted version in IEEE Journalof Oceanic Engineering\\u00a7r"}']}
{title:'Shahmansoori et al. (§72023§r)', author: 'Arash Shahmansoori; Utz Roedig', display:{Lore:['[{"text": "arXiv:2205.08459", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDynamic Recognition of Speakers for Consent Management by Contrastive Embedding Replay\\u00a7r\\n\\n\\u00a78\\u00a7oArash Shahmansoori\\nUtz Roedig\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.08459\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 2 Feb 2023 13:11:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. The current version includes 36 pages, 8 figures, "}','{"text": "and 3 tables\\u00a7r"}']}
{title:'Fakotakis et al. (§72023§r)', author: 'Nikos D. Fakotakis; Stavros Nousias; Gerasimos Arvanitis; Evangelia I. Zacharaki; Konstantinos Moustakas', display:{Lore:['[{"text": "arXiv:2205.15360", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.CY\\u00a7r, \\u00a7acs.GL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAI-enabled Sound Pattern Recognition on Asthma Medication Adherence: Evaluation with the RDA Benchmark Suite\\u00a7r\\n\\n\\u00a78\\u00a7oNikos D. Fakotakis\\nStavros Nousias\\nGerasimos Arvanitis\\nEvangelia I. Zacharaki\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.15360\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ACCESS.2023.3243547\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 16 Apr 2023 17:32:06 GMT)\\u00a7r"}']}
{title:'Campbell et al. (§72023§r)', author: 'Edward L. Campbell; Judith Dineley; Pauline Conde; Faith Matcham; Femke Lamers; Sara Siddi; Laura Docio-Fernandez; Carmen Garcia-Mateo; Nicholas Cummins; the RADAR-CNS Consortium', display:{Lore:['[{"text": "arXiv:2206.01542", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetecting the Severity of Major Depressive Disorder from Speech: A Novel HARD-Training Methodology\\u00a7r\\n\\n\\u00a78\\u00a7oEdward L. Campbell\\nJudith Dineley\\nPauline Conde\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.01542\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 May 2023 17:24:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oError in Training Code\\u00a7r"}']}
{title:'Passos et al. (§72023§r)', author: 'Leandro A. Passos; João Paulo Papa; Amir Hussain; Ahsan Adeel', display:{Lore:['[{"text": "arXiv:2206.02671", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCanonical Cortical Graph Neural Networks and its Application for Speech Enhancement in Audio-Visual Hearing Aids\\u00a7r\\n\\n\\u00a78\\u00a7oLeandro A. Passos\\nJo\\u00e3o Paulo Papa\\nAmir Hussain\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.02671\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.neucom.2022.11.081\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 31 Jan 2023 14:14:49 GMT)\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Sang-gil Lee; Wei Ping; Boris Ginsburg; Bryan Catanzaro; Sungroh Yoon', display:{Lore:['[{"text": "arXiv:2206.04658", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBigVGAN: A Universal Neural Vocoder with Large-Scale Training\\u00a7r\\n\\n\\u00a78\\u00a7oSang-gil Lee\\nWei Ping\\nBoris Ginsburg\\nBryan Catanzaro\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.04658\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 16 Feb 2023 18:48:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at ICLR 2023. Listen to audio samples from BigVGAN at: https://bigvgan-demo.github.io/\\u00a7r"}']}
{title:'Irino et al. (§72023§r)', author: 'Toshio Irino; Honoka Tamaru; Ayako Yamamoto', display:{Lore:['[{"text": "arXiv:2206.06573", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech intelligibility of simulated hearing loss sounds and its prediction using the Gammachirp Envelope Similarity Index (GESI)\\u00a7r\\n\\n\\u00a78\\u00a7oToshio Irino\\nHonoka Tamaru\\nAyako Yamamoto\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.06573\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-211\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2022\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 28 Nov 2023 05:52:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis preprint is a copy of the final version accepted for Interspeech 2022. See https://doi.org/10.21437/Interspeech.2022-211\\u00a7r"}']}
{title:'Irino (§72023§r)', author: 'Toshio Irino', display:{Lore:['[{"text": "arXiv:2206.06604", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWHIS: Hearing impairment simulator based on the gammachirp auditory filterbank\\u00a7r\\n\\n\\u00a78\\u00a7oToshio Irino\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.06604\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ACCESS.2023.3298673\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE access, 25 July 2023\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 28 Nov 2023 06:09:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis preprint was an originalversion that was unsuccessfully submitted to Trends in Hearing on June 5, 2022. The revised version has been accepted for publication in IEEE access. See https://doi.org/10.1109/ACCESS.20"}','{"text": "23.3298673 ( https://ieeexplore.ieee.org/document/10193769 )\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Kai Li; Yi Luo', display:{Lore:['[{"text": "arXiv:2206.07340", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Design and Training Strategies for RNN-based Online Neural Speech Separation Systems\\u00a7r\\n\\n\\u00a78\\u00a7oKai Li\\nYi Luo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.07340\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 21 Feb 2023 02:10:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2023\\u00a7r"}']}
{title:'Verma (§72023§r)', author: 'Prateek Verma', display:{Lore:['[{"text": "arXiv:2206.08297", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Language Model With Million Sample Context For Raw Audio Using Transformer Architectures\\u00a7r\\n\\n\\u00a78\\u00a7oPrateek Verma\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.08297\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 May 2023 20:50:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 1 figure. Technical Report at Stanford University\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Changan Chen; Carl Schissler; Sanchit Garg; Philip Kobernik; Alexander Clegg; Paul Calamia; Dhruv Batra; Philip W Robinson; Kristen Grauman', display:{Lore:['[{"text": "arXiv:2206.08312", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSoundSpaces 2.0: A Simulation Platform for Visual-Acoustic Learning\\u00a7r\\n\\n\\u00a78\\u00a7oChangan Chen\\nCarl Schissler\\nSanchit Garg\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.08312\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 23 Jan 2023 18:49:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCamera-ready version. Website: https://soundspaces.org. Project page: https://vision.cs.utexas.edu/projects/soundspaces2\\u00a7r"}']}
{title:'Gao et al. (§72023§r)', author: 'Zhifu Gao; Shiliang Zhang; Ian McLoughlin; Zhijie Yan', display:{Lore:['[{"text": "arXiv:2206.08317", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParaformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhifu Gao\\nShiliang Zhang\\nIan McLoughlin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.08317\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 30 Mar 2023 07:00:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted by INTERSPEECH 2022\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Zhengyang Chen; Bei Liu; Bing Han; Leying Zhang; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2206.11699", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe SJTU X-LANCE Lab System for CNSRC 2022\\u00a7r\\n\\n\\u00a78\\u00a7oZhengyang Chen\\nBei Liu\\nBing Han\\nLeying Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.11699\\u00a7r\\n\\nVersion:\\u00a77v5 (Sun, 14 May 2023 07:25:54 GMT)\\u00a7r"}']}
{title:'Ooi et al. (§72023§r)', author: 'Kenneth Ooi; Zhen-Ting Ong; Karn N. Watcharasupat; Bhan Lam; Joo Young Hong; Woon-Seng Gan', display:{Lore:['[{"text": "arXiv:2207.01078", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lARAUS: A Large-Scale Dataset and Baseline Models of Affective Responses to Augmented Urban Soundscapes\\u00a7r\\n\\n\\u00a78\\u00a7oKenneth Ooi\\nZhen-Ting Ong\\nKarn N. Watcharasupat\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.01078\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TAFFC.2023.3247914\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Trans. Affect. Comput., pp. 1-17, 2023\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 6 Mar 2023 03:24:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o[v1, v2] 25 pages, 11 figures. [v3] 33 pages, 18 figures. v3 updated with changes made after peer review. in IEEE Transactions on Affective Computing, 2023\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Zewang Zhang; Yibin Zheng; Xinhui Li; Li Lu', display:{Lore:['[{"text": "arXiv:2207.01886", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeSinger 2: Fully Parallel Singing Voice Synthesis via Multi-Singer Conditional Adversarial Training\\u00a7r\\n\\n\\u00a78\\u00a7oZewang Zhang\\nYibin Zheng\\nXinhui Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.01886\\u00a7r\\n\\nVersion:\\u00a77v8 (Thu, 16 Feb 2023 14:29:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at ICASSP 2023\\u00a7r"}']}
{title:'Yu et al. (§72023§r)', author: 'Jiashuo Yu; Junfu Pu; Ying Cheng; Rui Feng; Ying Shan', display:{Lore:['[{"text": "arXiv:2207.03190", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Music-Dance Representations through Explicit-Implicit Rhythm Synchronization\\u00a7r\\n\\n\\u00a78\\u00a7oJiashuo Yu\\nJunfu Pu\\nYing Cheng\\nRui Feng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.03190\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TMM.2023.3303690\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 10 Aug 2023 08:06:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in IEEE Transactions on Multimedia\\u00a7r"}']}
{title:'Gomes et al. (§72023§r)', author: 'Clive Gomes; Hyejin Park; Patrick Kollman; Yi Song; Iffanice Houndayi; Ankit Shah', display:{Lore:['[{"text": "arXiv:2207.04156", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomated Audio Captioning and Language-Based Audio Retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oClive Gomes\\nHyejin Park\\nPatrick Kollman\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.04156\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 15 May 2023 13:54:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDCASE 2022 Competition (Task 6)\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Harlin Lee; Aaqib Saeed', display:{Lore:['[{"text": "arXiv:2207.05784", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDistilled Non-Semantic Speech Embeddings with Binary Neural Networks for Low-Resource Devices\\u00a7r\\n\\n\\u00a78\\u00a7oHarlin Lee\\nAaqib Saeed\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.05784\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.patrec.2023.11.028\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPattern Recognition Letters, vol. 177, pp. 15-19, 2024\\u00a7r\\n\\nVersion:\\u00a77v4 (Sat, 2 Dec 2023 20:36:35 GMT)\\u00a7r"}']}
{title:'Huang et al. (§72023§r)', author: 'Po-Yao Huang; Hu Xu; Juncheng Li; Alexei Baevski; Michael Auli; Wojciech Galuba; Florian Metze; Christoph Feichtenhofer', display:{Lore:['[{"text": "arXiv:2207.06405", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMasked Autoencoders that Listen\\u00a7r\\n\\n\\u00a78\\u00a7oPo-Yao Huang\\nHu Xu\\nJuncheng Li\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.06405\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 12 Jan 2023 05:22:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at NeurIPS 2022\\u00a7r"}']}
{title:'Agarla et al. (§72023§r)', author: 'Mirko Agarla; Simone Bianco; Luigi Celona; Paolo Napoletano; Alexey Petrovsky; Flavio Piccoli; Raimondo Schettini; Ivan Shanin', display:{Lore:['[{"text": "arXiv:2207.06767", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-supervised cross-lingual speech emotion recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMirko Agarla\\nSimone Bianco\\nLuigi Celona\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.06767\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.eswa.2023.121368\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nElsevier Expert Systems with Applications, 237 (2024), 121368\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 17 Jul 2023 06:11:59 GMT)\\u00a7r"}']}
{title:'Dong et al. (§72023§r)', author: 'Hao-Wen Dong; Ke Chen; Shlomo Dubnov; Julian McAuley; Taylor Berg-Kirkpatrick', display:{Lore:['[{"text": "arXiv:2207.06983", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultitrack Music Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oHao-Wen Dong\\nKe Chen\\nShlomo Dubnov\\nJulian McAuley\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.06983\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 24 May 2023 20:49:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023. Demo: https://salu133445.github.io/mmt/ . Code: https://github.com/salu133445/mmt\\u00a7r"}']}
{title:'Jiang et al. (§72023§r)', author: 'Xue Jiang; Xiulian Peng; Huaying Xue; Yuan Zhang; Yan Lu', display:{Lore:['[{"text": "arXiv:2207.08363", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLatent-Domain Predictive Neural Speech Coding\\u00a7r\\n\\n\\u00a78\\u00a7oXue Jiang\\nXiulian Peng\\nHuaying Xue\\nYuan Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.08363\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3277693\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 May 2023 12:59:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING (TASLP)\\u00a7r"}']}
{title:'Aytekin et al. (§72023§r)', author: 'Idil Aytekin; Onat Dalmaz; Kaan Gonc; Haydar Ankishan; Emine U Saritas; Ulas Bagci; Haydar Celik; Tolga Cukur', display:{Lore:['[{"text": "arXiv:2207.09529", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCOVID-19 Detection from Respiratory Sounds with Hierarchical Spectrogram Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oIdil Aytekin\\nOnat Dalmaz\\nKaan Gonc\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.09529\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 27 May 2023 00:25:56 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Dongchao Yang; Jianwei Yu; Helin Wang; Wen Wang; Chao Weng; Yuexian Zou; Dong Yu', display:{Lore:['[{"text": "arXiv:2207.09983", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffsound: Discrete Diffusion Model for Text-to-sound Generation\\u00a7r\\n\\n\\u00a78\\u00a7oDongchao Yang\\nJianwei Yu\\nHelin Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.09983\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 28 Apr 2023 07:45:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by TASLP2022\\u00a7r"}']}
{title:'Ma et al. (§72023§r)', author: 'Haoxin Ma; Jiangyan Yi; Chenglong Wang; Xinrui Yan; Jianhua Tao; Tao Wang; Shiming Wang; Ruibo Fu', display:{Lore:['[{"text": "arXiv:2207.12308", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCFAD: A Chinese Dataset for Fake Audio Detection\\u00a7r\\n\\n\\u00a78\\u00a7oHaoxin Ma\\nJiangyan Yi\\nChenglong Wang\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.12308\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 18 Jul 2023 04:21:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oFAD renamed as CFAD\\u00a7r"}']}
{title:'Cozzatti et al. (§72023§r)', author: 'Michele Cozzatti; Federico Simonetta; Stavros Ntalampiras', display:{Lore:['[{"text": "arXiv:2208.03326", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVariational Autoencoders for Anomaly Detection in Respiratory Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oMichele Cozzatti\\nFederico Simonetta\\nStavros Ntalampiras\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.03326\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-031-15937-4_28\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 3 Dec 2023 11:03:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at ICANN 2022\\u00a7r"}']}
{title:'Boland et al. (§72023§r)', author: 'Jeffrey R. Boland; Lane P. Hughston', display:{Lore:['[{"text": "arXiv:2208.04974", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a72math.NT\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMathematical Foundations of Complex Tonality\\u00a7r\\n\\n\\u00a78\\u00a7oJeffrey R. Boland\\nLane P. Hughston\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.04974\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 10 Jul 2023 10:30:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o35 pages, to appear in Journal of Mathematics and Music\\u00a7r"}']}
{title:'Lv et al. (§72023§r)', author: 'Ang Lv; Xu Tan; Tao Qin; Tie-Yan Liu; Rui Yan', display:{Lore:['[{"text": "arXiv:2208.05697", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRe-creation of Creations: A New Paradigm for Lyric-to-Melody Generation\\u00a7r\\n\\n\\u00a78\\u00a7oAng Lv\\nXu Tan\\nTao Qin\\nTie-Yan Liu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.05697\\u00a7r\\n\\nVersion:\\u00a77v4 (Sat, 28 Jan 2023 09:43:42 GMT)\\u00a7r"}']}
{title:'Yan et al. (§72023§r)', author: 'Xinrui Yan; Jiangyan Yi; Chenglong Wang; Jianhua Tao; Junzuo Zhou; Hao Gu; Ruibo Fu', display:{Lore:['[{"text": "arXiv:2208.10489", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSystem Fingerprint Recognition for Deepfake Audio: An Initial Dataset and Investigation\\u00a7r\\n\\n\\u00a78\\u00a7oXinrui Yan\\nJiangyan Yi\\nChenglong Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.10489\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 15 Sep 2023 07:19:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 4 figures. Submitto IEEE Transactions on Audio, Speech and Language Processing (TASLP). arXiv admin note: text overlap with arXiv:2208.09646\\u00a7r"}']}
{title:'Champion et al. (§72023§r)', author: 'Pierre Champion; Denis Jouvet; Anthony Larcher', display:{Lore:['[{"text": "arXiv:2208.10497", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAre disentangled representations all you need to build speaker anonymization systems?\\u00a7r\\n\\n\\u00a78\\u00a7oPierre Champion\\nDenis Jouvet\\nAnthony Larcher\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.10497\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nINTERSPEECH 2022 - Human and Humanizing Speech Technology, Sep\\n  2022, incheon, South Korea\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 13 Jan 2023 14:31:07 GMT)\\u00a7r"}']}
{title:'Indenbom et al. (§72023§r)', author: 'Evgenii Indenbom; Nicolae-Cătălin Ristea; Ando Saabas; Tanel Pärnamaa; Jegor Gužvin', display:{Lore:['[{"text": "arXiv:2208.11308", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep model with built-in cross-attention alignment for acoustic echo cancellation\\u00a7r\\n\\n\\u00a78\\u00a7oEvgenii Indenbom\\nNicolae-C\\u0103t\\u0103lin Ristea\\nAndo Saabas\\nTanel P\\u00e4rnamaa\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.11308\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 14 Mar 2023 08:16:51 GMT)\\u00a7r"}']}
{title:'Borsos et al. (§72023§r)', author: 'Zalán Borsos; Raphaël Marinier; Damien Vincent; Eugene Kharitonov; Olivier Pietquin; Matt Sharifi; Dominik Roblek; Olivier Teboul; David Grangier; Marco Tagliasacchi; Neil Zeghidour', display:{Lore:['[{"text": "arXiv:2209.03143", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudioLM: a Language Modeling Approach to Audio Generation\\u00a7r\\n\\n\\u00a78\\u00a7oZal\\u00e1n Borsos\\nRapha\\u00ebl Marinier\\nDamien Vincent\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.03143\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 26 Jul 2023 03:52:36 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Zhong-Qiu Wang; Samuele Cornell; Shukjae Choi; Younglo Lee; Byeong-Yeol Kim; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2209.03952", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTF-GridNet: Making Time-Frequency Domain Models Great Again for Monaural Speaker Separation\\u00a7r\\n\\n\\u00a78\\u00a7oZhong-Qiu Wang\\nSamuele Cornell\\nShukjae Choi\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.03952\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 15 Mar 2023 16:01:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oin IEEEICASSP 2023\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Shih-Lun Wu; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2209.08212", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCompose     Embellish: Well-Structured Piano Performance Generation via A Two-Stage Approach\\u00a7r\\n\\n\\u00a78\\u00a7oShih-Lun Wu\\nYi-Hsuan Yang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.08212\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 7 Mar 2023 14:19:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2023\\u00a7r"}']}
{title:'Hahne (§72023§r)', author: 'Christopher Hahne', display:{Lore:['[{"text": "arXiv:2209.12202", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.app-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultimodal Exponentially Modified Gaussian Oscillators\\u00a7r\\n\\n\\u00a78\\u00a7oChristopher Hahne\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.12202\\u00a7r\\n\\nVersion:\\u00a77v6 (Sun, 22 Jan 2023 10:16:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE International Ultrasonic Symposium 2022\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Tung-Yu Wu; Chen-An Li; Tzu-Han Lin; Tsu-Yuan Hsu; Hung-Yi Lee', display:{Lore:['[{"text": "arXiv:2209.12900", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Efficacy of Self-Supervised Speech Models for Audio Representations\\u00a7r\\n\\n\\u00a78\\u00a7oTung-Yu Wu\\nChen-An Li\\nTzu-Han Lin\\nTsu-Yuan Hsu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.12900\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 31 Jan 2023 10:49:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oto appear in Proceedings of Machine Learning Research (PMLR): NeurIPS 2021 Competition Track\\u00a7r"}']}
{title:'Carofilis et al. (§72023§r)', author: 'Andrés Carofilis; Laura Fernández-Robles; Enrique Alegre; Eduardo Fidalgo', display:{Lore:['[{"text": "arXiv:2209.14078", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMeWEHV: Mel and Wave Embeddings for Human Voice Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oAndr\\u00e9s Carofilis\\nLaura Fern\\u00e1ndez-Robles\\nEnrique Alegre\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.14078\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 24 Jun 2023 10:41:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEEAccess\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Kai Li; Runxuan Yang; Xiaolin Hu', display:{Lore:['[{"text": "arXiv:2209.15200", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn efficient encoder-decoder architecture with top-down attention for speech separation\\u00a7r\\n\\n\\u00a78\\u00a7oKai Li\\nRunxuan Yang\\nXiaolin Hu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.15200\\u00a7r\\n\\nVersion:\\u00a77v5 (Thu, 30 Mar 2023 06:01:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICLR 2023; Code Demos: https://cslikai.cn/project/TDANet/\\u00a7r"}']}
{title:'Kreuk et al. (§72023§r)', author: 'Felix Kreuk; Gabriel Synnaeve; Adam Polyak; Uriel Singer; Alexandre Défossez; Jade Copet; Devi Parikh; Yaniv Taigman; Yossi Adi', display:{Lore:['[{"text": "arXiv:2209.15352", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudioGen: Textually Guided Audio Generation\\u00a7r\\n\\n\\u00a78\\u00a7oFelix Kreuk\\nGabriel Synnaeve\\nAdam Polyak\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.15352\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 5 Mar 2023 09:14:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICLR2023\\u00a7r"}']}
{title:'Ao et al. (§72023§r)', author: 'Tenglong Ao; Qingzhe Gao; Yuke Lou; Baoquan Chen; Libin Liu', display:{Lore:['[{"text": "arXiv:2210.01448", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRhythmic Gesticulator: Rhythm-Aware Co-Speech Gesture Synthesis with Hierarchical Neural Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oTenglong Ao\\nQingzhe Gao\\nYuke Lou\\nBaoquan Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.01448\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3550454.3555435\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 4 May 2023 12:13:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSIGGRAPH Asia 2022 (Journal Track); Project Page: https://pku-mocca.github.io/Rhythmic-Gesticulator-Page/\\u00a7r"}']}
{title:'Bovbjerg et al. (§72023§r)', author: 'Holger Severin Bovbjerg; Zheng-Hua Tan', display:{Lore:['[{"text": "arXiv:2210.01703", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Label-Deficient Keyword Spotting Through Self-Supervised Pretraining\\u00a7r\\n\\n\\u00a78\\u00a7oHolger Severin Bovbjerg\\nZheng-Hua Tan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.01703\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 24 May 2023 12:17:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published at ICASSP2023 Workshop on Self-supervision in Audio, Speech and Beyond, 10th of June 2023, Rhodes, Greece. Copyright (c) 2023 IEEE. 5 pages, 3 figures,3 tables\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Xuechen Liu; Xin Wang; Md Sahidullah; Jose Patino; Héctor Delgado; Tomi Kinnunen; Massimiliano Todisco; Junichi Yamagishi; Nicholas Evans; Andreas Nautsch; Kong Aik Lee', display:{Lore:['[{"text": "arXiv:2210.02437", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lASVspoof 2021: Towards Spoofed and Deepfake Speech Detection in the Wild\\u00a7r\\n\\n\\u00a78\\u00a7oXuechen Liu\\nXin Wang\\nMd Sahidullah\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.02437\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3285283\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 22 Jun 2023 13:00:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE/ACM Transactions on Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yangfu Li; Xiaodan Lin; Jiaxin Yang', display:{Lore:['[{"text": "arXiv:2210.02731", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPSVRF: Learning to restore Pitch-Shifted Voice without reference\\u00a7r\\n\\n\\u00a78\\u00a7oYangfu Li\\nXiaodan Lin\\nJiaxin Yang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.02731\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 13 Mar 2023 09:42:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oHave some errors\\u00a7r"}']}
{title:'Meng et al. (§72023§r)', author: 'Chutong Meng; Junyi Ao; Tom Ko; Mingxuan Wang; Haizhou Li', display:{Lore:['[{"text": "arXiv:2210.04062", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCoBERT: Self-Supervised Speech Representation Learning Through Code Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oChutong Meng\\nJunyi Ao\\nTom Ko\\nMingxuan Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.04062\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 5 Jul 2023 16:30:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2023\\u00a7r"}']}
{title:'Park et al. (§72023§r)', author: 'Junwoo Park; Youngwoo Cho; Gyuhyeon Sim; Hojoon Lee; Jaegul Choo', display:{Lore:['[{"text": "arXiv:2210.05917", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnemy Spotted: in-game gun sound dataset for gunshot classification and localization\\u00a7r\\n\\n\\u00a78\\u00a7oJunwoo Park\\nYoungwoo Cho\\nGyuhyeon Sim\\nHojoon Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.05917\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/CoG51982.2022.9893670\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 17 Feb 2023 02:04:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at IEEEConference on Games (GoG) 2022\\u00a7r"}']}
{title:'Matsunaga et al. (§72023§r)', author: 'Yuta Matsunaga; Takaaki Saeki; Shinnosuke Takamichi; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2210.07559", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmpirical Study Incorporating Linguistic Knowledge on Filled Pauses for Personalized Spontaneous Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYuta Matsunaga\\nTakaaki Saeki\\nShinnosuke Takamichi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.07559\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 19 Sep 2023 06:51:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to APSIPA ASC 2022\\u00a7r"}']}
{title:'Matsunaga et al. (§72023§r)', author: 'Yuta Matsunaga; Takaaki Saeki; Shinnosuke Takamichi; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2210.09815", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving robustness of spontaneous speech synthesis with linguistic speech regularization and pseudo-filled-pause insertion\\u00a7r\\n\\n\\u00a78\\u00a7oYuta Matsunaga\\nTakaaki Saeki\\nShinnosuke Takamichi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.09815\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 19 Sep 2023 06:45:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to SSW12\\u00a7r"}']}
{title:'Takahashi et al. (§72023§r)', author: 'Naoya Takahashi; Mayank Kumar Singh; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2210.11096", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust One-Shot Singing Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oNaoya Takahashi\\nMayank Kumar Singh\\nYuki Mitsufuji\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.11096\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 6 Oct 2023 16:18:32 GMT)\\u00a7r"}']}
{title:'Stergiou et al. (§72023§r)', author: 'Alexandros Stergiou; Dima Damen', display:{Lore:['[{"text": "arXiv:2210.11328", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPlay It Back: Iterative Attention for Audio Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAlexandros Stergiou\\nDima Damen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.11328\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 12 Mar 2023 12:03:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at IEEEInternational Conference on Acoustics, Speech and Signal Processing (ICASSP) 2023\\u00a7r"}']}
{title:'Postolache et al. (§72023§r)', author: 'Emilian Postolache; Jordi Pons; Santiago Pascual; Joan Serrà', display:{Lore:['[{"text": "arXiv:2210.12108", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Permutation Invariant Training for Universal Sound Separation\\u00a7r\\n\\n\\u00a78\\u00a7oEmilian Postolache\\nJordi Pons\\nSantiago Pascual\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.12108\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 6 Mar 2023 13:59:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDemo page: http://jordipons.me/apps/adversarialPIT/, Accepted at ICASSP-23\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Evonne P. C. Lee; Guangzhi Sun; Chao Zhang; Philip C. Woodland', display:{Lore:['[{"text": "arXiv:2210.13576", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpectral Clustering-aware Learning of Embeddings for Speaker Diarisation\\u00a7r\\n\\n\\u00a78\\u00a7oEvonne P. C. Lee\\nGuangzhi Sun\\nChao Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.13576\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 14 Mar 2023 23:04:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP 2023, 5 pages\\u00a7r"}']}
{title:'Gambardella et al. (§72023§r)', author: 'Andrew Gambardella; Youngjun Choi; Doyo Choi; Jinjoon Lee', display:{Lore:['[{"text": "arXiv:2210.14602", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.AP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Data Mosaicing with Simulation-based Inference\\u00a7r\\n\\n\\u00a78\\u00a7oAndrew Gambardella\\nYoungjun Choi\\nDoyo Choi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14602\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 1 Feb 2023 08:01:32 GMT)\\u00a7r"}']}
{title:'Chang et al. (§72023§r)', author: 'Yi Chang; Zhao Ren; Thanh Tam Nguyen; Kun Qian; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2210.14977", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKnowledge Transfer For On-Device Speech Emotion Recognition with Neural Structured Learning\\u00a7r\\n\\n\\u00a78\\u00a7oYi Chang\\nZhao Ren\\nThanh Tam Nguyen\\nKun Qian\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14977\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 11 May 2023 13:54:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Ito et al. (§72023§r)', author: 'Nobutaka Ito; Masashi Sugiyama', display:{Lore:['[{"text": "arXiv:2210.15143", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Signal Enhancement with Learning from Positive and Unlabelled Data\\u00a7r\\n\\n\\u00a78\\u00a7oNobutaka Ito\\nMasashi Sugiyama\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15143\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 26 Apr 2023 15:03:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP2023\\u00a7r"}']}
{title:'Beguš et al. (§72023§r)', author: 'Gašper Beguš; Alan Zhou; Peter Wu; Gopala K Anumanchipalli', display:{Lore:['[{"text": "arXiv:2210.15173", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lArticulation GAN: Unsupervised modeling of articulatory learning\\u00a7r\\n\\n\\u00a78\\u00a7oGa\\u0161per Begu\\u0161\\nAlan Zhou\\nPeter Wu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15173\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10096800\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2023 - 2023 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 12 Mar 2023 20:28:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Bukhsh et al. (§72023§r)', author: 'Zaharah Bukhsh; Aaqib Saeed', display:{Lore:['[{"text": "arXiv:2210.15283", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Out-of-Distribution Detection for Audio with Deep Nearest Neighbors\\u00a7r\\n\\n\\u00a78\\u00a7oZaharah Bukhsh\\nAaqib Saeed\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15283\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 25 Feb 2023 21:27:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP\'23. Webpage: https://zaharah.github.io/ood_audio, Code: https://github.com/Zaharah/ood_audio\\u00a7r"}']}
{title:'Ravenscroft et al. (§72023§r)', author: 'William Ravenscroft; Stefan Goetze; Thomas Hain', display:{Lore:['[{"text": "arXiv:2210.15305", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeformable Temporal Convolutional Networks for Monaural Noisy Reverberant Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oWilliam Ravenscroft\\nStefan Goetze\\nThomas Hain\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15305\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 10 Mar 2023 16:14:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP 2023\\u00a7r"}']}
{title:'Marmoret et al. (§72023§r)', author: 'Axel Marmoret; Jérémy E. Cohen; Frédéric Bimbot', display:{Lore:['[{"text": "arXiv:2210.15356", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConvolutive Block-Matching Segmentation Algorithm with Application to Music Structure Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oAxel Marmoret\\nJ\\u00e9r\\u00e9my E. Cohen\\nFr\\u00e9d\\u00e9ric Bimbot\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15356\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 26 Sep 2023 11:02:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 7 figures. Accepted for publication at WASPAA 2023. The associated toolbox is available at https://gitlab.inria.fr/amarmore/autosimilarity_segmentation/-/tree/WASPAA23\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Li-Wei Chen; Yao-Fei Cheng; Hung-Shin Lee; Yu Tsao; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:2210.15368", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Training and Inference Strategy Using Noisy and Enhanced Speech as Target for Speech Enhancement without Clean Speech\\u00a7r\\n\\n\\u00a78\\u00a7oLi-Wei Chen\\nYao-Fei Cheng\\nHung-Shin Lee\\nYu Tsao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15368\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 22 May 2023 14:02:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2023\\u00a7r"}']}
{title:'Saeki et al. (§72023§r)', author: 'Takaaki Saeki; Heiga Zen; Zhehuai Chen; Nobuyuki Morioka; Gary Wang; Yu Zhang; Ankur Bapna; Andrew Rosenberg; Bhuvana Ramabhadran', display:{Lore:['[{"text": "arXiv:2210.15447", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVirtuoso: Massive Multilingual Speech-Text Joint Semi-Supervised Learning for Text-To-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oTakaaki Saeki\\nHeiga Zen\\nZhehuai Chen\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15447\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 15 Mar 2023 10:52:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP 2023\\u00a7r"}']}
{title:'Yoneyama et al. (§72023§r)', author: 'Reo Yoneyama; Yi-Chiao Wu; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2210.15533", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSource-Filter HiFi-GAN: Fast and Pitch Controllable High-Fidelity Neural Vocoder\\u00a7r\\n\\n\\u00a78\\u00a7oReo Yoneyama\\nYi-Chiao Wu\\nTomoki Toda\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15533\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 27 Feb 2023 08:09:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Tuo Zhang; Tiantian Feng; Samiul Alam; Sunwoo Lee; Mi Zhang; Shrikanth S. Narayanan; Salman Avestimehr', display:{Lore:['[{"text": "arXiv:2210.15707", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.DC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFedAudio: A Federated Learning Benchmark for Audio Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oTuo Zhang\\nTiantian Feng\\nSamiul Alam\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15707\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 8 Feb 2023 18:42:17 GMT)\\u00a7r"}']}
{title:'Cai et al. (§72023§r)', author: 'Yiqiang Cai; Shengchen Li', display:{Lore:['[{"text": "arXiv:2210.15960", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring the Effects of Channel Sparsity on Neural Network Pruning for Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oYiqiang Cai\\nShengchen Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15960\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 20 Jul 2023 05:49:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by CSMT 2023\\u00a7r"}']}
{title:'Moummad et al. (§72023§r)', author: 'Ilyass Moummad; Nicolas Farrugia', display:{Lore:['[{"text": "arXiv:2210.16192", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPretraining Respiratory Sound Representations using Metadata and Contrastive Learning\\u00a7r\\n\\n\\u00a78\\u00a7oIlyass Moummad\\nNicolas Farrugia\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16192\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 11 Aug 2023 11:58:28 GMT)\\u00a7r"}']}
{title:'Vaillant et al. (§72023§r)', author: 'Gwendal Le Vaillant; Thierry Dutoit', display:{Lore:['[{"text": "arXiv:2210.16984", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynthesizer Preset Interpolation using Transformer Auto-Encoders\\u00a7r\\n\\n\\u00a78\\u00a7oGwendal Le Vaillant\\nThierry Dutoit\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16984\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 9 Mar 2023 16:12:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEICASSP 2023\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Eungbeom Kim; Jinhee Kim; Yoori Oh; Kyungsu Kim; Minju Park; Jaeheon Sim; Jinwoo Lee; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2210.17143", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Train and Test-Time Augmentations for Audio-Language Learning\\u00a7r\\n\\n\\u00a78\\u00a7oEungbeom Kim\\nJinhee Kim\\nYoori Oh\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.17143\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 23 May 2023 08:54:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures\\u00a7r"}']}
{title:'Chu et al. (§72023§r)', author: 'Ernie Chu; Ju-Ting Chen; Chia-Ping Chen', display:{Lore:['[{"text": "arXiv:2210.17152", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Time-Scale Modification with Temporal Compressing Networks\\u00a7r\\n\\n\\u00a78\\u00a7oErnie Chu\\nJu-Ting Chen\\nChia-Ping Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.17152\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 6 Oct 2023 04:32:16 GMT)\\u00a7r"}']}
{title:'Jiang et al. (§72023§r)', author: 'Junyan Jiang; Gus Xia', display:{Lore:['[{"text": "arXiv:2210.17183", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Hierarchical Metrical Structure Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oJunyan Jiang\\nGus Xia\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.17183\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 25 Jan 2023 04:55:11 GMT)\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Harlin Lee; Aaqib Saeed; Andrea L. Bertozzi', display:{Lore:['[{"text": "arXiv:2211.00119", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lActive Learning of Non-semantic Speech Tasks with Pretrained Models\\u00a7r\\n\\n\\u00a78\\u00a7oHarlin Lee\\nAaqib Saeed\\nAndrea L. Bertozzi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00119\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10096465\\u00a7r\\n\\nVersion:\\u00a77v4 (Sat, 25 Feb 2023 21:35:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at: ICASSP\'23, Code: https://github.com/HarlinLee/ALOE\\u00a7r"}']}
{title:'Vioni et al. (§72023§r)', author: 'Alexandra Vioni; Georgia Maniati; Nikolaos Ellinas; June Sig Sung; Inchul Hwang; Aimilios Chalamandaris; Pirros Tsiakoulis', display:{Lore:['[{"text": "arXiv:2211.00342", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating Content-Aware Neural Text-To-Speech MOS Prediction Using Prosodic and Linguistic Features\\u00a7r\\n\\n\\u00a78\\u00a7oAlexandra Vioni\\nGeorgia Maniati\\nNikolaos Ellinas\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00342\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10096255\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 7 May 2023 13:43:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of ICASSP 2023\\u00a7r"}']}
{title:'Markopoulos et al. (§72023§r)', author: 'Konstantinos Markopoulos; Georgia Maniati; Georgios Vamvoukakis; Nikolaos Ellinas; Georgios Vardaxoglou; Panos Kakoulidis; Junkwang Oh; Gunu Jho; Inchul Hwang; Aimilios Chalamandaris; Pirros Tsiakoulis; Spyros Raptis', display:{Lore:['[{"text": "arXiv:2211.00375", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerating Multilingual Gender-Ambiguous Text-to-Speech Voices\\u00a7r\\n\\n\\u00a78\\u00a7oKonstantinos Markopoulos\\nGeorgia Maniati\\nGeorgios Vamvoukakis\\n+ 8 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00375\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 11 Jun 2023 21:33:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2023\\u00a7r"}']}
{title:'Comunità et al. (§72023§r)', author: 'Marco Comunità; Christian J. Steinmetz; Huy Phan; Joshua D. Reiss', display:{Lore:['[{"text": "arXiv:2211.00497", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModelling black-box audio effects with time-varying feature modulation\\u00a7r\\n\\n\\u00a78\\u00a7oMarco Comunit\\u00e0\\nChristian J. Steinmetz\\nHuy Phan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00497\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10097173\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 May 2023 19:42:06 GMT)\\u00a7r"}']}
{title:'Song et al. (§72023§r)', author: 'Xingchen Song; Di Wu; Zhiyong Wu; Binbin Zhang; Yuekai Zhang; Zhendong Peng; Wenpeng Li; Fuping Pan; Changbao Zhu', display:{Lore:['[{"text": "arXiv:2211.00522", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTrimTail: Low-Latency Streaming ASR with Simple but Effective Spectrogram-Level Length Penalty\\u00a7r\\n\\n\\u00a78\\u00a7oXingchen Song\\nDi Wu\\nZhiyong Wu\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00522\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 22 Jan 2023 07:24:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Zhengyang Chen; Bing Han; Xu Xiang; Houjun Huang; Bei Liu; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2211.00815", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBuild a SRE Challenge System: Lessons from VoxSRC 2022 and CNSRC 2022\\u00a7r\\n\\n\\u00a78\\u00a7oZhengyang Chen\\nBing Han\\nXu Xiang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00815\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 1 Jun 2023 05:39:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by InterSpeech 2023\\u00a7r"}']}
{title:'Choi et al. (§72023§r)', author: 'Jongho Choi; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2211.00895", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPop2Piano : Pop Audio-based Piano Cover Generation\\u00a7r\\n\\n\\u00a78\\u00a7oJongho Choi\\nKyogu Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00895\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 1 Apr 2023 06:02:16 GMT)\\u00a7r"}']}
{title:'Kheir et al. (§72023§r)', author: 'Yassine El Kheir; Shammur Absar Chowdhury; Ahmed Ali; Hamdy Mubarak; Shazia Afzal', display:{Lore:['[{"text": "arXiv:2211.00923", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeechBlender: Speech Augmentation Framework for Mispronunciation Data Generation\\u00a7r\\n\\n\\u00a78\\u00a7oYassine El Kheir\\nShammur Absar Chowdhury\\nAhmed Ali\\nHamdy Mubarak\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00923\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 12 Jul 2023 12:28:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'López-García (§72023§r)', author: 'Aarón López-García', display:{Lore:['[{"text": "arXiv:2211.00982", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpectroMap: Peak detection algorithm for audio fingerprinting\\u00a7r\\n\\n\\u00a78\\u00a7oAar\\u00f3n L\\u00f3pez-Garc\\u00eda\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00982\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 2 May 2023 14:21:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 5 figures\\u00a7r"}']}
{title:'Golmakani et al. (§72023§r)', author: 'Ali Golmakani; Mostafa Sadeghi; Xavier Alameda-Pineda; Romain Serizel', display:{Lore:['[{"text": "arXiv:2211.00990", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA weighted-variance variational autoencoder model for speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oAli Golmakani\\nMostafa Sadeghi\\nXavier Alameda-Pineda\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00990\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 26 Oct 2023 11:47:25 GMT)\\u00a7r"}']}
{title:'Violeta et al. (§72023§r)', author: 'Lester Phillip Violeta; Ding Ma; Wen-Chin Huang; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2211.01079", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntermediate Fine-Tuning Using Imperfect Synthetic Speech for Improving Electrolaryngeal Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oLester Phillip Violeta\\nDing Ma\\nWen-Chin Huang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01079\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 May 2023 16:00:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Song et al. (§72023§r)', author: 'Kun Song; Yongmao Zhang; Yi Lei; Jian Cong; Hanzhao Li; Lei Xie; Gang He; Jinfeng Bai', display:{Lore:['[{"text": "arXiv:2211.01087", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDSPGAN: a GAN-based universal vocoder for high-fidelity TTS by time-frequency domain supervision from DSP\\u00a7r\\n\\n\\u00a78\\u00a7oKun Song\\nYongmao Zhang\\nYi Lei\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01087\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 28 May 2023 16:15:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Hung et al. (§72023§r)', author: 'Yun-Ning Hung; Chao-Han Huck Yang; Pin-Yu Chen; Alexander Lerch', display:{Lore:['[{"text": "arXiv:2211.01317", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Resource Music Genre Classification with Cross-Modal Neural Model Reprogramming\\u00a7r\\n\\n\\u00a78\\u00a7oYun-Ning Hung\\nChao-Han Huck Yang\\nPin-Yu Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01317\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 3 May 2023 04:22:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEICASSP 2023. The implementation is available at https://github.com/biboamy/music-repro\\u00a7r"}']}
{title:'Andreev et al. (§72023§r)', author: 'Pavel Andreev; Nicholas Babaev; Azat Saginbaev; Ivan Shchekotov; Aibek Alanov', display:{Lore:['[{"text": "arXiv:2211.01751", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIterative autoregression: a novel trick to improve your low-latency speech enhancement model\\u00a7r\\n\\n\\u00a78\\u00a7oPavel Andreev\\nNicholas Babaev\\nAzat Saginbaev\\nIvan Shchekotov\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01751\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 5 Dec 2023 11:36:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023\\u00a7r"}']}
{title:'Ollerenshaw et al. (§72023§r)', author: 'Anna Ollerenshaw; Md Asif Jalal; Thomas Hain', display:{Lore:['[{"text": "arXiv:2211.02000", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDynamic Kernels and Channel Attention for Low Resource Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oAnna Ollerenshaw\\nMd Asif Jalal\\nThomas Hain\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02000\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Feb 2023 10:07:42 GMT)\\u00a7r"}']}
{title:'Veluri et al. (§72023§r)', author: 'Bandhav Veluri; Justin Chan; Malek Itani; Tuochao Chen; Takuya Yoshioka; Shyamnath Gollakota', display:{Lore:['[{"text": "arXiv:2211.02250", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-Time Target Sound Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oBandhav Veluri\\nJustin Chan\\nMalek Itani\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02250\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 19 Apr 2023 09:43:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023 camera-ready\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Yunhao Chen; Yunjie Zhu; Zihui Yan; Yifan Huang; Zhen Ren; Jianlu Shen; Lifang Chen', display:{Lore:['[{"text": "arXiv:2211.02940", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEffective Audio Classification Network Based on Paired Inverse Pyramid Structure and Dense MLP Block\\u00a7r\\n\\n\\u00a78\\u00a7oYunhao Chen\\nYunjie Zhu\\nZihui Yan\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02940\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 31 May 2023 03:27:32 GMT)\\u00a7r"}']}
{title:'Sheffer et al. (§72023§r)', author: 'Roy Sheffer; Yossi Adi', display:{Lore:['[{"text": "arXiv:2211.03089", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lI Hear Your True Colors: Image Guided Audio Generation\\u00a7r\\n\\n\\u00a78\\u00a7oRoy Sheffer\\nYossi Adi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.03089\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Feb 2023 11:15:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2023\\u00a7r"}']}
{title:'Sharma et al. (§72023§r)', author: 'Roshan Sharma; Weipeng He; Ju Lin; Egor Lakomkin; Yang Liu; Kaustubh Kalgaonkar', display:{Lore:['[{"text": "arXiv:2211.03643", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEgocentric Audio-Visual Noise Suppression\\u00a7r\\n\\n\\u00a78\\u00a7oRoshan Sharma\\nWeipeng He\\nJu Lin\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.03643\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 3 May 2023 02:34:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Nguyen et al. (§72023§r)', author: 'Viet-Anh Nguyen; Anh H. T. Nguyen; Andy W. H. Khong', display:{Lore:['[{"text": "arXiv:2211.04071", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving performance of real-time full-band blind packet-loss concealment with predictive network\\u00a7r\\n\\n\\u00a78\\u00a7oViet-Anh Nguyen\\nAnh H. T. Nguyen\\nAndy W. H. Khong\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.04071\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10097132\\u00a7r\\n\\nVersion:\\u00a77v6 (Fri, 12 May 2023 08:29:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings ICASSP 2023, 5 pages, 1 figure, 4 tables\\u00a7r"}']}
{title:'Ratnarajah et al. (§72023§r)', author: 'Anton Ratnarajah; Ishwarya Ananthabhotla; Vamsi Krishna Ithapu; Pablo Hoffmann; Dinesh Manocha; Paul Calamia', display:{Lore:['[{"text": "arXiv:2211.04473", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Improved Room Impulse Response Estimation for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAnton Ratnarajah\\nIshwarya Ananthabhotla\\nVamsi Krishna Ithapu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.04473\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 19 Mar 2023 20:23:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2023. More results are available athttps://anton-jeran.github.io/S2IR/\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Yu Chen; Wen Ding; Junjie Lai', display:{Lore:['[{"text": "arXiv:2211.04717", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Noisy Student Training on Non-target Domain Data for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYu Chen\\nWen Ding\\nJunjie Lai\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.04717\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 1 Mar 2023 17:59:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper is accepted by the ICASSP 2023 conference\\u00a7r"}']}
{title:'Schmid et al. (§72023§r)', author: 'Florian Schmid; Khaled Koutini; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2211.04772", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Large-scale Audio Tagging via Transformer-to-CNN Knowledge Distillation\\u00a7r\\n\\n\\u00a78\\u00a7oFlorian Schmid\\nKhaled Koutini\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.04772\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 23 Jun 2023 07:21:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2023. Source Code available at: https://github.com/fschmid56/EfficientAT\\u00a7r"}']}
{title:'Goudarzi (§72023§r)', author: 'Armin Goudarzi', display:{Lore:['[{"text": "arXiv:2211.04921", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGlobal, and Local Optimization Beamforming for Broadband Sources\\u00a7r\\n\\n\\u00a78\\u00a7oArmin Goudarzi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.04921\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 25 Oct 2023 17:43:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to JASA\\u00a7r"}']}
{title:'Zhao et al. (§72023§r)', author: 'Yan Zhao; Jiangyan Yi; Jianhua Tao; Chenglong Wang; Xiaohui Zhang; Yongfeng Dong', display:{Lore:['[{"text": "arXiv:2211.05363", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmoFake: An Initial Dataset for Emotion Fake Audio Detection\\u00a7r\\n\\n\\u00a78\\u00a7oYan Zhao\\nJiangyan Yi\\nJianhua Tao\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.05363\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 14 Sep 2023 08:56:11 GMT)\\u00a7r"}']}
{title:'Narita et al. (§72023§r)', author: 'Gaku Narita; Junichi Shimizu; Taketo Akama', display:{Lore:['[{"text": "arXiv:2211.05385", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGANStrument: Adversarial Instrument Sound Synthesis with Pitch-invariant Instance Conditioning\\u00a7r\\n\\n\\u00a78\\u00a7oGaku Narita\\nJunichi Shimizu\\nTaketo Akama\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.05385\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 7 Mar 2023 05:54:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, Accepted to 2023 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Audio examples: https://ganstrument.github.io/ganstrument-demo/\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Ya-Jie Zhang; Wei Song; Yanghao Yue; Zhengchen Zhang; Youzheng Wu; Xiaodong He', display:{Lore:['[{"text": "arXiv:2211.06170", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMaskedSpeech: Context-aware Speech Synthesis with Masking Strategy\\u00a7r\\n\\n\\u00a78\\u00a7oYa-Jie Zhang\\nWei Song\\nYanghao Yue\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.06170\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 18 May 2023 07:24:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in Interspeech 2023\\u00a7r"}']}
{title:'Webber et al. (§72023§r)', author: 'Jacob J Webber; Cassia Valentini-Botinhao; Evelyn Williams; Gustav Eje Henter; Simon King', display:{Lore:['[{"text": "arXiv:2211.06989", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutovocoder: Fast Waveform Generation from a Learned Speech Representation using Differentiable Digital Signal Processing\\u00a7r\\n\\n\\u00a78\\u00a7oJacob J Webber\\nCassia Valentini-Botinhao\\nEvelyn Williams\\nGustav Eje Henter\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.06989\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10095729\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2023 - 2023 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-5\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 24 May 2023 11:23:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the 2023 IEEE InternationalConference on Acoustics, Speech, and Signal Processing (ICASSP 2023)\\u00a7r"}']}
{title:'Jeon et al. (§72023§r)', author: 'Chang-Bin Jeon; Hyeongi Moon; Keunwoo Choi; Ben Sangbae Chon; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2211.07302", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMedleyVox: An Evaluation Dataset for Multiple Singing Voices Separation\\u00a7r\\n\\n\\u00a78\\u00a7oChang-Bin Jeon\\nHyeongi Moon\\nKeunwoo Choi\\nBen Sangbae Chon\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.07302\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 4 May 2023 14:13:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 6 tables, To appear in ICASSP 2023 (camera-ready version)\\u00a7r"}']}
{title:'Ye et al. (§72023§r)', author: 'Jiaxin Ye; Xin-cheng Wen; Yujie Wei; Yong Xu; Kunhong Liu; Hongming Shan', display:{Lore:['[{"text": "arXiv:2211.08233", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTemporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJiaxin Ye\\nXin-cheng Wen\\nYujie Wei\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08233\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10096370\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE ICASSP 2023\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 14 Aug 2023 11:50:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Singh et al. (§72023§r)', author: 'Rajsuryan Singh; Pablo Zinemanas; Xavier Serra; Juan Pablo Bello; Magdalena Fuentes', display:{Lore:['[{"text": "arXiv:2211.08367", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFlowGrad: Using Motion for Visual Sound Source Localization\\u00a7r\\n\\n\\u00a78\\u00a7oRajsuryan Singh\\nPablo Zinemanas\\nXavier Serra\\nJuan Pablo Bello\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08367\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 14 Apr 2023 18:14:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ICASSP 2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Kuan-Lin Chen; Daniel D. E. Wong; Ke Tan; Buye Xu; Anurag Kumar; Vamsi Krishna Ithapu', display:{Lore:['[{"text": "arXiv:2211.08624", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Heteroscedastic Uncertainty in Learning Complex Spectral Mapping for Single-channel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oKuan-Lin Chen\\nDaniel D. E. Wong\\nKe Tan\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08624\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 8 Mar 2023 11:09:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages. Accepted at ICASSP 2023\\u00a7r"}']}
{title:'Wagner et al. (§72023§r)', author: 'Dominik Wagner; Ilja Baumann; Sebastian P. Bayerl; Korbinian Riedhammer; Tobias Bocklet', display:{Lore:['[{"text": "arXiv:2211.08774", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Adaptation for End-To-End Speech Recognition Systems in Noisy Environments\\u00a7r\\n\\n\\u00a78\\u00a7oDominik Wagner\\nIlja Baumann\\nSebastian P. Bayerl\\nKorbinian Riedhammer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08774\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 7 Dec 2023 09:32:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ASRU 2023\\u00a7r"}']}
{title:'Qu et al. (§72023§r)', author: 'Leyuan Qu; Wei Wang; Cornelius Weber; Pengcheng Yue; Taihao Li; Stefan Wermter', display:{Lore:['[{"text": "arXiv:2211.08843", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Speech Emotion Recognition with Unsupervised Speaking Style Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oLeyuan Qu\\nWei Wang\\nCornelius Weber\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08843\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 28 Dec 2023 11:09:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2024\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Lei Wang; Ziyi Zhao; Hanwei Liu; Junwei Pang; Yi Qin; Qidi Wu', display:{Lore:['[{"text": "arXiv:2211.09124", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Review of Intelligent Music Generation Systems\\u00a7r\\n\\n\\u00a78\\u00a7oLei Wang\\nZiyi Zhao\\nHanwei Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.09124\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 17 Nov 2023 13:11:53 GMT)\\u00a7r"}']}
{title:'Yuan et al. (§72023§r)', author: 'Xin Yuan; Robin Feng; Mingming Ye', display:{Lore:['[{"text": "arXiv:2211.09365", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Resource Mongolian Speech Synthesis Based on Automatic Prosody Annotation\\u00a7r\\n\\n\\u00a78\\u00a7oXin Yuan\\nRobin Feng\\nMingming Ye\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.09365\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 4 Jan 2023 09:51:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by NCMMSC 2022\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Jianwei Zhang; Julie Liss; Suren Jayasuriya; Visar Berisha', display:{Lore:['[{"text": "arXiv:2211.09858", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Vocal Quality Feature Embeddings for Dysphonic Voice Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJianwei Zhang\\nJulie Liss\\nSuren Jayasuriya\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.09858\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 26 Jan 2023 15:22:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis manuscript is submitted on July 06, 2022 to IEEE/ACM Transactionson Audio, Speech, and Language Processing for peer-review\\u00a7r"}']}
{title:'Mira et al. (§72023§r)', author: 'Rodrigo Mira; Buye Xu; Jacob Donley; Anurag Kumar; Stavros Petridis; Vamsi Krishna Ithapu; Maja Pantic', display:{Lore:['[{"text": "arXiv:2211.10999", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLA-VocE: Low-SNR Audio-visual Speech Enhancement using Neural Vocoders\\u00a7r\\n\\n\\u00a78\\u00a7oRodrigo Mira\\nBuye Xu\\nJacob Donley\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.10999\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 13 Mar 2023 16:51:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to ICASSP 2023\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Shangda Wu; Maosong Sun', display:{Lore:['[{"text": "arXiv:2211.11216", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring the Efficacy of Pre-trained Checkpoints in Text-to-Music Generation Task\\u00a7r\\n\\n\\u00a78\\u00a7oShangda Wu\\nMaosong Sun\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.11216\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 4 Jan 2023 01:06:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the Creative AI Across Modalities workshop at AAAI 2023\\u00a7r"}']}
{title:'Bralios et al. (§72023§r)', author: 'Dimitrios Bralios; Efthymios Tzinis; Gordon Wichern; Paris Smaragdis; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2211.11917", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLatent Iterative Refinement for Modular Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oDimitrios Bralios\\nEfthymios Tzinis\\nGordon Wichern\\nParis Smaragdis\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.11917\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10096897\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2023 - 2023 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 16 Oct 2023 03:06:50 GMT)\\u00a7r"}']}
{title:'Jiang et al. (§72023§r)', author: 'Xue Jiang; Xiulian Peng; Yuan Zhang; Yan Lu', display:{Lore:['[{"text": "arXiv:2211.11960", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDisentangled Feature Learning for Real-Time Neural Speech Coding\\u00a7r\\n\\n\\u00a78\\u00a7oXue Jiang\\nXiulian Peng\\nYuan Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.11960\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 25 Feb 2023 02:30:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023 (Accepted)\\u00a7r"}']}
{title:'Fang et al. (§72023§r)', author: 'Zhihua Fang; Liang He; Hanhan Ma; Xiaochen Guo; Lin Li', display:{Lore:['[{"text": "arXiv:2211.12080", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Training for Speaker Verification against Noisy Labels\\u00a7r\\n\\n\\u00a78\\u00a7oZhihua Fang\\nLiang He\\nHanhan Ma\\nXiaochen Guo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.12080\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 May 2023 16:17:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Mandel et al. (§72023§r)', author: 'Moshe Mandel; Or Tal; Yossi Adi', display:{Lore:['[{"text": "arXiv:2211.12232", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAERO: Audio Super Resolution in the Spectral Domain\\u00a7r\\n\\n\\u00a78\\u00a7oMoshe Mandel\\nOr Tal\\nYossi Adi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.12232\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 26 Feb 2023 22:18:24 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Zhong-Qiu Wang; Samuele Cornell; Shukjae Choi; Younglo Lee; Byeong-Yeol Kim; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2211.12433", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTF-GridNet: Integrating Full- and Sub-Band Modeling for Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oZhong-Qiu Wang\\nSamuele Cornell\\nShukjae Choi\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.12433\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 4 Aug 2023 17:44:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn IEEE/ACM Transactionson Audio, Speech, and Language Processing. A sound demo is available at https://zqwang7.github.io/demos/TF-GridNet-demo/index.html, and the code is available at https://github.com/espnet/espnet"}','{"text": "/pull/5395\\u00a7r"}']}
{title:'Jin et al. (§72023§r)', author: 'Mumin Jin; Prashant Serai; Jilong Wu; Andros Tjandra; Vimal Manohar; Qing He', display:{Lore:['[{"text": "arXiv:2211.13282", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice-preserving Zero-shot Multiple Accent Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oMumin Jin\\nPrashant Serai\\nJilong Wu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.13282\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 14 Oct 2023 06:27:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEICASSP 2023\\u00a7r"}']}
{title:'Shi et al. (§72023§r)', author: 'Xuan Shi; Erica Cooper; Xin Wang; Junichi Yamagishi; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2211.13868", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCan Knowledge of End-to-End Text-to-Speech Models Improve Neural MIDI-to-Audio Synthesis Systems?\\u00a7r\\n\\n\\u00a78\\u00a7oXuan Shi\\nErica Cooper\\nXin Wang\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.13868\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 21 Mar 2023 02:50:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Koutini et al. (§72023§r)', author: 'Khaled Koutini; Shahed Masoudian; Florian Schmid; Hamid Eghbal-zadeh; Jan Schlüter; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2211.13956", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning General Audio Representations with Large-Scale Training of Patchout Audio Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oKhaled Koutini\\nShahed Masoudian\\nFlorian Schmid\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.13956\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of Machine Learning Research v166 (2022) 65-89\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 2 Mar 2023 10:53:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7owill apear in HEAR: Holistic Evaluation of Audio Representations Proceedings of MachineLearning Research PMLR 166. Source code: https://github.com/kkoutini/passt_hear21\\u00a7r"}']}
{title:'Watts et al. (§72023§r)', author: 'Oliver Watts; Lovisa Wihlborg; Cassia Valentini-Botinhao', display:{Lore:['[{"text": "arXiv:2211.14130", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPuffin: pitch-synchronous neural waveform generation for fullband speech on modest devices\\u00a7r\\n\\n\\u00a78\\u00a7oOliver Watts\\nLovisa Wihlborg\\nCassia Valentini-Botinhao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.14130\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10094729\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 8 Jun 2023 12:38:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Ai et al. (§72023§r)', author: 'Yang Ai; Zhen-Hua Ling', display:{Lore:['[{"text": "arXiv:2211.15974", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Speech Phase Prediction based on Parallel Estimation Architecture and Anti-Wrapping Losses\\u00a7r\\n\\n\\u00a78\\u00a7oYang Ai\\nZhen-Hua Ling\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.15974\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 16 Feb 2023 09:15:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023. Codes areavailable\\u00a7r"}']}
{title:'Hsu et al. (§72023§r)', author: 'Tsu-Yuan Hsu; Chen-An Li; Tung-Yu Wu; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2211.16044", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModel Extraction Attack against Self-supervised Speech Models\\u00a7r\\n\\n\\u00a78\\u00a7oTsu-Yuan Hsu\\nChen-An Li\\nTung-Yu Wu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.16044\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 8 Oct 2023 09:41:22 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Andong Li; Guochen Yu; Chengshi Zheng; Wenzhe Liu; Xiaodong Li', display:{Lore:['[{"text": "arXiv:2211.16764", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA General Unfolding Speech Enhancement Method Motivated by Taylor\'s Theorem\\u00a7r\\n\\n\\u00a78\\u00a7oAndong Li\\nGuochen Yu\\nChengshi Zheng\\nWenzhe Liu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.16764\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 28 Mar 2023 04:33:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to TASLP, revised version,17 pages\\u00a7r"}']}
{title:'Srivastava et al. (§72023§r)', author: 'Prerak Srivastava; Antoine Deleforge; Archontis Politis; Emmanuel Vincent', display:{Lore:['[{"text": "arXiv:2211.16958", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHow to (virtually) train your speaker localizer\\u00a7r\\n\\n\\u00a78\\u00a7oPrerak Srivastava\\nAntoine Deleforge\\nArchontis Politis\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.16958\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 May 2023 14:51:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in INTERSPEECH 2023\\u00a7r"}']}
{title:'Tulchinskii et al. (§72023§r)', author: 'Eduard Tulchinskii; Kristian Kuznetsov; Laida Kushnareva; Daniil Cherniavskii; Serguei Barannikov; Irina Piontkovskaya; Sergey Nikolenko; Evgeny Burnaev', display:{Lore:['[{"text": "arXiv:2211.17223", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a72math.AT\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTopological Data Analysis for Speech Processing\\u00a7r\\n\\n\\u00a78\\u00a7oEduard Tulchinskii\\nKristian Kuznetsov\\nLaida Kushnareva\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.17223\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1861\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. INTERSPEECH 2023, pages 311--315\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 6 Jun 2023 11:25:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2023 conference\\u00a7r"}']}
{title:'Ochieng (§72023§r)', author: 'Peter Ochieng', display:{Lore:['[{"text": "arXiv:2212.00369", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep neural network techniques for monaural speech enhancement: state of the art analysis\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Ochieng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.00369\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 20 Jun 2023 14:23:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oconference\\u00a7r"}']}
{title:'Wagner et al. (§72023§r)', author: 'Dominik Wagner; Sebastian P. Bayerl; Hector A. Cordourier Maruri; Tobias Bocklet', display:{Lore:['[{"text": "arXiv:2212.01775", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerative Models for Improved Naturalness, Intelligibility, and Voicing of Whispered Speech\\u00a7r\\n\\n\\u00a78\\u00a7oDominik Wagner\\nSebastian P. Bayerl\\nHector A. Cordourier Maruri\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.01775\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/SLT54892.2023.10022796\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 30 Jan 2023 10:03:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at SLT 2022\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Chang Liu; Jie Zhang; Han Fang; Zehua Ma; Weiming Zhang; Nenghai Yu', display:{Lore:['[{"text": "arXiv:2212.02339", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeAR: A Deep-learning-based Audio Re-recording Resilient Watermarking\\u00a7r\\n\\n\\u00a78\\u00a7oChang Liu\\nJie Zhang\\nHan Fang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.02339\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 3 Apr 2023 06:33:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by AAAI2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Chen Chen; Yuchen Hu; Qiang Zhang; Heqing Zou; Beier Zhu; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2212.05301", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Modality-specific Representations for Audio-visual Speech Recognition via Reinforcement Learning\\u00a7r\\n\\n\\u00a78\\u00a7oChen Chen\\nYuchen Hu\\nQiang Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.05301\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 2 Feb 2023 09:30:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by AAAI2023\\u00a7r"}']}
{title:'Qu et al. (§72023§r)', author: 'Leyuan Qu; Taihao Li; Cornelius Weber; Theresa Pekarek-Rosin; Fuji Ren; Stefan Wermter', display:{Lore:['[{"text": "arXiv:2212.06972", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDisentangling Prosody Representations with Unsupervised Speech Reconstruction\\u00a7r\\n\\n\\u00a78\\u00a7oLeyuan Qu\\nTaihao Li\\nCornelius Weber\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.06972\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 26 Sep 2023 02:42:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE/ACM Transactionson Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Dong et al. (§72023§r)', author: 'Hao-Wen Dong; Naoya Takahashi; Yuki Mitsufuji; Julian McAuley; Taylor Berg-Kirkpatrick', display:{Lore:['[{"text": "arXiv:2212.07065", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCLIPSep: Learning Text-queried Sound Separation with Noisy Unlabeled Videos\\u00a7r\\n\\n\\u00a78\\u00a7oHao-Wen Dong\\nNaoya Takahashi\\nYuki Mitsufuji\\nJulian McAuley\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.07065\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 3 Mar 2023 08:37:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICLR 2023. Audio samples can be found at https://sony.github.io/CLIPSep/\\u00a7r"}']}
{title:'Budd et al. (§72023§r)', author: 'Jobie Budd; Kieran Baker; Emma Karoune; Harry Coppock; Selina Patel; Ana Tendero Cañadas; Alexander Titcomb; Richard Payne; David Hurley; Sabrina Egglestone; Lorraine Butler; Jonathon Mellor; George Nicholson; Ivan Kiskin; Vasiliki Koutra; Radka Jersakova; Rachel A. McKendry; Peter Diggle; Sylvia Richardson; Björn W. Schuller; Steven Gilmour; Davide Pigoli; Stephen Roberts; Josef Packham; Tracey Thornley; Chris Holmes', display:{Lore:['[{"text": "arXiv:2212.07738", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA large-scale and PCR-referenced vocal audio dataset for COVID-19\\u00a7r\\n\\n\\u00a78\\u00a7oJobie Budd\\nKieran Baker\\nEmma Karoune\\n+ 22 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.07738\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 3 Nov 2023 11:28:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o39 pages, 4 figures\\u00a7r"}']}
{title:'Coppock et al. (§72023§r)', author: 'Harry Coppock; George Nicholson; Ivan Kiskin; Vasiliki Koutra; Kieran Baker; Jobie Budd; Richard Payne; Emma Karoune; David Hurley; Alexander Titcomb; Sabrina Egglestone; Ana Tendero Cañadas; Lorraine Butler; Radka Jersakova; Jonathon Mellor; Selina Patel; Tracey Thornley; Peter Diggle; Sylvia Richardson; Josef Packham; Björn W. Schuller; Davide Pigoli; Steven Gilmour; Stephen Roberts; Chris Holmes', display:{Lore:['[{"text": "arXiv:2212.08570", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-based AI classifiers show no evidence of improved COVID-19 screening over simple symptoms checkers\\u00a7r\\n\\n\\u00a78\\u00a7oHarry Coppock\\nGeorge Nicholson\\nIvan Kiskin\\n+ 21 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.08570\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 2 Mar 2023 18:12:11 GMT)\\u00a7r"}']}
{title:'Pigoli et al. (§72023§r)', author: 'Davide Pigoli; Kieran Baker; Jobie Budd; Lorraine Butler; Harry Coppock; Sabrina Egglestone; Steven G. Gilmour; Chris Holmes; David Hurley; Radka Jersakova; Ivan Kiskin; Vasiliki Koutra; Jonathon Mellor; George Nicholson; Joe Packham; Selina Patel; Richard Payne; Stephen J. Roberts; Björn W. Schuller; Ana Tendero-Cañadas; Tracey Thornley; Alexander Titcomb', display:{Lore:['[{"text": "arXiv:2212.08571", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.AP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStatistical Design and Analysis for Robust Machine Learning: A Case Study from COVID-19\\u00a7r\\n\\n\\u00a78\\u00a7oDavide Pigoli\\nKieran Baker\\nJobie Budd\\n+ 18 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.08571\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Feb 2023 13:39:00 GMT)\\u00a7r"}']}
{title:'Feng et al. (§72023§r)', author: 'Tiantian Feng; Rajat Hebbar; Nicholas Mehlman; Xuan Shi; Aditya Kommineni; and Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2212.09006", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Review of Speech-centric Trustworthy Machine Learning: Privacy, Safety, and Fairness\\u00a7r\\n\\n\\u00a78\\u00a7oTiantian Feng\\nRajat Hebbar\\nNicholas Mehlman\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.09006\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1561/116.00000084\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nAPSIPA Transactions on Signal and Information Processing, vol. 12,\\n  no. 3, 2023\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 16 Apr 2023 10:08:40 GMT)\\u00a7r"}']}
{title:'Maimon et al. (§72023§r)', author: 'Gallil Maimon; Yossi Adi', display:{Lore:['[{"text": "arXiv:2212.09730", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units\\u00a7r\\n\\n\\u00a78\\u00a7oGallil Maimon\\nYossi Adi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.09730\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 18 Oct 2023 19:23:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at EMNLP 2023\\u00a7r"}']}
{title:'Shougat et al. (§72023§r)', author: 'Md Raf E Ul Shougat; XiaoFu Li; Siyao Shao; Kathleen Walden McGarvey; Edmon Perkins', display:{Lore:['[{"text": "arXiv:2212.10370", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHopf Physical Reservoir Computer for Reconfigurable Sound Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMd Raf E Ul Shougat\\nXiaoFu Li\\nSiyao Shao\\nKathleen Walden McGarvey\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.10370\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 26 Jan 2023 19:22:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o21 pages, 11 figures\\u00a7r"}']}
{title:'Sudo et al. (§72023§r)', author: 'Yui Sudo; Muhammad Shakeel; Brian Yan; Jiatong Shi; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2212.10818", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l4D ASR: Joint modeling of CTC, Attention, Transducer, and Mask-Predict decoders\\u00a7r\\n\\n\\u00a78\\u00a7oYui Sudo\\nMuhammad Shakeel\\nBrian Yan\\nJiatong Shi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.10818\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 29 May 2023 23:16:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERRSPEECH2023\\u00a7r"}']}
{title:'He et al. (§72023§r)', author: 'Zihao He; Weituo Hao; Wei-Tsung Lu; Changyou Chen; Kristina Lerman; Xuchen Song', display:{Lore:['[{"text": "arXiv:2212.10901", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lALCAP: Alignment-Augmented Music Captioner\\u00a7r\\n\\n\\u00a78\\u00a7oZihao He\\nWeituo Hao\\nWei-Tsung Lu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.10901\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 22 Oct 2023 02:12:16 GMT)\\u00a7r"}']}
{title:'Kawa et al. (§72023§r)', author: 'Piotr Kawa; Marcin Plata; Piotr Syga', display:{Lore:['[{"text": "arXiv:2212.14597", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDefense Against Adversarial Attacks on Audio DeepFake Detection\\u00a7r\\n\\n\\u00a78\\u00a7oPiotr Kawa\\nMarcin Plata\\nPiotr Syga\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.14597\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 10 Jun 2023 18:48:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2023\\u00a7r"}']}
{title:'Ince et al. (§72023§r)', author: 'Turker Ince; Serkan Kiranyaz; Ozer Can Devecioglu; Muhammad Salman Khan; Muhammad Chowdhury; Moncef Gabbouj', display:{Lore:['[{"text": "arXiv:2212.14618", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBlind Restoration of Real-World Audio by 1D Operational GANs\\u00a7r\\n\\n\\u00a78\\u00a7oTurker Ince\\nSerkan Kiranyaz\\nOzer Can Devecioglu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.14618\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 20 Jan 2023 18:03:53 GMT)\\u00a7r"}']}
{title:'Buhl (§72023§r)', author: 'Fred W. Buhl', display:{Lore:['[{"text": "arXiv:2301.00508", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmoGator: A New Open Source Vocal Burst Dataset with Baseline Machine Learning Classification Methodologies\\u00a7r\\n\\n\\u00a78\\u00a7oFred W. Buhl\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.00508\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 6 Apr 2023 15:25:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 4 tables, 2 figures\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Li Zhang; Chris Callison-Burch', display:{Lore:['[{"text": "arXiv:2301.01162", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLanguage Models are Drummers: Drum Composition with Natural Language Pre-Training\\u00a7r\\n\\n\\u00a78\\u00a7oLi Zhang\\nChris Callison-Burch\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.01162\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 Jan 2023 15:47:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the 1st workshop on Creative AI across Modalities in AAAI 2023\\u00a7r"}']}
{title:'Calik et al. (§72023§r)', author: 'Sukru Selim Calik; Ayhan Kucukmanisa; Zeynep Hilal Kilimci', display:{Lore:['[{"text": "arXiv:2301.01378", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn ensemble-based framework for mispronunciation detection of Arabic phonemes\\u00a7r\\n\\n\\u00a78\\u00a7oSukru Selim Calik\\nAyhan Kucukmanisa\\nZeynep Hilal Kilimci\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.01378\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 Jan 2023 22:17:08 GMT)\\u00a7r"}']}
{title:'Sturm et al. (§72023§r)', author: 'Bob L. T. Sturm; Arthur Flexer', display:{Lore:['[{"text": "arXiv:2301.01578", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lValidity in Music Information Research Experiments\\u00a7r\\n\\n\\u00a78\\u00a7oBob L. T. Sturm\\nArthur Flexer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.01578\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 4 Jan 2023 12:52:47 GMT)\\u00a7r"}']}
{title:'Keshari (§72023§r)', author: 'Abhinav Kaushal Keshari', display:{Lore:['[{"text": "arXiv:2301.02385", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Genre Music Transformer \\u2013 Composing Full Length Musical Piece\\u00a7r\\n\\n\\u00a78\\u00a7oAbhinav Kaushal Keshari\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.02385\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 Jan 2023 05:27:55 GMT)\\u00a7r"}']}
{title:'Liao et al. (§72023§r)', author: 'Callie C. Liao; Duoduo Liao; Jesse Guessford', display:{Lore:['[{"text": "arXiv:2301.02732", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultimodal Lyrics-Rhythm Matching\\u00a7r\\n\\n\\u00a78\\u00a7oCallie C. Liao\\nDuoduo Liao\\nJesse Guessford\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.02732\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/BigData55660.2022.10021009\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 15 Mar 2023 01:12:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by 2022 IEEE InternationalConference on Big Data(IEEE Big Data 2022)\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Shangda Wu; Xiaobing Li; Feng Yu; Maosong Sun', display:{Lore:['[{"text": "arXiv:2301.02884", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTunesFormer: Forming Irish Tunes with Control Codes by Bar Patching\\u00a7r\\n\\n\\u00a78\\u00a7oShangda Wu\\nXiaobing Li\\nFeng Yu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.02884\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 12 Dec 2023 13:47:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 1 figure, 1 table, accepted by HCMIR 2023\\u00a7r"}']}
{title:'Han et al. (§72023§r)', author: 'Han Han; Vincent Lostanlen; Mathieu Lagrange', display:{Lore:['[{"text": "arXiv:2301.02886", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerceptual-Neural-Physical Sound Matching\\u00a7r\\n\\n\\u00a78\\u00a7oHan Han\\nVincent Lostanlen\\nMathieu Lagrange\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.02886\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 13 Mar 2023 17:16:37 GMT)\\u00a7r"}']}
{title:'Pizzi et al. (§72023§r)', author: 'Karla Pizzi; Franziska Boenisch; Ugur Sahin; Konstantin Böttinger', display:{Lore:['[{"text": "arXiv:2301.03206", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntroducing Model Inversion Attacks on Automatic Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oKarla Pizzi\\nFranziska Boenisch\\nUgur Sahin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.03206\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/SPSC.2022-3\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. 2nd Symposium on Security and Privacy in Speech\\n  Communication, 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 9 Jan 2023 08:51:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ofor associated pdf, see https://www.isca-speech.org/archive/pdfs/spsc_2022/pizzi22_spsc.pdf\\u00a7r"}']}
{title:'Shahid et al. (§72023§r)', author: 'Abdullah Shahid; Siddique Latif; Junaid Qadir', display:{Lore:['[{"text": "arXiv:2301.03751", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerative Emotional AI for Speech Emotion Recognition: The Case for Synthetic Emotional Speech Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oAbdullah Shahid\\nSiddique Latif\\nJunaid Qadir\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.03751\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Jan 2023 02:03:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Haogeng Liu; Tao Wang; Ruibo Fu; Jiangyan Yi; Zhengqi Wen; Jianhua Tao', display:{Lore:['[{"text": "arXiv:2301.03801", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnifySpeech: A Unified Framework for Zero-shot Text-to-Speech and Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oHaogeng Liu\\nTao Wang\\nRuibo Fu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.03801\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Jan 2023 06:06:57 GMT)\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Haibin Wu; Ke Tan; Buye Xu; Anurag Kumar; Daniel Wong', display:{Lore:['[{"text": "arXiv:2301.04320", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRethinking complex-valued deep neural networks for monaural speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oHaibin Wu\\nKe Tan\\nBuye Xu\\nAnurag Kumar\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.04320\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 11 Jan 2023 05:59:50 GMT)\\u00a7r"}']}
{title:'Close et al. (§72023§r)', author: 'George Close; William Ravenscroft; Thomas Hain; Stefan Goetze', display:{Lore:['[{"text": "arXiv:2301.04388", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerceive and predict: self-supervised speech representation based loss functions for speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oGeorge Close\\nWilliam Ravenscroft\\nThomas Hain\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.04388\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10095666\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2023 - 2023 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 26 Jun 2023 09:31:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, accepted at ICASSP 2023\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Kejun Zhang; Xinda Wu; Tieyao Zhang; Zhijie Huang; Xu Tan; Qihao Liang; Songruoyao Wu; Lingyun Sun', display:{Lore:['[{"text": "arXiv:2301.04488", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWuYun: Exploring hierarchical skeleton-guided melody generation using knowledge-enhanced deep learning\\u00a7r\\n\\n\\u00a78\\u00a7oKejun Zhang\\nXinda Wu\\nTieyao Zhang\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.04488\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 14 Mar 2023 09:23:25 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Yuran Zhang; Jiajie Zou; Nai Ding', display:{Lore:['[{"text": "arXiv:2301.05898", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic correlates of the syllabic rhythm of speech: Modulation spectrum or local features of the temporal envelope\\u00a7r\\n\\n\\u00a78\\u00a7oYuran Zhang\\nJiajie Zou\\nNai Ding\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.05898\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.neubiorev.2023.105111\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 14 Jan 2023 11:32:52 GMT)\\u00a7r"}']}
{title:'Jin et al. (§72023§r)', author: 'Xin Jin; Wu Zhou; Jinyu Wang; Duo Xu; Yiqing Rong; Shuai Cui', display:{Lore:['[{"text": "arXiv:2301.05908", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Order-Complexity Model for Aesthetic Quality Assessment of Symbolic Homophony Music Scores\\u00a7r\\n\\n\\u00a78\\u00a7oXin Jin\\nWu Zhou\\nJinyu Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.05908\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 14 Jan 2023 12:30:16 GMT)\\u00a7r"}']}
{title:'Melms et al. (§72023§r)', author: 'Leander Melms; Robert R. Ilesan; Ulrich Köhler; Olaf Hildebrandt; Regina Conradt; Jens Eckstein; Cihan Atila; Sami Matrood; Bernhard Schieffer; Jürgen R. Schaefer; Tobias Müller; Julius Obergassel; Nadine Schlicker; Martin C. Hirsch', display:{Lore:['[{"text": "arXiv:2301.06078", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTraining one model to detect heart and lung sound events from single point auscultations\\u00a7r\\n\\n\\u00a78\\u00a7oLeander Melms\\nRobert R. Ilesan\\nUlrich K\\u00f6hler\\n+ 10 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.06078\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 15 Jan 2023 12:13:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages, 8 figures\\u00a7r"}']}
{title:'Kilpatrick (§72023§r)', author: 'Alexander Kilpatrick', display:{Lore:['[{"text": "arXiv:2301.06211", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhat artificial intelligence might teach us about the origin of human language\\u00a7r\\n\\n\\u00a78\\u00a7oAlexander Kilpatrick\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.06211\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 15 Jan 2023 23:25:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICPHS2023 Conference Submission.5 pages\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Kai Liu; Xucheng Wan; Ziqing Du; Huan Zhou', display:{Lore:['[{"text": "arXiv:2301.06277", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Target Speaker Extraction with Sparse LDA-transformed Speaker Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oKai Liu\\nXucheng Wan\\nZiqing Du\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.06277\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 16 Jan 2023 06:30:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oACCEPTED by NCMMSC 2022\\u00a7r"}']}
{title:'Maina (§72023§r)', author: 'Kinyugo Maina', display:{Lore:['[{"text": "arXiv:2301.06468", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMsanii: High Fidelity Music Synthesis on a Shoestring Budget\\u00a7r\\n\\n\\u00a78\\u00a7oKinyugo Maina\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.06468\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 16 Jan 2023 15:18:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 8 figures, for demo see https://kinyugo.github.io/msanii-demo/ and for code, see https://github.com/Kinyugo/msanii, this paper is a work in progress\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Zhanheng Yang; Sining Sun; Xiong Wang; Yike Zhang; Long Ma; Lei Xie', display:{Lore:['[{"text": "arXiv:2301.06735", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTwo Stage Contextual Word Filtering for Context bias in Unified Streaming and Non-streaming Transducer\\u00a7r\\n\\n\\u00a78\\u00a7oZhanheng Yang\\nSining Sun\\nXiong Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.06735\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 8 Jun 2023 13:29:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by interspeech 2023\\u00a7r"}']}
{title:'Tevissen et al. (§72023§r)', author: 'Yannis Tevissen; Jérôme Boudy; Frédéric Petitpont', display:{Lore:['[{"text": "arXiv:2301.07491", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Newsbridge -Telecom SudParis VoxCeleb Speaker Recognition Challenge 2022 System Description\\u00a7r\\n\\n\\u00a78\\u00a7oYannis Tevissen\\nJ\\u00e9r\\u00f4me Boudy\\nFr\\u00e9d\\u00e9ric Petitpont\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.07491\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 17 Jan 2023 15:52:39 GMT)\\u00a7r"}']}
{title:'Natsiou et al. (§72023§r)', author: "Anastasia Natsiou; Luca Longo; Sean O'Leary", display:{Lore:['[{"text": "arXiv:2301.07665", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn investigation of the reconstruction capacity of stacked convolutional autoencoders for log-mel-spectrograms\\u00a7r\\n\\n\\u00a78\\u00a7oAnastasia Natsiou\\nLuca Longo\\nSean O\'Leary\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.07665\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 18 Jan 2023 17:19:04 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Chao-Han Huck Yang; Bo Li; Yu Zhang; Nanxin Chen; Rohit Prabhavalkar; Tara N. Sainath; Trevor Strohman', display:{Lore:['[{"text": "arXiv:2301.07851", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrom English to More Languages: Parameter-Efficient Model Reprogramming for Cross-Lingual Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oChao-Han Huck Yang\\nBo Li\\nYu Zhang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.07851\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10094903\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 19 Jan 2023 02:37:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023. The project was initiated in May 2022 during a research internship at Google Research\\u00a7r"}']}
{title:'Dang et al. (§72023§r)', author: 'Feng Dang; Qi Hu; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2301.07939", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTHLNet: two-stage heterogeneous lightweight network for monaural speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oFeng Dang\\nQi Hu\\nPengyuan Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.07939\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 19 May 2023 07:11:08 GMT)\\u00a7r"}']}
{title:'Dimolitsas et al. (§72023§r)', author: 'Ioannis Dimolitsas; Spyridon Kantarelis; Afroditi Fouka', display:{Lore:['[{"text": "arXiv:2301.07978", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpotHitPy: A Study For ML-Based Song Hit Prediction Using Spotify\\u00a7r\\n\\n\\u00a78\\u00a7oIoannis Dimolitsas\\nSpyridon Kantarelis\\nAfroditi Fouka\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.07978\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 19 Jan 2023 10:13:52 GMT)\\u00a7r"}']}
{title:'Lemke et al. (§72023§r)', author: 'Mathias Lemke; Lewin Stein', display:{Lore:['[{"text": "arXiv:2301.08620", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.flu-dyn\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdjoint-Based Identification of Sound Sources for Sound Reinforcement and Source Localization\\u00a7r\\n\\n\\u00a78\\u00a7oMathias Lemke\\nLewin Stein\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.08620\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-030-52429-6_17\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nNotes on Numerical Fluid Mechanics and Multidisciplinary Design,\\n  vol 145. Springer (2021)\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 20 Jan 2023 15:01:46 GMT)\\u00a7r"}']}
{title:'Shu et al. (§72023§r)', author: 'Amanda Shu; Hamza Khalid; Haohui Liu; Shikhar Agnihotri; Joseph Konan; Ojas Bhargave', display:{Lore:['[{"text": "arXiv:2301.09027", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCellular Network Speech Enhancement: Removing Background and Transmission Noise\\u00a7r\\n\\n\\u00a78\\u00a7oAmanda Shu\\nHamza Khalid\\nHaohui Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.09027\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 22 Jan 2023 00:18:10 GMT)\\u00a7r"}']}
{title:'M et al. (§72023§r)', author: 'Gurunath Reddy M; Zhe Zhang; Yi Yu; Florian Harscoet; Simon Canales; Suhua Tang', display:{Lore:['[{"text": "arXiv:2301.10015", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Attention-Based Alignment Network for Melody Generation from Incomplete Lyrics\\u00a7r\\n\\n\\u00a78\\u00a7oGurunath Reddy M\\nZhe Zhang\\nYi Yu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.10015\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 23 Jan 2023 03:41:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2011.06380\\u00a7r"}']}
{title:'Vahidi et al. (§72023§r)', author: 'Cyrus Vahidi; Han Han; Changhong Wang; Mathieu Lagrange; György Fazekas; Vincent Lostanlen', display:{Lore:['[{"text": "arXiv:2301.10183", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMesostructures: Beyond Spectrogram Loss in Differentiable Time-Frequency Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oCyrus Vahidi\\nHan Han\\nChanghong Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.10183\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 24 Jan 2023 17:50:19 GMT)\\u00a7r"}']}
{title:'Badlani et al. (§72023§r)', author: 'Rohan Badlani; Rafael Valle; Kevin J. Shih; João Felipe Santos; Siddharth Gururani; Bryan Catanzaro', display:{Lore:['[{"text": "arXiv:2301.10335", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultilingual Multiaccented Multispeaker TTS with RADTTS\\u00a7r\\n\\n\\u00a78\\u00a7oRohan Badlani\\nRafael Valle\\nKevin J. Shih\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.10335\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 24 Jan 2023 22:39:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, submitted to ICASSP 2023\\u00a7r"}']}
{title:'Triantafyllopoulos et al. (§72023§r)', author: 'Andreas Triantafyllopoulos; Alexander Kathan; Alice Baird; Lukas Christ; Alexander Gebhard; Maurice Gerczuk; Vincent Karas; Tobias Hübner; Xin Jing; Shuo Liu; Adria Mallol-Ragolta; Manuel Milling; Sandra Ottl; Anastasia Semertzidou; Srividya Tirunellai Rajamani; Tianhao Yan; Zijiang Yang; Judith Dineley; Shahin Amiriparian; Katrin D. Bartl-Pokorny; Anton Batliner; Florian B. Pokorny; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2301.10477", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CY\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHEAR4Health: A blueprint for making computer audition a staple of modern healthcare\\u00a7r\\n\\n\\u00a78\\u00a7oAndreas Triantafyllopoulos\\nAlexander Kathan\\nAlice Baird\\n+ 19 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.10477\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 25 Jan 2023 09:25:08 GMT)\\u00a7r"}']}
{title:'Gonzalez et al. (§72023§r)', author: 'Philippe Gonzalez; Tommy Sonne Alstrøm; Tobias May', display:{Lore:['[{"text": "arXiv:2301.10587", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Batching Variable Size Inputs for Training End-to-End Speech Enhancement Systems\\u00a7r\\n\\n\\u00a78\\u00a7oPhilippe Gonzalez\\nTommy Sonne Alstr\\u00f8m\\nTobias May\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.10587\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10097075\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 31 Mar 2023 11:37:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Agostinelli et al. (§72023§r)', author: 'Andrea Agostinelli; Timo I. Denk; Zalán Borsos; Jesse Engel; Mauro Verzetti; Antoine Caillon; Qingqing Huang; Aren Jansen; Adam Roberts; Marco Tagliasacchi; Matt Sharifi; Neil Zeghidour; Christian Frank', display:{Lore:['[{"text": "arXiv:2301.11325", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusicLM: Generating Music From Text\\u00a7r\\n\\n\\u00a78\\u00a7oAndrea Agostinelli\\nTimo I. Denk\\nZal\\u00e1n Borsos\\n+ 9 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.11325\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 26 Jan 2023 18:58:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSupplementary material at https://google-research.github.io/seanet/musiclm/examples and https://kaggle.com/datasets/googleai/musiccaps\\u00a7r"}']}
{title:'Mccloskey et al. (§72023§r)', author: 'Matthew Mccloskey; Gabrielle Curcio; Amulya Badineni; Kevin Mcgrath; Dimitris Papamichail', display:{Lore:['[{"text": "arXiv:2301.12084", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomated Arrangements of Multi-Part Music for Sets of Monophonic Instruments\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew Mccloskey\\nGabrielle Curcio\\nAmulya Badineni\\nKevin Mcgrath\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.12084\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 28 Jan 2023 04:13:45 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Shenghao Li; Jagmohan Chauhan', display:{Lore:['[{"text": "arXiv:2301.12209", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lwho is snoring? snore based user recognition\\u00a7r\\n\\n\\u00a78\\u00a7oShenghao Li\\nJagmohan Chauhan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.12209\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 28 Jan 2023 14:28:57 GMT)\\u00a7r"}']}
{title:'Shi et al. (§72023§r)', author: 'Xian Shi; Yanni Chen; Shiliang Zhang; Zhijie Yan', display:{Lore:['[{"text": "arXiv:2301.12343", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAchieving Timestamp Prediction While Recognizing with Non-Autoregressive End-to-End ASR Model\\u00a7r\\n\\n\\u00a78\\u00a7oXian Shi\\nYanni Chen\\nShiliang Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.12343\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 29 Jan 2023 03:47:59 GMT)\\u00a7r"}']}
{title:'Tralie (§72023§r)', author: 'Christopher J. Tralie', display:{Lore:['[{"text": "arXiv:2301.12354", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lArtistic Curve Steganography Carried by Musical Audio\\u00a7r\\n\\n\\u00a78\\u00a7oChristopher J. Tralie\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.12354\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 29 Jan 2023 04:15:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages, 14 figures, in Proceedings of EvoMUSART 2023\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Haohe Liu; Zehua Chen; Yi Yuan; Xinhao Mei; Xubo Liu; Danilo Mandic; Wenwu Wang; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:2301.12503", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudioLDM: Text-to-Audio Generation with Latent Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oHaohe Liu\\nZehua Chen\\nYi Yuan\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.12503\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 9 Sep 2023 15:27:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICML 2023. Demo and implementation at https://audioldm.github.io. Evaluation toolbox at https://github.com/haoheliu/audioldm_eval\\u00a7r"}']}
{title:'Malandro (§72023§r)', author: 'Martin E. Malandro', display:{Lore:['[{"text": "arXiv:2301.12525", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComposer\'s Assistant: An Interactive Transformer for Multi-Track MIDI Infilling\\u00a7r\\n\\n\\u00a78\\u00a7oMartin E. Malandro\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.12525\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 14 Jul 2023 20:53:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 6 figures, 3 tables. To be published in ISMIR 2023\\u00a7r"}']}
{title:'Huang et al. (§72023§r)', author: 'Rongjie Huang; Jiawei Huang; Dongchao Yang; Yi Ren; Luping Liu; Mingze Li; Zhenhui Ye; Jinglin Liu; Xiang Yin; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2301.12661", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMake-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oRongjie Huang\\nJiawei Huang\\nDongchao Yang\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.12661\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 Jan 2023 04:44:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAudio samples are available at https://Text-to-Audio.github.io\\u00a7r"}']}
{title:'Donahue et al. (§72023§r)', author: 'Chris Donahue; Antoine Caillon; Adam Roberts; Ethan Manilow; Philippe Esling; Andrea Agostinelli; Mauro Verzetti; Ian Simon; Olivier Pietquin; Neil Zeghidour; Jesse Engel', display:{Lore:['[{"text": "arXiv:2301.12662", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSingSong: Generating musical accompaniments from singing\\u00a7r\\n\\n\\u00a78\\u00a7oChris Donahue\\nAntoine Caillon\\nAdam Roberts\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.12662\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 Jan 2023 04:53:23 GMT)\\u00a7r"}']}
{title:'Schneider (§72023§r)', author: 'Flavio Schneider', display:{Lore:['[{"text": "arXiv:2301.13267", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lArchiSound: Audio Generation with Diffusion\\u00a7r\\n\\n\\u00a78\\u00a7oFlavio Schneider\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.13267\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 Jan 2023 20:23:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMaster Thesis at ETH Zurich\\u00a7r"}']}
{title:'Robinson et al. (§72023§r)', author: 'Kyle Robinson; Dan Brown', display:{Lore:['[{"text": "arXiv:2301.13380", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomated Time-frequency Domain Audio Crossfades using Graph Cuts\\u00a7r\\n\\n\\u00a78\\u00a7oKyle Robinson\\nDan Brown\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.13380\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nLate Breaking/Demo at the 20th International Society for Music\\n  Information Retrieval, Delft, The Netherlands, 2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 31 Jan 2023 03:05:48 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yuqiang Li; Shengchen Li; George Fazekas', display:{Lore:['[{"text": "arXiv:2301.13383", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Comparative Analysis of Different Pitch and Metrical Grid Encoding Methods in the Task of Sequential Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oYuqiang Li\\nShengchen Li\\nGeorge Fazekas\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.13383\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 31 Jan 2023 03:19:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis is a draft before submitted to TISMIR as ajournal paper\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Dongchao Yang; Songxiang Liu; Rongjie Huang; Chao Weng; Helen Meng', display:{Lore:['[{"text": "arXiv:2301.13662", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt\\u00a7r\\n\\n\\u00a78\\u00a7oDongchao Yang\\nSongxiang Liu\\nRongjie Huang\\nChao Weng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.13662\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 25 Jun 2023 11:42:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmit to TASLP\\u00a7r"}']}
{title:'Cheuk et al. (§72023§r)', author: 'Kin Wai Cheuk; Keunwoo Choi; Qiuqiang Kong; Bochen Li; Minz Won; Ju-Chiang Wang; Yun-Ning Hung; Dorien Herremans', display:{Lore:['[{"text": "arXiv:2302.00286", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJointist: Simultaneous Improvement of Multi-instrument Transcription and Music Source Separation via Joint Training\\u00a7r\\n\\n\\u00a78\\u00a7oKin Wai Cheuk\\nKeunwoo Choi\\nQiuqiang Kong\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.00286\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 2 Feb 2023 01:58:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2206.10805\\u00a7r"}']}
{title:'Huh et al. (§72023§r)', author: 'Jaesung Huh; Jacob Chalk; Evangelos Kazakos; Dima Damen; Andrew Zisserman', display:{Lore:['[{"text": "arXiv:2302.00646", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEpic-Sounds: A Large-scale Dataset of Actions That Sound\\u00a7r\\n\\n\\u00a78\\u00a7oJaesung Huh\\nJacob Chalk\\nEvangelos Kazakos\\nDima Damen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.00646\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Feb 2023 18:19:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 4 figures\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Hojeong Lee; Minseon Gwak; Kawon Lee; Minjeong Kim; Joseph Konan; Ojas Bhargave', display:{Lore:['[{"text": "arXiv:2302.00868", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Enhancement for Virtual Meetings on Cellular Networks\\u00a7r\\n\\n\\u00a78\\u00a7oHojeong Lee\\nMinseon Gwak\\nKawon Lee\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.00868\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 16 Feb 2023 17:12:35 GMT)\\u00a7r"}']}
{title:'Ziemer (§72023§r)', author: 'Tim Ziemer', display:{Lore:['[{"text": "arXiv:2302.01090", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGoniometers are a Powerful Acoustic Feature for Music Information Retrieval Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oTim Ziemer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.01090\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 Feb 2023 13:23:54 GMT)\\u00a7r"}']}
{title:'Hajavi et al. (§72023§r)', author: 'Amirhossein Hajavi; Ali Etemad', display:{Lore:['[{"text": "arXiv:2302.02845", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Representation Learning by Distilling Video as Privileged Information\\u00a7r\\n\\n\\u00a78\\u00a7oAmirhossein Hajavi\\nAli Etemad\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.02845\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 Feb 2023 15:09:34 GMT)\\u00a7r"}']}
{title:'Ashhad et al. (§72023§r)', author: 'Mohd Ashhad; Umang Goenka; Aaryan Jagetia; Parwin Akhtari; Sooraj K. Ambat; Mary Samuel', display:{Lore:['[{"text": "arXiv:2302.02945", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Vehicle Sub-type Classification for Acoustic Traffic Monitoring\\u00a7r\\n\\n\\u00a78\\u00a7oMohd Ashhad\\nUmang Goenka\\nAaryan Jagetia\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.02945\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 Feb 2023 17:26:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Twenty-Ninth National Conference on Communications(NCC) 23- 26 February, Indian Institute of TechnologyGuwahati\\u00a7r"}']}
{title:'Kharitonov et al. (§72023§r)', author: 'Eugene Kharitonov; Damien Vincent; Zalán Borsos; Raphaël Marinier; Sertan Girgin; Olivier Pietquin; Matt Sharifi; Marco Tagliasacchi; Neil Zeghidour', display:{Lore:['[{"text": "arXiv:2302.03540", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision\\u00a7r\\n\\n\\u00a78\\u00a7oEugene Kharitonov\\nDamien Vincent\\nZal\\u00e1n Borsos\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.03540\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Feb 2023 15:48:31 GMT)\\u00a7r"}']}
{title:'Huang et al. (§72023§r)', author: 'Qingqing Huang; Daniel S. Park; Tao Wang; Timo I. Denk; Andy Ly; Nanxin Chen; Zhengdong Zhang; Zhishuai Zhang; Jiahui Yu; Christian Frank; Jesse Engel; Quoc V. Le; William Chan; Zhifeng Chen; Wei Han', display:{Lore:['[{"text": "arXiv:2302.03917", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoise2Music: Text-conditioned Music Generation with Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oQingqing Huang\\nDaniel S. Park\\nTao Wang\\n+ 11 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.03917\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 6 Mar 2023 18:09:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Pengfei Zhu; Chao Pang; Yekun Chai; Lei Li; Shuohuan Wang; Yu Sun; Hao Tian; Hua Wu', display:{Lore:['[{"text": "arXiv:2302.04456", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oPengfei Zhu\\nChao Pang\\nYekun Chai\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.04456\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 21 Sep 2023 09:30:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by AACL demo 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Ziteng Wang; Yueyue Na; Biao Tian; Qiang Fu', display:{Lore:['[{"text": "arXiv:2302.04469", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Acoustic Echo Cancellation and Speech Dereverberation Using Kalman filters\\u00a7r\\n\\n\\u00a78\\u00a7oZiteng Wang\\nYueyue Na\\nBiao Tian\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.04469\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Feb 2023 07:18:10 GMT)\\u00a7r"}']}
{title:'Ranjan et al. (§72023§r)', author: 'Shivangi Ranjan; Vishal Srivastava', display:{Lore:['[{"text": "arXiv:2302.04577", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIncorporating Total Variation Regularization in the design of an intelligent Query by Humming system\\u00a7r\\n\\n\\u00a78\\u00a7oShivangi Ranjan\\nVishal Srivastava\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.04577\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Feb 2023 11:34:23 GMT)\\u00a7r"}']}
{title:'Sarmento et al. (§72023§r)', author: 'Pedro Sarmento; Adarsh Kumar; Yu-Hua Chen; CJ Carr; Zack Zukowski; Mathieu Barthet', display:{Lore:['[{"text": "arXiv:2302.05393", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGTR-CTRL: Instrument and Genre Conditioning for Guitar-Focused Music Generation with Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oPedro Sarmento\\nAdarsh Kumar\\nYu-Hua Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.05393\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nEvoMUSART: International Conference on Computational Intelligence\\n  in Music, Sound, Art and Design (Part of EvoStar) 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 10 Feb 2023 17:43:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis preprint is licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). The Version of Record of this contribution is published in Proceedings of EvoMUSART: International Conference on "}','{"text": "Computational Intelligence in Music, Sound, Art and Design (Part of EvoStar) 2023\\u00a7r"}']}
{title:'Hou et al. (§72023§r)', author: 'Zhongshu Hou; Qinwen Hu; Kai Chen; Jing Lu', display:{Lore:['[{"text": "arXiv:2302.05690", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention does not guarantee best performance in speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oZhongshu Hou\\nQinwen Hu\\nKai Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.05690\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 11 Feb 2023 13:17:59 GMT)\\u00a7r"}']}
{title:'Hou et al. (§72023§r)', author: 'Zhongshu Hou; Qinwen Hu; Kai Chen; Jing Lu', display:{Lore:['[{"text": "arXiv:2302.05693", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLocal spectral attention for full-band speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oZhongshu Hou\\nQinwen Hu\\nKai Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.05693\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 11 Feb 2023 13:25:19 GMT)\\u00a7r"}']}
{title:'Ukolov (§72023§r)', author: 'Dominik Ukolov', display:{Lore:['[{"text": "arXiv:2302.05725", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParameterizable Acoustical Modeling and Auralization of Cultural Heritage Sites based on Photogrammetry\\u00a7r\\n\\n\\u00a78\\u00a7oDominik Ukolov\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.05725\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 11 Feb 2023 15:44:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 3 figures, 27th Conference on CulturalHeritage and New Technologies (Vienna, 2022)\\u00a7r"}']}
{title:'Xiao et al. (§72023§r)', author: 'Yicheng Xiao; Yue Ma; Shuyan Li; Hantao Zhou; Ran Liao; Xiu Li', display:{Lore:['[{"text": "arXiv:2302.05940", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemanticAC: Semantics-Assisted Framework for Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oYicheng Xiao\\nYue Ma\\nShuyan Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.05940\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 12 Feb 2023 15:30:28 GMT)\\u00a7r"}']}
{title:'Bonafos et al. (§72023§r)', author: 'Guillem Bonafos; Pierre Pudlo; Jean-Marc Freyermuth; Thierry Legou; Joël Fagot; Samuel Tronçon; Arnaud Rey', display:{Lore:['[{"text": "arXiv:2302.07640", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.AP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetection and classification of vocal productions in large scale audio recordings\\u00a7r\\n\\n\\u00a78\\u00a7oGuillem Bonafos\\nPierre Pudlo\\nJean-Marc Freyermuth\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.07640\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 11 Aug 2023 17:50:41 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Muqiao Yang; Joseph Konan; David Bick; Yunyang Zeng; Shuo Han; Anurag Kumar; Shinji Watanabe; Bhiksha Raj', display:{Lore:['[{"text": "arXiv:2302.08095", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPAAPLoss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oMuqiao Yang\\nJoseph Konan\\nDavid Bick\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.08095\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Feb 2023 05:17:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Chung-Che Wang; Yu-Chun Lin; Yu-Teng Hsu; Jyh-Shing Roger Jang', display:{Lore:['[{"text": "arXiv:2302.08130", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPersonalized Audio Quality Preference Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oChung-Che Wang\\nYu-Chun Lin\\nYu-Teng Hsu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.08130\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Feb 2023 07:49:06 GMT)\\u00a7r"}']}
{title:'Zhong et al. (§72023§r)', author: 'Zhi Zhong; Masato Hirano; Kazuki Shimada; Kazuya Tateishi; Shusuke Takahashi; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2302.08136", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Attention-based Approach to Hierarchical Multi-label Music Instrument Classification\\u00a7r\\n\\n\\u00a78\\u00a7oZhi Zhong\\nMasato Hirano\\nKazuki Shimada\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.08136\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Feb 2023 08:07:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at ICASSP 2023\\u00a7r"}']}
{title:'Hussain et al. (§72023§r)', author: 'Shehzeen Hussain; Paarth Neekhara; Jocelyn Huang; Jason Li; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2302.08137", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lACE-VC: Adaptive and Controllable Voice Conversion using Explicitly Disentangled Self-supervised Speech Representations\\u00a7r\\n\\n\\u00a78\\u00a7oShehzeen Hussain\\nPaarth Neekhara\\nJocelyn Huang\\nJason Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.08137\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Feb 2023 08:10:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished as a conference paper at ICASSP 2023\\u00a7r"}']}
{title:'Guo et al. (§72023§r)', author: 'Houjian Guo; Chaoran Liu; Carlos Toshinori Ishi; Hiroshi Ishiguro', display:{Lore:['[{"text": "arXiv:2302.08296", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQuickVC: Any-to-many Voice Conversion Using Inverse Short-time Fourier Transform for Faster Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oHoujian Guo\\nChaoran Liu\\nCarlos Toshinori Ishi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.08296\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 23 Feb 2023 05:43:07 GMT)\\u00a7r"}']}
{title:'Adegbija (§72023§r)', author: 'Tosiron Adegbija', display:{Lore:['[{"text": "arXiv:2302.08632", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7ljazznet: A Dataset of Fundamental Piano Patterns for Music Audio Machine Learning Research\\u00a7r\\n\\n\\u00a78\\u00a7oTosiron Adegbija\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.08632\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 17 Feb 2023 00:13:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo Appear at IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2023\\u00a7r"}']}
{title:'Liang et al. (§72023§r)', author: 'Xuefeng Liang; Hexin Jiang; Wenxin Xu; Ying Zhou', display:{Lore:['[{"text": "arXiv:2302.08650", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGaussian-smoothed Imbalance Data Improves Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXuefeng Liang\\nHexin Jiang\\nWenxin Xu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.08650\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 17 Feb 2023 01:50:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Minsu Kim; Joanna Hong; Yong Man Ro', display:{Lore:['[{"text": "arXiv:2302.08841", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLip-to-Speech Synthesis in the Wild with Multi-task Learning\\u00a7r\\n\\n\\u00a78\\u00a7oMinsu Kim\\nJoanna Hong\\nYong Man Ro\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.08841\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 17 Feb 2023 12:31:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2023. Demo available: https://github.com/joannahong/Lip-to-Speech-Synthesis-in-the-Wild\\u00a7r"}']}
{title:'Zhao et al. (§72023§r)', author: 'Yan Zhao; Jincen Wang; Yuan Zong; Wenming Zheng; Hailun Lian; Li Zhao', display:{Lore:['[{"text": "arXiv:2302.08921", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Implicit Distribution Alignment Networks for Cross-Corpus Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYan Zhao\\nJincen Wang\\nYuan Zong\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.08921\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 17 Feb 2023 14:51:37 GMT)\\u00a7r"}']}
{title:'Sun et al. (§72023§r)', author: 'Chengzhe Sun; Shan Jia; Shuwei Hou; Ehab AlBadawy; Siwei Lyu', display:{Lore:['[{"text": "arXiv:2302.09198", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExposing AI-Synthesized Human Voices Using Neural Vocoder Artifacts\\u00a7r\\n\\n\\u00a78\\u00a7oChengzhe Sun\\nShan Jia\\nShuwei Hou\\nEhab AlBadawy\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.09198\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 27 Apr 2023 08:48:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDataset and codes will be available at https://github.com/csun22/LibriVoc-Dataset\\u00a7r"}']}
{title:'Tasnim et al. (§72023§r)', author: 'Mashrura Tasnim; Jekaterina Novikova', display:{Lore:['[{"text": "arXiv:2302.09214", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCost-effective Models for Detecting Depression from Speech\\u00a7r\\n\\n\\u00a78\\u00a7oMashrura Tasnim\\nJekaterina Novikova\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.09214\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 18 Feb 2023 02:46:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICMLA 2022\\u00a7r"}']}
{title:'Meng et al. (§72023§r)', author: 'Lingwei Meng; Jiawen Kang; Mingyu Cui; Yuejiao Wang; Xixin Wu; Helen Meng', display:{Lore:['[{"text": "arXiv:2302.09908", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Sidecar Separator Can Convert a Single-Talker Speech Recognition System to a Multi-Talker One\\u00a7r\\n\\n\\u00a78\\u00a7oLingwei Meng\\nJiawen Kang\\nMingyu Cui\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.09908\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 5 Mar 2023 23:10:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2023\\u00a7r"}']}
{title:'Tevissen et al. (§72023§r)', author: 'Yannis Tevissen; Jérôme Boudy; Gérard Chollet; Frédéric Petitpont', display:{Lore:['[{"text": "arXiv:2302.09991", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Measuring and Scoring Speaker Diarization Fairness\\u00a7r\\n\\n\\u00a78\\u00a7oYannis Tevissen\\nJ\\u00e9r\\u00f4me Boudy\\nG\\u00e9rard Chollet\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.09991\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Feb 2023 14:07:43 GMT)\\u00a7r"}']}
{title:'Huh et al. (§72023§r)', author: 'Jaesung Huh; Andrew Brown; Jee-weon Jung; Joon Son Chung; Arsha Nagrani; Daniel Garcia-Romero; Andrew Zisserman', display:{Lore:['[{"text": "arXiv:2302.10248", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoxSRC 2022: The Fourth VoxCeleb Speaker Recognition Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oJaesung Huh\\nAndrew Brown\\nJee-weon Jung\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.10248\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 6 Mar 2023 13:38:57 GMT)\\u00a7r"}']}
{title:'Recalde (§72023§r)', author: 'Nilo Merino Recalde', display:{Lore:['[{"text": "arXiv:2302.10340", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.PE\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lpykanto: a python library to accelerate research on wild bird song\\u00a7r\\n\\n\\u00a78\\u00a7oNilo Merino Recalde\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.10340\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1111/2041-210X.14155\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nMethods in Ecology and Evolution, 00, 1-9\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Feb 2023 22:05:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 3 figures\\u00a7r"}']}
{title:'Shah et al. (§72023§r)', author: 'Nirmesh Shah; Mayank Kumar Singh; Naoya Takahashi; Naoyuki Onoe', display:{Lore:['[{"text": "arXiv:2302.10536", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNonparallel Emotional Voice Conversion For Unseen Speaker-Emotion Pairs Using Dual Domain Adversarial Network     Virtual Domain Pairing\\u00a7r\\n\\n\\u00a78\\u00a7oNirmesh Shah\\nMayank Kumar Singh\\nNaoya Takahashi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.10536\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Feb 2023 09:06:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDemo Samples at https://demosamplesites.github.io/EVCUP/\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Shuo Wang; Xiangyu Kong; Xiulian Peng; Mahmood Movassagh; Vinod Prakash; Yan Lu', display:{Lore:['[{"text": "arXiv:2302.10657", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDasFormer: Deep Alternating Spectrogram Transformer for Multi/Single-Channel Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oShuo Wang\\nXiangyu Kong\\nXiulian Peng\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.10657\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 14 Mar 2023 11:35:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted by ICASSP2023\\u00a7r"}']}
{title:'Yao et al. (§72023§r)', author: 'Jiadi Yao; Hong Luo; Xiao-Lei Zhang', display:{Lore:['[{"text": "arXiv:2302.10686", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInterpretable Spectrum Transformation Attacks to Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJiadi Yao\\nHong Luo\\nXiao-Lei Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.10686\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Feb 2023 14:12:29 GMT)\\u00a7r"}']}
{title:'Lin et al. (§72023§r)', author: 'Baihan Lin; Xinxin Zhang', display:{Lore:['[{"text": "arXiv:2302.10924", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Reinforcement Learning Framework for Online Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oBaihan Lin\\nXinxin Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.10924\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Feb 2023 15:42:25 GMT)\\u00a7r"}']}
{title:'Sandholm (§72023§r)', author: 'Sophia Sandholm', display:{Lore:['[{"text": "arXiv:2302.10983", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDo Orcas Have Semantic Language? Machine Learning to Predict Orca Behaviors Using Partially Labeled Vocalization Data\\u00a7r\\n\\n\\u00a78\\u00a7oSophia Sandholm\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.10983\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 28 Jan 2023 06:04:22 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Xiaoqiang Wang; Yanqing Liu; Jinyu Li; Sheng Zhao', display:{Lore:['[{"text": "arXiv:2302.11192", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Contextual Spelling Correction by External Acoustics Attention and Semantic Aware Data Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoqiang Wang\\nYanqing Liu\\nJinyu Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.11192\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 Feb 2023 08:00:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Meng Liu; Kong Aik Lee; Longbiao Wang; Hanyi Zhang; Chang Zeng; Jianwu Dang', display:{Lore:['[{"text": "arXiv:2302.11254", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-modal Audio-visual Co-learning for Text-independent Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oMeng Liu\\nKong Aik Lee\\nLongbiao Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.11254\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 Feb 2023 10:06:37 GMT)\\u00a7r"}']}
{title:'Sheikh et al. (§72023§r)', author: 'Shakeel A. Sheikh; Md Sahidullah; Fabrice Hirsch; Slim Ouni', display:{Lore:['[{"text": "arXiv:2302.11343", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdvancing Stuttering Detection via Data Augmentation, Class-Balanced Loss and Multi-Contextual Deep Learning\\u00a7r\\n\\n\\u00a78\\u00a7oShakeel A. Sheikh\\nMd Sahidullah\\nFabrice Hirsch\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.11343\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Feb 2023 14:03:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in IEEE Journal of Biomedical Health Informatics 2023\\u00a7r"}']}
{title:'Xin et al. (§72023§r)', author: 'Yifei Xin; Xiulian Peng; Yan Lu', display:{Lore:['[{"text": "arXiv:2302.11558", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Speech Enhancement via Event-based Query\\u00a7r\\n\\n\\u00a78\\u00a7oYifei Xin\\nXiulian Peng\\nYan Lu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.11558\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 24 Feb 2023 07:42:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2023\\u00a7r"}']}
{title:'Zhao et al. (§72023§r)', author: 'Shengkui Zhao; Bin Ma', display:{Lore:['[{"text": "arXiv:2302.11824", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMossFormer: Pushing the Performance Limit of Monaural Speech Separation using Gated Single-Head Transformer with Convolution-Augmented Joint Self-Attentions\\u00a7r\\n\\n\\u00a78\\u00a7oShengkui Zhao\\nBin Ma\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.11824\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Feb 2023 07:17:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted by ICASSP 2023\\u00a7r"}']}
{title:'Zhao et al. (§72023§r)', author: 'Shengkui Zhao; Bin Ma', display:{Lore:['[{"text": "arXiv:2302.11832", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lD2Former: A Fully Complex Dual-Path Dual-Decoder Conformer Network using Joint Complex Masking and Complex Spectral Mapping for Monaural Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oShengkui Zhao\\nBin Ma\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.11832\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Feb 2023 07:43:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted by ICASSP 2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Chen Chen; Yuchen Hu; Heqing Zou; Linhui Sun; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2302.11981", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Noise adaptation using Data Simulation\\u00a7r\\n\\n\\u00a78\\u00a7oChen Chen\\nYuchen Hu\\nHeqing Zou\\nLinhui Sun\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.11981\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Feb 2023 12:57:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Chen Chen; Yuchen Hu; Weiwei Weng; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2302.11989", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMetric-oriented Speech Enhancement using Diffusion Probabilistic Model\\u00a7r\\n\\n\\u00a78\\u00a7oChen Chen\\nYuchen Hu\\nWeiwei Weng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.11989\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Feb 2023 13:12:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2023\\u00a7r"}']}
{title:'Weck et al. (§72023§r)', author: 'Benno Weck; Xavier Serra', display:{Lore:['[{"text": "arXiv:2302.12258", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData leakage in cross-modal retrieval training: A case study\\u00a7r\\n\\n\\u00a78\\u00a7oBenno Weck\\nXavier Serra\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.12258\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10094617\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Feb 2023 09:51:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages. Accepted at ICASSP2023\\u00a7r"}']}
{title:'Deng et al. (§72023§r)', author: 'Jiangyi Deng; Yanjiao Chen; Yinan Zhong; Qianhao Miao; Xueluan Gong; Wenyuan Xu', display:{Lore:['[{"text": "arXiv:2302.12434", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCatch You and I Can: Revealing Source Voiceprint Against Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oJiangyi Deng\\nYanjiao Chen\\nYinan Zhong\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.12434\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Feb 2023 03:33:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by USENIX Security Symposium 2023. Pleasecite this paper as \\"Jiangyi Deng, Yanjiao Chen, Yinan Zhong, Qianhao Miao, Xueluan Gong, Wenyuan Xu. Catch You and I Can: Revealing Source Voiceprint Against Voice "}','{"text": "Conversion. In 32nd USENIX Security Symposium (USENIX Security 23).\\"\\u00a7r"}']}
{title:'Singh et al. (§72023§r)', author: 'Prachi Singh; Amrit Kaul; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:2302.12716", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSupervised Hierarchical Clustering using Graph Neural Networks for Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oPrachi Singh\\nAmrit Kaul\\nSriram Ganapathy\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.12716\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Feb 2023 16:16:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages including references. Accepted in ICASSP 2023\\u00a7r"}']}
{title:'Vaessen et al. (§72023§r)', author: 'Nik Vaessen; David A. van Leeuwen', display:{Lore:['[{"text": "arXiv:2302.12773", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards multi-task learning of speech and speaker recognition\\u00a7r\\n\\n\\u00a78\\u00a7oNik Vaessen\\nDavid A. van Leeuwen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.12773\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 26 May 2023 11:41:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to interspeech 2023\\u00a7r"}']}
{title:'Shah et al. (§72023§r)', author: 'Saqlain Hussain Shah; Muhammad Saad Saeed; Shah Nawaz; Muhammad Haroon Yousaf', display:{Lore:['[{"text": "arXiv:2302.13033", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Recognition in Realistic Scenario Using Multimodal Data\\u00a7r\\n\\n\\u00a78\\u00a7oSaqlain Hussain Shah\\nMuhammad Saad Saeed\\nShah Nawaz\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.13033\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 25 Feb 2023 09:11:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the International Conference on Artificial Intelligence (ICAI\'2023)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Jianrong Wang; Jinyu Liu; Li Liu; Xuewei Li; Mei Yu; Jie Gao; Qiang Fang', display:{Lore:['[{"text": "arXiv:2302.13273", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTwo-Stream Joint-Training for Speaker Independent Acoustic-to-Articulatory Inversion\\u00a7r\\n\\n\\u00a78\\u00a7oJianrong Wang\\nJinyu Liu\\nLi Liu\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.13273\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 26 Feb 2023 08:53:20 GMT)\\u00a7r"}']}
{title:'Shen et al. (§72023§r)', author: 'Siyuan Shen; Feng Liu; Aimin Zhou', display:{Lore:['[{"text": "arXiv:2302.13277", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMingling or Misalignment? Temporal Shift for Speech Emotion Recognition with Pre-trained Representations\\u00a7r\\n\\n\\u00a78\\u00a7oSiyuan Shen\\nFeng Liu\\nAimin Zhou\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.13277\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 1 Mar 2023 06:02:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by 48th IEEE InternationalConference on Acoustics, Speech, and Signal Processing (ICASSP 2023)\\u00a7r"}']}
{title:'Xue et al. (§72023§r)', author: 'Huaying Xue; Xiulian Peng; Yan Lu', display:{Lore:['[{"text": "arXiv:2302.13284", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContrast-PLC: Contrastive Learning for Packet Loss Concealment\\u00a7r\\n\\n\\u00a78\\u00a7oHuaying Xue\\nXiulian Peng\\nYan Lu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.13284\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 26 Feb 2023 10:24:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, ICASSP 2023(Accepted)\\u00a7r"}']}
{title:'Schoder (§72023§r)', author: 'Stefan Schoder', display:{Lore:['[{"text": "arXiv:2302.13290", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImplementation of an aeroacoustic simulation pipeline using openCFS-Acoustics and openCFS-Data applied to human phonation\\u00a7r\\n\\n\\u00a78\\u00a7oStefan Schoder\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.13290\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 26 Feb 2023 10:46:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 4 figures\\u00a7r"}']}
{title:'Krols et al. (§72023§r)', author: 'Tibor Krols; Yana Nikolova; Ninell Oldenburg', display:{Lore:['[{"text": "arXiv:2302.13321", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Modality in Music: Predicting Emotion in Music from High-Level Audio Features and Lyrics\\u00a7r\\n\\n\\u00a78\\u00a7oTibor Krols\\nYana Nikolova\\nNinell Oldenburg\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.13321\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 26 Feb 2023 13:38:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, incl. 2 pages appendix\\u00a7r"}']}
{title:'Yuan et al. (§72023§r)', author: 'Shenli Yuan; Lingjie Kong; Jiushuang Guo', display:{Lore:['[{"text": "arXiv:2302.13401", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrom Audio to Symbolic Encoding\\u00a7r\\n\\n\\u00a78\\u00a7oShenli Yuan\\nLingjie Kong\\nJiushuang Guo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.13401\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 26 Feb 2023 20:15:00 GMT)\\u00a7r"}']}
{title:'Gu et al. (§72023§r)', author: 'Rongzhi Gu; Shi-Xiong Zhang; Dong Yu', display:{Lore:['[{"text": "arXiv:2302.13462", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l3D Neural Beamforming for Multi-channel Speech Separation Against Location Uncertainty\\u00a7r\\n\\n\\u00a78\\u00a7oRongzhi Gu\\nShi-Xiong Zhang\\nDong Yu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.13462\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Feb 2023 01:27:37 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Ao Zhang; He Wang; Pengcheng Guo; Yihui Fu; Lei Xie; Yingying Gao; Shilei Zhang; Junlan Feng', display:{Lore:['[{"text": "arXiv:2302.13523", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVE-KWS: Visual Modality Enhanced End-to-End Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oAo Zhang\\nHe Wang\\nPengcheng Guo\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.13523\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 14 Mar 2023 12:02:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages. Accepted at ICASSP2023\\u00a7r"}']}
{title:'Devis et al. (§72023§r)', author: 'Ninon Devis; Nils Demerlé; Sarah Nabi; David Genova; Philippe Esling', display:{Lore:['[{"text": "arXiv:2302.13542", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContinuous descriptor-based control for deep audio synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oNinon Devis\\nNils Demerl\\u00e9\\nSarah Nabi\\nDavid Genova\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.13542\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Feb 2023 06:40:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:"O'Connor et al. (§72023§r)", author: "Brendan O'Connor; Simon Dixon", display:{Lore:['[{"text": "arXiv:2302.13678", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparative Analysis Of Latent Regressor Losses For Singing Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oBrendan O\'Connor\\nSimon Dixon\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.13678\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Feb 2023 11:26:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to the Sound and Music Computing Conference 2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Weidong Chen; Xiaofen Xing; Xiangmin Xu; Jianxin Pang; Lan Du', display:{Lore:['[{"text": "arXiv:2302.13729", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDST: Deformable Speech Transformer for Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oWeidong Chen\\nXiaofen Xing\\nXiangmin Xu\\nJianxin Pang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.13729\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Feb 2023 12:52:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, 2tables, accepted by ICASSP 2023\\u00a7r"}']}
{title:'Riera et al. (§72023§r)', author: 'Pablo Riera; Manuela Cerdeiro; Leonardo Pepino; Luciana Ferrer', display:{Lore:['[{"text": "arXiv:2302.14055", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhone and speaker spatial organization in self-supervised speech representations\\u00a7r\\n\\n\\u00a78\\u00a7oPablo Riera\\nManuela Cerdeiro\\nLeonardo Pepino\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.14055\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSPW59220.2023.10193460\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Feb 2023 19:39:42 GMT)\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Xiaoliang Wu; Peter Bell; Ajitha Rajan', display:{Lore:['[{"text": "arXiv:2302.14062", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExplanations for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoliang Wu\\nPeter Bell\\nAjitha Rajan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.14062\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Feb 2023 11:09:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Speech Track, ICASSP 2023\\u00a7r"}']}
{title:'Yu et al. (§72023§r)', author: 'Zhongjie Yu; Shuyang Wang; Lin Chen; Zhongwei Cheng', display:{Lore:['[{"text": "arXiv:2302.14204", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHalluAudio: Hallucinating Frequency as Concepts for Few-Shot Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oZhongjie Yu\\nShuyang Wang\\nLin Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.14204\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Feb 2023 23:56:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2023\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Ji-Hoon Kim; Hong-Sun Yang; Yoon-Cheol Ju; Il-Hwan Kim; Byeong-Yeol Kim', display:{Lore:['[{"text": "arXiv:2302.14370", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCrossSpeech: Speaker-independent Acoustic Representation for Cross-lingual Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oJi-Hoon Kim\\nHong-Sun Yang\\nYoon-Cheol Ju\\nIl-Hwan Kim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.14370\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 12 Jun 2023 04:14:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Hu et al. (§72023§r)', author: 'Shujie Hu; Xurong Xie; Zengrui Jin; Mengzhe Geng; Yi Wang; Mingyu Cui; Jiajun Deng; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2302.14564", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Self-supervised Pre-trained ASR Models For Dysarthric and Elderly Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oShujie Hu\\nXurong Xie\\nZengrui Jin\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.14564\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 22 Jun 2023 06:45:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by ICASSP 2023\\u00a7r"}']}
{title:'Ng et al. (§72023§r)', author: 'Dianwen Ng; Ruixi Zhang; Jia Qi Yip; Zhao Yang; Jinjie Ni; Chong Zhang; Yukun Ma; Chongjia Ni; Eng Siong Chng; Bin Ma', display:{Lore:['[{"text": "arXiv:2302.14597", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7ldeHuBERT: Disentangling Noise in a Self-supervised Model for Robust Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDianwen Ng\\nRuixi Zhang\\nJia Qi Yip\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.14597\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Feb 2023 14:33:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Zhao et al. (§72023§r)', author: 'Zhenduo Zhao; Zhuo Li; Wenchao Wang; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2303.00204", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPCF: ECAPA-TDNN with Progressive Channel Fusion for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oZhenduo Zhao\\nZhuo Li\\nWenchao Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.00204\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Mar 2023 03:12:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Li Zhang; Qing Wang; Hongji Wang; Yue Li; Wei Rao; Yannan Wang; Lei Xie', display:{Lore:['[{"text": "arXiv:2303.00264", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDistance-based Weight Transfer from Near-field to Far-field Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oLi Zhang\\nQing Wang\\nHongji Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.00264\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 15 Mar 2023 03:05:35 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Hui Wang; Siqi Zheng; Yafeng Chen; Luyao Cheng; Qian Chen', display:{Lore:['[{"text": "arXiv:2303.00332", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCAM++: A Fast and Efficient Network for Speaker Verification Using Context-Aware Masking\\u00a7r\\n\\n\\u00a78\\u00a7oHui Wang\\nSiqi Zheng\\nYafeng Chen\\nLuyao Cheng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.00332\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 16 Jun 2023 09:07:11 GMT)\\u00a7r"}']}
{title:'Niu et al. (§72023§r)', author: 'Zhe Niu; Brian Mak', display:{Lore:['[{"text": "arXiv:2303.00502", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Audio-visual Synchronization for Lip-to-Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oZhe Niu\\nBrian Mak\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.00502\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Mar 2023 13:35:35 GMT)\\u00a7r"}']}
{title:'Bain et al. (§72023§r)', author: 'Max Bain; Jaesung Huh; Tengda Han; Andrew Zisserman', display:{Lore:['[{"text": "arXiv:2303.00747", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhisperX: Time-Accurate Speech Transcription of Long-Form Audio\\u00a7r\\n\\n\\u00a78\\u00a7oMax Bain\\nJaesung Huh\\nTengda Han\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.00747\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 11 Jul 2023 17:07:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2023\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Xuechen Liu; Md Sahidullah; Tomi Kinnunen', display:{Lore:['[{"text": "arXiv:2303.01125", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDistilling Multi-Level X-vector Knowledge for Small-footprint Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oXuechen Liu\\nMd Sahidullah\\nTomi Kinnunen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.01125\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 19 Dec 2023 23:25:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to DataKnowledge Engineering at Dec. 2023. Copyright may be transferred without notice\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Xuechen Liu; Md Sahidullah; Kong Aik Lee; Tomi Kinnunen', display:{Lore:['[{"text": "arXiv:2303.01126", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker-Aware Anti-Spoofing\\u00a7r\\n\\n\\u00a78\\u00a7oXuechen Liu\\nMd Sahidullah\\nKong Aik Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.01126\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 8 Jun 2023 04:26:30 GMT)\\u00a7r"}']}
{title:'Xue et al. (§72023§r)', author: 'Jun Xue; Cunhang Fan; Jiangyan Yi; Chenglong Wang; Zhengqi Wen; Dan Zhang; Zhao Lv', display:{Lore:['[{"text": "arXiv:2303.01211", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning From Yourself: A Self-Distillation Method for Fake Speech Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJun Xue\\nCunhang Fan\\nJiangyan Yi\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.01211\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 Mar 2023 12:52:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Shutong Wu; Jiongxiao Wang; Wei Ping; Weili Nie; Chaowei Xiao', display:{Lore:['[{"text": "arXiv:2303.01507", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDefending against Adversarial Audio via Diffusion Model\\u00a7r\\n\\n\\u00a78\\u00a7oShutong Wu\\nJiongxiao Wang\\nWei Ping\\nWeili Nie\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.01507\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 Mar 2023 07:15:47 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Shijun Wang; Jón Guðnason; Damian Borth', display:{Lore:['[{"text": "arXiv:2303.01508", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFine-grained Emotional Control of Text-To-Speech: Learning To Rank Inter- And Intra-Class Emotion Intensities\\u00a7r\\n\\n\\u00a78\\u00a7oShijun Wang\\nJ\\u00f3n Gu\\u00f0nason\\nDamian Borth\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.01508\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 11 Mar 2023 13:07:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2023\\u00a7r"}']}
{title:'Rekimoto (§72023§r)', author: 'Jun Rekimoto', display:{Lore:['[{"text": "arXiv:2303.01639", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWESPER: Zero-shot and Realtime Whisper to Normal Voice Conversion for Whisper-based Speech Interactions\\u00a7r\\n\\n\\u00a78\\u00a7oJun Rekimoto\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.01639\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3544548.3580706\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 2023 CHI Conference on Human Factors in\\n  Computing Systems (CHI \'23), April 23--28, 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 3 Mar 2023 00:10:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oACM CHI 2023 paper\\u00a7r"}']}
{title:'Koizumi et al. (§72023§r)', author: 'Yuma Koizumi; Heiga Zen; Shigeki Karita; Yifan Ding; Kohei Yatabe; Nobuyuki Morioka; Yu Zhang; Wei Han; Ankur Bapna; Michiel Bacchiani', display:{Lore:['[{"text": "arXiv:2303.01664", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMiipher: A Robust Speech Restoration Model Integrating Self-Supervised Speech and Text Representations\\u00a7r\\n\\n\\u00a78\\u00a7oYuma Koizumi\\nHeiga Zen\\nShigeki Karita\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.01664\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 14 Aug 2023 09:22:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to WASPAA 2023\\u00a7r"}']}
{title:'Adkins et al. (§72023§r)', author: 'Sara Adkins; Pedro Sarmento; Mathieu Barthet', display:{Lore:['[{"text": "arXiv:2303.01665", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLooperGP: A Loopable Sequence Model for Live Coding Performance using GuitarPro Tablature\\u00a7r\\n\\n\\u00a78\\u00a7oSara Adkins\\nPedro Sarmento\\nMathieu Barthet\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.01665\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nEvoMUSART: International Conference on Computational Intelligence\\n  in Music, Sound, Art and Design (Part of EvoStar) 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 3 Mar 2023 02:00:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe Version of Record of this contribution is published in Proceedings of EvoMUSART: International Conference on Computational Intelligence in Music, Sound, Art and Design (Part of EvoStar) 2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Shuaiqi Chen; Xiaofen Xing; Weibin Zhang; Weidong Chen; Xiangmin Xu', display:{Lore:['[{"text": "arXiv:2303.01694", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDWFormer: Dynamic Window transFormer for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oShuaiqi Chen\\nXiaofen Xing\\nWeibin Zhang\\nWeidong Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.01694\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 3 Mar 2023 03:26:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 5 figures, 3 tables, accepted by 2023 International Conference on Acoustics, Speech, and Signal Processing (ICASSP2023)\\u00a7r"}']}
{title:'Dinkel et al. (§72023§r)', author: 'Heinrich Dinkel; Yongqing Wang; Zhiyong Yan; Junbo Zhang; Yujun Wang', display:{Lore:['[{"text": "arXiv:2303.01812", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnified Keyword Spotting and Audio Tagging on Mobile Devices with Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oHeinrich Dinkel\\nYongqing Wang\\nZhiyong Yan\\nJunbo Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.01812\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 3 Mar 2023 09:38:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Magron et al. (§72023§r)', author: 'Paul Magron; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2303.01864", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpectrogram Inversion for Audio Source Separation via Consistency, Mixing, and Magnitude Constraints\\u00a7r\\n\\n\\u00a78\\u00a7oPaul Magron\\nTuomas Virtanen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.01864\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 30 Jun 2023 10:31:48 GMT)\\u00a7r"}']}
{title:'Chowdhury et al. (§72023§r)', author: 'Shreyan Chowdhury; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2303.01875", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDecoding and Visualising Intended Emotion in an Expressive Piano Performance\\u00a7r\\n\\n\\u00a78\\u00a7oShreyan Chowdhury\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.01875\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 3 Mar 2023 12:10:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oExtended version of Late-Breaking Demo Session paper accepted at ISMIR 2022 (23rd Int. Society for Music Information Retrieval Conf., Bengaluru, India, 2022)\\u00a7r"}']}
{title:'Schmid et al. (§72023§r)', author: 'Florian Schmid; Khaled Koutini; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2303.01879", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Complexity Audio Embedding Extractors\\u00a7r\\n\\n\\u00a78\\u00a7oFlorian Schmid\\nKhaled Koutini\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.01879\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 23 Jun 2023 07:30:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of the 31st European Signal Processing Conference, EUSIPCO 2023. Source Code available at: https://github.com/fschmid56/EfficientAT_HEAR\\u00a7r"}']}
{title:'Pei et al. (§72023§r)', author: 'Sen Pei; Jingya Yu; Qi Chen; Wozhou He', display:{Lore:['[{"text": "arXiv:2303.01884", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutoMatch: A Large-scale Audio Beat Matching Benchmark for Boosting Deep Learning Assistant Video Editing\\u00a7r\\n\\n\\u00a78\\u00a7oSen Pei\\nJingya Yu\\nQi Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.01884\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 3 Mar 2023 12:30:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 5 figures\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Haoxu Wang; Ming Cheng; Qiang Fu; Ming Li', display:{Lore:['[{"text": "arXiv:2303.02348", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe DKU Post-Challenge Audio-Visual Wake Word Spotting System for the 2021 MISP Challenge: Deep Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oHaoxu Wang\\nMing Cheng\\nQiang Fu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.02348\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 4 Mar 2023 07:30:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Serrano et al. (§72023§r)', author: 'Danzel Serrano; Mark Cartwright', display:{Lore:['[{"text": "arXiv:2303.02396", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA General Framework for Learning Procedural Audio Models of Environmental Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oDanzel Serrano\\nMark Cartwright\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.02396\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 4 Mar 2023 12:12:26 GMT)\\u00a7r"}']}
{title:'Fernando et al. (§72023§r)', author: 'Rashen Fernando; Pamudu Ranasinghe; Udula Ranasinghe; Janaka Wijayakulasooriya; Pantaleon Perera', display:{Lore:['[{"text": "arXiv:2303.02599", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHybrid Y-Net Architecture for Singing Voice Separation\\u00a7r\\n\\n\\u00a78\\u00a7oRashen Fernando\\nPamudu Ranasinghe\\nUdula Ranasinghe\\nJanaka Wijayakulasooriya\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.02599\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 5 Mar 2023 07:54:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted for EUSIPCO23: 5 Pages, 7 figures\\u00a7r"}']}
{title:'Shirian et al. (§72023§r)', author: 'Amir Shirian; Mona Ahmadian; Krishna Somandepalli; Tanaya Guha', display:{Lore:['[{"text": "arXiv:2303.02665", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHeterogeneous Graph Learning for Acoustic Event Classification\\u00a7r\\n\\n\\u00a78\\u00a7oAmir Shirian\\nMona Ahmadian\\nKrishna Somandepalli\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.02665\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 12 Mar 2023 10:26:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2207.07935\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Jiguo Li; Tianzi Zhang; Xiaobin Liu; Lirong Zheng', display:{Lore:['[{"text": "arXiv:2303.02673", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTime-frequency Network for Robust Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJiguo Li\\nTianzi Zhang\\nXiaobin Liu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.02673\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 7 Mar 2023 02:38:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5pages, 3 figures\\u00a7r"}']}
{title:'Ali et al. (§72023§r)', author: 'Mohamed Nabih Ali; Francesco Paissan; Daniele Falavigna; Alessio Brutti', display:{Lore:['[{"text": "arXiv:2303.03005", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScaling strategies for on-device low-complexity source separation with Conv-Tasnet\\u00a7r\\n\\n\\u00a78\\u00a7oMohamed Nabih Ali\\nFrancesco Paissan\\nDaniele Falavigna\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.03005\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 Mar 2023 10:15:14 GMT)\\u00a7r"}']}
{title:'Shah et al. (§72023§r)', author: 'Ankit Shah; Shuyi Chen; Kejun Zhou; Yue Chen; Bhiksha Raj', display:{Lore:['[{"text": "arXiv:2303.03591", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lApproach to Learning Generalized Audio Representation Through Batch Embedding Covariance Regularization and Constant-Q Transforms\\u00a7r\\n\\n\\u00a78\\u00a7oAnkit Shah\\nShuyi Chen\\nKejun Zhou\\nYue Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.03591\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Mar 2023 01:54:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTechnical report, 10 pages\\u00a7r"}']}
{title:'Morsali et al. (§72023§r)', author: 'M. Mehrdad Morsali; Hoda Mohammadzade; Saeed Bagheri Shouraki', display:{Lore:['[{"text": "arXiv:2303.03666", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFace: Fast, Accurate and Context-Aware Audio Annotation and Classification\\u00a7r\\n\\n\\u00a78\\u00a7oM. Mehrdad Morsali\\nHoda Mohammadzade\\nSaeed Bagheri Shouraki\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.03666\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Mar 2023 06:04:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 4 figures\\u00a7r"}']}
{title:'Nguyen et al. (§72023§r)', author: 'Bac Nguyen; Stefan Uhlich; Fabien Cardinaux', display:{Lore:['[{"text": "arXiv:2303.03717", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Self-Supervised Learning for Audio Representations by Feature Diversity and Decorrelation\\u00a7r\\n\\n\\u00a78\\u00a7oBac Nguyen\\nStefan Uhlich\\nFabien Cardinaux\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.03717\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Mar 2023 07:57:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Mu et al. (§72023§r)', author: 'Zhaoxi Mu; Xinyu Yang; Xiangyuan Yang; Wenjing Zhu', display:{Lore:['[{"text": "arXiv:2303.03732", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Multi-Stage Triple-Path Method for Speech Separation in Noisy and Reverberant Environments\\u00a7r\\n\\n\\u00a78\\u00a7oZhaoxi Mu\\nXinyu Yang\\nXiangyuan Yang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.03732\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Mar 2023 08:44:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Mu et al. (§72023§r)', author: 'Zhaoxi Mu; Xinyu Yang; Wenjing Zhu', display:{Lore:['[{"text": "arXiv:2303.03737", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Dimensional and Multi-Scale Modeling for Speech Separation Optimized by Discriminative Learning\\u00a7r\\n\\n\\u00a78\\u00a7oZhaoxi Mu\\nXinyu Yang\\nWenjing Zhu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.03737\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Mar 2023 08:53:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Yuan et al. (§72023§r)', author: 'Yi Yuan; Haohe Liu; Jinhua Liang; Xubo Liu; Mark D. Plumbley; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2303.03857", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Pre-trained AudioLDM for Text to Sound Generation: A Benchmark Study\\u00a7r\\n\\n\\u00a78\\u00a7oYi Yuan\\nHaohe Liu\\nJinhua Liang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.03857\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 11 Mar 2023 09:22:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oEUSIPCO 2023\\u00a7r"}']}
{title:'Ngo et al. (§72023§r)', author: 'Dat Ngo; Lam Pham; Huy Phan; Minh Tran; Delaram Jarchi; Sefki Kolozali', display:{Lore:['[{"text": "arXiv:2303.04104", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Inception-Residual-Based Architecture with Multi-Objective Loss for Detecting Respiratory Anomalies\\u00a7r\\n\\n\\u00a78\\u00a7oDat Ngo\\nLam Pham\\nHuy Phan\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.04104\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 19 Jun 2023 21:42:15 GMT)\\u00a7r"}']}
{title:'Gao et al. (§72023§r)', author: 'Chenyang Gao; Yue Gu; Francesco Caliva; Yuzong Liu', display:{Lore:['[{"text": "arXiv:2303.04255", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervised speech representation learning for keyword-spotting with light-weight transformers\\u00a7r\\n\\n\\u00a78\\u00a7oChenyang Gao\\nYue Gu\\nFrancesco Caliva\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.04255\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Mar 2023 21:54:35 GMT)\\u00a7r"}']}
{title:'Fernandez (§72023§r)', author: 'Andres Fernandez', display:{Lore:['[{"text": "arXiv:2303.04485", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOnsets and Velocities: Affordable Real-Time Piano Transcription Using Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oAndres Fernandez\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.04485\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 1 Jun 2023 00:42:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at EUSIPCO 2023\\u00a7r"}']}
{title:'Fang et al. (§72023§r)', author: 'Sen Fang; Yangjian Wu; Bowen Gao; Jingwen Cai; Teik Toe Teoh', display:{Lore:['[{"text": "arXiv:2303.04585", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Efficient-Tuned Learning Audio Representation Method from BriVL\\u00a7r\\n\\n\\u00a78\\u00a7oSen Fang\\nYangjian Wu\\nBowen Gao\\nJingwen Cai\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.04585\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 29 Jul 2023 03:42:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 2023.3 Finished\\u00a7r"}']}
{title:'Zhao et al. (§72023§r)', author: 'Ziping Zhao; Huan Wang; Haishuai Wang; Bjorn Schuller', display:{Lore:['[{"text": "arXiv:2303.05134", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lhierarchical network with decoupled knowledge distillation for speech emotion recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZiping Zhao\\nHuan Wang\\nHaishuai Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.05134\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Mar 2023 09:40:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages,4 figures,icassp 2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Qi Chen; Ziyang Ma; Tao Liu; Xu Tan; Qu Lu; Xie Chen; Kai Yu', display:{Lore:['[{"text": "arXiv:2303.05322", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Few-Shot Learning for Talking Face System with TTS Data Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oQi Chen\\nZiyang Ma\\nTao Liu\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.05322\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Mar 2023 15:13:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages. Accepted by ICASSP 2023\\u00a7r"}']}
{title:'Xu et al. (§72023§r)', author: 'Ruize Xu; Ruoxuan Feng; Shi-Xiong Zhang; Di Hu', display:{Lore:['[{"text": "arXiv:2303.05338", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMMCosine: Multi-Modal Cosine Loss Towards Balanced Audio-Visual Fine-Grained Learning\\u00a7r\\n\\n\\u00a78\\u00a7oRuize Xu\\nRuoxuan Feng\\nShi-Xiong Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.05338\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 11 Mar 2023 16:11:24 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Jiaming Wang; Zhihao Du; Shiliang Zhang', display:{Lore:['[{"text": "arXiv:2303.05397", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTOLD: A Novel Two-Stage Overlap-Aware Framework for Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oJiaming Wang\\nZhihao Du\\nShiliang Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.05397\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 13 Dec 2023 12:03:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2023\\u00a7r"}']}
{title:'Xin et al. (§72023§r)', author: 'Yifei Xin; Dongchao Yang; Fan Cui; Yujun Wang; Yuexian Zou', display:{Lore:['[{"text": "arXiv:2303.05678", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Weakly Supervised Sound Event Detection with Causal Intervention\\u00a7r\\n\\n\\u00a78\\u00a7oYifei Xin\\nDongchao Yang\\nFan Cui\\nYujun Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.05678\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 10 Mar 2023 03:13:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2023\\u00a7r"}']}
{title:'Xin et al. (§72023§r)', author: 'Yifei Xin; Dongchao Yang; Yuexian Zou', display:{Lore:['[{"text": "arXiv:2303.05681", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Text-Audio Retrieval by Text-aware Attention Pooling and Prior Matrix Revised Loss\\u00a7r\\n\\n\\u00a78\\u00a7oYifei Xin\\nDongchao Yang\\nYuexian Zou\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.05681\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 30 Mar 2023 10:17:02 GMT)\\u00a7r"}']}
{title:'Huq et al. (§72023§r)', author: 'Aminul Huq; Weiyi Zhang; Xiaolin Hu', display:{Lore:['[{"text": "arXiv:2303.05758", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMIXPGD: Hybrid Adversarial Training for Speech Recognition Systems\\u00a7r\\n\\n\\u00a78\\u00a7oAminul Huq\\nWeiyi Zhang\\nXiaolin Hu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.05758\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 10 Mar 2023 07:52:28 GMT)\\u00a7r"}']}
{title:'Xu et al. (§72023§r)', author: 'Weiming Xu; Zhihao Guo', display:{Lore:['[{"text": "arXiv:2303.06379", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTaylorAECNet: A Taylor Style Neural Network for Full-Band Echo Cancellation\\u00a7r\\n\\n\\u00a78\\u00a7oWeiming Xu\\nZhihao Guo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.06379\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 11 Mar 2023 11:12:49 GMT)\\u00a7r"}']}
{title:'Eisenberg et al. (§72023§r)', author: 'Aviad Eisenberg; Sharon Gannot; Shlomo E. Chazan', display:{Lore:['[{"text": "arXiv:2303.07072", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA two-stage speaker extraction algorithm under adverse acoustic conditions using a single-microphone\\u00a7r\\n\\n\\u00a78\\u00a7oAviad Eisenberg\\nSharon Gannot\\nShlomo E. Chazan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07072\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Mar 2023 12:48:15 GMT)\\u00a7r"}']}
{title:'Badlani et al. (§72023§r)', author: 'Rohan Badlani; Akshit Arora; Subhankar Ghosh; Rafael Valle; Kevin J. Shih; João Felipe Santos; Boris Ginsburg; Bryan Catanzaro', display:{Lore:['[{"text": "arXiv:2303.07578", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVANI: Very-lightweight Accent-controllable TTS for Native and Non-native speakers with Identity Preservation\\u00a7r\\n\\n\\u00a78\\u00a7oRohan Badlani\\nAkshit Arora\\nSubhankar Ghosh\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07578\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Mar 2023 01:55:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresentation accepted at ICASSP 2023\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Xiaoyu Liu; Hanlin Lu; Jianbo Yuan; Xinyu Li', display:{Lore:['[{"text": "arXiv:2303.07626", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCAT: Causal Audio Transformer for Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoyu Liu\\nHanlin Lu\\nJianbo Yuan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07626\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Mar 2023 04:50:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Kang et al. (§72023§r)', author: 'Zuheng Kang; Yayun He; Jianzong Wang; Junqing Peng; Xiaoyang Qu; Jing Xiao', display:{Lore:['[{"text": "arXiv:2303.07643", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFeature-Rich Audio Model Inversion for Data-Free Knowledge Distillation Towards General Sound Classification\\u00a7r\\n\\n\\u00a78\\u00a7oZuheng Kang\\nYayun He\\nJianzong Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07643\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Mar 2023 06:04:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023. International Conference on Acoustics, Speech and Signal Processing (ICASSP 2023)\\u00a7r"}']}
{title:'Ru et al. (§72023§r)', author: 'Ganghui Ru; Xulong Zhang; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2303.07667", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Music Genre Classification from Multi-Modal Properties of Music and Genre Correlations Perspective\\u00a7r\\n\\n\\u00a78\\u00a7oGanghui Ru\\nXulong Zhang\\nJianzong Wang\\nNing Cheng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07667\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 12 Jun 2023 01:58:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Tang et al. (§72023§r)', author: 'Haobin Tang; Xulong Zhang; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2303.07682", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQI-TTS: Questioning Intonation Control for Emotional Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oHaobin Tang\\nXulong Zhang\\nJianzong Wang\\nNing Cheng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07682\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Mar 2023 07:53:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Xulong Zhang; Haobin Tang; Jianzong Wang; Ning Cheng; Jian Luo; Jing Xiao', display:{Lore:['[{"text": "arXiv:2303.07687", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDynamic Alignment Mask CTC: Improved Mask-CTC with Aligned Cross Entropy\\u00a7r\\n\\n\\u00a78\\u00a7oXulong Zhang\\nHaobin Tang\\nJianzong Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07687\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Mar 2023 08:01:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Qiang et al. (§72023§r)', author: 'Chunyu Qiang; Peng Yang; Hao Che; Ying Zhang; Xiaorui Wang; Zhongyuan Wang', display:{Lore:['[{"text": "arXiv:2303.07711", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Prosody for Cross-Speaker Style Transfer by Semi-Supervised Style Extractor and Hierarchical Modeling in Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oChunyu Qiang\\nPeng Yang\\nHao Che\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07711\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Mar 2023 08:52:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2023\\u00a7r"}']}
{title:'Wang (§72023§r)', author: 'Hongfei Wang', display:{Lore:['[{"text": "arXiv:2303.07794", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffuseRoll: Multi-track multi-category music generation based on diffusion model\\u00a7r\\n\\n\\u00a78\\u00a7oHongfei Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07794\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Mar 2023 11:09:12 GMT)\\u00a7r"}']}
{title:'Hajavi et al. (§72023§r)', author: 'Amirhossein Hajavi; Ali Etemad', display:{Lore:['[{"text": "arXiv:2303.08026", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Study on Bias and Fairness In Deep Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAmirhossein Hajavi\\nAli Etemad\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08026\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Mar 2023 16:08:19 GMT)\\u00a7r"}']}
{title:'Kulvicius et al. (§72023§r)', author: 'Tomas Kulvicius; Sigrun Lang; Claudius AA Widmann; Nina Hansmann; Daniel Holzinger; Luise Poustka; Dajie Zhang; Peter B Marschik', display:{Lore:['[{"text": "arXiv:2303.08239", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFacilitating deep acoustic phenotyping: A basic coding scheme of infant vocalisations preluding computational analysis, machine learning and clinical reasoning\\u00a7r\\n\\n\\u00a78\\u00a7oTomas Kulvicius\\nSigrun Lang\\nClaudius AA Widmann\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08239\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Mar 2023 21:10:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper is in German\\u00a7r"}']}
{title:'Jo et al. (§72023§r)', author: 'Suhee Jo; Younggun Lee; Yookyung Shin; Yeongtae Hwang; Taesu Kim', display:{Lore:['[{"text": "arXiv:2303.08329", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-speaker Emotion Transfer by Manipulating Speech Style Latents\\u00a7r\\n\\n\\u00a78\\u00a7oSuhee Jo\\nYounggun Lee\\nYookyung Shin\\nYeongtae Hwang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08329\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Mar 2023 02:34:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to ICASSP 2023\\u00a7r"}']}
{title:'Ooi et al. (§72023§r)', author: 'Kenneth Ooi; Karn N. Watcharasupat; Bhan Lam; Zhen-Ting Ong; Woon-Seng Gan', display:{Lore:['[{"text": "arXiv:2303.08342", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutonomous Soundscape Augmentation with Multimodal Fusion of Visual and Participant-linked Inputs\\u00a7r\\n\\n\\u00a78\\u00a7oKenneth Ooi\\nKarn N. Watcharasupat\\nBhan Lam\\nZhen-Ting Ong\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08342\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10094866\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2023 - 2023 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP), Jun. 2023, pp. 1-5\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Mar 2023 03:17:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures. Submitted to 2023 IEEE International Conference on Acoustics, Speech and Signal Processing\\u00a7r"}']}
{title:'Gulzar et al. (§72023§r)', author: 'Hafsa Gulzar; Jiyun Li; Arslan Manzoor; Sadaf Rehmat; Usman Amjad; Hadiqa Jalil Khan', display:{Lore:['[{"text": "arXiv:2303.08362", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransfer Learning Based Diagnosis and Analysis of Lung Sound Aberrations\\u00a7r\\n\\n\\u00a78\\u00a7oHafsa Gulzar\\nJiyun Li\\nArslan Manzoor\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08362\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Mar 2023 04:46:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 9 figures\\u00a7r"}']}
{title:'Atassi (§72023§r)', author: 'Lilac Atassi', display:{Lore:['[{"text": "arXiv:2303.08385", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerating symbolic music using diffusion models\\u00a7r\\n\\n\\u00a78\\u00a7oLilac Atassi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08385\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 15 May 2023 04:09:19 GMT)\\u00a7r"}']}
{title:'Pan et al. (§72023§r)', author: 'Yulin Pan; Xiangteng He; Biao Gong; Yuxin Peng; Yiliang Lv', display:{Lore:['[{"text": "arXiv:2303.08561", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Unsupervised Audio Representation Learning via Adversarial Sample Generation\\u00a7r\\n\\n\\u00a78\\u00a7oYulin Pan\\nXiangteng He\\nBiao Gong\\nYuxin Peng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08561\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Mar 2023 12:25:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 4 figures\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Yuning Wu; Jiatong Shi; Tao Qian; Dongji Gao; Qin Jin', display:{Lore:['[{"text": "arXiv:2303.08607", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPHONEix: Acoustic Feature Processing Strategy for Enhanced Singing Pronunciation with Phoneme Distribution Predictor\\u00a7r\\n\\n\\u00a78\\u00a7oYuning Wu\\nJiatong Shi\\nTao Qian\\nDongji Gao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08607\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Mar 2023 13:27:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Sungho Lee; Jaehyun Park; Seungryeol Paik; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2303.08610", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBlind Estimation of Audio Processing Graph\\u00a7r\\n\\n\\u00a78\\u00a7oSungho Lee\\nJaehyun Park\\nSeungryeol Paik\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08610\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 May 2023 00:48:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Konan et al. (§72023§r)', author: 'Joseph Konan; Ojas Bhargave; Shikhar Agnihotri; Hojeong Lee; Ankit Shah; Shuo Han; Yunyang Zeng; Amanda Shu; Haohui Liu; Xuankai Chang; Hamza Khalid; Minseon Gwak; Kawon Lee; Minjeong Kim; Bhiksha Raj', display:{Lore:['[{"text": "arXiv:2303.09048", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Perceptual Quality, Intelligibility, and Acoustics on VoIP Platforms\\u00a7r\\n\\n\\u00a78\\u00a7oJoseph Konan\\nOjas Bhargave\\nShikhar Agnihotri\\n+ 11 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.09048\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Mar 2023 02:36:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review at European Association for Signal Processing. 5 pages\\u00a7r"}']}
{title:'Lin et al. (§72023§r)', author: 'Yi-Han Lin; Xunquan Chen; Ryoichi Takashima; Tetsuya Takiguchi', display:{Lore:['[{"text": "arXiv:2303.10316", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero-shot Sound Event Classification Using a Sound Attribute Vector with Global and Local Feature Learning\\u00a7r\\n\\n\\u00a78\\u00a7oYi-Han Lin\\nXunquan Chen\\nRyoichi Takashima\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.10316\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 18 Mar 2023 03:21:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2023\\u00a7r"}']}
{title:'Lin et al. (§72023§r)', author: 'Guan-Ting Lin; Qingming Tang; Chieh-Chi Kao; Viktor Rozgic; Chao Wang', display:{Lore:['[{"text": "arXiv:2303.10351", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeight-sharing Supernet for Searching Specialized Acoustic Event Classification Networks Across Device Constraints\\u00a7r\\n\\n\\u00a78\\u00a7oGuan-Ting Lin\\nQingming Tang\\nChieh-Chi Kao\\nViktor Rozgic\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.10351\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 18 Mar 2023 07:10:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Xiyuxing Zhang; Yuntao Wang; Jingru Zhang; Yaqing Yang; Shwetak Patel; Yuanchun Shi', display:{Lore:['[{"text": "arXiv:2303.10445", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEarCough: Enabling Continuous Subject Cough Event Detection on Hearables\\u00a7r\\n\\n\\u00a78\\u00a7oXiyuxing Zhang\\nYuntao Wang\\nJingru Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.10445\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 18 Mar 2023 16:03:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper has been accepted by ACM CHI 2023\\u00a7r"}']}
{title:'Verma et al. (§72023§r)', author: 'Prateek Verma; Chris Chafe', display:{Lore:['[{"text": "arXiv:2303.10446", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContent Adaptive Front End For Audio Signal Processing\\u00a7r\\n\\n\\u00a78\\u00a7oPrateek Verma\\nChris Chafe\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.10446\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 29 Apr 2023 14:54:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures. 2023 IEEE International Conference on Acoustics, Speech, and Signal Processing, Rhodes, Greece Minor Edits; Updated title\\u00a7r"}']}
{title:'Doh et al. (§72023§r)', author: 'SeungHeon Doh; Minz Won; Keunwoo Choi; Juhan Nam', display:{Lore:['[{"text": "arXiv:2303.10539", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTextless Speech-to-Music Retrieval Using Emotion Similarity\\u00a7r\\n\\n\\u00a78\\u00a7oSeungHeon Doh\\nMinz Won\\nKeunwoo Choi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.10539\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 19 Mar 2023 02:33:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo Appear IEEE ICASSP 2023\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Ho-Hsiang Wu; Oriol Nieto; Juan Pablo Bello; Justin Salamon', display:{Lore:['[{"text": "arXiv:2303.10667", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-Text Models Do Not Yet Leverage Natural Language\\u00a7r\\n\\n\\u00a78\\u00a7oHo-Hsiang Wu\\nOriol Nieto\\nJuan Pablo Bello\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.10667\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 19 Mar 2023 14:14:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCopyright 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising "}','{"text": "or promotional purposes, creating newcollective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Wentao Zhu; Mohamed Omar', display:{Lore:['[{"text": "arXiv:2303.10757", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultiscale Audio Spectrogram Transformer for Efficient Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oWentao Zhu\\nMohamed Omar\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.10757\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 19 Mar 2023 20:21:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Cui et al. (§72023§r)', author: 'Fan Cui; Liyong Guo; Lang He; Jiyao Liu; ErCheng Pei; Yujun Wang; Dongmei Jiang', display:{Lore:['[{"text": "arXiv:2303.10897", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRelate auditory speech to EEG by shallow-deep attention-based network\\u00a7r\\n\\n\\u00a78\\u00a7oFan Cui\\nLiyong Guo\\nLang He\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.10897\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Mar 2023 06:34:22 GMT)\\u00a7r"}']}
{title:'Cui et al. (§72023§r)', author: 'Fan Cui; Liyong Guo; Quandong Wang; Peng Gao; Yujun Wang', display:{Lore:['[{"text": "arXiv:2303.10912", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Representation Learning for Small-Footprint Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oFan Cui\\nLiyong Guo\\nQuandong Wang\\nPeng Gao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.10912\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10558\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Mar 2023 07:09:26 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yangfu Li; Jiapan Gan; Xiaodan Lin', display:{Lore:['[{"text": "arXiv:2303.11020", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDS-TDNN: Dual-stream Time-delay Neural Network with Global-aware Filter for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oYangfu Li\\nJiapan Gan\\nXiaodan Lin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.11020\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 1 Aug 2023 07:09:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages 4 figures\\u00a7r"}']}
{title:'Dubey et al. (§72023§r)', author: 'Harishchandra Dubey; Ashkan Aazami; Vishak Gopal; Babak Naderi; Sebastian Braun; Ross Cutler; Alex Ju; Mehdi Zohourian; Min Tang; Hannes Gamper; Mehrsa Golestaneh; Robert Aichner', display:{Lore:['[{"text": "arXiv:2303.11510", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lICASSP 2023 Deep Noise Suppression Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oHarishchandra Dubey\\nAshkan Aazami\\nVishak Gopal\\n+ 8 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.11510\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 May 2023 00:26:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 1 figure. arXiv admin note: text overlapwith arXiv:2202.13288\\u00a7r"}']}
{title:'Du et al. (§72023§r)', author: 'Xingjian Du; Zijie Wang; Xia Liang; Huidong Liang; Bilei Zhu; Zejun Ma', display:{Lore:['[{"text": "arXiv:2303.11692", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lByteCover3: Accurate Cover Song Identification on Short Queries\\u00a7r\\n\\n\\u00a78\\u00a7oXingjian Du\\nZijie Wang\\nXia Liang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.11692\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Mar 2023 09:27:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepeted by ICASSP 2023\\u00a7r"}']}
{title:'Huang et al. (§72023§r)', author: 'Sung-Feng Huang; Chia-ping Chen; Zhi-Sheng Chen; Yu-Pao Tsai; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2303.11816", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPersonalized Lightweight Text-to-Speech: Voice Cloning with Adaptive Structured Pruning\\u00a7r\\n\\n\\u00a78\\u00a7oSung-Feng Huang\\nChia-ping Chen\\nZhi-Sheng Chen\\nYu-Pao Tsai\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.11816\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Mar 2023 12:59:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Ren et al. (§72023§r)', author: 'Zeyu Ren; Nurmement Yolwas; Huiru Wang; Wushour Slamu', display:{Lore:['[{"text": "arXiv:2303.12300", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Turkish Speech Recognition via Hybrid CTC/Attention Architecture and Multi-feature Fusion Network\\u00a7r\\n\\n\\u00a78\\u00a7oZeyu Ren\\nNurmement Yolwas\\nHuiru Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.12300\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 Mar 2023 04:11:35 GMT)\\u00a7r"}']}
{title:'Kenwright (§72023§r)', author: 'Benjamin Kenwright', display:{Lore:['[{"text": "arXiv:2303.12692", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDual-Quaternions: Theory and Applications in Sound\\u00a7r\\n\\n\\u00a78\\u00a7oBenjamin Kenwright\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.12692\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 23 Mar 2023 17:09:59 GMT)\\u00a7r"}']}
{title:'Jenrungrot et al. (§72023§r)', author: 'Teerapat Jenrungrot; Michael Chinen; W. Bastiaan Kleijn; Jan Skoglund; Zalán Borsos; Neil Zeghidour; Marco Tagliasacchi', display:{Lore:['[{"text": "arXiv:2303.12984", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLMCodec: A Low Bitrate Speech Codec With Causal Transformer Models\\u00a7r\\n\\n\\u00a78\\u00a7oTeerapat Jenrungrot\\nMichael Chinen\\nW. Bastiaan Kleijn\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.12984\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Mar 2023 01:27:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted to ICASSP 2023, project page: https://mjenrungrot.github.io/chrome-media-audio-papers/publications/lmcodec\\u00a7r"}']}
{title:'Tang et al. (§72023§r)', author: 'Haoyu Tang; Zhaoyi Liu; Chang Zeng; Xinfeng Li', display:{Lore:['[{"text": "arXiv:2303.13072", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBeyond Universal Transformer: block reusing with adaptor in Transformer for automatic speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHaoyu Tang\\nZhaoyi Liu\\nChang Zeng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.13072\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 5 Apr 2023 08:36:34 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Dichucheng Li; Mingjin Che; Wenwu Meng; Yulun Wu; Yi Yu; Fan Xia; Wei Li', display:{Lore:['[{"text": "arXiv:2303.13272", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrame-Level Multi-Label Playing Technique Detection Using Multi-Scale Network and Self-Attention Mechanism\\u00a7r\\n\\n\\u00a78\\u00a7oDichucheng Li\\nMingjin Che\\nWenwu Meng\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.13272\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Mar 2023 13:52:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Chenshuang Zhang; Chaoning Zhang; Sheng Zheng; Mengchun Zhang; Maryam Qamar; Sung-Ho Bae; In So Kweon', display:{Lore:['[{"text": "arXiv:2303.13336", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI\\u00a7r\\n\\n\\u00a78\\u00a7oChenshuang Zhang\\nChaoning Zhang\\nSheng Zheng\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.13336\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 2 Apr 2023 09:27:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages\\u00a7r"}']}
{title:'Hernandez-Olivan et al. (§72023§r)', author: 'Carlos Hernandez-Olivan; Sonia Rubio Llamas; Jose R. Beltran', display:{Lore:['[{"text": "arXiv:2303.13881", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSymbolic Music Structure Analysis with Graph Representations and Changepoint Detection Methods\\u00a7r\\n\\n\\u00a78\\u00a7oCarlos Hernandez-Olivan\\nSonia Rubio Llamas\\nJose R. Beltran\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.13881\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Mar 2023 09:45:11 GMT)\\u00a7r"}']}
{title:'Kaneko et al. (§72023§r)', author: 'Takuhiro Kaneko; Hirokazu Kameoka; Kou Tanaka; Shogo Seki', display:{Lore:['[{"text": "arXiv:2303.13909", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWave-U-Net Discriminator: Fast and Lightweight Discriminator for Generative Adversarial Network-Based Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oTakuhiro Kaneko\\nHirokazu Kameoka\\nKou Tanaka\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.13909\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Mar 2023 10:46:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023. Project page: https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/waveunetd/\\u00a7r"}']}
{title:'Shi et al. (§72023§r)', author: 'Hao Shi; Masato Mimura; Longbiao Wang; Jianwu Dang; Tatsuya Kawahara', display:{Lore:['[{"text": "arXiv:2303.14593", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTime-domain Speech Enhancement Assisted by Multi-resolution Frequency Encoder and Decoder\\u00a7r\\n\\n\\u00a78\\u00a7oHao Shi\\nMasato Mimura\\nLongbiao Wang\\nJianwu Dang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.14593\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 26 Mar 2023 00:30:06 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Yunhao Chen; Yunjie Zhu; Zihui Yan; Jianlu Shen; Zhen Ren; Yifan Huang', display:{Lore:['[{"text": "arXiv:2303.15161", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData Augmentation for Environmental Sound Classification Using Diffusion Probabilistic Model with Top-k Selection Discriminator\\u00a7r\\n\\n\\u00a78\\u00a7oYunhao Chen\\nYunjie Zhu\\nZihui Yan\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.15161\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 4 Apr 2023 17:18:12 GMT)\\u00a7r"}']}
{title:'Lazzari et al. (§72023§r)', author: 'Nicolas Lazzari; Andrea Poltronieri; Valentina Presutti', display:{Lore:['[{"text": "arXiv:2303.15306", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPitchclass2vec: Symbolic Music Structure Segmentation with Chord Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oNicolas Lazzari\\nAndrea Poltronieri\\nValentina Presutti\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.15306\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 1st Workshop on Artificial Intelligence and\\n  Creativity co-located with 21th International Conference of the Italian\\n  Association for Artificial Intelligence(AIxIA 2022), Udine, Italy, November\\n  28 - December 3, 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Mar 2023 10:23:15 GMT)\\u00a7r"}']}
{title:'Gege et al. (§72023§r)', author: 'Qi Gege; Yuefeng Chen; Xiaofeng Mao; Yao Zhu; Binyuan Hui; Xiaodan Li; Rong Zhang; Hui Xue', display:{Lore:['[{"text": "arXiv:2303.15940", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransAudio: Towards the Transferable Adversarial Audio Attack via Learning Contextualized Perturbations\\u00a7r\\n\\n\\u00a78\\u00a7oQi Gege\\nYuefeng Chen\\nXiaofeng Mao\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.15940\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Mar 2023 12:53:02 GMT)\\u00a7r"}']}
{title:'Jiang et al. (§72023§r)', author: 'Anbai Jiang; Wei-Qiang Zhang; Yufeng Deng; Pingyi Fan; Jia Liu', display:{Lore:['[{"text": "arXiv:2303.17949", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Anomaly Detection and Localization of Machine Audio: A GAN-based Approach\\u00a7r\\n\\n\\u00a78\\u00a7oAnbai Jiang\\nWei-Qiang Zhang\\nYufeng Deng\\nPingyi Fan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.17949\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 31 Mar 2023 10:27:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Bo-Kyeong Kim; Jaemin Kang; Daeun Seo; Hancheol Park; Shinkook Choi; Hyoung-Kyu Song; Hyungshin Kim; Sungsu Lim', display:{Lore:['[{"text": "arXiv:2304.00471", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Unified Compression Framework for Efficient Speech-Driven Talking-Face Generation\\u00a7r\\n\\n\\u00a78\\u00a7oBo-Kyeong Kim\\nJaemin Kang\\nDaeun Seo\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.00471\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 28 Apr 2023 15:55:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMLSys Workshop on On-DeviceIntelligence, 2023; Demo: https://huggingface.co/spaces/nota-ai/compressed_wav2lip\\u00a7r"}']}
{title:'Maksymov (§72023§r)', author: 'Ivan S. Maksymov', display:{Lore:['[{"text": "arXiv:2304.00822", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.flu-dyn\\u00a7r, \\u00a75physics.soc-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusical creativity enabled by nonlinear oscillations of a bubble in water\\u00a7r\\n\\n\\u00a78\\u00a7oIvan S. Maksymov\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.00822\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 3 Apr 2023 09:08:52 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Yuancheng Wang; Zeqian Ju; Xu Tan; Lei He; Zhizheng Wu; Jiang Bian; Sheng Zhao', display:{Lore:['[{"text": "arXiv:2304.00830", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAUDIT: Audio Editing by Following Instructions with Latent Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oYuancheng Wang\\nZeqian Ju\\nXu Tan\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.00830\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 5 Apr 2023 12:13:48 GMT)\\u00a7r"}']}
{title:'Antoniou et al. (§72023§r)', author: 'Nikolaos Antoniou; Athanasios Katsamanis; Theodoros Giannakopoulos; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2304.00860", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDesigning and Evaluating Speech Emotion Recognition Systems: A reality check case study with IEMOCAP\\u00a7r\\n\\n\\u00a78\\u00a7oNikolaos Antoniou\\nAthanasios Katsamanis\\nTheodoros Giannakopoulos\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.00860\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10096808\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 3 Apr 2023 10:16:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2023\\u00a7r"}']}
{title:'Rios-Navarro et al. (§72023§r)', author: 'Antonio Rios-Navarro; Enrique Piñero-Fuentes; Salvador Canas-Moreno; Aqib Javed; Jin Harkin; Alejandro Linares-Barranco', display:{Lore:['[{"text": "arXiv:2304.01080", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLIPSFUS: A neuromorphic dataset for audio-visual sensory fusion of lip reading\\u00a7r\\n\\n\\u00a78\\u00a7oAntonio Rios-Navarro\\nEnrique Pi\\u00f1ero-Fuentes\\nSalvador Canas-Moreno\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.01080\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Mar 2023 12:27:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ISCAS2023, 4 pages, plus references, github link provided\\u00a7r"}']}
{title:'Sahai et al. (§72023§r)', author: 'Saumya Y. Sahai; Jing Liu; Thejaswi Muniyappa; Kanthashree M. Sathyendra; Anastasios Alexandridis; Grant P. Strimel; Ross McGowan; Ariya Rastrow; Feng-Ju Chang; Athanasios Mouchtaris; Siegfried Kunzmann', display:{Lore:['[{"text": "arXiv:2304.01905", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDual-Attention Neural Transducers for Efficient Wake Word Spotting in Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSaumya Y. Sahai\\nJing Liu\\nThejaswi Muniyappa\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.01905\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 5 Apr 2023 01:22:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Proc. IEEE ICASSP 2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Ke Chen; Gordon Wichern; François G. Germain; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2304.02160", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPac-HuBERT: Self-Supervised Music Source Separation via Primitive Auditory Clustering and Hidden-Unit BERT\\u00a7r\\n\\n\\u00a78\\u00a7oKe Chen\\nGordon Wichern\\nFran\\u00e7ois G. Germain\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.02160\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 4 Apr 2023 23:19:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 3 tables\\u00a7r"}']}
{title:'Kouzelis et al. (§72023§r)', author: 'Thodoris Kouzelis; Grigoris Bastas; Athanasios Katsamanis; Alexandros Potamianos', display:{Lore:['[{"text": "arXiv:2304.02916", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Audio Captioning Transformer with Patchout and Text Guidance\\u00a7r\\n\\n\\u00a78\\u00a7oThodoris Kouzelis\\nGrigoris Bastas\\nAthanasios Katsamanis\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.02916\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 6 Apr 2023 07:58:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Euihyoek Lee; Chulhong Min; Jeaseung Lee; Jin Yu; Seungwoo Kang', display:{Lore:['[{"text": "arXiv:2304.03295", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Detection of Reactions to Music via Earable Sensing\\u00a7r\\n\\n\\u00a78\\u00a7oEuihyoek Lee\\nChulhong Min\\nJeaseung Lee\\nJin Yu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.03295\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 6 Apr 2023 08:11:03 GMT)\\u00a7r"}']}
{title:'Yadav et al. (§72023§r)', author: 'Amit Kumar Singh Yadav; Kratika Bhagtani; Ziyue Xiang; Paolo Bestagini; Stefano Tubaro; Edward J. Delp', display:{Lore:['[{"text": "arXiv:2304.03323", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDSVAE: Interpretable Disentangled Representation for Synthetic Speech Detection\\u00a7r\\n\\n\\u00a78\\u00a7oAmit Kumar Singh Yadav\\nKratika Bhagtani\\nZiyue Xiang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.03323\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 28 Jul 2023 20:38:31 GMT)\\u00a7r"}']}
{title:'Yi et al. (§72023§r)', author: 'Wonjun Yi; Jung-Woo Choi', display:{Lore:['[{"text": "arXiv:2304.03522", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn-site Noise Exposure technique for noise-robust machine fault classification\\u00a7r\\n\\n\\u00a78\\u00a7oWonjun Yi\\nJung-Woo Choi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.03522\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 7 Apr 2023 07:35:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at 24th International Congress on Acoustics (ICA) 2022\\u00a7r"}']}
{title:'Xiao et al. (§72023§r)', author: 'Feiyang Xiao; Jian Guan; Qiaoxi Zhu; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2304.03586", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGraph Attention for Automated Audio Captioning\\u00a7r\\n\\n\\u00a78\\u00a7oFeiyang Xiao\\nJian Guan\\nQiaoxi Zhu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.03586\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2023.3266114\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 10 Apr 2023 04:08:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE Signal Processing Letters\\u00a7r"}']}
{title:'Guan et al. (§72023§r)', author: 'Jian Guan; Feiyang Xiao; Youde Liu; Qiaoxi Zhu; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2304.03588", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnomalous Sound Detection using Audio Representation with Machine ID based Contrastive Learning Pretraining\\u00a7r\\n\\n\\u00a78\\u00a7oJian Guan\\nFeiyang Xiao\\nYoude Liu\\nQiaoxi Zhu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.03588\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 10 Apr 2023 04:04:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2023)\\u00a7r"}']}
{title:'Yan et al. (§72023§r)', author: 'Brian Yan; Jiatong Shi; Yun Tang; Hirofumi Inaguma; Yifan Peng; Siddharth Dalmia; Peter Polák; Patrick Fernandes; Dan Berrebbi; Tomoki Hayashi; Xiaohui Zhang; Zhaoheng Ni; Moto Hira; Soumi Maiti; Juan Pino; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2304.04596", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit\\u00a7r\\n\\n\\u00a78\\u00a7oBrian Yan\\nJiatong Shi\\nYun Tang\\n+ 12 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.04596\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 6 Jul 2023 20:07:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oACL 2023; System Demonstration\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Lequn Chen; Xiling Yao; Chaolin Tan; Weiyang He; Jinlong Su; Fei Weng; Youxiang Chew; Nicholas Poh Huat Ng; Seung Ki Moon', display:{Lore:['[{"text": "arXiv:2304.04598", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIn-situ crack and keyhole pore detection in laser directed energy deposition through acoustic signal and deep learning\\u00a7r\\n\\n\\u00a78\\u00a7oLequn Chen\\nXiling Yao\\nChaolin Tan\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.04598\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.addma.2023.103547\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 Apr 2023 14:12:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o36 Pages, 16 Figures, accepted at journal Additive Manufacturing\\u00a7r"}']}
{title:'Shi et al. (§72023§r)', author: 'Jiatong Shi; Yun Tang; Ann Lee; Hirofumi Inaguma; Changhan Wang; Juan Pino; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2304.04618", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Speech-to-Speech Translation with Multiple TTS Targets\\u00a7r\\n\\n\\u00a78\\u00a7oJiatong Shi\\nYun Tang\\nAnn Lee\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.04618\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 Apr 2023 14:33:33 GMT)\\u00a7r"}']}
{title:'Agres et al. (§72023§r)', author: 'Kat R. Agres; Adyasha Dash; Phoebe Chua', display:{Lore:['[{"text": "arXiv:2304.04915", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAffectMachine-Classical: A novel system for generating affective classical music\\u00a7r\\n\\n\\u00a78\\u00a7oKat R. Agres\\nAdyasha Dash\\nPhoebe Chua\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.04915\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Apr 2023 01:06:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oK. Agres and A. Dash share first authorship\\u00a7r"}']}
{title:'Wei et al. (§72023§r)', author: 'Guangyong Wei; Zhikui Duan; Shiren Li; Guangguang Yang; Xinmei Yu; Junhua Li', display:{Lore:['[{"text": "arXiv:2304.04991", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSim-T: Simplify the Transformer Network by Multiplexing Technique for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGuangyong Wei\\nZhikui Duan\\nShiren Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.04991\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Apr 2023 05:25:00 GMT)\\u00a7r"}']}
{title:'Krause et al. (§72023§r)', author: 'Michael Krause; Christof Weiß; Meinard Müller', display:{Lore:['[{"text": "arXiv:2304.05032", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSoft Dynamic Time Warping for Multi-Pitch Estimation and Beyond\\u00a7r\\n\\n\\u00a78\\u00a7oMichael Krause\\nChristof Wei\\u00df\\nMeinard M\\u00fcller\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.05032\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Apr 2023 07:39:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2023\\u00a7r"}']}
{title:'Singh et al. (§72023§r)', author: 'Nikhil Singh; Chih-Wei Wu; Iroro Orife; Mahdi Kalayeh', display:{Lore:['[{"text": "arXiv:2304.05600", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLooking Similar, Sounding Different: Leveraging Counterfactual Cross-Modal Pairs for Audiovisual Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oNikhil Singh\\nChih-Wei Wu\\nIroro Orife\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.05600\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 12 Apr 2023 04:17:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o17 pages, 5 figures\\u00a7r"}']}
{title:'Han et al. (§72023§r)', author: 'Bing Han; Zhengyang Chen; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2304.05754", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Learning with Cluster-Aware-DINO for High-Performance Robust Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oBing Han\\nZhengyang Chen\\nYanmin Qian\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.05754\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 12 Apr 2023 10:32:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to TASLP in July 19, 2022\\u00a7r"}']}
{title:'Yong et al. (§72023§r)', author: 'Sangeon Yong; Li Su; Juhan Nam', display:{Lore:['[{"text": "arXiv:2304.05917", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Phoneme-Informed Neural Network Model for Note-Level Singing Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oSangeon Yong\\nLi Su\\nJuhan Nam\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.05917\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 12 Apr 2023 15:36:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2023\\u00a7r"}']}
{title:'Ghaheri et al. (§72023§r)', author: 'Paria Ghaheri; Ahmadreza Shateri; Hamid Nasiri', display:{Lore:['[{"text": "arXiv:2304.06016", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPD-ADSV: An Automated Diagnosing System Using Voice Signals and Hard Voting Ensemble Method for Parkinson\'s Disease\\u00a7r\\n\\n\\u00a78\\u00a7oParia Ghaheri\\nAhmadreza Shateri\\nHamid Nasiri\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.06016\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Apr 2023 17:24:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to the Software Impacts journal (Elsevier) for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\\u00a7r"}']}
{title:'Lei et al. (§72023§r)', author: 'Shun Lei; Yixuan Zhou; Liyang Chen; Zhiyong Wu; Shiyin Kang; Helen Meng', display:{Lore:['[{"text": "arXiv:2304.06359", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContext-aware Coherent Speaking Style Prediction with Hierarchical Transformers for Audiobook Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oShun Lei\\nYixuan Zhou\\nLiyang Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.06359\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 13 Apr 2023 09:26:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Rizhko (§72023§r)', author: 'Mariia Rizhko', display:{Lore:['[{"text": "arXiv:2304.06809", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLevel generation for rhythm VR games\\u00a7r\\n\\n\\u00a78\\u00a7oMariia Rizhko\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.06809\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 13 Apr 2023 20:24:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oBachelor\'s thesis, 35 pages, 25 figures\\u00a7r"}']}
{title:'Morais et al. (§72023§r)', author: 'Giovana Morais; Matthew E. P. Davies; Marcelo Queiroz; Magdalena Fuentes', display:{Lore:['[{"text": "arXiv:2304.06868", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTempo vs. Pitch: understanding self-supervised tempo estimation\\u00a7r\\n\\n\\u00a78\\u00a7oGiovana Morais\\nMatthew E. P. Davies\\nMarcelo Queiroz\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.06868\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10095292\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 Apr 2023 00:08:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, published on 2023 IEEE International Conference on Acoustics, Speech, and Signal Processing\\u00a7r"}']}
{title:'Ravenscroft et al. (§72023§r)', author: 'William Ravenscroft; Stefan Goetze; Thomas Hain', display:{Lore:['[{"text": "arXiv:2304.07142", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Data Sampling Strategies for Training Neural Network Speech Separation Models\\u00a7r\\n\\n\\u00a78\\u00a7oWilliam Ravenscroft\\nStefan Goetze\\nThomas Hain\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.07142\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 16 Jun 2023 13:42:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for EUSIPCO 2023\\u00a7r"}']}
{title:'Maia et al. (§72023§r)', author: 'Lucas S. Maia; Martín Rocamora; Luiz W. P. Biscainho; Magdalena Fuentes', display:{Lore:['[{"text": "arXiv:2304.07186", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdapting Meter Tracking Models to Latin American Music\\u00a7r\\n\\n\\u00a78\\u00a7oLucas S. Maia\\nMart\\u00edn Rocamora\\nLuiz W. P. Biscainho\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.07186\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 Apr 2023 14:57:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ISMIR 2022. This version was made aftera bug fix in the code, which lead to minor modifications in the results (updated in Figure 1 and Table 1). The paper\'s conclusions remain unchanged\\u00a7r"}']}
{title:'Akama et al. (§72023§r)', author: 'Taketo Akama; Hiroaki Kitano; Katsuhiro Takematsu; Yasushi Miyajima; Natalia Polouliakh', display:{Lore:['[{"text": "arXiv:2304.07449", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervised Auxiliary Loss for Metric Learning in Music Similarity-based Retrieval and Auto-tagging\\u00a7r\\n\\n\\u00a78\\u00a7oTaketo Akama\\nHiroaki Kitano\\nKatsuhiro Takematsu\\nYasushi Miyajima\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.07449\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 15 Apr 2023 02:00:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages\\u00a7r"}']}
{title:'Luo et al. (§72023§r)', author: 'Yi Luo; Rongzhi Gu', display:{Lore:['[{"text": "arXiv:2304.08052", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast Random Approximation of Multi-channel Room Impulse Response\\u00a7r\\n\\n\\u00a78\\u00a7oYi Luo\\nRongzhi Gu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.08052\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Apr 2023 08:11:39 GMT)\\u00a7r"}']}
{title:'Momeni et al. (§72023§r)', author: 'Ali Momeni; Xinxin Guo; Herve Lissek; Romain Fleury', display:{Lore:['[{"text": "arXiv:2304.08380", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.app-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhysics-inspired Neuroacoustic Computing Based on Tunable Nonlinear Multiple-scattering\\u00a7r\\n\\n\\u00a78\\u00a7oAli Momeni\\nXinxin Guo\\nHerve Lissek\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.08380\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Apr 2023 15:48:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o28 pages\\u00a7r"}']}
{title:'Xie et al. (§72023§r)', author: 'Xiaoping Xie; Hao Cai; Can Li; Fei Ding', display:{Lore:['[{"text": "arXiv:2304.08708", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Voice Disease Detection Method Based on MFCCs and Shallow CNN\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoping Xie\\nHao Cai\\nCan Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.08708\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 Apr 2023 02:41:59 GMT)\\u00a7r"}']}
{title:'Kumar et al. (§72023§r)', author: 'Adarsh Kumar; Pedro Sarmento', display:{Lore:['[{"text": "arXiv:2304.08953", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrom Words to Music: A Study of Subword Tokenization Techniques in Symbolic Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oAdarsh Kumar\\nPedro Sarmento\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.08953\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 25 Apr 2023 11:16:53 GMT)\\u00a7r"}']}
{title:'Xu et al. (§72023§r)', author: 'Yaoxun Xu; Baiji Liu; Qiaochu Huang and; Xingchen Song; Zhiyong Wu; Shiyin Kang; Helen Meng', display:{Lore:['[{"text": "arXiv:2304.09607", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCB-Conformer: Contextual biasing Conformer for biased word recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYaoxun Xu\\nBaiji Liu\\nQiaochu Huang and\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.09607\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 25 Apr 2023 07:45:40 GMT)\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Shangda Wu; Dingyao Yu; Xu Tan; Maosong Sun', display:{Lore:['[{"text": "arXiv:2304.11029", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCLaMP: Contrastive Language-Music Pre-training for Cross-Modal Symbolic Music Information Retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oShangda Wu\\nDingyao Yu\\nXu Tan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.11029\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 18 Oct 2023 17:16:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 5 figures, 5 tables, accepted by ISMIR 2023\\u00a7r"}']}
{title:'Hosain et al. (§72023§r)', author: 'Mehrab Hosain; Most. Yeasmin Arafat; Gazi Zahirul Islam; Jia Uddin; Md. Mobarak Hossain; Fatema Alam', display:{Lore:['[{"text": "arXiv:2304.11040", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotional Expression Detection in Spoken Language Employing Machine Learning Algorithms\\u00a7r\\n\\n\\u00a78\\u00a7oMehrab Hosain\\nMost. Yeasmin Arafat\\nGazi Zahirul Islam\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.11040\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 20 Apr 2023 17:57:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oJournal Pre-print (15 Pages, 9 Figures, 3 Tables)\\u00a7r"}']}
{title:'Mamun et al. (§72023§r)', author: 'Md. Adyelullahil Mamun; Hasnat Md. Abdullah; Md. Golam Rabiul Alam; Muhammad Mehedi Hassan; Md. Zia Uddin', display:{Lore:['[{"text": "arXiv:2304.11046", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAffective social anthropomorphic intelligent system\\u00a7r\\n\\n\\u00a78\\u00a7oMd. Adyelullahil Mamun\\nHasnat Md. Abdullah\\nMd. Golam Rabiul Alam\\nMuhammad Mehedi Hassan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.11046\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s11042-023-14597-6\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 19 Apr 2023 18:24:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMultimedia Tools and Applications (2023)\\u00a7r"}']}
{title:'Mirjafari et al. (§72023§r)', author: 'Shayan Mirjafari; Subigya Nepal; Weichen Wang; Andrew T. Campbell', display:{Lore:['[{"text": "arXiv:2304.11049", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing Mobile Data and Deep Models to Assess Auditory Verbal Hallucinations\\u00a7r\\n\\n\\u00a78\\u00a7oShayan Mirjafari\\nSubigya Nepal\\nWeichen Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.11049\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 20 Apr 2023 15:37:34 GMT)\\u00a7r"}']}
{title:'Sadok et al. (§72023§r)', author: 'Samir Sadok; Simon Leglaive; Renaud Séguier', display:{Lore:['[{"text": "arXiv:2304.11117", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA vector quantized masked autoencoder for speech emotion recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSamir Sadok\\nSimon Leglaive\\nRenaud S\\u00e9guier\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.11117\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 Apr 2023 16:37:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ohttps://samsad35.github.io/VQ-MAE-Speech/\\u00a7r"}']}
{title:'Nada et al. (§72023§r)', author: 'Ahlam Husni Abu Nada; Siddique Latif; Junaid Qadir', display:{Lore:['[{"text": "arXiv:2304.11408", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLightweight Toxicity Detection in Spoken Language: A Transformer-based Approach for Edge Devices\\u00a7r\\n\\n\\u00a78\\u00a7oAhlam Husni Abu Nada\\nSiddique Latif\\nJunaid Qadir\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.11408\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 22 Apr 2023 13:45:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder Rewiew\\u00a7r"}']}
{title:'Jin et al. (§72023§r)', author: 'Xin Jin; Wu Zhou; Jinyu Wang; Duo Xu; Yiqing Rong; Jialin Sun', display:{Lore:['[{"text": "arXiv:2304.11521", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Order-Complexity Model for Aesthetic Quality Assessment of Homophony Music Performance\\u00a7r\\n\\n\\u00a78\\u00a7oXin Jin\\nWu Zhou\\nJinyu Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.11521\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nAIART 2023 ICME Workshop\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 23 Apr 2023 03:02:24 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Jianzong Wang; Xulong Zhang; Haobin Tang; Aolan Sun; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2304.11547", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSAR: Self-Supervised Anti-Distortion Representation for End-To-End Speech Model\\u00a7r\\n\\n\\u00a78\\u00a7oJianzong Wang\\nXulong Zhang\\nHaobin Tang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.11547\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 23 Apr 2023 06:10:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IJCNN2023. 2023 International Joint Conference on Neural Networks (IJCNN2023)\\u00a7r"}']}
{title:'Yi et al. (§72023§r)', author: 'Wonjun Yi; Jung-Woo Choi; Jae-Woo Lee', display:{Lore:['[{"text": "arXiv:2304.11708", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound-based drone fault classification using multitask learning\\u00a7r\\n\\n\\u00a78\\u00a7oWonjun Yi\\nJung-Woo Choi\\nJae-Woo Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.11708\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 23 Apr 2023 17:55:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at 29thInternational Congresson Sound and Vibration (ICSV29). Dataset available: https://zenodo.org/record/7779574#.ZEVncnZBwQ-\\u00a7r"}']}
{title:'Fujita et al. (§72023§r)', author: 'Kenichi Fujita; Takanori Ashihara; Hiroki Kanagawa; Takafumi Moriya; Yusuke Ijima', display:{Lore:['[{"text": "arXiv:2304.11976", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero-shot text-to-speech synthesis conditioned using self-supervised speech representation model\\u00a7r\\n\\n\\u00a78\\u00a7oKenichi Fujita\\nTakanori Ashihara\\nHiroki Kanagawa\\nTakafumi Moriya\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.11976\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSPW59220.2023.10193459\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2023 IEEE International Conference on Acoustics, Speech, and\\n  Signal Processing Workshops (ICASSPW), 2023, pp. 1-5,\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Apr 2023 10:15:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages,3 figures, Accepted to IEEE ICASSP 2023 workshop Self-supervision in Audio, Speech and Beyond\\u00a7r"}']}
{title:'Gu et al. (§72023§r)', author: 'Xiangming Gu; Wei Zeng; Jianan Zhang; Longshen Ou; Ye Wang', display:{Lore:['[{"text": "arXiv:2304.12082", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Audio-Visual Singing Voice Transcription based on Self-Supervised Learning Models\\u00a7r\\n\\n\\u00a78\\u00a7oXiangming Gu\\nWei Zeng\\nJianan Zhang\\nLongshen Ou\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.12082\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Apr 2023 13:25:22 GMT)\\u00a7r"}']}
{title:'Akhtar et al. (§72023§r)', author: 'Zuhaib Akhtar; Mohammad Omar Khursheed; Dongsu Du; Yuzong Liu', display:{Lore:['[{"text": "arXiv:2304.12183", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSmall-footprint slimmable networks for keyword spotting\\u00a7r\\n\\n\\u00a78\\u00a7oZuhaib Akhtar\\nMohammad Omar Khursheed\\nDongsu Du\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.12183\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 Apr 2023 12:59:37 GMT)\\u00a7r"}']}
{title:'Alonso-Jiménez et al. (§72023§r)', author: 'Pablo Alonso-Jiménez; Xavier Favory; Hadrien Foroughmand; Grigoris Bourdalas; Xavier Serra; Thomas Lidy; Dmitry Bogdanov', display:{Lore:['[{"text": "arXiv:2304.12257", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPre-Training Strategies Using Contrastive Learning and Playlist Information for Music Classification and Similarity\\u00a7r\\n\\n\\u00a78\\u00a7oPablo Alonso-Jim\\u00e9nez\\nXavier Favory\\nHadrien Foroughmand\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.12257\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Apr 2023 16:49:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the 2023 International Conference on Acoustics, Speech, and Signal Processing (ICASSP\'23)\\u00a7r"}']}
{title:'Choi et al. (§72023§r)', author: 'Keunwoo Choi; Jaekwon Im; Laurie Heller; Brian McFee; Keisuke Imoto; Yuki Okamoto; Mathieu Lagrange; Shinosuke Takamichi', display:{Lore:['[{"text": "arXiv:2304.12521", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFoley Sound Synthesis at the DCASE 2023 Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oKeunwoo Choi\\nJaekwon Im\\nLaurie Heller\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.12521\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 28 Sep 2023 18:38:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDCASE 2023 Challenge - Task 7- Technical Report (Submitted to DCASE 2023 Workshop)\\u00a7r"}']}
{title:'Zhuang et al. (§72023§r)', author: 'Haolin Zhuang; Shun Lei; Long Xiao; Weiqin Li; Liyang Chen; Sicheng Yang; Zhiyong Wu; Shiyin Kang; Helen Meng', display:{Lore:['[{"text": "arXiv:2304.12704", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGTN-Bailando: Genre Consistent Long-Term 3D Dance Generation based on Pre-trained Genre Token Network\\u00a7r\\n\\n\\u00a78\\u00a7oHaolin Zhuang\\nShun Lei\\nLong Xiao\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.12704\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Apr 2023 10:17:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2023.Demo page: https://im1eon.github.io/ICASSP23-GTNB-DG/\\u00a7r"}']}
{title:'Faiß et al. (§72023§r)', author: 'Marius Faiß; Dan Stowell', display:{Lore:['[{"text": "arXiv:2304.12739", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptive Representations of Sound for Automatic Insect Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMarius Fai\\u00df\\nDan Stowell\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.12739\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1371/journal.pcbi.1011541\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Apr 2023 11:28:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o35 pages, 11 figures. arXivadmin note: substantial text overlap with arXiv:2211.09503\\u00a7r"}']}
{title:'Cancino-Chacón et al. (§72023§r)', author: 'Carlos Cancino-Chacón; Silvan Peter; Patricia Hu; Emmanouil Karystinaios; Florian Henkel; Francesco Foscarin; Nimrod Varga; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2304.12939", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe ACCompanion: Combining Reactivity, Robustness, and Musical Expressivity in an Automatic Piano Accompanist\\u00a7r\\n\\n\\u00a78\\u00a7oCarlos Cancino-Chac\\u00f3n\\nSilvan Peter\\nPatricia Hu\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.12939\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 May 2023 14:53:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of the 32nd International Joint Conference on Artificial Intelligence (IJCAI-23), Macao, China. The differences/extensions with the previous version include a technical appendix, added missinglinks, and "}','{"text": "minor text updates. 10 pages, 4 figures\\u00a7r"}']}
{title:'Xia et al. (§72023§r)', author: 'Yuanxin Xia; Cheol-Ho Jeong', display:{Lore:['[{"text": "arXiv:2304.12993", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRoom dimensions and absorption inference from room transfer function via machine learning\\u00a7r\\n\\n\\u00a78\\u00a7oYuanxin Xia\\nCheol-Ho Jeong\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.12993\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Apr 2023 17:03:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 10 figures\\u00a7r"}']}
{title:'Sun et al. (§72023§r)', author: 'Chengzhe Sun; Shan Jia; Shuwei Hou; Siwei Lyu', display:{Lore:['[{"text": "arXiv:2304.13085", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAI-Synthesized Voice Detection Using Neural Vocoder Artifacts\\u00a7r\\n\\n\\u00a78\\u00a7oChengzhe Sun\\nShan Jia\\nShuwei Hou\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.13085\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 27 Apr 2023 14:20:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper accepted in CVPRW 2023. Codes and data can be found at https://github.com/csun22/Synthetic-Voice-Detection-Vocoder-Artifacts.arXiv admin note: substantial text overlap with arXiv:2302.09198\\u00a7r"}']}
{title:'Du et al. (§72023§r)', author: 'Chenpeng Du; Yiwei Guo; Feiyu Shen; Kai Yu', display:{Lore:['[{"text": "arXiv:2304.13121", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Speaker Multi-Lingual VQTTS System for LIMMITS 2023 Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oChenpeng Du\\nYiwei Guo\\nFeiyu Shen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.13121\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Apr 2023 19:54:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023 Special Session for Grand Challenges\\u00a7r"}']}
{title:'Frommholz et al. (§72023§r)', author: 'Annika Frommholz; Fabian Seipel; Sebastian Lapuschkin; Wojciech Samek; Johanna Vielhaben', display:{Lore:['[{"text": "arXiv:2304.14019", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lXAI-based Comparison of Input Representations for Audio Event Classification\\u00a7r\\n\\n\\u00a78\\u00a7oAnnika Frommholz\\nFabian Seipel\\nSebastian Lapuschkin\\nWojciech Samek\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.14019\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Apr 2023 08:30:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 4 figures\\u00a7r"}']}
{title:'Kheddar et al. (§72023§r)', author: 'Hamza Kheddar; Yassine Himeur; Somaya Al-Maadeed; Abbes Amira; Faycal Bensaali', display:{Lore:['[{"text": "arXiv:2304.14535", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Transfer Learning for Automatic Speech Recognition: Towards Better Generalization\\u00a7r\\n\\n\\u00a78\\u00a7oHamza Kheddar\\nYassine Himeur\\nSomaya Al-Maadeed\\nAbbes Amira\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.14535\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.knosys.2023.110851\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nKnowledge-Based Systems, Elsevier, 2023\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 31 Jul 2023 11:58:18 GMT)\\u00a7r"}']}
{title:'Karystinaios et al. (§72023§r)', author: 'Emmanouil Karystinaios; Francesco Foscarin; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2304.14848", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusical Voice Separation as Link Prediction: Modeling a Musical Perception Task as a Multi-Trajectory Tracking Problem\\u00a7r\\n\\n\\u00a78\\u00a7oEmmanouil Karystinaios\\nFrancesco Foscarin\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.14848\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Apr 2023 13:48:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the 32nd International Joint Conference on Artificial Intelligence (IJCAI-23)\\u00a7r"}']}
{title:'Schuller et al. (§72023§r)', author: 'Björn W. Schuller; Anton Batliner; Shahin Amiriparian; Alexander Barnhill; Maurice Gerczuk; Andreas Triantafyllopoulos; Alice Baird; Panagiotis Tzirakis; Chris Gagne; Alan S. Cowen; Nikola Lackovic; Marie-José Caraty; Claude Montacié', display:{Lore:['[{"text": "arXiv:2304.14882", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe ACM Multimedia 2023 Computational Paralinguistics Challenge: Emotion Share     Requests\\u00a7r\\n\\n\\u00a78\\u00a7oBj\\u00f6rn W. Schuller\\nAnton Batliner\\nShahin Amiriparian\\n+ 9 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.14882\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 1 May 2023 07:59:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, part of the ACM Multimedia 2023 Grand Challenge \\"The ACM Multimedia 2023 Computational Paralinguistics Challenge (ComParE 2023). arXiv admin note: text overlap with arXiv:2205.06799\\u00a7r"}']}
{title:'Fan et al. (§72023§r)', author: 'Peng Fan; Dongyue Guo; JianWei Zhang; Bo Yang; Yi Lin', display:{Lore:['[{"text": "arXiv:2305.00170", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing multilingual speech recognition in air traffic control by sentence-level language identification\\u00a7r\\n\\n\\u00a78\\u00a7oPeng Fan\\nDongyue Guo\\nJianWei Zhang\\nBo Yang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.00170\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 29 Apr 2023 04:47:40 GMT)\\u00a7r"}']}
{title:'Okamoto et al. (§72023§r)', author: 'Yuki Okamoto; Keisuke Imoto; Shinnosuke Takamichi; Ryotaro Nagase; Takahiro Fukumori; Yoichi Yamashita', display:{Lore:['[{"text": "arXiv:2305.00302", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnvironmental sound synthesis from vocal imitations and sound event labels\\u00a7r\\n\\n\\u00a78\\u00a7oYuki Okamoto\\nKeisuke Imoto\\nShinnosuke Takamichi\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.00302\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Sep 2023 10:13:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP2024\\u00a7r"}']}
{title:'Sonali et al. (§72023§r)', author: 'C. S. Sonali; Chinmayi B S; Ahana Balasubramanian', display:{Lore:['[{"text": "arXiv:2305.00417", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransformer-based Sequence Labeling for Audio Classification based on MFCCs\\u00a7r\\n\\n\\u00a78\\u00a7oC. S. Sonali\\nChinmayi B S\\nAhana Balasubramanian\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.00417\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 5 Jul 2023 05:50:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oError in the explanation as well inadequate resultsand conclusion\\u00a7r"}']}
{title:'Leś et al. (§72023§r)', author: 'Michał Leś; Michał Woźniak', display:{Lore:['[{"text": "arXiv:2305.00426", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransfer of knowledge among instruments in automatic music transcription\\u00a7r\\n\\n\\u00a78\\u00a7oMicha\\u0142 Le\\u015b\\nMicha\\u0142 Wo\\u017aniak\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.00426\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 30 Apr 2023 08:37:41 GMT)\\u00a7r"}']}
{title:'Malik et al. (§72023§r)', author: 'Ibrahim Malik; Siddique Latif; Sanaullah Manzoor; Muhammad Usama; Junaid Qadir; Raja Jurdak', display:{Lore:['[{"text": "arXiv:2305.00725", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotions Beyond Words: Non-Speech Audio Emotion Recognition With Edge Computing\\u00a7r\\n\\n\\u00a78\\u00a7oIbrahim Malik\\nSiddique Latif\\nSanaullah Manzoor\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.00725\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 May 2023 09:01:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review\\u00a7r"}']}
{title:'Li (§72023§r)', author: 'Xinyu Li', display:{Lore:['[{"text": "arXiv:2305.01051", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLooPy: A Research-Friendly Mix Framework for Music Information Retrieval on Electronic Dance Music\\u00a7r\\n\\n\\u00a78\\u00a7oXinyu Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.01051\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 May 2023 19:30:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ACM MM 2023. arXiv admin note: substantial text overlap with arXiv:2201.05194\\u00a7r"}']}
{title:'Ng et al. (§72023§r)', author: 'Dianwen Ng; Ruixi Zhang; Jia Qi Yip; Chong Zhang; Yukun Ma; Trung Hieu Nguyen; Chongjia Ni; Eng Siong Chng; Bin Ma', display:{Lore:['[{"text": "arXiv:2305.01170", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContrastive Speech Mixup for Low-resource Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oDianwen Ng\\nRuixi Zhang\\nJia Qi Yip\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.01170\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 May 2023 03:07:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Yu et al. (§72023§r)', author: 'Jiashuo Yu; Yaohui Wang; Xinyuan Chen; Xiao Sun; Yu Qiao', display:{Lore:['[{"text": "arXiv:2305.01319", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLong-Term Rhythmic Video Soundtracker\\u00a7r\\n\\n\\u00a78\\u00a7oJiashuo Yu\\nYaohui Wang\\nXinyuan Chen\\nXiao Sun\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.01319\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 May 2023 11:04:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICML2023\\u00a7r"}']}
{title:'Pham et al. (§72023§r)', author: 'Lam Pham; Trang Le; Cam Le; Dat Ngo; Weissenfeld Axel; Alexander Schindler', display:{Lore:['[{"text": "arXiv:2305.01476", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning Based Multimodal with Two-phase Training Strategy for Daily Life Video Classification\\u00a7r\\n\\n\\u00a78\\u00a7oLam Pham\\nTrang Le\\nCam Le\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.01476\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 30 Apr 2023 19:12:34 GMT)\\u00a7r"}']}
{title:'Labbé et al. (§72023§r)', author: 'Etienne Labbé; Julien Pinquier; Thomas Pellegrini', display:{Lore:['[{"text": "arXiv:2305.01482", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultitask learning in Audio Captioning: a sentence embedding regression loss acts as a regularizer\\u00a7r\\n\\n\\u00a78\\u00a7oEtienne Labb\\u00e9\\nJulien Pinquier\\nThomas Pellegrini\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.01482\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 May 2023 15:03:20 GMT)\\u00a7r"}']}
{title:'Gorin et al. (§72023§r)', author: 'Arsenii Gorin; Cem Subakan; Sajjad Abdoli; Junhao Wang; Samantha Latremouille; Charles Onu', display:{Lore:['[{"text": "arXiv:2305.01578", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervised learning for infant cry analysis\\u00a7r\\n\\n\\u00a78\\u00a7oArsenii Gorin\\nCem Subakan\\nSajjad Abdoli\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.01578\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 May 2023 16:27:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEICASSP 2023 workshop Self-supervision in Audio, Speech and Beyond\\u00a7r"}']}
{title:'Guo et al. (§72023§r)', author: 'Dongyue Guo; Jianwei Zhang; Yi Lin', display:{Lore:['[{"text": "arXiv:2305.01661", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSIA-FTP: A Spoken Instruction Aware Flight Trajectory Prediction Framework\\u00a7r\\n\\n\\u00a78\\u00a7oDongyue Guo\\nJianwei Zhang\\nYi Lin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.01661\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 May 2023 08:28:55 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Zhepei Wang; Cem Subakan; Krishna Subramani; Junkai Wu; Tiago Tavares; Fabio Ayres; Paris Smaragdis', display:{Lore:['[{"text": "arXiv:2305.01864", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Improvement of Audio-Text Cross-Modal Representations\\u00a7r\\n\\n\\u00a78\\u00a7oZhepei Wang\\nCem Subakan\\nKrishna Subramani\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.01864\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 31 Jul 2023 18:28:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to WASPAA 2023\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Guangwei Li; Xuenan Xu; Lingfeng Dai; Mengyue Wu; Kai Yu', display:{Lore:['[{"text": "arXiv:2305.01980", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiverse and Vivid Sound Generation from Text Descriptions\\u00a7r\\n\\n\\u00a78\\u00a7oGuangwei Li\\nXuenan Xu\\nLingfeng Dai\\nMengyue Wu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.01980\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 May 2023 08:52:40 GMT)\\u00a7r"}']}
{title:'Xue et al. (§72023§r)', author: 'Jinlong Xue; Yayue Deng; Fengping Wang; Ya Li; Yingming Gao; Jianhua Tao; Jianqing Sun; Jiaen Liang', display:{Lore:['[{"text": "arXiv:2305.02269", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lM2-CTTS: End-to-End Multi-scale Multi-modal Conversational Text-to-Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oJinlong Xue\\nYayue Deng\\nFengping Wang\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.02269\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 May 2023 16:59:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figures, 2 tables. Accepted by ICASSP 2023\\u00a7r"}']}
{title:'Kowtha et al. (§72023§r)', author: 'Vasudha Kowtha; Miquel Espi Marques; Jonathan Huang; Yichi Zhang; Carlos Avendano', display:{Lore:['[{"text": "arXiv:2305.02382", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning to Detect Novel and Fine-Grained Acoustic Sequences Using Pretrained Audio Representations\\u00a7r\\n\\n\\u00a78\\u00a7oVasudha Kowtha\\nMiquel Espi Marques\\nJonathan Huang\\nYichi Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.02382\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 May 2023 18:41:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE ICASSP 2023\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Dongchao Yang; Songxiang Liu; Rongjie Huang; Jinchuan Tian; Chao Weng; Yuexian Zou', display:{Lore:['[{"text": "arXiv:2305.02765", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHiFi-Codec: Group-residual Vector quantization for High Fidelity Audio Codec\\u00a7r\\n\\n\\u00a78\\u00a7oDongchao Yang\\nSongxiang Liu\\nRongjie Huang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.02765\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 7 May 2023 09:22:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe second version of HiFi-Codec\\u00a7r"}']}
{title:'King et al. (§72023§r)', author: 'James A King; Arshdeep Singh; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:2305.03391", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCompressing audio CNNs with graph centrality based filter pruning\\u00a7r\\n\\n\\u00a78\\u00a7oJames A King\\nArshdeep Singh\\nMark D. Plumbley\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.03391\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 May 2023 09:38:05 GMT)\\u00a7r"}']}
{title:'Jonason et al. (§72023§r)', author: 'Nicolas Jonason; Bob L. T. Sturm', display:{Lore:['[{"text": "arXiv:2305.03530", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Softly Masked Language Modelling for Controllable Symbolic Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oNicolas Jonason\\nBob L. T. Sturm\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.03530\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 11 May 2023 04:44:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oVersion 1.1\\u00a7r"}']}
{title:'Stanziola et al. (§72023§r)', author: 'Antonio Stanziola; Ben T. Cox; Bradley E. Treeby; Michael D. Brown', display:{Lore:['[{"text": "arXiv:2305.03625", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.app-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhysics-Based Acoustic Holograms\\u00a7r\\n\\n\\u00a78\\u00a7oAntonio Stanziola\\nBen T. Cox\\nBradley E. Treeby\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.03625\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 May 2023 15:37:17 GMT)\\u00a7r"}']}
{title:'Hung et al. (§72023§r)', author: 'Yu Cheng Hung; Ping Hung Chen; Jian Jiun Ding', display:{Lore:['[{"text": "arXiv:2305.03982", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPitch Estimation by Denoising Preprocessor and Hybrid Estimation Model\\u00a7r\\n\\n\\u00a78\\u00a7oYu Cheng Hung\\nPing Hung Chen\\nJian Jiun Ding\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.03982\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 6 May 2023 09:05:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oFrom ICCE-Taiwan\\u00a7r"}']}
{title:'Takeuchi et al. (§72023§r)', author: 'Makoto Takeuchi; Haruo Saito', display:{Lore:['[{"text": "arXiv:2305.04531", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.ins-det\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA method for analyzing sampling jitter in audio equipment\\u00a7r\\n\\n\\u00a78\\u00a7oMakoto Takeuchi\\nHaruo Saito\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.04531\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 May 2023 08:06:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 12 figures\\u00a7r"}']}
{title:'Saleh (§72023§r)', author: 'Yahya Saleh', display:{Lore:['[{"text": "arXiv:2305.04810", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynthesizing Cough Audio with GAN for COVID-19 Detection\\u00a7r\\n\\n\\u00a78\\u00a7oYahya Saleh\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.04810\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 May 2023 10:36:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oBachelor\'s thesis\\u00a7r"}']}
{title:'Hung et al. (§72023§r)', author: 'Yu Cheng Hung; Jian-Jiun Ding', display:{Lore:['[{"text": "arXiv:2305.05139", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTemporal Convolution Network Based Onset Detection and Query by Humming System Design\\u00a7r\\n\\n\\u00a78\\u00a7oYu Cheng Hung\\nJian-Jiun Ding\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.05139\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 8 Jun 2023 01:36:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper has been withdrawn by the author due to a crucial definition of probabilitythreshold and several grammer and vocabulary mistakes\\u00a7r"}']}
{title:'Ren et al. (§72023§r)', author: 'Yanzhen Ren; Hongcheng Zhu; Liming Zhai; Zongkun Sun; Rubing Shen; Lina Wang', display:{Lore:['[{"text": "arXiv:2305.05152", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWho is Speaking Actually? Robust and Versatile Speaker Traceability for Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oYanzhen Ren\\nHongcheng Zhu\\nLiming Zhai\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.05152\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 26 Jul 2023 05:12:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ohas been accepted by ACM MM 2023\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Jingbei Li; Sipan Li; Ping Chen; Luwen Zhang; Yi Meng; Zhiyong Wu; Helen Meng; Qiao Tian; Yuping Wang; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2305.05203", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Multi-scale Cross-lingual Speaking Style Transfer with Bidirectional Attention Mechanism for Automatic Dubbing\\u00a7r\\n\\n\\u00a78\\u00a7oJingbei Li\\nSipan Li\\nPing Chen\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.05203\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 May 2023 06:44:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to TASLP\\u00a7r"}']}
{title:'Xue et al. (§72023§r)', author: 'Wei Xue; Yiwen Wang; Qifeng Liu; Yike Guo', display:{Lore:['[{"text": "arXiv:2305.05401", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearn to Sing by Listening: Building Controllable Virtual Singer by Unsupervised Learning from Voice Recordings\\u00a7r\\n\\n\\u00a78\\u00a7oWei Xue\\nYiwen Wang\\nQifeng Liu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.05401\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 May 2023 12:45:45 GMT)\\u00a7r"}']}
{title:'Reddy et al. (§72023§r)', author: 'Pradyumna Reddy; Scott Wisdom; Klaus Greff; John R. Hershey; Thomas Kipf', display:{Lore:['[{"text": "arXiv:2305.05591", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudioSlots: A slot-centric generative model for audio separation\\u00a7r\\n\\n\\u00a78\\u00a7oPradyumna Reddy\\nScott Wisdom\\nKlaus Greff\\nJohn R. Hershey\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.05591\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 May 2023 16:28:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the Self-supervision in Audio, Speech and Beyond (SASB) Workshop at ICASSP 2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Jun Chen; Wei Rao; Zilin Wang; Jiuxin Lin; Zhiyong Wu; Yannan Wang; Shidong Shang; Helen Meng', display:{Lore:['[{"text": "arXiv:2305.05599", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInter-SubNet: Speech Enhancement with Subband Interaction\\u00a7r\\n\\n\\u00a78\\u00a7oJun Chen\\nWei Rao\\nZilin Wang\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.05599\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 May 2023 16:47:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Yuanda Wang; Hanqing Guo; Guangjing Wang; Bocheng Chen; Qiben Yan', display:{Lore:['[{"text": "arXiv:2305.05736", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVSMask: Defending Against Voice Synthesis Attack via Real-Time Predictive Perturbation\\u00a7r\\n\\n\\u00a78\\u00a7oYuanda Wang\\nHanqing Guo\\nGuangjing Wang\\nBocheng Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.05736\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3558482.3590189\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 May 2023 19:31:58 GMT)\\u00a7r"}']}
{title:'Strods et al. (§72023§r)', author: 'Deniss Strods; Alan F. Smeaton', display:{Lore:['[{"text": "arXiv:2305.05780", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Gappy Speech Audio Signals with Generative Adversarial Networks\\u00a7r\\n\\n\\u00a78\\u00a7oDeniss Strods\\nAlan F. Smeaton\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.05780\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 May 2023 21:58:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 4 figures, 4 tables. 34th Irish Signals and Systems Conferences, 13-14 June 2023\\u00a7r"}']}
{title:'Harere et al. (§72023§r)', author: 'Ahmad Al Harere; Khloud Al Jallad', display:{Lore:['[{"text": "arXiv:2305.06429", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMispronunciation Detection of Basic Quranic Recitation Rules using Deep Learning\\u00a7r\\n\\n\\u00a78\\u00a7oAhmad Al Harere\\nKhloud Al Jallad\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.06429\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 May 2023 19:31:25 GMT)\\u00a7r"}']}
{title:'Zhong et al. (§72023§r)', author: 'Zhi Zhong; Hao Shi; Masato Hirano; Kazuki Shimada; Kazuya Tateishi; Takashi Shibuya; Shusuke Takahashi; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2305.06701", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExtending Audio Masked Autoencoders Toward Audio Restoration\\u00a7r\\n\\n\\u00a78\\u00a7oZhi Zhong\\nHao Shi\\nMasato Hirano\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.06701\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 17 Aug 2023 12:22:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWASPAA 2023.Copyright 2023 IEEE.Personal use of this material is permitted.Permission from IEEE must be obtained for all other uses,in any current or future media,including reprinting/republishing this material for "}','{"text": "advertising or promotional purposes, creating new collective works,for resale or redistribution to servers or lists,or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Piao et al. (§72023§r)', author: 'Zhenyu Piao; Miseul Kim; Hyungchan Yoon; Hong-Goo Kang', display:{Lore:['[{"text": "arXiv:2305.06806", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHappyQuokka System for ICASSP 2023 Auditory EEG Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oZhenyu Piao\\nMiseul Kim\\nHyungchan Yoon\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.06806\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 May 2023 07:57:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oFirst Place in Task 2 of Auditory EEG decoding Challenge, which is part of ICASSP Signal Processing Grand Challenge (SPGC) 2023\\u00a7r"}']}
{title:'Ye et al. (§72023§r)', author: 'Zhen Ye; Wei Xue; Xu Tan; Jie Chen; Qifeng Liu; Yike Guo', display:{Lore:['[{"text": "arXiv:2305.06908", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model\\u00a7r\\n\\n\\u00a78\\u00a7oZhen Ye\\nWei Xue\\nXu Tan\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.06908\\u00a7r\\n\\nVersion:\\u00a77v4 (Sun, 29 Oct 2023 14:12:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ACM MM 2023\\u00a7r"}']}
{title:'Parekh et al. (§72023§r)', author: "Jayneel Parekh; Sanjeel Parekh; Pavlo Mozharovskyi; Gaël Richard; Florence d'Alché-Buc", display:{Lore:['[{"text": "arXiv:2305.07132", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTackling Interpretability in Audio Classification Networks with Non-negative Matrix Factorization\\u00a7r\\n\\n\\u00a78\\u00a7oJayneel Parekh\\nSanjeel Parekh\\nPavlo Mozharovskyi\\nGa\\u00ebl Richard\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.07132\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 May 2023 20:50:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder submission at IEEE/ACM TASLP. arXiv admin note:text overlap with arXiv:2202.11479\\u00a7r"}']}
{title:'Ling et al. (§72023§r)', author: 'Yuhang Ling; Yuxi Li; Zhenye Gan; Jiangning Zhang; Mingmin Chi; Yabiao Wang', display:{Lore:['[{"text": "arXiv:2305.07223", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransavs: End-To-End Audio-Visual Segmentation With Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oYuhang Ling\\nYuxi Li\\nZhenye Gan\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.07223\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 26 Dec 2023 12:00:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 3 figures\\u00a7r"}']}
{title:'Betker (§72023§r)', author: 'James Betker', display:{Lore:['[{"text": "arXiv:2305.07243", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBetter speech synthesis through scaling\\u00a7r\\n\\n\\u00a78\\u00a7oJames Betker\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.07243\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 23 May 2023 21:41:54 GMT)\\u00a7r"}']}
{title:'Plachouras et al. (§72023§r)', author: 'Christos Plachouras; Marius Miron', display:{Lore:['[{"text": "arXiv:2305.07347", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Rearrangement Using Hierarchical Segmentation\\u00a7r\\n\\n\\u00a78\\u00a7oChristos Plachouras\\nMarius Miron\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.07347\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10097212\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 May 2023 09:50:54 GMT)\\u00a7r"}']}
{title:'Kong et al. (§72023§r)', author: 'Qiuqiang Kong; Ke Chen; Haohe Liu; Xingjian Du; Taylor Berg-Kirkpatrick; Shlomo Dubnov; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:2305.07447", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUniversal Source Separation with Weakly Labelled Data\\u00a7r\\n\\n\\u00a78\\u00a7oQiuqiang Kong\\nKe Chen\\nHaohe Liu\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.07447\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 May 2023 16:41:55 GMT)\\u00a7r"}']}
{title:'Morocutti et al. (§72023§r)', author: 'Tobias Morocutti; Florian Schmid; Khaled Koutini; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2305.07499", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDevice-Robust Acoustic Scene Classification via Impulse Response Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oTobias Morocutti\\nFlorian Schmid\\nKhaled Koutini\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.07499\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 27 Jun 2023 08:43:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of the 31st European Signal Processing Conference, EUSIPCO 2023. Source Code available at: https://github.com/theMoro/DIRAugmentation/\\u00a7r"}']}
{title:'Cai et al. (§72023§r)', author: 'Jinglun Cai; Monica Sunkara; Xilai Li; Anshu Bhatia; Xiao Pan; Sravan Bodapati', display:{Lore:['[{"text": "arXiv:2305.07677", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMasked Audio Text Encoders are Effective Multi-Modal Rescorers\\u00a7r\\n\\n\\u00a78\\u00a7oJinglun Cai\\nMonica Sunkara\\nXilai Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.07677\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 24 May 2023 18:18:00 GMT)\\u00a7r"}']}
{title:'Dohi et al. (§72023§r)', author: 'Kota Dohi; Keisuke Imoto; Noboru Harada; Daisuke Niizumi; Yuma Koizumi; Tomoya Nishida; Harsh Purohit; Ryo Tanabe; Takashi Endo; Yohei Kawaguchi', display:{Lore:['[{"text": "arXiv:2305.07828", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDescription and Discussion on DCASE 2023 Challenge Task 2: First-Shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring\\u00a7r\\n\\n\\u00a78\\u00a7oKota Dohi\\nKeisuke Imoto\\nNoboru Harada\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.07828\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 2 Nov 2023 05:02:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oanomaly detection, acoustic condition monitoring, domain shift, first-shot problem, DCASE Challenge, Accepted in DCASE2023 Workshop\\u00a7r"}']}
{title:'Lazzarini et al. (§72023§r)', author: 'Victor Lazzarini; Joseph Timoney', display:{Lore:['[{"text": "arXiv:2305.07909", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHigher-Order Frequency Modulation Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oVictor Lazzarini\\nJoseph Timoney\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.07909\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 13 May 2023 12:25:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 6 figures\\u00a7r"}']}
{title:'Ai et al. (§72023§r)', author: 'Yang Ai; Zhen-Hua Ling', display:{Lore:['[{"text": "arXiv:2305.07952", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAPNet: An All-Frame-Level Neural Vocoder Incorporating Direct Prediction of Amplitude and Phase Spectra\\u00a7r\\n\\n\\u00a78\\u00a7oYang Ai\\nZhen-Hua Ling\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.07952\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 13 May 2023 15:51:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE/ACM Transactionson Audio, Speech, and Language Processing. Codes are available\\u00a7r"}']}
{title:'Devecioglu et al. (§72023§r)', author: 'Ozer Can Devecioglu; Serkan Kiranyaz; Amer Elhmes; Sadok Sassi; Turker Ince; Onur Avci; Mohammad Hesam Soleimani-Babakamali; Ertugrul Taciroglu; Moncef Gabbouj', display:{Lore:['[{"text": "arXiv:2305.07960", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound-to-Vibration Transformation for Sensorless Motor Health Monitoring\\u00a7r\\n\\n\\u00a78\\u00a7oOzer Can Devecioglu\\nSerkan Kiranyaz\\nAmer Elhmes\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.07960\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 13 May 2023 16:37:18 GMT)\\u00a7r"}']}
{title:'Lin et al. (§72023§r)', author: 'Weiwei Lin; Chenhang He; Man-Wai Mak; Youzhi Tu', display:{Lore:['[{"text": "arXiv:2305.08099", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervised Neural Factor Analysis for Disentangling Utterance-level Speech Representations\\u00a7r\\n\\n\\u00a78\\u00a7oWeiwei Lin\\nChenhang He\\nMan-Wai Mak\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.08099\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 4 Oct 2023 12:15:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by ICML 2023\\u00a7r"}']}
{title:'Dang et al. (§72023§r)', author: 'Feng Dang; Qi Hu; Pengyuan Zhang; Yonghong Yan', display:{Lore:['[{"text": "arXiv:2305.08292", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lForkNet: Simultaneous Time and Time-Frequency Domain Modeling for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oFeng Dang\\nQi Hu\\nPengyuan Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.08292\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 May 2023 01:24:49 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Qiquan Zhang; Hongxu Zhu; Qi Song; Xinyuan Qian; Zhaoheng Ni; Haizhou Li', display:{Lore:['[{"text": "arXiv:2305.08541", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRipple sparse self-attention for monaural speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oQiquan Zhang\\nHongxu Zhu\\nQi Song\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.08541\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 May 2023 11:12:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, ICASSP 2023 published\\u00a7r"}']}
{title:'Zhao et al. (§72023§r)', author: 'Xintao Zhao; Shuai Wang; Yang Chao; Zhiyong Wu; Helen Meng', display:{Lore:['[{"text": "arXiv:2305.09167", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Speaker Disentanglement Using Unannotated External Data for Self-supervised Representation Based Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oXintao Zhao\\nShuai Wang\\nYang Chao\\nZhiyong Wu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.09167\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 May 2023 04:52:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICME 2023\\u00a7r"}']}
{title:'Pham et al. (§72023§r)', author: 'Lam Pham; Dat Ngo; Cam Le; Anahid Jalali; Alexander Schindler', display:{Lore:['[{"text": "arXiv:2305.09463", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-complexity deep learning frameworks for acoustic scene classification using teacher-student scheme and multiple spectrograms\\u00a7r\\n\\n\\u00a78\\u00a7oLam Pham\\nDat Ngo\\nCam Le\\nAnahid Jalali\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.09463\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 May 2023 14:21:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2206.06057\\u00a7r"}']}
{title:'Plasser et al. (§72023§r)', author: 'Matthias Plasser; Silvan Peter; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2305.09489", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiscrete Diffusion Probabilistic Models for Symbolic Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oMatthias Plasser\\nSilvan Peter\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.09489\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 May 2023 14:43:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of the 32nd International Joint Conference on Artificial Intelligence (IJCAI-23), Macau, China\\u00a7r"}']}
{title:'Agarwaal et al. (§72023§r)', author: 'Anoubhav Agarwaal; Prabhat Kanaujia; Sartaki Sinha Roy; Susmita Ghose', display:{Lore:['[{"text": "arXiv:2305.09559", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust and lightweight audio fingerprint for Automatic Content Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAnoubhav Agarwaal\\nPrabhat Kanaujia\\nSartaki Sinha Roy\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.09559\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 17 May 2023 06:28:30 GMT)\\u00a7r"}']}
{title:'Borsos et al. (§72023§r)', author: 'Zalán Borsos; Matt Sharifi; Damien Vincent; Eugene Kharitonov; Neil Zeghidour; Marco Tagliasacchi', display:{Lore:['[{"text": "arXiv:2305.09636", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSoundStorm: Efficient Parallel Audio Generation\\u00a7r\\n\\n\\u00a78\\u00a7oZal\\u00e1n Borsos\\nMatt Sharifi\\nDamien Vincent\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.09636\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 May 2023 17:41:25 GMT)\\u00a7r"}']}
{title:'Kadlčík et al. (§72023§r)', author: 'Marek Kadlčík; Adam Hájek; Jürgen Kieslich; Radosław Winiecki', display:{Lore:['[{"text": "arXiv:2305.09690", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Whisper transformer for audio captioning trained with synthetic captions and transfer learning\\u00a7r\\n\\n\\u00a78\\u00a7oMarek Kadl\\u010d\\u00edk\\nAdam H\\u00e1jek\\nJ\\u00fcrgen Kieslich\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.09690\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 May 2023 22:20:07 GMT)\\u00a7r"}']}
{title:'Shi et al. (§72023§r)', author: 'Jiatong Shi; Dan Berrebbi; William Chen; Ho-Lam Chung; En-Pei Hu; Wei Ping Huang; Xuankai Chang; Shang-Wen Li; Abdelrahman Mohamed; Hung-yi Lee; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2305.10615", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lML-SUPERB: Multilingual Speech Universal PERformance Benchmark\\u00a7r\\n\\n\\u00a78\\u00a7oJiatong Shi\\nDan Berrebbi\\nWilliam Chen\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10615\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 11 Aug 2023 17:39:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech\\u00a7r"}']}
{title:'Song et al. (§72023§r)', author: 'Xingchen Song; Di Wu; Binbin Zhang; Zhendong Peng; Bo Dang; Fuping Pan; Zhiyong Wu', display:{Lore:['[{"text": "arXiv:2305.10649", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs\\u00a7r\\n\\n\\u00a78\\u00a7oXingchen Song\\nDi Wu\\nBinbin Zhang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10649\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1497\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n@inproceedings{song23c_interspeech, year=2023, booktitle={Proc.\\n  INTERSPEECH 2023}, pages={1648--1652}}\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 May 2023 02:08:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by interspeech 2023\\u00a7r"}']}
{title:'Ochieng (§72023§r)', author: 'Peter Ochieng', display:{Lore:['[{"text": "arXiv:2305.10652", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Separation based on Contrastive Learning and Deep Modularization\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Ochieng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10652\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 12 Sep 2023 06:47:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2212.00369\\u00a7r"}']}
{title:'Shi et al. (§72023§r)', author: 'Xian Shi; Haoneng Luo; Zhifu Gao; Shiliang Zhang; Zhijie Yan', display:{Lore:['[{"text": "arXiv:2305.10680", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAccurate and Reliable Confidence Estimation Based on Non-Autoregressive End-to-End Speech Recognition System\\u00a7r\\n\\n\\u00a78\\u00a7oXian Shi\\nHaoneng Luo\\nZhifu Gao\\nShiliang Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10680\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 May 2023 02:26:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, Interspeech2023\\u00a7r"}']}
{title:'He et al. (§72023§r)', author: 'Jinzheng He; Jinglin Liu; Zhenhui Ye; Rongjie Huang; Chenye Cui; Huadai Liu; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2305.10686", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRMSSinger: Realistic-Music-Score based Singing Voice Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oJinzheng He\\nJinglin Liu\\nZhenhui Ye\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10686\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 May 2023 03:57:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Finding of ACL2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Zhengyang Chen; Bing Han; Shuai Wang; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2305.10704", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention-based Encoder-Decoder Network for End-to-End Neural Speaker Diarization with Target Speaker Attractor\\u00a7r\\n\\n\\u00a78\\u00a7oZhengyang Chen\\nBing Han\\nShuai Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10704\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 15 Aug 2023 10:31:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by InterSpeech 2023\\u00a7r"}']}
{title:'Ye et al. (§72023§r)', author: 'Zhenhui Ye; Rongjie Huang; Yi Ren; Ziyue Jiang; Jinglin Liu; Jinzheng He; Xiang Yin; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2305.10763", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCLAPSpeech: Learning Prosody from Text Context with Contrastive Language-Audio Pre-training\\u00a7r\\n\\n\\u00a78\\u00a7oZhenhui Ye\\nRongjie Huang\\nYi Ren\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10763\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 May 2023 07:07:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ACL 2023 (Main Conference)\\u00a7r"}']}
{title:'Shao et al. (§72023§r)', author: 'Hang Shao; Wei Wang; Bei Liu; Xun Gong; Haoyu Wang; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2305.10788", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhisper-KDQ: A Lightweight Whisper via Guided Knowledge Distillation and Quantization for Efficient ASR\\u00a7r\\n\\n\\u00a78\\u00a7oHang Shao\\nWei Wang\\nBei Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10788\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 May 2023 08:00:09 GMT)\\u00a7r"}']}
{title:'Sigona et al. (§72023§r)', author: 'Francesco Sigona; Mirko Grimaldi', display:{Lore:['[{"text": "arXiv:2305.10805", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lValidation of an ECAPA-TDNN system for Forensic Automatic Speaker Recognition under case work conditions\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesco Sigona\\nMirko Grimaldi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10805\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 May 2023 08:37:14 GMT)\\u00a7r"}']}
{title:'Lv et al. (§72023§r)', author: 'Ang Lv; Xu Tan; Peiling Lu; Wei Ye; Shikun Zhang; Jiang Bian; Rui Yan', display:{Lore:['[{"text": "arXiv:2305.10841", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGETMusic: Generating Any Music Tracks with a Unified Representation and Diffusion Framework\\u00a7r\\n\\n\\u00a78\\u00a7oAng Lv\\nXu Tan\\nPeiling Lu\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10841\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 29 Sep 2023 11:05:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 4 figures\\u00a7r"}']}
{title:'Gao et al. (§72023§r)', author: 'Zhifu Gao; Zerui Li; Jiaming Wang; Haoneng Luo; Xian Shi; Mengzhe Chen; Yabin Li; Lingyun Zuo; Zhihao Du; Zhangyu Xiao; Shiliang Zhang', display:{Lore:['[{"text": "arXiv:2305.11013", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFunASR: A Fundamental End-to-End Speech Recognition Toolkit\\u00a7r\\n\\n\\u00a78\\u00a7oZhifu Gao\\nZerui Li\\nJiaming Wang\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11013\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 May 2023 14:45:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Feng et al. (§72023§r)', author: 'Tiantian Feng; Rajat Hebbar; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2305.11229", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTrustSER: On the Trustworthiness of Fine-tuning Pre-trained Speech Embeddings For Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oTiantian Feng\\nRajat Hebbar\\nShrikanth Narayanan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11229\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 May 2023 18:00:36 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Li-Jen Yang; Chao-Han Huck Yang; Jen-Tzung Chien', display:{Lore:['[{"text": "arXiv:2305.11320", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParameter-Efficient Learning for Text-to-Speech Accent Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oLi-Jen Yang\\nChao-Han Huck Yang\\nJen-Tzung Chien\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11320\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1212\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 May 2023 22:02:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023\\u00a7r"}']}
{title:'Ho et al. (§72023§r)', author: 'Chun-Wei Ho; Chao-Han Huck Yang; Sabato Marco Siniscalchi', display:{Lore:['[{"text": "arXiv:2305.11360", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDifferentially Private Adapters for Parameter Efficient Acoustic Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oChun-Wei Ho\\nChao-Han Huck Yang\\nSabato Marco Siniscalchi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11360\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-551\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 May 2023 00:36:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023. Code will be available at:https://github.com/Chun-wei-Ho/Private-Speech-Adapter. The authors would like to express their gratitude to Prof. Chin-Hui Lee from Georgia Tech for providing "}','{"text": "helpful insights and suggestions\\u00a7r"}']}
{title:'Malik et al. (§72023§r)', author: 'Ibrahim Malik; Siddique Latif; Raja Jurdak; Björn Schuller', display:{Lore:['[{"text": "arXiv:2305.11413", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Preliminary Study on Augmenting Speech Emotion Recognition using a Diffusion Model\\u00a7r\\n\\n\\u00a78\\u00a7oIbrahim Malik\\nSiddique Latif\\nRaja Jurdak\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11413\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 May 2023 03:51:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted Interspeech 2023\\u00a7r"}']}
{title:'Namgyal et al. (§72023§r)', author: 'Tashi Namgyal; Alexander Hepburn; Raul Santos-Rodriguez; Valero Laparra; Jesus Malo', display:{Lore:['[{"text": "arXiv:2305.11582", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhat You Hear Is What You See: Audio Quality Metrics From Image Quality Metrics\\u00a7r\\n\\n\\u00a78\\u00a7oTashi Namgyal\\nAlexander Hepburn\\nRaul Santos-Rodriguez\\nValero Laparra\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11582\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 30 Aug 2023 16:06:27 GMT)\\u00a7r"}']}
{title:'Namgyal et al. (§72023§r)', author: 'Tashi Namgyal; Peter Flach; Raul Santos-Rodriguez', display:{Lore:['[{"text": "arXiv:2305.11605", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMIDI-Draw: Sketching to Control Melody Generation\\u00a7r\\n\\n\\u00a78\\u00a7oTashi Namgyal\\nPeter Flach\\nRaul Santos-Rodriguez\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11605\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 May 2023 11:31:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oLate-Breaking / Demo Session Extended Abstract, ISMIR 2022 Conference\\u00a7r"}']}
{title:'Härmä et al. (§72023§r)', author: 'Aki Härmä; Ulf Grossekathöfer; Okke Ouweltjes; Venkata Srikanth Nallanthighal', display:{Lore:['[{"text": "arXiv:2305.11683", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSensing of inspiration events from speech: comparison of deep learning and linguistic methods\\u00a7r\\n\\n\\u00a78\\u00a7oAki H\\u00e4rm\\u00e4\\nUlf Grossekath\\u00f6fer\\nOkke Ouweltjes\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11683\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 May 2023 14:06:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages\\u00a7r"}']}
{title:'Lluís et al. (§72023§r)', author: 'Francesc Lluís; Nils Meyer-Kahlen; Vasileios Chatziioannou; Alex Hofmann', display:{Lore:['[{"text": "arXiv:2305.11727", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDirection Specific Ambisonics Source Separation with End-To-End Deep Learning\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesc Llu\\u00eds\\nNils Meyer-Kahlen\\nVasileios Chatziioannou\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11727\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nActa Acustica 2023, 7, 29\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 20 Jun 2023 15:15:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCode and listening examples: https://github.com/francesclluis/direction-ambisonics-source-separation\\u00a7r"}']}
{title:'Shah et al. (§72023§r)', author: 'Neil Shah; Vishal Tambrahalli; Saiteja Kosgi; Niranjan Pedanekar; Vineet Gandhi', display:{Lore:['[{"text": "arXiv:2305.11926", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMParrotTTS: Multilingual Multi-speaker Text to Speech Synthesis in Low Resource Setting\\u00a7r\\n\\n\\u00a78\\u00a7oNeil Shah\\nVishal Tambrahalli\\nSaiteja Kosgi\\nNiranjan Pedanekar\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11926\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 May 2023 13:43:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure\\u00a7r"}']}
{title:'Yip et al. (§72023§r)', author: 'Jia Qi Yip; Tuan Truong; Dianwen Ng; Chong Zhang; Yukun Ma; Trung Hieu Nguyen; Chongjia Ni; Shengkui Zhao; Eng Siong Chng; Bin Ma', display:{Lore:['[{"text": "arXiv:2305.12121", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lACA-Net: Towards Lightweight Speaker Verification using Asymmetric Cross Attention\\u00a7r\\n\\n\\u00a78\\u00a7oJia Qi Yip\\nTuan Truong\\nDianwen Ng\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12121\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 20 May 2023 06:56:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Yuyue Wang; Huan Xiao; Yihan Wu; Ruihua Song', display:{Lore:['[{"text": "arXiv:2305.12200", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComedicSpeech: Text To Speech For Stand-up Comedies in Low-Resource Scenarios\\u00a7r\\n\\n\\u00a78\\u00a7oYuyue Wang\\nHuan Xiao\\nYihan Wu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12200\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 20 May 2023 14:24:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 tables, 2 figure\\u00a7r"}']}
{title:'Xin et al. (§72023§r)', author: 'Detai Xin; Shinnosuke Takamichi; Ai Morimatsu; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2305.12442", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLaughter Synthesis using Pseudo Phonetic Tokens with a Large-scale In-the-wild Laughter Corpus\\u00a7r\\n\\n\\u00a78\\u00a7oDetai Xin\\nShinnosuke Takamichi\\nAi Morimatsu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12442\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 26 May 2023 13:17:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Xin et al. (§72023§r)', author: 'Detai Xin; Shinnosuke Takamichi; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2305.12445", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJNV Corpus: A Corpus of Japanese Nonverbal Vocalizations with Diverse Phrases and Emotions\\u00a7r\\n\\n\\u00a78\\u00a7oDetai Xin\\nShinnosuke Takamichi\\nHiroshi Saruwatari\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12445\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 21 May 2023 12:32:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 3 figures\\u00a7r"}']}
{title:'Maben et al. (§72023§r)', author: 'Leander Melroy Maben; Zixun Guo; Chen Chen; Utkarsh Chudiwal; Chng Eng Siong', display:{Lore:['[{"text": "arXiv:2305.12460", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStudy of GANs for Noisy Speech Simulation from Clean Speech\\u00a7r\\n\\n\\u00a78\\u00a7oLeander Melroy Maben\\nZixun Guo\\nChen Chen\\nUtkarsh Chudiwal\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12460\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 21 May 2023 13:43:14 GMT)\\u00a7r"}']}
{title:'Dineley et al. (§72023§r)', author: 'Judith Dineley; Ewan Carr; Faith Matcham; Johnny Downs; Richard Dobson; Thomas F Quatieri; Nicholas Cummins', display:{Lore:['[{"text": "arXiv:2305.12514", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards robust paralinguistic assessment for real-world mobile health (mHealth) monitoring: an initial study of reverberation effects on speech\\u00a7r\\n\\n\\u00a78\\u00a7oJudith Dineley\\nEwan Carr\\nFaith Matcham\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12514\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 31 May 2023 16:21:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at Interspeech 2023\\u00a7r"}']}
{title:'Zhao et al. (§72023§r)', author: 'Zhenduo Zhao; Zhuo Li; Wenchao Wang; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2305.12642", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe HCCL system for VoxCeleb Speaker Recognition Challenge 2022\\u00a7r\\n\\n\\u00a78\\u00a7oZhenduo Zhao\\nZhuo Li\\nWenchao Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12642\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 May 2023 02:32:34 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Shipei Liu; Xiaoya Fan; Guowei Wu', display:{Lore:['[{"text": "arXiv:2305.12701", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMore Perspectives Mean Better: Underwater Target Recognition and Localization with Multimodal Data via Symbiotic Transformer and Multiview Regression\\u00a7r\\n\\n\\u00a78\\u00a7oShipei Liu\\nXiaoya Fan\\nGuowei Wu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12701\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 May 2023 04:16:28 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Zhuo Li; Jingze Lu; Zhenduo Zhao; Wenchao Wang; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2305.12703", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProgressive Sub-Graph Clustering Algorithm for Semi-Supervised Domain Adaptation Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oZhuo Li\\nJingze Lu\\nZhenduo Zhao\\nWenchao Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12703\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 May 2023 04:26:18 GMT)\\u00a7r"}']}
{title:'Choudhary et al. (§72023§r)', author: 'Shwetank Choudhary; CR Karthik; Punuru Sri Lakshmi; Sumit Kumar', display:{Lore:['[{"text": "arXiv:2305.12712", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLEAN: Light and Efficient Audio Classification Network\\u00a7r\\n\\n\\u00a78\\u00a7oShwetank Choudhary\\nCR Karthik\\nPunuru Sri Lakshmi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12712\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/INDICON56171.2022.10039921\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 May 2023 04:45:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INDICON 2022\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'J. Li; Z. Duan; S. Li; X. Yu; G. Yang', display:{Lore:['[{"text": "arXiv:2305.12755", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGNCformer Enhanced Self-attention for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJ. Li\\nZ. Duan\\nS. Li\\nX. Yu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12755\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 May 2023 06:26:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages,3 figures,\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Haibin Wu; Jiawen Kang; Lingwei Meng; Helen Meng; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2305.12804", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe defender\'s perspective on automatic speaker verification: An overview\\u00a7r\\n\\n\\u00a78\\u00a7oHaibin Wu\\nJiawen Kang\\nLingwei Meng\\nHelen Meng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12804\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 25 Jun 2023 09:22:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IJCAI 2023 Workshop\\u00a7r"}']}
{title:'Ye et al. (§72023§r)', author: 'Zhen Ye; Wei Xue; Xu Tan; Qifeng Liu; Yike Guo', display:{Lore:['[{"text": "arXiv:2305.12868", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNAS-FM: Neural Architecture Search for Tunable and Interpretable Sound Synthesis based on Frequency Modulation\\u00a7r\\n\\n\\u00a78\\u00a7oZhen Ye\\nWei Xue\\nXu Tan\\nQifeng Liu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12868\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIJCAI 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 May 2023 09:46:10 GMT)\\u00a7r"}']}
{title:'Yariv et al. (§72023§r)', author: 'Guy Yariv; Itai Gat; Lior Wolf; Yossi Adi; Idan Schwartz', display:{Lore:['[{"text": "arXiv:2305.13050", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudioToken: Adaptation of Text-Conditioned Diffusion Models for Audio-to-Image Generation\\u00a7r\\n\\n\\u00a78\\u00a7oGuy Yariv\\nItai Gat\\nLior Wolf\\nYossi Adi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13050\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 May 2023 14:02:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2023\\u00a7r"}']}
{title:'Jing et al. (§72023§r)', author: 'Xin Jing; Yi Chang; Zijiang Yang; Jiangjian Xie; Andreas Triantafyllopoulos; Bjoern W. Schuller', display:{Lore:['[{"text": "arXiv:2305.13195", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lU-DiT TTS: U-Diffusion Vision Transformer for Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oXin Jing\\nYi Chang\\nZijiang Yang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13195\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 May 2023 16:25:19 GMT)\\u00a7r"}']}
{title:'Nolasco et al. (§72023§r)', author: 'Inês Nolasco; Shubhr Singh; Veronica Morfi; Vincent Lostanlen; Ariana Strandburg-Peshkin; Ester Vidaña-Vila; Lisa Gill; Hanna Pamuła; Helen Whitehead; Ivan Kiskin; Frants H. Jensen; Joe Morford; Michael G. Emmerson; Elisabetta Versace; Emily Grout; Haohe Liu; Dan Stowell', display:{Lore:['[{"text": "arXiv:2305.13210", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning to detect an animal sound from five examples\\u00a7r\\n\\n\\u00a78\\u00a7oIn\\u00eas Nolasco\\nShubhr Singh\\nVeronica Morfi\\n+ 13 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13210\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.ecoinf.2023.102258\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 May 2023 16:43:39 GMT)\\u00a7r"}']}
{title:'Isoyama et al. (§72023§r)', author: 'Takuto Isoyama; Shunsuke Kidani; Masashi Unoki', display:{Lore:['[{"text": "arXiv:2305.13213", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComputational models of sound-quality metrics using method for calculating loudness with gammatone/gammachirp auditory filterbank\\u00a7r\\n\\n\\u00a78\\u00a7oTakuto Isoyama\\nShunsuke Kidani\\nMasashi Unoki\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13213\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 May 2023 02:37:45 GMT)\\u00a7r"}']}
{title:'Mitcheltree et al. (§72023§r)', author: 'Christopher Mitcheltree; Christian J. Steinmetz; Marco Comunità; Joshua D. Reiss', display:{Lore:['[{"text": "arXiv:2305.13262", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModulation Extraction for LFO-driven Audio Effects\\u00a7r\\n\\n\\u00a78\\u00a7oChristopher Mitcheltree\\nChristian J. Steinmetz\\nMarco Comunit\\u00e0\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13262\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 May 2023 17:33:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to DAFx 2023. Listening samples and plugins can be found at https://christhetree.github.io/mod_extraction/\\u00a7r"}']}
{title:'Jiang et al. (§72023§r)', author: 'Ziyue Jiang; Qian Yang; Jialong Zuo; Zhenhui Ye; Rongjie Huang; Yi Ren; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2305.13612", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFluentSpeech: Stutter-Oriented Automatic Speech Editing with Context-Aware Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oZiyue Jiang\\nQian Yang\\nJialong Zuo\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13612\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 May 2023 02:20:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ACL 2023 (Findings)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Chenglong Wang; Jiangyan Yi; Jianhua Tao; Chuyuan Zhang; Shuai Zhang; Xun Chen', display:{Lore:['[{"text": "arXiv:2305.13700", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetection of Cross-Dataset Fake Audio Based on Prosodic and Pronunciation Features\\u00a7r\\n\\n\\u00a78\\u00a7oChenglong Wang\\nJiangyan Yi\\nJianhua Tao\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13700\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 May 2023 05:27:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Chenglong Wang; Jiangyan Yi; Jianhua Tao; Chuyuan Zhang; Shuai Zhang; Ruibo Fu; Xun Chen', display:{Lore:['[{"text": "arXiv:2305.13701", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTO-Rawnet: Improving RawNet with TCN and Orthogonal Regularization for Fake Audio Detection\\u00a7r\\n\\n\\u00a78\\u00a7oChenglong Wang\\nJiangyan Yi\\nJianhua Tao\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13701\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 May 2023 05:30:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech2023\\u00a7r"}']}
{title:'Saito et al. (§72023§r)', author: 'Yuki Saito; Eiji Iimori; Shinnosuke Takamichi; Kentaro Tachibana; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2305.13713", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCALLS: Japanese Empathetic Dialogue Speech Corpus of Complaint Handling and Attentive Listening in Customer Center\\u00a7r\\n\\n\\u00a78\\u00a7oYuki Saito\\nEiji Iimori\\nShinnosuke Takamichi\\nKentaro Tachibana\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13713\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 May 2023 06:04:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted for INTERSPEECH2023\\u00a7r"}']}
{title:'Liang et al. (§72023§r)', author: 'Yuhao Liang; Fan Yu; Yangze Li; Pengcheng Guo; Shiliang Zhang; Qian Chen; Lei Xie', display:{Lore:['[{"text": "arXiv:2305.13716", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBA-SOT: Boundary-Aware Serialized Output Training for Multi-Talker ASR\\u00a7r\\n\\n\\u00a78\\u00a7oYuhao Liang\\nFan Yu\\nYangze Li\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13716\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 5 Oct 2023 11:44:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Saito et al. (§72023§r)', author: 'Yuki Saito; Shinnosuke Takamichi; Eiji Iimori; Kentaro Tachibana; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2305.13724", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChatGPT-EDSS: Empathetic Dialogue Speech Synthesis Trained from ChatGPT-derived Context Word Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oYuki Saito\\nShinnosuke Takamichi\\nEiji Iimori\\nKentaro Tachibana\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13724\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 May 2023 06:19:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted for INTERSPEECH 2023\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Hyemi Kim; Jiyun Park; Taegyun Kwon; Dasaem Jeong; Juhan Nam', display:{Lore:['[{"text": "arXiv:2305.13758", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA study of audio mixing methods for piano transcription in violin-piano ensembles\\u00a7r\\n\\n\\u00a78\\u00a7oHyemi Kim\\nJiyun Park\\nTaegyun Kwon\\nDasaem Jeong\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13758\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 May 2023 07:19:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo Appear IEEE ICASSP 2023\\u00a7r"}']}
{title:'Yi et al. (§72023§r)', author: 'Jiangyan Yi; Jianhua Tao; Ruibo Fu; Xinrui Yan; Chenglong Wang; Tao Wang; Chu Yuan Zhang; Xiaohui Zhang; Yan Zhao; Yong Ren; Le Xu; Junzuo Zhou; Hao Gu; Zhengqi Wen; Shan Liang; Zheng Lian; Shuai Nie; Haizhou Li', display:{Lore:['[{"text": "arXiv:2305.13774", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lADD 2023: the Second Audio Deepfake Detection Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oJiangyan Yi\\nJianhua Tao\\nRuibo Fu\\n+ 14 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13774\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 May 2023 07:42:52 GMT)\\u00a7r"}']}
{title:'Qiu et al. (§72023§r)', author: 'Zhibin Qiu; Mengfan Fu; Fuchun Sun; Gulila Altenbek; Hao Huang', display:{Lore:['[{"text": "arXiv:2305.13796", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSE-Bridge: Speech Enhancement with Consistent Brownian Bridge\\u00a7r\\n\\n\\u00a78\\u00a7oZhibin Qiu\\nMengfan Fu\\nFuchun Sun\\nGulila Altenbek\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13796\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 May 2023 08:06:36 GMT)\\u00a7r"}']}
{title:'Kang et al. (§72023§r)', author: 'Minki Kang; Wooseok Han; Sung Ju Hwang; Eunho Yang', display:{Lore:['[{"text": "arXiv:2305.13831", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZET-Speech: Zero-shot adaptive Emotion-controllable Text-to-Speech Synthesis with Diffusion and Style-based Models\\u00a7r\\n\\n\\u00a78\\u00a7oMinki Kang\\nWooseok Han\\nSung Ju Hwang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13831\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 May 2023 08:52:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Düsterhöft et al. (§72023§r)', author: 'Aljoscha Düsterhöft; Felix Burkhardt; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2305.14023", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHappy or Evil Laughter? Analysing a Database of Natural Audio Samples\\u00a7r\\n\\n\\u00a78\\u00a7oAljoscha D\\u00fcsterh\\u00f6ft\\nFelix Burkhardt\\nBj\\u00f6rn W. Schuller\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.14023\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 May 2023 12:56:35 GMT)\\u00a7r"}']}
{title:'Diaz et al. (§72023§r)', author: 'Rodrigo Diaz; Charalampos Saitis; Mark Sandler', display:{Lore:['[{"text": "arXiv:2305.14867", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInteractive Neural Resonators\\u00a7r\\n\\n\\u00a78\\u00a7oRodrigo Diaz\\nCharalampos Saitis\\nMark Sandler\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.14867\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 May 2023 08:19:42 GMT)\\u00a7r"}']}
{title:'Singh et al. (§72023§r)', author: 'Mayank Kumar Singh; Naoya Takahashi; Onoe Naoyuki', display:{Lore:['[{"text": "arXiv:2305.15055", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIteratively Improving Speech Recognition and Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oMayank Kumar Singh\\nNaoya Takahashi\\nOnoe Naoyuki\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.15055\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 May 2023 11:45:42 GMT)\\u00a7r"}']}
{title:'Diener et al. (§72023§r)', author: 'Lorenz Diener; Marju Purin; Sten Sootla; Ando Saabas; Robert Aichner; Ross Cutler', display:{Lore:['[{"text": "arXiv:2305.15127", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPLCMOS \\u2013 a data-driven non-intrusive metric for the evaluation of packet loss concealment algorithms\\u00a7r\\n\\n\\u00a78\\u00a7oLorenz Diener\\nMarju Purin\\nSten Sootla\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.15127\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 May 2023 13:21:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oto appear: INTERSPEECH 2023, associated model release: https://aka.ms/PLCMOS\\u00a7r"}']}
{title:'Tatar et al. (§72023§r)', author: 'Kıvanç Tatar; Kelsey Cotton; Daniel Bisig', display:{Lore:['[{"text": "arXiv:2305.15571", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Design Strategies for Latent Audio Space Explorations Using Deep Learning Architectures\\u00a7r\\n\\n\\u00a78\\u00a7oK\\u0131van\\u00e7 Tatar\\nKelsey Cotton\\nDaniel Bisig\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.15571\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 19 Jun 2023 09:59:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of Sound and Music Computing 2023, ISBN 978-91-527-7372-7\\u00a7r"}']}
{title:'Gogins (§72023§r)', author: 'Michael Gogins', display:{Lore:['[{"text": "arXiv:2305.15601", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMetamathematics of Algorithmic Composition\\u00a7r\\n\\n\\u00a78\\u00a7oMichael Gogins\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.15601\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 May 2023 22:21:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 0 figures. Comments are very welcome\\u00a7r"}']}
{title:'Lam et al. (§72023§r)', author: 'Max W. Y. Lam; Qiao Tian; Tang Li; Zongyu Yin; Siyuan Feng; Ming Tu; Yuliang Ji; Rui Xia; Mingbo Ma; Xuchen Song; Jitong Chen; Yuping Wang; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2305.15719", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Neural Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oMax W. Y. Lam\\nQiao Tian\\nTang Li\\n+ 9 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.15719\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 May 2023 05:02:35 GMT)\\u00a7r"}']}
{title:'Melhem et al. (§72023§r)', author: 'Rawad Melhem; Assef Jafar; Oumayma Al Dakkak', display:{Lore:['[{"text": "arXiv:2305.15758", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Solving Cocktail-Party: The First Method to Build a Realistic Dataset with Ground Truths for Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oRawad Melhem\\nAssef Jafar\\nOumayma Al Dakkak\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.15758\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 May 2023 06:17:03 GMT)\\u00a7r"}']}
{title:'Shimonishi et al. (§72023§r)', author: 'Kanta Shimonishi; Kota Dohi; Yohei Kawaguchi', display:{Lore:['[{"text": "arXiv:2305.15859", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnomalous Sound Detection Based on Sound Separation\\u00a7r\\n\\n\\u00a78\\u00a7oKanta Shimonishi\\nKota Dohi\\nYohei Kawaguchi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.15859\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 May 2023 08:49:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH2023\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Kyungyun Lee; Jeonghun Seo; Keunwoo Choi; Sangmoon Lee; Ben Sangbae Chon', display:{Lore:['[{"text": "arXiv:2305.15898", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRoom Impulse Response Estimation in a Multiple Source Environment\\u00a7r\\n\\n\\u00a78\\u00a7oKyungyun Lee\\nJeonghun Seo\\nKeunwoo Choi\\nSangmoon Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.15898\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 May 2023 09:54:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2023 AES International Conference on Spatial and Immersive Audio\\u00a7r"}']}
{title:'Yuan et al. (§72023§r)', author: 'Yi Yuan; Haohe Liu; Xubo Liu; Xiyuan Kang; Mark D. Plumbley; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2305.15905", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLatent Diffusion Model Based Foley Sound Generation System For DCASE Challenge 2023 Task 7\\u00a7r\\n\\n\\u00a78\\u00a7oYi Yuan\\nHaohe Liu\\nXubo Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.15905\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 15 Sep 2023 16:45:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDCASE 2023 task 7 technical report, ranked 1st in the challenge\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Jiaying Wang; Xianglong Wang; Namin Wang; Lantian Li; Dong Wang', display:{Lore:['[{"text": "arXiv:2305.16043", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOrdered and Binary Speaker Embedding\\u00a7r\\n\\n\\u00a78\\u00a7oJiaying Wang\\nXianglong Wang\\nNamin Wang\\nLantian Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.16043\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 May 2023 13:21:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oto be published in INTERSPEECH 2023\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Pengqi Li; Lantian Li; Askar Hamdulla; Dong Wang', display:{Lore:['[{"text": "arXiv:2305.16070", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisualizing data augmentation in deep speaker recognition\\u00a7r\\n\\n\\u00a78\\u00a7oPengqi Li\\nLantian Li\\nAskar Hamdulla\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.16070\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 May 2023 14:01:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oto be published in INTERSPEECH 2023\\u00a7r"}']}
{title:'Meng et al. (§72023§r)', author: 'Lingwei Meng; Jiawen Kang; Mingyu Cui; Haibin Wu; Xixin Wu; Helen Meng', display:{Lore:['[{"text": "arXiv:2305.16263", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnified Modeling of Multi-Talker Overlapped Speech Recognition and Diarization with a Sidecar Separator\\u00a7r\\n\\n\\u00a78\\u00a7oLingwei Meng\\nJiawen Kang\\nMingyu Cui\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.16263\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 May 2023 17:18:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2023\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Rui Liu; Jinhua Zhang; Guanglai Gao; Haizhou Li', display:{Lore:['[{"text": "arXiv:2305.16353", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBetray Oneself: A Novel Audio DeepFake Detection Model via Mono-to-Stereo Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oRui Liu\\nJinhua Zhang\\nGuanglai Gao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.16353\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 May 2023 02:54:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at InterSpeech2023\\u00a7r"}']}
{title:'Monjur et al. (§72023§r)', author: 'Mahathir Monjur; Yubo Luo; Zhenyu Wang; Shahriar Nirjon', display:{Lore:['[{"text": "arXiv:2305.16445", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.DC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSoundSieve: Seconds-Long Audio Event Recognition on Intermittently-Powered Systems\\u00a7r\\n\\n\\u00a78\\u00a7oMahathir Monjur\\nYubo Luo\\nZhenyu Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.16445\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 May 2023 19:43:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe 21st ACM International Conference on Mobile Systems, Applications, and Services (Mobisys 2023)\\u00a7r"}']}
{title:'Wei et al. (§72023§r)', author: 'Xipin Wei; Junhui Chen; Zirui Zheng; Li Guo; Lantian Li; Dong Wang', display:{Lore:['[{"text": "arXiv:2305.16592", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Multi-Scale Attentive Transformer for Multi-Instrument Symbolic Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oXipin Wei\\nJunhui Chen\\nZirui Zheng\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.16592\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 May 2023 02:41:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oto be published in INTERSPEECH 2023\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Xiang Li; Songxiang Liu; Max W. Y. Lam; Zhiyong Wu; Chao Weng; Helen Meng', display:{Lore:['[{"text": "arXiv:2305.16749", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiverse and Expressive Speech Prosody Prediction with Denoising Diffusion Probabilistic Model\\u00a7r\\n\\n\\u00a78\\u00a7oXiang Li\\nSongxiang Liu\\nMax W. Y. Lam\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.16749\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 7 Oct 2023 05:15:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of Interspeech 2023 (doi: 10.21437/Interspeech.2023-715), demo site at https://thuhcsi.github.io/interspeech2023-DiffVar/\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Chen Chen; Chao-Han Huck Yang; Kai Li; Yuchen Hu; Pin-Jui Ku; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2305.16932", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Neural State-Space Model Approach to Efficient Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oChen Chen\\nChao-Han Huck Yang\\nKai Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.16932\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 May 2023 13:47:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by InterSpeech 2023\\u00a7r"}']}
{title:'Shi et al. (§72023§r)', author: 'Ying Shi; Dong Wang; Lantian Li; Jiqing Han; Shi Yin', display:{Lore:['[{"text": "arXiv:2305.17706", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpot keywords from very noisy and mixed speech\\u00a7r\\n\\n\\u00a78\\u00a7oYing Shi\\nDong Wang\\nLantian Li\\nJiqing Han\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.17706\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 28 May 2023 12:26:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023\\u00a7r"}']}
{title:'Song et al. (§72023§r)', author: 'Kun Song; Yi Ren; Yi Lei; Chunfeng Wang; Kun Wei; Lei Xie; Xiang Yin; Zejun Ma', display:{Lore:['[{"text": "arXiv:2305.17732", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStyleS2ST: Zero-shot Style Transfer for Direct Speech-to-speech Translation\\u00a7r\\n\\n\\u00a78\\u00a7oKun Song\\nYi Ren\\nYi Lei\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.17732\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 25 Jul 2023 09:24:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Lin Zhang; Xin Wang; Erica Cooper; Nicholas Evans; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2305.17739", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRange-Based Equal Error Rate for Spoof Localization\\u00a7r\\n\\n\\u00a78\\u00a7oLin Zhang\\nXin Wang\\nErica Cooper\\nNicholas Evans\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.17739\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 28 May 2023 14:46:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023\\u00a7r"}']}
{title:'Huang et al. (§72023§r)', author: 'Yongchao Huang; Yuhang He; Hong Ge', display:{Lore:['[{"text": "arXiv:2305.17749", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.data-an\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBayesian inference and neural estimation of acoustic wave propagation\\u00a7r\\n\\n\\u00a78\\u00a7oYongchao Huang\\nYuhang He\\nHong Ge\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.17749\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 28 May 2023 15:14:46 GMT)\\u00a7r"}']}
{title:'Okamoto et al. (§72023§r)', author: 'Yuki Okamoto; Kanta Shimonishi; Keisuke Imoto; Kota Dohi; Shota Horiguchi; Yohei Kawaguchi', display:{Lore:['[{"text": "arXiv:2305.17758", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCAPTDURE: Captioned Sound Dataset of Single Sources\\u00a7r\\n\\n\\u00a78\\u00a7oYuki Okamoto\\nKanta Shimonishi\\nKeisuke Imoto\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.17758\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 28 May 2023 15:56:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH2023\\u00a7r"}']}
{title:'Dinkel et al. (§72023§r)', author: 'Heinrich Dinkel; Zhiyong Yan; Yongqing Wang; Junbo Zhang; Yujun Wang', display:{Lore:['[{"text": "arXiv:2305.17834", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreaming Audio Transformers for Online Audio Tagging\\u00a7r\\n\\n\\u00a78\\u00a7oHeinrich Dinkel\\nZhiyong Yan\\nYongqing Wang\\nJunbo Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.17834\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 May 2023 00:32:11 GMT)\\u00a7r"}']}
{title:'Sudo et al. (§72023§r)', author: 'Yui Sudo; Kazuya Hata; Kazuhiro Nakadai', display:{Lore:['[{"text": "arXiv:2305.17846", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRetraining-free Customized ASR for Enharmonic Words Based on a Named-Entity-Aware Model and Phoneme Similarity Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oYui Sudo\\nKazuya Hata\\nKazuhiro Nakadai\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.17846\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 May 2023 02:10:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by INTERSPEECH2023\\u00a7r"}']}
{title:'Lu et al. (§72023§r)', author: 'Haoyu Lu; Nan Li; Tongtong Song; Longbiao Wang; Jianwu Dang; Xiaobao Wang; Shiliang Zhang', display:{Lore:['[{"text": "arXiv:2305.17860", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lspeech and noise dual-stream spectrogram refine network with speech distortion loss for robust speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHaoyu Lu\\nNan Li\\nTongtong Song\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.17860\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 May 2023 07:33:48 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Guangyao Li; Yixin Xu; Di Hu', display:{Lore:['[{"text": "arXiv:2305.17993", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Scale Attention for Audio Question Answering\\u00a7r\\n\\n\\u00a78\\u00a7oGuangyao Li\\nYixin Xu\\nDi Hu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.17993\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 May 2023 10:06:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by InterSpeech 2023\\u00a7r"}']}
{title:'Mehrish et al. (§72023§r)', author: 'Ambuj Mehrish; Abhinav Ramesh Kashyap; Li Yingting; Navonil Majumder; Soujanya Poria', display:{Lore:['[{"text": "arXiv:2305.18028", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lADAPTERMIX: Exploring the Efficacy of Mixture of Adapters for Low-Resource TTS Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oAmbuj Mehrish\\nAbhinav Ramesh Kashyap\\nLi Yingting\\nNavonil Majumder\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.18028\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 May 2023 11:39:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023\\u00a7r"}']}
{title:'Xie et al. (§72023§r)', author: 'Wei Xie; Yanxiong Li; Qianhua He; Wenchang Cao; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2305.18045", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFew-shot Class-incremental Audio Classification Using Adaptively-refined Prototypes\\u00a7r\\n\\n\\u00a78\\u00a7oWei Xie\\nYanxiong Li\\nQianhua He\\nWenchang Cao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.18045\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 May 2023 12:16:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages,2 figures, Accepted by Interspeech 2023\\u00a7r"}']}
{title:'Chang et al. (§72023§r)', author: 'Xuankai Chang; Brian Yan; Yuya Fujita; Takashi Maekaku; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2305.18108", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning\\u00a7r\\n\\n\\u00a78\\u00a7oXuankai Chang\\nBrian Yan\\nYuya Fujita\\nTakashi Maekaku\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.18108\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 May 2023 14:23:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2023\\u00a7r"}']}
{title:'Kong et al. (§72023§r)', author: 'Fei Kong; Jinhao Duan; RuiPeng Ma; Hengtao Shen; Xiaofeng Zhu; Xiaoshuang Shi; Kaidi Xu', display:{Lore:['[{"text": "arXiv:2305.18355", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Efficient Membership Inference Attack for the Diffusion Model by Proximal Initialization\\u00a7r\\n\\n\\u00a78\\u00a7oFei Kong\\nJinhao Duan\\nRuiPeng Ma\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.18355\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 9 Oct 2023 06:26:35 GMT)\\u00a7r"}']}
{title:'Yeo et al. (§72023§r)', author: 'Eun Jung Yeo; Kwanghee Choi; Sunhee Kim; Minhwa Chung', display:{Lore:['[{"text": "arXiv:2305.18392", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Intelligibility Assessment of Dysarthric Speech by using Goodness of Pronunciation with Uncertainty Quantification\\u00a7r\\n\\n\\u00a78\\u00a7oEun Jung Yeo\\nKwanghee Choi\\nSunhee Kim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.18392\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 28 May 2023 11:48:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023\\u00a7r"}']}
{title:'Huang et al. (§72023§r)', author: 'Jiawei Huang; Yi Ren; Rongjie Huang; Dongchao Yang; Zhenhui Ye; Chen Zhang; Jinglin Liu; Xiang Yin; Zejun Ma; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2305.18474", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMake-An-Audio 2: Temporal-Enhanced Text-to-Audio Generation\\u00a7r\\n\\n\\u00a78\\u00a7oJiawei Huang\\nYi Ren\\nRongjie Huang\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.18474\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 May 2023 10:41:28 GMT)\\u00a7r"}']}
{title:'Goyal et al. (§72023§r)', author: 'Abhinav Goyal; Nikesh Garera', display:{Lore:['[{"text": "arXiv:2305.18596", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBuilding Accurate Low Latency ASR for Streaming Voice Search\\u00a7r\\n\\n\\u00a78\\u00a7oAbhinav Goyal\\nNikesh Garera\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.18596\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 May 2023 20:24:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ACL 2023 Industry Track\\u00a7r"}']}
{title:'Singh et al. (§72023§r)', author: 'Arshdeep Singh; Haohe Liu; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:2305.18665", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lE-PANNs: Sound Recognition Using Efficient Pre-trained Audio Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oArshdeep Singh\\nHaohe Liu\\nMark D. Plumbley\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.18665\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 May 2023 00:08:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in Internoise 2023 conference\\u00a7r"}']}
{title:'Dinkel et al. (§72023§r)', author: 'Heinrich Dinkel; Weiji Zhuang; Zhiyong Yan; Yongqing Wang; Junbo Zhang; Yujun Wang', display:{Lore:['[{"text": "arXiv:2305.18794", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnderstanding temporally weakly supervised training: A case study for keyword spotting\\u00a7r\\n\\n\\u00a78\\u00a7oHeinrich Dinkel\\nWeiji Zhuang\\nZhiyong Yan\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.18794\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 May 2023 07:12:29 GMT)\\u00a7r"}']}
{title:'Miao et al. (§72023§r)', author: 'Xiaoxiao Miao; Xin Wang; Erica Cooper; Junichi Yamagishi; Natalia Tomashenko', display:{Lore:['[{"text": "arXiv:2305.18823", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker anonymization using orthogonal Householder neural network\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoxiao Miao\\nXin Wang\\nErica Cooper\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.18823\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 13 Sep 2023 01:21:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE/ACM Transactionson Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Qing Wang; Jixun Yao; Ziqian Wang; Pengcheng Guo; Lei Xie', display:{Lore:['[{"text": "arXiv:2305.19020", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPseudo-Siamese Network based Timbre-reserved Black-box Adversarial Attack in Speaker Identification\\u00a7r\\n\\n\\u00a78\\u00a7oQing Wang\\nJixun Yao\\nZiqian Wang\\nPengcheng Guo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19020\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 May 2023 13:20:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Tóth et al. (§72023§r)', author: 'László Tóth; Amin Honarmandi Shandiz; Gábor Gosztolya; Csapó Tamás Gábor', display:{Lore:['[{"text": "arXiv:2305.19130", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using Spatial Transformer Networks\\u00a7r\\n\\n\\u00a78\\u00a7oL\\u00e1szl\\u00f3 T\\u00f3th\\nAmin Honarmandi Shandiz\\nG\\u00e1bor Gosztolya\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19130\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1607\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nthe Proceedings of Interspeech 2023\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 17 Oct 2023 08:01:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 3 tables\\u00a7r"}']}
{title:'Kumar (§72023§r)', author: 'Krishna Kumar', display:{Lore:['[{"text": "arXiv:2305.19304", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio classification using ML methods\\u00a7r\\n\\n\\u00a78\\u00a7oKrishna Kumar\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19304\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 May 2023 15:42:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o3 pages, 8 figures\\u00a7r"}']}
{title:'Mo et al. (§72023§r)', author: 'Shentong Mo; Pedro Morgado', display:{Lore:['[{"text": "arXiv:2305.19458", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Unified Audio-Visual Learning Framework for Localization, Separation, and Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oShentong Mo\\nPedro Morgado\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19458\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 May 2023 23:53:12 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Guanghou Liu; Yongmao Zhang; Yi Lei; Yunlin Chen; Rui Wang; Zhifei Li; Lei Xie', display:{Lore:['[{"text": "arXiv:2305.19522", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPromptStyle: Controllable Style Transfer for Text-to-Speech with Natural Language Descriptions\\u00a7r\\n\\n\\u00a78\\u00a7oGuanghou Liu\\nYongmao Zhang\\nYi Lei\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19522\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 1 Jun 2023 09:30:41 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Hongfu Liu; Mingqian Shi; Ye Wang', display:{Lore:['[{"text": "arXiv:2305.19563", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero-Shot Automatic Pronunciation Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oHongfu Liu\\nMingqian Shi\\nYe Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19563\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 May 2023 05:17:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023\\u00a7r"}']}
{title:'Choi et al. (§72023§r)', author: 'Yerin Choi; Myoung-Wan Koo', display:{Lore:['[{"text": "arXiv:2305.19567", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDC CoMix TTS: An End-to-End Expressive TTS with Discrete Code Collaborated with Mixer\\u00a7r\\n\\n\\u00a78\\u00a7oYerin Choi\\nMyoung-Wan Koo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19567\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 28 Jun 2023 11:42:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in Interspeech 2023\\u00a7r"}']}
{title:'Kang et al. (§72023§r)', author: 'Zuheng Kang; Jianzong Wang; Junqing Peng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2305.19581", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSVVAD: Personal Voice Activity Detection for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oZuheng Kang\\nJianzong Wang\\nJunqing Peng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19581\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 May 2023 05:59:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Tianyu Chen; Yuan Xie; Shuai Zhang; Shaohan Huang; Haoyi Zhou; Jianxin Li', display:{Lore:['[{"text": "arXiv:2305.19602", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Music Sequence Representation from Text Supervision\\u00a7r\\n\\n\\u00a78\\u00a7oTianyu Chen\\nYuan Xie\\nShuai Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19602\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746131\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE International Conference on Acoustics, Speech and Signal\\n  Processing (ICASSP). IEEE, 2022: 4583-4587\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 May 2023 07:15:06 GMT)\\u00a7r"}']}
{title:'Choi et al. (§72023§r)', author: 'Jeongsoo Choi; Minsu Kim; Yong Man Ro', display:{Lore:['[{"text": "arXiv:2305.19603", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntelligible Lip-to-Speech Synthesis with Speech Units\\u00a7r\\n\\n\\u00a78\\u00a7oJeongsoo Choi\\nMinsu Kim\\nYong Man Ro\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19603\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 May 2023 07:17:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023\\u00a7r"}']}
{title:'Shim et al. (§72023§r)', author: 'Hye-jin Shim; Jee-weon Jung; Tomi Kinnunen', display:{Lore:['[{"text": "arXiv:2305.19953", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Dataset Co-Training with Sharpness-Aware Optimization for Audio Anti-spoofing\\u00a7r\\n\\n\\u00a78\\u00a7oHye-jin Shim\\nJee-weon Jung\\nTomi Kinnunen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19953\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 1 Jun 2023 06:50:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Zhong-Qiu Wang; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2305.20054", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures\\u00a7r\\n\\n\\u00a78\\u00a7oZhong-Qiu Wang\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.20054\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 29 Oct 2023 14:55:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oin Conference on Neural Information ProcessingSystems (NeurIPS), 2023\\u00a7r"}']}
{title:'Lu et al. (§72023§r)', author: 'Peiling Lu; Xin Xu; Chenfei Kang; Botao Yu; Chengyi Xing; Xu Tan; Jiang Bian', display:{Lore:['[{"text": "arXiv:2306.00110", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMuseCoco: Generating Symbolic Music from Text\\u00a7r\\n\\n\\u00a78\\u00a7oPeiling Lu\\nXin Xu\\nChenfei Kang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00110\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 May 2023 18:34:16 GMT)\\u00a7r"}']}
{title:'Montesinos et al. (§72023§r)', author: 'Juan F. Montesinos; Daniel Michelsanti; Gloria Haro; Zheng-Hua Tan; Jesper Jensen', display:{Lore:['[{"text": "arXiv:2306.00489", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech inpainting: Context-based speech synthesis guided by video\\u00a7r\\n\\n\\u00a78\\u00a7oJuan F. Montesinos\\nDaniel Michelsanti\\nGloria Haro\\nZheng-Hua Tan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00489\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jun 2023 09:40:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in Interspeech23\\u00a7r"}']}
{title:'Yadav et al. (§72023§r)', author: 'Sarthak Yadav; Sergios Theodoridis; Lars Kai Hansen; Zheng-Hua Tan', display:{Lore:['[{"text": "arXiv:2306.00561", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMasked Autoencoders with Multi-Window Local-Global Attention Are Better Audio Learners\\u00a7r\\n\\n\\u00a78\\u00a7oSarthak Yadav\\nSergios Theodoridis\\nLars Kai Hansen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00561\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 1 Oct 2023 21:53:36 GMT)\\u00a7r"}']}
{title:'Nakilcioglu et al. (§72023§r)', author: 'Emin Cagatay Nakilcioglu; Maximilian Reimann; Ole John', display:{Lore:['[{"text": "arXiv:2306.00614", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptation and Optimization of Automatic Speech Recognition (ASR) for the Maritime Domain in the Field of VHF Communication\\u00a7r\\n\\n\\u00a78\\u00a7oEmin Cagatay Nakilcioglu\\nMaximilian Reimann\\nOle John\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00614\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the COMPIT Conference 22 (2023) 345-354\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jun 2023 12:38:11 GMT)\\u00a7r"}']}
{title:'Tang et al. (§72023§r)', author: 'Haobin Tang; Xulong Zhang; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2306.00648", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmoMix: Emotion Mixing via Diffusion Models for Emotional Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oHaobin Tang\\nXulong Zhang\\nJianzong Wang\\nNing Cheng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00648\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jun 2023 13:14:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by 24th Annual Conference of the International Speech Communication Association (INTERSPEECH 2023)\\u00a7r"}']}
{title:'Jung et al. (§72023§r)', author: 'Jee-weon Jung; Soonshin Seo; Hee-Soo Heo; Geonmin Kim; You Jin Kim; Young-ki Kwon; Minjae Lee; Bong-Jin Lee', display:{Lore:['[{"text": "arXiv:2306.00680", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEncoder-decoder multimodal speaker change detection\\u00a7r\\n\\n\\u00a78\\u00a7oJee-weon Jung\\nSoonshin Seo\\nHee-Soo Heo\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00680\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jun 2023 13:55:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted for presentation at INTERSPEECH 2023\\u00a7r"}']}
{title:'Sheikh et al. (§72023§r)', author: 'Shakeel A. Sheikh; Md Sahidullah; Fabrice Hirsch; Slim Ouni', display:{Lore:['[{"text": "arXiv:2306.00689", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStuttering Detection Using Speaker Representations and Self-supervised Contextual Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oShakeel A. Sheikh\\nMd Sahidullah\\nFabrice Hirsch\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00689\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jun 2023 14:00:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in International Journal of Speech Technology, Springer 2023 substantial overlap with arXiv:2204.01564\\u00a7r"}']}
{title:'Iashchenko et al. (§72023§r)', author: 'Anastasiia Iashchenko; Pavel Andreev; Ivan Shchekotov; Nicholas Babaev; Dmitry Vetrov', display:{Lore:['[{"text": "arXiv:2306.00721", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnDiff: Unsupervised Voice Restoration with Unconditional Diffusion Model\\u00a7r\\n\\n\\u00a78\\u00a7oAnastasiia Iashchenko\\nPavel Andreev\\nIvan Shchekotov\\nNicholas Babaev\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00721\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 12 Oct 2023 10:32:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023\\u00a7r"}']}
{title:'Haque et al. (§72023§r)', author: 'Mirazul Haque; Rutvij Shah; Simin Chen; Berrak Şişman; Cong Liu; Wei Yang', display:{Lore:['[{"text": "arXiv:2306.00794", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSlothSpeech: Denial-of-service Attack Against Speech Recognition Models\\u00a7r\\n\\n\\u00a78\\u00a7oMirazul Haque\\nRutvij Shah\\nSimin Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00794\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jun 2023 15:25:14 GMT)\\u00a7r"}']}
{title:'Xu et al. (§72023§r)', author: 'Tianyi Xu; Zhanheng Yang; Kaixun Huang; Pengcheng Guo; Ao Zhang; Biao Li; Changru Chen; Chao Li; Lei Xie', display:{Lore:['[{"text": "arXiv:2306.00804", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptive Contextual Biasing for Transducer Based Streaming Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oTianyi Xu\\nZhanheng Yang\\nKaixun Huang\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00804\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 15 Aug 2023 04:36:14 GMT)\\u00a7r"}']}
{title:'Pellegrini et al. (§72023§r)', author: 'Thomas Pellegrini; Ismail Khalfaoui-Hassani; Etienne Labbé; Timothée Masquelier', display:{Lore:['[{"text": "arXiv:2306.00830", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdapting a ConvNeXt model to audio classification on AudioSet\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Pellegrini\\nIsmail Khalfaoui-Hassani\\nEtienne Labb\\u00e9\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00830\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jun 2023 15:52:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2023\\u00a7r"}']}
{title:'Bargum et al. (§72023§r)', author: 'Anders R. Bargum; Stefania Serafin; Cumhur Erkut; Julian D. Parker', display:{Lore:['[{"text": "arXiv:2306.00860", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDifferentiable Allpass Filters for Phase Response Estimation and Automatic Signal Alignment\\u00a7r\\n\\n\\u00a78\\u00a7oAnders R. Bargum\\nStefania Serafin\\nCumhur Erkut\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00860\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 2 Jun 2023 12:05:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCollaboration done while interning/employed at Native Instruments. Accepted for publication in Proc. DAFX\'23, Copenhagen, Denmark, September 2023. Sound examples at https://abargum.github.io v2: 10 pages, LaTeX; "}','{"text": "figures resized, pdf optimized\\u00a7r"}']}
{title:'Shi et al. (§72023§r)', author: 'Jiatong Shi; Yun Tang; Hirofumi Inaguma; Hongyu GOng; Juan Pino; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2306.01084", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploration on HuBERT with Multiple Resolutions\\u00a7r\\n\\n\\u00a78\\u00a7oJiatong Shi\\nYun Tang\\nHirofumi Inaguma\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01084\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 22 Jun 2023 18:34:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech2023\\u00a7r"}']}
{title:'Wei et al. (§72023§r)', author: 'Haojie Wei; Jun Yuan; Rui Zhang; Yueguo Chen; Gang Wang', display:{Lore:['[{"text": "arXiv:2306.01304", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJEPOO: Highly Accurate Joint Estimation of Pitch, Onset and Offset for Music Information Retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oHaojie Wei\\nJun Yuan\\nRui Zhang\\nYueguo Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01304\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.24963/ijcai.2023/544\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 7 Jul 2023 09:57:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper has been accepted by IJCAI 2023; 11 pages, 6 figures\\u00a7r"}']}
{title:'Kawa et al. (§72023§r)', author: 'Piotr Kawa; Marcin Plata; Michał Czuba; Piotr Szymański; Piotr Syga', display:{Lore:['[{"text": "arXiv:2306.01428", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved DeepFake Detection Using Whisper Features\\u00a7r\\n\\n\\u00a78\\u00a7oPiotr Kawa\\nMarcin Plata\\nMicha\\u0142 Czuba\\nPiotr Szyma\\u0144ski\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01428\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Jun 2023 10:34:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2023\\u00a7r"}']}
{title:'Kögel et al. (§72023§r)', author: 'Fabian Kögel; Bac Nguyen; Fabien Cardinaux', display:{Lore:['[{"text": "arXiv:2306.01442", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Robust FastSpeech 2 by Modelling Residual Multimodality\\u00a7r\\n\\n\\u00a78\\u00a7oFabian K\\u00f6gel\\nBac Nguyen\\nFabien Cardinaux\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01442\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Jun 2023 11:03:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2023\\u00a7r"}']}
{title:'Lu et al. (§72023§r)', author: 'Cheng Lu; Hailun Lian; Wenming Zheng; Yuan Zong; Yan Zhao; Sunan Li', display:{Lore:['[{"text": "arXiv:2306.01491", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Local to Global Feature Aggregation for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oCheng Lu\\nHailun Lian\\nWenming Zheng\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01491\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Jun 2023 12:34:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper has been accepted on INTERSPEECH 2023\\u00a7r"}']}
{title:'Xie et al. (§72023§r)', author: 'Zeyu Xie; Xuenan Xu; Mengyue Wu; Kai Yu', display:{Lore:['[{"text": "arXiv:2306.01533", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhance Temporal Relations in Audio Captioning with Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oZeyu Xie\\nXuenan Xu\\nMengyue Wu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01533\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Jun 2023 13:36:34 GMT)\\u00a7r"}']}
{title:'Zhao et al. (§72023§r)', author: 'Jingwei Zhao; Gus Xia; Ye Wang', display:{Lore:['[{"text": "arXiv:2306.01635", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQ   A: Query-Based Representation Learning for Multi-Track Symbolic Music re-Arrangement\\u00a7r\\n\\n\\u00a78\\u00a7oJingwei Zhao\\nGus Xia\\nYe Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01635\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Jun 2023 15:53:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IJCAI 2023 Special Track for AI the Arts and Creativity\\u00a7r"}']}
{title:'Hwang et al. (§72023§r)', author: 'Dongseong Hwang; Changwan Ryu; Khe Chai Sim', display:{Lore:['[{"text": "arXiv:2306.01789", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEdit Distance based RL for RNNT decoding\\u00a7r\\n\\n\\u00a78\\u00a7oDongseong Hwang\\nChangwan Ryu\\nKhe Chai Sim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01789\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 14 Jul 2023 16:53:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures\\u00a7r"}']}
{title:'Kheir et al. (§72023§r)', author: 'Yassine El Kheir; Shammur Absar Chowdhury; Ahmed Ali', display:{Lore:['[{"text": "arXiv:2306.01845", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-View Multi-Task Representation Learning for Mispronunciation Detection\\u00a7r\\n\\n\\u00a78\\u00a7oYassine El Kheir\\nShammur Absar Chowdhury\\nAhmed Ali\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01845\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 7 Aug 2023 08:49:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, Accepted SLaTE23\\u00a7r"}']}
{title:'Cao et al. (§72023§r)', author: 'Chunxiao Cao; Zili An; Zhong Ren; Dinesh Manocha; Kun Zhou', display:{Lore:['[{"text": "arXiv:2306.01974", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBEDRF: Bidirectional Edge Diffraction Response Function for Interactive Sound Propagation\\u00a7r\\n\\n\\u00a78\\u00a7oChunxiao Cao\\nZili An\\nZhong Ren\\nDinesh Manocha\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01974\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 3 Jun 2023 01:06:54 GMT)\\u00a7r"}']}
{title:'Hu et al. (§72023§r)', author: 'Yiying Hu; Hui Feng; Qinghua Zhao; Aijun Li', display:{Lore:['[{"text": "arXiv:2306.02251", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEffects of Tonal Coarticulation and Prosodic Positions on Tonal Contours of Low Rising Tones: In the Case of Xiamen Dialect\\u00a7r\\n\\n\\u00a78\\u00a7oYiying Hu\\nHui Feng\\nQinghua Zhao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.02251\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 4 Jun 2023 03:49:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in InterSpeech 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Jianrong Wang; Yuchen Huo; Li Liu; Tianyi Xu; Qi Li; Sen Li', display:{Lore:['[{"text": "arXiv:2306.02263", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMAVD: The First Open Large-Scale Mandarin Audio-Visual Dataset with Depth Information\\u00a7r\\n\\n\\u00a78\\u00a7oJianrong Wang\\nYuchen Huo\\nLi Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.02263\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 4 Jun 2023 05:00:12 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Zhe Zhang; Yi Yu; Atsuhiro Takasu', display:{Lore:['[{"text": "arXiv:2306.02613", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lControllable Lyrics-to-Melody Generation\\u00a7r\\n\\n\\u00a78\\u00a7oZhe Zhang\\nYi Yu\\nAtsuhiro Takasu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.02613\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Jun 2023 06:14:08 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Junjie Li; Meng Ge; Zexu pan; Rui Cao; Longbiao Wang; Jianwu Dang; Shiliang Zhang', display:{Lore:['[{"text": "arXiv:2306.02625", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRethinking the visual cues in audio-visual speaker extraction\\u00a7r\\n\\n\\u00a78\\u00a7oJunjie Li\\nMeng Ge\\nZexu pan\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.02625\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Jun 2023 06:53:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in Interspeech 2023\\u00a7r"}']}
{title:'Flax (§72023§r)', author: 'Matt R. Flax', display:{Lore:['[{"text": "arXiv:2306.02750", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Learning Prescription, A Neural Network Hearing Aid Core\\u00a7r\\n\\n\\u00a78\\u00a7oMatt R. Flax\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.02750\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Jun 2023 10:12:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ohttps://github.com/flatmax/hearing.aid-neural.network-core\\u00a7r"}']}
{title:'Indenbom et al. (§72023§r)', author: 'Evgenii Indenbom; Nicolae-Catalin Ristea; Ando Saabas; Tanel Parnamaa; Jegor Guzvin; Ross Cutler', display:{Lore:['[{"text": "arXiv:2306.03177", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeepVQE: Real Time Deep Voice Quality Enhancement for Joint Acoustic Echo Cancellation, Noise Suppression and Dereverberation\\u00a7r\\n\\n\\u00a78\\u00a7oEvgenii Indenbom\\nNicolae-Catalin Ristea\\nAndo Saabas\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.03177\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Jun 2023 18:37:05 GMT)\\u00a7r"}']}
{title:'Kalonaris (§72023§r)', author: 'Stefano Kalonaris', display:{Lore:['[{"text": "arXiv:2306.03307", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReef Elegy: An Auditory Display of Hawaii\'s 2019 Coral Bleaching Data\\u00a7r\\n\\n\\u00a78\\u00a7oStefano Kalonaris\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.03307\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 6 Jul 2023 13:44:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in: Proceedings of the 28th International Conference on Auditory Display (ICAD 2023) NOTE: This version (v2) replaces Figure 2, which was incorrectly rendered. Do not use or cite the previous version (v1)\\u00a7r"}','{"text": ""}']}
{title:'Zang et al. (§72023§r)', author: 'Yongyi Zang; You Zhang; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2306.03389", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhase perturbation improves channel robustness for speech spoofing countermeasures\\u00a7r\\n\\n\\u00a78\\u00a7oYongyi Zang\\nYou Zhang\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.03389\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-2039\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 7 Oct 2023 01:32:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages; Proceedings of Interspeech 2023\\u00a7r"}']}
{title:'Ji et al. (§72023§r)', author: 'Shulei Ji; Xinyu Yang', display:{Lore:['[{"text": "arXiv:2306.03718", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotion-Conditioned Melody Harmonization with Hierarchical Variational Autoencoder\\u00a7r\\n\\n\\u00a78\\u00a7oShulei Ji\\nXinyu Yang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.03718\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 20 Jul 2023 03:06:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE SMC 2023\\u00a7r"}']}
{title:'Fukumori et al. (§72023§r)', author: 'Takahiro Fukumori; Taito Ishida; Yoichi Yamashita', display:{Lore:['[{"text": "arXiv:2306.04143", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRISC: A Corpus for Shout Type Classification and Shout Intensity Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oTakahiro Fukumori\\nTaito Ishida\\nYoichi Yamashita\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.04143\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Jun 2023 04:30:02 GMT)\\u00a7r"}']}
{title:'Misra et al. (§72023§r)', author: 'Chandan Misra; Swarup Chattopadhyay', display:{Lore:['[{"text": "arXiv:2306.04148", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSANGEET: A XML based Open Dataset for Research in Hindustani Sangeet\\u00a7r\\n\\n\\u00a78\\u00a7oChandan Misra\\nSwarup Chattopadhyay\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.04148\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Jun 2023 04:50:09 GMT)\\u00a7r"}']}
{title:'Mariotte et al. (§72023§r)', author: 'Théo Mariotte; Anthony Larcher; Silvio Montrésor; Jean-Hugh Thomas', display:{Lore:['[{"text": "arXiv:2306.04268", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-microphone Automatic Speech Segmentation in Meetings Based on Circular Harmonics Features\\u00a7r\\n\\n\\u00a78\\u00a7oTh\\u00e9o Mariotte\\nAnthony Larcher\\nSilvio Montr\\u00e9sor\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.04268\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Jun 2023 09:09:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023, international Speech Communication Association (ISCA), Aug 2023, Dublin, Ireland\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Liang Liu; Haixin Guan; Jinlong Ma; Wei Dai; Guangyong Wang; Shaowei Ding', display:{Lore:['[{"text": "arXiv:2306.04286", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Mask Free Neural Network for Monaural Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oLiang Liu\\nHaixin Guan\\nJinlong Ma\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.04286\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Jun 2023 09:39:07 GMT)\\u00a7r"}']}
{title:'Guan et al. (§72023§r)', author: 'Wenhao Guan; Tao Li; Yishuang Li; Hukai Huang; Qingyang Hong; Lin Li', display:{Lore:['[{"text": "arXiv:2306.04301", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInterpretable Style Transfer for Text-to-Speech with ControlVAE and Diffusion Bridge\\u00a7r\\n\\n\\u00a78\\u00a7oWenhao Guan\\nTao Li\\nYishuang Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.04301\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 11 Jul 2023 15:03:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech2023\\u00a7r"}']}
{title:'Baali et al. (§72023§r)', author: 'Massa Baali; Ibrahim Almakky; Shady Shehata; Fakhri Karray', display:{Lore:['[{"text": "arXiv:2306.04368", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lArabic Dysarthric Speech Recognition Using Adversarial and Signal-Based Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oMassa Baali\\nIbrahim Almakky\\nShady Shehata\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.04368\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Jun 2023 12:01:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023\\u00a7r"}']}
{title:'Han et al. (§72023§r)', author: 'Sangjun Han; Hyeongrae Ihm; Woohyung Lim', display:{Lore:['[{"text": "arXiv:2306.04628", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSystematic Analysis of Music Representations from BERT\\u00a7r\\n\\n\\u00a78\\u00a7oSangjun Han\\nHyeongrae Ihm\\nWoohyung Lim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.04628\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Jun 2023 13:26:55 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Xiaohui Zhang; Jiangyan Yi; Jianhua Tao; Chenlong Wang; Le Xu; Ruibo Fu', display:{Lore:['[{"text": "arXiv:2306.04956", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptive Fake Audio Detection with Low-Rank Model Squeezing\\u00a7r\\n\\n\\u00a78\\u00a7oXiaohui Zhang\\nJiangyan Yi\\nJianhua Tao\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.04956\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nDADA workshop on IJCAI 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Jun 2023 06:06:42 GMT)\\u00a7r"}']}
{title:'Fan et al. (§72023§r)', author: 'Zhiyun Fan; Linhao Dong; Chen Shen; Zhenlin Liang; Jun Zhang; Lu Lu; Zejun Ma', display:{Lore:['[{"text": "arXiv:2306.05279", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLanguage-specific Acoustic Boundary Learning for Mandarin-English Code-switching Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhiyun Fan\\nLinhao Dong\\nChen Shen\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.05279\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Jun 2023 15:27:40 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Chenglong Wang; Jiangyan Yi; Xiaohui Zhang; Jianhua Tao; Le Xu; Ruibo Fu', display:{Lore:['[{"text": "arXiv:2306.05617", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-rank Adaptation Method for Wav2vec2-based Fake Audio Detection\\u00a7r\\n\\n\\u00a78\\u00a7oChenglong Wang\\nJiangyan Yi\\nXiaohui Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.05617\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIJCAI 2023 Workshop on Deepfake Audio Detection and Analysis\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Jun 2023 01:43:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6pages\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Haogeng Liu; Tao Wang; Jie Cao; Ran He; Jianhua Tao', display:{Lore:['[{"text": "arXiv:2306.05708", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBoosting Fast and High-Quality Speech Synthesis with Linear Diffusion\\u00a7r\\n\\n\\u00a78\\u00a7oHaogeng Liu\\nTao Wang\\nJie Cao\\nRan He\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.05708\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 12 Jun 2023 06:12:41 GMT)\\u00a7r"}']}
{title:'Tang et al. (§72023§r)', author: 'Jingjing Tang; Geraint Wiggins; Gyorgy Fazekas', display:{Lore:['[{"text": "arXiv:2306.06040", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReconstructing Human Expressiveness in Piano Performances with a Transformer Network\\u00a7r\\n\\n\\u00a78\\u00a7oJingjing Tang\\nGeraint Wiggins\\nGyorgy Fazekas\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.06040\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 1 Oct 2023 15:14:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 5 figures, accepted by CMMR2023, the 16th International Symposium on Computer Music Multidisciplinary Research\\u00a7r"}']}
{title:'Veliche et al. (§72023§r)', author: 'Irina-Elena Veliche; Pascale Fung', display:{Lore:['[{"text": "arXiv:2306.06083", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Fairness and Robustness in End-to-End Speech Recognition through unsupervised clustering\\u00a7r\\n\\n\\u00a78\\u00a7oIrina-Elena Veliche\\nPascale Fung\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.06083\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Jun 2023 21:13:08 GMT)\\u00a7r"}']}
{title:'Shen et al. (§72023§r)', author: 'Conghao Shen; Violet Z. Yao; Yixin Liu', display:{Lore:['[{"text": "arXiv:2306.06284", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEverybody Compose: Deep Beats To Music\\u00a7r\\n\\n\\u00a78\\u00a7oConghao Shen\\nViolet Z. Yao\\nYixin Liu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.06284\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3587819.3592542\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 14th Conference on ACM Multimedia Systems\\n  (2023)\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Jun 2023 22:24:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted MMSys \'23\\u00a7r"}']}
{title:'Wagner et al. (§72023§r)', author: 'Dominik Wagner; Ilja Baumann; Tobias Bocklet', display:{Lore:['[{"text": "arXiv:2306.06514", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVocoder-Free Non-Parallel Conversion of Whispered Speech With Masked Cycle-Consistent Generative Adversarial Networks\\u00a7r\\n\\n\\u00a78\\u00a7oDominik Wagner\\nIlja Baumann\\nTobias Bocklet\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.06514\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 10 Jun 2023 19:33:05 GMT)\\u00a7r"}']}
{title:'Kumar et al. (§72023§r)', author: 'Rithesh Kumar; Prem Seetharaman; Alejandro Luebs; Ishaan Kumar; Kundan Kumar', display:{Lore:['[{"text": "arXiv:2306.06546", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHigh-Fidelity Audio Compression with Improved RVQGAN\\u00a7r\\n\\n\\u00a78\\u00a7oRithesh Kumar\\nPrem Seetharaman\\nAlejandro Luebs\\nIshaan Kumar\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.06546\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 26 Oct 2023 22:17:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at NeurIPS 2023 (spotlight)\\u00a7r"}']}
{title:'Chien et al. (§72023§r)', author: 'Yung-Lun Chien; Hsin-Hao Chen; Ming-Chi Yen; Shu-Wei Tsai; Hsin-Min Wang; Yu Tsao; Tai-Shih Chi', display:{Lore:['[{"text": "arXiv:2306.06652", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-Visual Mandarin Electrolaryngeal Speech Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oYung-Lun Chien\\nHsin-Hao Chen\\nMing-Chi Yen\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.06652\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 11 Jun 2023 11:25:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Hsin-Hao Chen; Yung-Lun Chien; Ming-Chi Yen; Shu-Wei Tsai; Yu Tsao; Tai-shih Chi; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:2306.06653", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMandarin Electrolaryngeal Speech Voice Conversion using Cross-domain Features\\u00a7r\\n\\n\\u00a78\\u00a7oHsin-Hao Chen\\nYung-Lun Chien\\nMing-Chi Yen\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.06653\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 11 Jun 2023 11:25:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2023\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Wen Wu; Chao Zhang; Philip C. Woodland', display:{Lore:['[{"text": "arXiv:2306.06760", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEstimating the Uncertainty in Emotion Attributes using Deep Evidential Regression\\u00a7r\\n\\n\\u00a78\\u00a7oWen Wu\\nChao Zhang\\nPhilip C. Woodland\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.06760\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.18653/v1/2023.acl-long.873\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 61st Annual Meeting of the Association for\\n  Computational Linguistics (Volume 1: Long Papers) 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 11 Jun 2023 20:07:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ACL 2023\\u00a7r"}']}
{title:'Durand et al. (§72023§r)', author: 'Simon Durand; Daniel Stoller; Sebastian Ewert', display:{Lore:['[{"text": "arXiv:2306.07744", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContrastive Learning-Based Audio to Lyrics Alignment for Multiple Languages\\u00a7r\\n\\n\\u00a78\\u00a7oSimon Durand\\nDaniel Stoller\\nSebastian Ewert\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.07744\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10096725\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2023 - 2023 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-5\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Jun 2023 13:01:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted at the International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2023\\u00a7r"}']}
{title:'Feng et al. (§72023§r)', author: 'Tiantian Feng; Digbalay Bose; Xuan Shi; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2306.07791", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnlocking Foundation Models for Privacy-Enhancing Speech Understanding: An Early Study on Low Resource Speech Training Leveraging Label-guided Synthetic Speech Content\\u00a7r\\n\\n\\u00a78\\u00a7oTiantian Feng\\nDigbalay Bose\\nXuan Shi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.07791\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Jun 2023 14:13:08 GMT)\\u00a7r"}']}
{title:'Masoudian et al. (§72023§r)', author: 'Shahed Masoudian; Khaled Koutini; Markus Schedl; Gerhard Widmer; Navid Rekabsaz', display:{Lore:['[{"text": "arXiv:2306.08010", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDomain Information Control at Inference Time for Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oShahed Masoudian\\nKhaled Koutini\\nMarkus Schedl\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08010\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Jun 2023 10:58:05 GMT)\\u00a7r"}']}
{title:'Kopparapu (§72023§r)', author: 'Sunil Kumar Kopparapu', display:{Lore:['[{"text": "arXiv:2306.08012", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Novel Scheme to classify Read and Spontaneous Speech\\u00a7r\\n\\n\\u00a78\\u00a7oSunil Kumar Kopparapu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08012\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Jun 2023 11:16:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages, 8 figures\\u00a7r"}']}
{title:'Ji et al. (§72023§r)', author: 'Weidong Ji; Shijie Zan; Guohui Zhou; Xu Wang', display:{Lore:['[{"text": "arXiv:2306.08329", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lResearch on an improved Conformer end-to-end Speech Recognition Model with R-Drop Structure\\u00a7r\\n\\n\\u00a78\\u00a7oWeidong Ji\\nShijie Zan\\nGuohui Zhou\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08329\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Jun 2023 08:01:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 9 figures\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Wenzhe Liu; Yupeng Shi; Jun Chen; Wei Rao; Shulin He; Andong Li; Yannan Wang; Zhiyong Wu', display:{Lore:['[{"text": "arXiv:2306.08454", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGesper: A Restoration-Enhancement Framework for General Speech Reconstruction\\u00a7r\\n\\n\\u00a78\\u00a7oWenzhe Liu\\nYupeng Shi\\nJun Chen\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08454\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Jun 2023 11:54:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by InterSpeech 2023\\u00a7r"}']}
{title:'Ramoneda et al. (§72023§r)', author: 'Pedro Ramoneda; Dasaem Jeong; Vsevolod Eremenko; Nazif Can Tamer; Marius Miron; Xavier Serra', display:{Lore:['[{"text": "arXiv:2306.08480", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCombining piano performance dimensions for score difficulty classification\\u00a7r\\n\\n\\u00a78\\u00a7oPedro Ramoneda\\nDasaem Jeong\\nVsevolod Eremenko\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08480\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 27 Sep 2023 14:15:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o36 pages\\u00a7r"}']}
{title:'Thickstun et al. (§72023§r)', author: 'John Thickstun; David Hall; Chris Donahue; Percy Liang', display:{Lore:['[{"text": "arXiv:2306.08620", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnticipatory Music Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oJohn Thickstun\\nDavid Hall\\nChris Donahue\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08620\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Jun 2023 16:27:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o33 pages, 6 figures\\u00a7r"}']}
{title:'Anand et al. (§72023§r)', author: 'Nayan Anand; Meenakshi Sirigiraju; Chiranjeevi Yarra', display:{Lore:['[{"text": "arXiv:2306.08845", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised speech intelligibility assessment with utterance level alignment distance between teacher and learner Wav2Vec-2.0 representations\\u00a7r\\n\\n\\u00a78\\u00a7oNayan Anand\\nMeenakshi Sirigiraju\\nChiranjeevi Yarra\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08845\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Jun 2023 04:18:30 GMT)\\u00a7r"}']}
{title:'Zhong et al. (§72023§r)', author: 'Lifan Zhong; Erica Cooper; Junichi Yamagishi; Nobuaki Minematsu', display:{Lore:['[{"text": "arXiv:2306.08850", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Isolated Musical Notes as Pre-training Data for Predominant Instrument Recognition in Polyphonic Music\\u00a7r\\n\\n\\u00a78\\u00a7oLifan Zhong\\nErica Cooper\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08850\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Jun 2023 04:27:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to APSIPA 2023\\u00a7r"}']}
{title:'Wan et al. (§72023§r)', author: 'Liang Wan; Hongqing Liu; Yi Zhou; Jie Ji', display:{Lore:['[{"text": "arXiv:2306.08956", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Loss Convolutional Network with Time-Frequency Attention for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oLiang Wan\\nHongqing Liu\\nYi Zhou\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08956\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Jun 2023 08:48:19 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yuqi Li; Yizhi Luo; Xiaoshuai Hao; Chuanguang Yang; Zhulin An; Dantong Song; Wei Yi', display:{Lore:['[{"text": "arXiv:2306.08998", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTeam AcieLee: Technical Report for EPIC-SOUNDS Audio-Based Interaction Recognition Challenge 2023\\u00a7r\\n\\n\\u00a78\\u00a7oYuqi Li\\nYizhi Luo\\nXiaoshuai Hao\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08998\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Jun 2023 09:49:07 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Feng Liu; Deyi Tuo; Yinan Xu; Xintong Han', display:{Lore:['[{"text": "arXiv:2306.09025", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCoverHunter: Cover Song Identification with Refined Attention and Alignments\\u00a7r\\n\\n\\u00a78\\u00a7oFeng Liu\\nDeyi Tuo\\nYinan Xu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.09025\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Jun 2023 10:34:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 3 figures\\u00a7r"}']}
{title:'Bibbo et al. (§72023§r)', author: 'Gabriel Bibbo; Arshdeep Singh; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:2306.09106", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SY\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SY\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Tagging on an Embedded Hardware Platform\\u00a7r\\n\\n\\u00a78\\u00a7oGabriel Bibbo\\nArshdeep Singh\\nMark D. Plumbley\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.09106\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Jun 2023 13:02:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to DCASE 2023 Workshop\\u00a7r"}']}
{title:'Shimada et al. (§72023§r)', author: 'Kazuki Shimada; Archontis Politis; Parthasaarathy Sudarsanam; Daniel Krause; Kengo Uchida; Sharath Adavanne; Aapo Hakala; Yuichiro Koyama; Naoya Takahashi; Shusuke Takahashi; Tuomas Virtanen; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2306.09126", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSTARSS23: An Audio-Visual Dataset of Spatial Recordings of Real Scenes with Spatiotemporal Annotations of Sound Events\\u00a7r\\n\\n\\u00a78\\u00a7oKazuki Shimada\\nArchontis Politis\\nParthasaarathy Sudarsanam\\n+ 8 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.09126\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 14 Nov 2023 08:29:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o27 pages, 9 figures, accepted for publication in NeurIPS 2023 Track on Datasetsand Benchmarks\\u00a7r"}']}
{title:'Nolasco et al. (§72023§r)', author: 'Ines Nolasco; Burooj Ghani; Shubhr Singh; Ester Vidaña-Vila; Helen Whitehead; Emily Grout; Michael Emmerson; Frants Jensen; Ivan Kiskin; Joe Morford; Ariana Strandburg-Peshkin; Lisa Gill; Hanna Pamuła; Vincent Lostanlen; Dan Stowell', display:{Lore:['[{"text": "arXiv:2306.09223", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFew-shot bioacoustic event detection at the DCASE 2023 challenge\\u00a7r\\n\\n\\u00a78\\u00a7oInes Nolasco\\nBurooj Ghani\\nShubhr Singh\\n+ 11 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.09223\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Jun 2023 15:59:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to DCASE 2023 workshop\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Minseok Kim; Jun Hyung Lee; Soonyoung Jung', display:{Lore:['[{"text": "arXiv:2306.09382", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Demixing Challenge 2023 Music Demixing Track Technical Report: TFC-TDF-UNet v3\\u00a7r\\n\\n\\u00a78\\u00a7oMinseok Kim\\nJun Hyung Lee\\nSoonyoung Jung\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.09382\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 21 Jul 2023 07:59:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 tables\\u00a7r"}']}
{title:'Raissi et al. (§72023§r)', author: 'Tina Raissi; Christoph Lüscher; Moritz Gunz; Ralf Schlüter; Hermann Ney', display:{Lore:['[{"text": "arXiv:2306.09517", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCompetitive and Resource Efficient Factored Hybrid HMM Systems are Simpler Than You Think\\u00a7r\\n\\n\\u00a78\\u00a7oTina Raissi\\nChristoph L\\u00fcscher\\nMoritz Gunz\\nRalf Schl\\u00fcter\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.09517\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Jun 2023 21:34:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for presentation at InterSpeech 2023\\u00a7r"}']}
{title:'Dong et al. (§72023§r)', author: 'Hao-Wen Dong; Xiaoyu Liu; Jordi Pons; Gautam Bhattacharya; Santiago Pascual; Joan Serrà; Taylor Berg-Kirkpatrick; Julian McAuley', display:{Lore:['[{"text": "arXiv:2306.09635", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models\\u00a7r\\n\\n\\u00a78\\u00a7oHao-Wen Dong\\nXiaoyu Liu\\nJordi Pons\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.09635\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 23 Jul 2023 07:53:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by WASPAA 2023. Demo: https://salu133445.github.io/clipsonic/\\u00a7r"}']}
{title:'Stein et al. (§72023§r)', author: 'David Stein; Bjoern Andres', display:{Lore:['[{"text": "arXiv:2306.09906", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCorrelation Clustering of Bird Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Stein\\nBjoern Andres\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.09906\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 16 Jun 2023 15:35:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages\\u00a7r"}']}
{title:'Clarke et al. (§72023§r)', author: 'Samuel Clarke; Ruohan Gao; Mason Wang; Mark Rau; Julia Xu; Jui-Hsien Wang; Doug L. James; Jiajun Wu', display:{Lore:['[{"text": "arXiv:2306.09944", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRealImpact: A Dataset of Impact Sound Fields for Real Objects\\u00a7r\\n\\n\\u00a78\\u00a7oSamuel Clarke\\nRuohan Gao\\nMason Wang\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.09944\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 16 Jun 2023 16:25:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCVPR 2023 (Highlight). Project page: https://samuelpclarke.com/realimpact/\\u00a7r"}']}
{title:'Oliveira et al. (§72023§r)', author: 'Frederico S. Oliveira; Edresson Casanova; Arnaldo Cândido Júnior; Lucas R. S. Gris; Anderson S. Soares; Arlindo R. Galvão Filho', display:{Lore:['[{"text": "arXiv:2306.09979", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluation of Speech Representations for MOS prediction\\u00a7r\\n\\n\\u00a78\\u00a7oFrederico S. Oliveira\\nEdresson Casanova\\nArnaldo C\\u00e2ndido J\\u00fanior\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.09979\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 16 Jun 2023 17:21:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 4 figures, Accepted to the 26th International Conference of Text, Speech and Dialogue (TSD2023)\\u00a7r"}']}
{title:'Paim et al. (§72023§r)', author: 'Kayuã Oleques Paim; Ricardo Rohweder; Mariana Recamonde-Mendoza; Rodrigo Brandão Mansilha1; Weverton Cordeiro', display:{Lore:['[{"text": "arXiv:2306.10091", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic Identification of Ae. aegypti Mosquitoes using Smartphone Apps and Residual Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oKayu\\u00e3 Oleques Paim\\nRicardo Rohweder\\nMariana Recamonde-Mendoza\\nRodrigo Brand\\u00e3o Mansilha1\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10091\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.bspc.2024.106342\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 16 Jun 2023 13:41:01 GMT)\\u00a7r"}']}
{title:'Chuipka (§72023§r)', author: 'Noah Chuipka', display:{Lore:['[{"text": "arXiv:2306.10093", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.flu-dyn\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusico-acoustic Depictions of Laminar and Turbulent Flows in Ligeti Piano Etude No. 9 and a Novel Method of Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oNoah Chuipka\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10093\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 17 Jun 2023 19:06:41 GMT)\\u00a7r"}']}
{title:'Bando et al. (§72023§r)', author: 'Yoshiaki Bando; Yoshiki Masuyama; Aditya Arie Nugraha; Kazuyoshi Yoshii', display:{Lore:['[{"text": "arXiv:2306.10240", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Fast Full-Rank Spatial Covariance Analysis for Blind Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oYoshiaki Bando\\nYoshiki Masuyama\\nAditya Arie Nugraha\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10240\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 17 Jun 2023 02:50:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, accepted to EUSIPCO 2023\\u00a7r"}']}
{title:'Yuan et al. (§72023§r)', author: 'Yi Yuan; Haohe Liu; Xubo Liu; Xiyuan Kang; Peipei Wu; Mark D. Plumbley; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2306.10359", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lText-Driven Foley Sound Generation With Latent Diffusion Model\\u00a7r\\n\\n\\u00a78\\u00a7oYi Yuan\\nHaohe Liu\\nXubo Liu\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10359\\u00a7r\\n\\nVersion:\\u00a77v5 (Mon, 18 Sep 2023 10:35:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmit to DCASE-workshop 2023, an extension and supersedes the previous technical report arXiv:2305.15905\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Kexin Wang; Yunlong Zhao; Qianqian Dong; Tom Ko; Mingxuan Wang', display:{Lore:['[{"text": "arXiv:2306.10493", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMOSPC: MOS Prediction Based on Pairwise Comparison\\u00a7r\\n\\n\\u00a78\\u00a7oKexin Wang\\nYunlong Zhao\\nQianqian Dong\\nTom Ko\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10493\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 18 Jun 2023 07:38:17 GMT)\\u00a7r"}']}
{title:'Yuan et al. (§72023§r)', author: 'Ruibin Yuan; Yinghao Ma; Yizhi Li; Ge Zhang; Xingran Chen; Hanzhi Yin; Le Zhuo; Yiqi Liu; Jiawen Huang; Zeyue Tian; Binyue Deng; Ningzhi Wang; Chenghua Lin; Emmanouil Benetos; Anton Ragni; Norbert Gyenge; Roger Dannenberg; Wenhu Chen; Gus Xia; Wei Xue; Si Liu; Shi Wang; Ruibo Liu; Yike Guo; Jie Fu', display:{Lore:['[{"text": "arXiv:2306.10548", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMARBLE: Music Audio Representation Benchmark for Universal Evaluation\\u00a7r\\n\\n\\u00a78\\u00a7oRuibin Yuan\\nYinghao Ma\\nYizhi Li\\n+ 21 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10548\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 23 Nov 2023 10:31:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ocamera-ready version for NeurIPS 2023\\u00a7r"}']}
{title:'Song et al. (§72023§r)', author: 'Zengjie Song; Zhaoxiang Zhang', display:{Lore:['[{"text": "arXiv:2306.10684", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisually-Guided Sound Source Separation with Audio-Visual Predictive Coding\\u00a7r\\n\\n\\u00a78\\u00a7oZengjie Song\\nZhaoxiang Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10684\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TNNLS.2023.3288022\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Jun 2023 03:10:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEETransactions on NeuralNetworks and Learning Systems (T-NNLS)\\u00a7r"}']}
{title:'Imamura et al. (§72023§r)', author: 'Kanami Imamura; Tomohiko Nakamura; Norihiro Takamune; Kohei Yatabe; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2306.10718", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAlgorithms of Sampling-Frequency-Independent Layers for Non-integer Strides\\u00a7r\\n\\n\\u00a78\\u00a7oKanami Imamura\\nTomohiko Nakamura\\nNorihiro Takamune\\nKohei Yatabe\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10718\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.23919/EUSIPCO58844.2023.10289819\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nEuropean Signal Processing Conference, Sep. 2023, pp. 326--330\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Jun 2023 06:33:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted for EuropeanSignal Processing Conference 2023 (EUSIPCO 2023)\\u00a7r"}']}
{title:'Liang et al. (§72023§r)', author: 'Hao Liang; Guanxing Zhou; Xiaotong Tu; Andreas Jakobsson; Xinghao Ding; Yue Huang', display:{Lore:['[{"text": "arXiv:2306.10772", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning an Interpretable End-to-End Network for Real-Time Acoustic Beamforming\\u00a7r\\n\\n\\u00a78\\u00a7oHao Liang\\nGuanxing Zhou\\nXiaotong Tu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10772\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Jun 2023 08:28:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 9 figures\\u00a7r"}']}
{title:'Lu et al. (§72023§r)', author: 'Wei-Tsung Lu; Ju-Chiang Wang; Yun-Ning Hung', display:{Lore:['[{"text": "arXiv:2306.10785", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultitrack Music Transcription with a Time-Frequency Perceiver\\u00a7r\\n\\n\\u00a78\\u00a7oWei-Tsung Lu\\nJu-Chiang Wang\\nYun-Ning Hung\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10785\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Jun 2023 08:58:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Naranjo-Alcazar et al. (§72023§r)', author: 'Javier Naranjo-Alcazar; Jordi Grau-Haro; David Almenar; Pedro Zuccarello', display:{Lore:['[{"text": "arXiv:2306.10843", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFemale mosquito detection by means of AI techniques inside release containers in the context of a Sterile Insect Technique program\\u00a7r\\n\\n\\u00a78\\u00a7oJavier Naranjo-Alcazar\\nJordi Grau-Haro\\nDavid Almenar\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10843\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Jun 2023 10:45:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review at DCASE 2023 Workshop\\u00a7r"}']}
{title:'Südholt et al. (§72023§r)', author: 'David Südholt; Cumhur Erkut', display:{Lore:['[{"text": "arXiv:2306.10886", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVocal Timbre Effects with Differentiable Digital Signal Processing\\u00a7r\\n\\n\\u00a78\\u00a7oDavid S\\u00fcdholt\\nCumhur Erkut\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10886\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Jun 2023 12:32:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in Proc DAFx2023, Copenhagen, Denmark. Sound Examples: https://dsuedholt.github.io/ddsp-vocal-effects/ Code: https://github.com/dsuedholt/ddsp_xsynth\\u00a7r"}']}
{title:'Nam et al. (§72023§r)', author: 'Hyeonuk Nam; Seong-Hu Kim; Deokki Min; Yong-Hwa Park', display:{Lore:['[{"text": "arXiv:2306.11277", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrequency     Channel Attention for Computationally Efficient Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oHyeonuk Nam\\nSeong-Hu Kim\\nDeokki Min\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.11277\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 28 Aug 2023 22:58:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to DCASE 2023 workshop\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Xuefei Wang; Yanhua Long; Yijie Li; Haoran Wei', display:{Lore:['[{"text": "arXiv:2306.11309", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-pass Training and Cross-information Fusion for Low-resource End-to-end Accented Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXuefei Wang\\nYanhua Long\\nYijie Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.11309\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Jun 2023 06:08:09 GMT)\\u00a7r"}']}
{title:'Pelinski et al. (§72023§r)', author: 'Teresa Pelinski; Rodrigo Diaz; Adán L. Benito Temprano; Andrew McPherson', display:{Lore:['[{"text": "arXiv:2306.11389", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPipeline for recording datasets and running neural networks on the Bela embedded hardware platform\\u00a7r\\n\\n\\u00a78\\u00a7oTeresa Pelinski\\nRodrigo Diaz\\nAd\\u00e1n L. Benito Temprano\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.11389\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Jun 2023 08:55:04 GMT)\\u00a7r"}']}
{title:'Park et al. (§72023§r)', author: 'Jong-Yun Park; Mitsuaki Tsukamoto; Misato Tanaka; Yukiyasu Kamitani', display:{Lore:['[{"text": "arXiv:2306.11629", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound reconstruction from human brain activity via a generative model with brain-like auditory features\\u00a7r\\n\\n\\u00a78\\u00a7oJong-Yun Park\\nMitsuaki Tsukamoto\\nMisato Tanaka\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.11629\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Jun 2023 15:59:09 GMT)\\u00a7r"}']}
{title:'Poltronieri (§72023§r)', author: 'Andrea Poltronieri', display:{Lore:['[{"text": "arXiv:2306.12249", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKnowledge-based Multimodal Music Similarity\\u00a7r\\n\\n\\u00a78\\u00a7oAndrea Poltronieri\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.12249\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Jun 2023 13:12:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 1 figure\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Zhonghua Liu; Shijun Wang; Ning Chen', display:{Lore:['[{"text": "arXiv:2306.12259", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Speech Disentanglement for Voice Conversion using Rank Module and Speech Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oZhonghua Liu\\nShijun Wang\\nNing Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.12259\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Jun 2023 13:28:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH2023\\u00a7r"}']}
{title:'Kushwaha et al. (§72023§r)', author: 'Saksham Singh Kushwaha; Magdalena Fuentes', display:{Lore:['[{"text": "arXiv:2306.12300", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Multimodal Prototypical Approach for Unsupervised Sound Classification\\u00a7r\\n\\n\\u00a78\\u00a7oSaksham Singh Kushwaha\\nMagdalena Fuentes\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.12300\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 17 Aug 2023 19:03:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2023\\u00a7r"}']}
{title:'Yamamoto (§72023§r)', author: 'Yuya Yamamoto', display:{Lore:['[{"text": "arXiv:2306.12714", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lToward Leveraging Pre-Trained Self-Supervised Frontends for Automatic Singing Voice Understanding Tasks: Three Case Studies\\u00a7r\\n\\n\\u00a78\\u00a7oYuya Yamamoto\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.12714\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 5 Sep 2023 06:20:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at APSIPA ASC 2023\\u00a7r"}']}
{title:'Gharavian (§72023§r)', author: 'Mohammad Reza Hasanabadi Majid Behdad Davood Gharavian', display:{Lore:['[{"text": "arXiv:2306.12785", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMFCCGAN: A Novel MFCC-Based Speech Synthesizer Using Adversarial Learning\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Reza Hasanabadi Majid Behdad Davood Gharavian\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.12785\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10095873\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Jun 2023 10:29:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023 - 2023 IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP)\\u00a7r"}']}
{title:'Oh et al. (§72023§r)', author: 'Sejin Oh; Jason A. Shaw; Karthik Durvasula; Alexei Kochotov', display:{Lore:['[{"text": "arXiv:2306.12789", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRussian assimilatory palatalization is incomplete neutralization\\u00a7r\\n\\n\\u00a78\\u00a7oSejin Oh\\nJason A. Shaw\\nKarthik Durvasula\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.12789\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Jun 2023 10:43:38 GMT)\\u00a7r"}']}
{title:'Nishida et al. (§72023§r)', author: 'Koki Nishida; Norihiro Takamune; Rintaro Ikeshita; Daichi Kitamura; Hiroshi Saruwatari; Tomohiro Nakatani', display:{Lore:['[{"text": "arXiv:2306.12820", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoisyILRMA: Diffuse-Noise-Aware Independent Low-Rank Matrix Analysis for Fast Blind Source Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oKoki Nishida\\nNorihiro Takamune\\nRintaro Ikeshita\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.12820\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Jun 2023 11:35:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted for EuropeanSignal Processing Conference 2023 (EUSIPCO 2023)\\u00a7r"}']}
{title:'Lanzendörfer et al. (§72023§r)', author: 'Luca A. Lanzendörfer; Roger Wattenhofer', display:{Lore:['[{"text": "arXiv:2306.12957", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSiamese SIREN: Audio Compression with Implicit Neural Representations\\u00a7r\\n\\n\\u00a78\\u00a7oLuca A. Lanzend\\u00f6rfer\\nRoger Wattenhofer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.12957\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Jun 2023 15:16:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished as a workshop paper at ICML2023 neural compression workshop\\u00a7r"}']}
{title:'Lanzendörfer et al. (§72023§r)', author: 'Luca A. Lanzendörfer; Florian Grötschla; Emil Funke; Roger Wattenhofer', display:{Lore:['[{"text": "arXiv:2306.13512", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDISCO-10M: A Large-Scale Music Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oLuca A. Lanzend\\u00f6rfer\\nFlorian Gr\\u00f6tschla\\nEmil Funke\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.13512\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 5 Oct 2023 09:45:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oNeurIPS 2023 Track on Datasets and Benchmarks\\u00a7r"}']}
{title:'Brown et al. (§72023§r)', author: 'Jason I. Brown; Ian George', display:{Lore:['[{"text": "arXiv:2306.13691", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModulation Graphs in Popular Music\\u00a7r\\n\\n\\u00a78\\u00a7oJason I. Brown\\nIan George\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.13691\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Jun 2023 00:36:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o17 pages, 6 figures, 6 tables\\u00a7r"}']}
{title:'Broughton et al. (§72023§r)', author: 'Samuel J. Broughton; Lahiru Samarakoon', display:{Lore:['[{"text": "arXiv:2306.13863", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving End-to-End Neural Diarization Using Conversational Summary Representations\\u00a7r\\n\\n\\u00a78\\u00a7oSamuel J. Broughton\\nLahiru Samarakoon\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.13863\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 24 Jun 2023 04:43:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, INTERSPEECH 2023\\u00a7r"}']}
{title:'Violeta et al. (§72023§r)', author: 'Lester Phillip Violeta; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2306.13953", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Analysis of Personalized Speech Recognition System Development for the Deaf and Hard-of-Hearing\\u00a7r\\n\\n\\u00a78\\u00a7oLester Phillip Violeta\\nTomoki Toda\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.13953\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 24 Jun 2023 12:49:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to APSIPA 2023\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Sen Liu; Yiwei Guo; Chenpeng Du; Xie Chen; Kai Yu', display:{Lore:['[{"text": "arXiv:2306.14145", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDSE-TTS: Dual Speaker Embedding for Cross-Lingual Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oSen Liu\\nYiwei Guo\\nChenpeng Du\\nXie Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.14145\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 25 Jun 2023 06:46:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023\\u00a7r"}']}
{title:'Yamamoto et al. (§72023§r)', author: 'Yuya Yamamoto; Juhan Nam; Hiroko Terasawa', display:{Lore:['[{"text": "arXiv:2306.14191", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrimaDNN\': A Characteristics-aware DNN Customization for Singing Technique Detection\\u00a7r\\n\\n\\u00a78\\u00a7oYuya Yamamoto\\nJuhan Nam\\nHiroko Terasawa\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.14191\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 25 Jun 2023 10:15:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at EUSIPCO 2023\\u00a7r"}']}
{title:'Xue et al. (§72023§r)', author: 'Weicheng Xue; Bing Yang; Shaohong Jia', display:{Lore:['[{"text": "arXiv:2306.14276", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAeroacoustic Source Localization\\u00a7r\\n\\n\\u00a78\\u00a7oWeicheng Xue\\nBing Yang\\nShaohong Jia\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.14276\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 5 Jul 2023 04:08:06 GMT)\\u00a7r"}']}
{title:'Huang et al. (§72023§r)', author: 'Wen-Chin Huang; Lester Phillip Violeta; Songxiang Liu; Jiatong Shi; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2306.14422", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Singing Voice Conversion Challenge 2023\\u00a7r\\n\\n\\u00a78\\u00a7oWen-Chin Huang\\nLester Phillip Violeta\\nSongxiang Liu\\nJiatong Shi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.14422\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 6 Jul 2023 08:17:31 GMT)\\u00a7r"}']}
{title:'Serrà et al. (§72023§r)', author: 'Joan Serrà; Davide Scaini; Santiago Pascual; Daniel Arteaga; Jordi Pons; Jeroen Breebaart; Giulio Cengarle', display:{Lore:['[{"text": "arXiv:2306.14647", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMono-to-stereo through parametric stereo generation\\u00a7r\\n\\n\\u00a78\\u00a7oJoan Serr\\u00e0\\nDavide Scaini\\nSantiago Pascual\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.14647\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Jun 2023 12:33:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 1 figure; accepted for ISMIR23\\u00a7r"}']}
{title:'Ngo et al. (§72023§r)', author: 'Dat Ngo; Lam Pham; Huy Phan; Minh Tran; Delaram Jarchi', display:{Lore:['[{"text": "arXiv:2306.14929", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Deep Learning Architecture with Spatio-Temporal Focusing for Detecting Respiratory Anomalies\\u00a7r\\n\\n\\u00a78\\u00a7oDat Ngo\\nLam Pham\\nHuy Phan\\nMinh Tran\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.14929\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 25 Jun 2023 12:24:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2303.04104\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Jie Liu; Zhiba Su; Hui Huang; Caiyan Wan; Quanxiu Wang; Jiangli Hong; Benlai Tang; Fengjie Zhu', display:{Lore:['[{"text": "arXiv:2306.15212", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTranssionADD: A multi-frame reinforcement based sequence tagging model for audio deepfake detection\\u00a7r\\n\\n\\u00a78\\u00a7oJie Liu\\nZhiba Su\\nHui Huang\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.15212\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Jun 2023 05:18:25 GMT)\\u00a7r"}']}
{title:'Dong et al. (§72023§r)', author: 'Shunbo Dong; Jun Xue; Cunhang Fan; Kang Zhu; Yujie Chen; Zhao Lv', display:{Lore:['[{"text": "arXiv:2306.15389", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-perspective Information Fusion Res2Net with RandomSpecmix for Fake Speech Detection\\u00a7r\\n\\n\\u00a78\\u00a7oShunbo Dong\\nJun Xue\\nCunhang Fan\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.15389\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Jun 2023 11:27:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by DADA2023\\u00a7r"}']}
{title:'Wei et al. (§72023§r)', author: 'Haojie Wei; Xueke Cao; Tangpeng Dan; Yueguo Chen', display:{Lore:['[{"text": "arXiv:2306.15412", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRMVPE: A Robust Model for Vocal Pitch Estimation in Polyphonic Music\\u00a7r\\n\\n\\u00a78\\u00a7oHaojie Wei\\nXueke Cao\\nTangpeng Dan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.15412\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-528\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 28 Jun 2023 01:53:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper has been accepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Kefalas et al. (§72023§r)', author: 'Triantafyllos Kefalas; Yannis Panagakis; Maja Pantic', display:{Lore:['[{"text": "arXiv:2306.15464", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLarge-scale unsupervised audio pre-training for video-to-speech synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oTriantafyllos Kefalas\\nYannis Panagakis\\nMaja Pantic\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.15464\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 31 Jul 2023 12:09:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCorrected typos. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\\u00a7r"}']}
{title:'Ye et al. (§72023§r)', author: 'Zhe Ye; Terui Mao; Li Dong; Diqun Yan', display:{Lore:['[{"text": "arXiv:2306.15875", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFake the Real: Backdoor Attack on Deep Speech Classification via Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oZhe Ye\\nTerui Mao\\nLi Dong\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.15875\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-733\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. INTERSPEECH 2023, pp. 4923-4927\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Jun 2023 02:19:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Guo et al. (§72023§r)', author: 'Aoqi Guo; Junnan Wu; Peng Gao; Wenbo Zhu; Qinwen Guo; Dazhi Gao; Yujun Wang', display:{Lore:['[{"text": "arXiv:2306.15942", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhanced Neural Beamformer with Spatial Information for Target Speech Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oAoqi Guo\\nJunnan Wu\\nPeng Gao\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.15942\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Jun 2023 06:03:10 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Xinfeng Li; Junning Ze; Chen Yan; Yushi Cheng; Xiaoyu Ji; Wenyuan Xu', display:{Lore:['[{"text": "arXiv:2306.16022", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnrollment-stage Backdoor Attacks on Speaker Recognition Systems via Adversarial Ultrasound\\u00a7r\\n\\n\\u00a78\\u00a7oXinfeng Li\\nJunning Ze\\nChen Yan\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.16022\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 11 Dec 2023 02:47:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in Internet of Things Journal (IoT-J)\\u00a7r"}']}
{title:'Kölle et al. (§72023§r)', author: 'Michael Kölle; Steffen Illium; Maximilian Zorn; Jonas Nüßlein; Patrick Suchostawski; Claudia Linnhoff-Popien', display:{Lore:['[{"text": "arXiv:2306.16054", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Primate Sounds Classification using Binary Presorting for Deep Learning\\u00a7r\\n\\n\\u00a78\\u00a7oMichael K\\u00f6lle\\nSteffen Illium\\nMaximilian Zorn\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.16054\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Jun 2023 09:35:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDeLTA\\u00a7r"}']}
{title:'Grinstein et al. (§72023§r)', author: 'Eric Grinstein; Mike Brookes; Patrick A. Naylor', display:{Lore:['[{"text": "arXiv:2306.16081", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGraph neural networks for sound source localization on distributed microphone networks\\u00a7r\\n\\n\\u00a78\\u00a7oEric Grinstein\\nMike Brookes\\nPatrick A. Naylor\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.16081\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Jun 2023 10:27:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented as a poster at ICASSP 2023\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Heeseung Kim; Sungwon Kim; Jiheum Yeom; Sungroh Yoon', display:{Lore:['[{"text": "arXiv:2306.16083", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnitSpeech: Speaker-adaptive Speech Synthesis with Untranscribed Data\\u00a7r\\n\\n\\u00a78\\u00a7oHeeseung Kim\\nSungwon Kim\\nJiheum Yeom\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.16083\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Jun 2023 10:30:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023, Oral\\u00a7r"}']}
{title:'Lin et al. (§72023§r)', author: 'Jiuxin Lin; Peng Wang; Heinrich Dinkel; Jun Chen; Zhiyong Wu; Zhiyong Yan; Yongqing Wang; Junbo Zhang; Yujun Wang', display:{Lore:['[{"text": "arXiv:2306.16241", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFocus on the Sound around You: Monaural Target Speaker Extraction via Distance and Speaker Information\\u00a7r\\n\\n\\u00a78\\u00a7oJiuxin Lin\\nPeng Wang\\nHeinrich Dinkel\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.16241\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 7 Oct 2023 09:25:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProc. INTERSPEECH 2023, 2488-2492, doi: 10.21437/Interspeech.2023-218\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Jun Chen; Wei Rao; Zilin Wang; Jiuxin Lin; Yukai Ju; Shulin He; Yannan Wang; Zhiyong Wu', display:{Lore:['[{"text": "arXiv:2306.16250", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMC-SpEx: Towards Effective Speaker Extraction with Multi-Scale Interfusion and Conditional Speaker Modulation\\u00a7r\\n\\n\\u00a78\\u00a7oJun Chen\\nWei Rao\\nZilin Wang\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.16250\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Jun 2023 14:21:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by InterSpeech 2023\\u00a7r"}']}
{title:'Rose et al. (§72023§r)', author: 'Richard Rose; Oscar Chang; Olivier Siohan', display:{Lore:['[{"text": "arXiv:2306.16398", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCascaded encoders for fine-tuning ASR models on overlapped speech\\u00a7r\\n\\n\\u00a78\\u00a7oRichard Rose\\nOscar Chang\\nOlivier Siohan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.16398\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Jun 2023 17:44:30 GMT)\\u00a7r"}']}
{title:'Miyaguchi et al. (§72023§r)', author: 'Anthony Miyaguchi; Nathan Zhong; Murilo Gustineli; Chris Hayduk', display:{Lore:['[{"text": "arXiv:2306.16760", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransfer Learning with Semi-Supervised Dataset Annotation for Birdcall Classification\\u00a7r\\n\\n\\u00a78\\u00a7oAnthony Miyaguchi\\nNathan Zhong\\nMurilo Gustineli\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.16760\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 29 Jun 2023 07:56:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oBirdCLEF workingnote submission to Multimedia Retrieval in Nature (LifeCLEF) for CLEF 2023\\u00a7r"}']}
{title:'Foscarin et al. (§72023§r)', author: 'Francesco Foscarin; Daniel Harasim; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2306.16955", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPredicting Music Hierarchies with a Graph-Based Neural Decoder\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesco Foscarin\\nDaniel Harasim\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.16955\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 29 Jun 2023 13:59:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in the Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)\\u00a7r"}']}
{title:'Burkhardt et al. (§72023§r)', author: 'Felix Burkhardt; Johannes Wagner; Hagen Wierstorf; Florian Eyben; Björn Schuller', display:{Lore:['[{"text": "arXiv:2306.16962", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech-based Age and Gender Prediction with Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oFelix Burkhardt\\nJohannes Wagner\\nHagen Wierstorf\\nFlorian Eyben\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.16962\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 29 Jun 2023 14:13:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, submitted to 15th ITG Conference onSpeech Communication\\u00a7r"}']}
{title:'Gündert et al. (§72023§r)', author: 'Siegfried Gündert; Stephan D. Ewert; Steven van de Par', display:{Lore:['[{"text": "arXiv:2306.16967", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.med-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the relevance of acoustic measurements for creating realistic virtual acoustic environments\\u00a7r\\n\\n\\u00a78\\u00a7oSiegfried G\\u00fcndert\\nStephan D. Ewert\\nSteven van de Par\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.16967\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 29 Jun 2023 14:16:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to the I3DA 2023 International Conference (IEEE Xplore Digital Library) for possible publication. Copyright may be transferred without notice, after which this version may no longer be "}','{"text": "accessible\\u00a7r"}']}
{title:'Luo et al. (§72023§r)', author: 'Simian Luo; Chuanhao Yan; Chenxu Hu; Hang Zhao', display:{Lore:['[{"text": "arXiv:2306.17203", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oSimian Luo\\nChuanhao Yan\\nChenxu Hu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.17203\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 29 Jun 2023 12:39:58 GMT)\\u00a7r"}']}
{title:'Ding et al. (§72023§r)', author: 'Yiwei Ding; Alexander Lerch', display:{Lore:['[{"text": "arXiv:2306.17424", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Embeddings as Teachers for Music Classification\\u00a7r\\n\\n\\u00a78\\u00a7oYiwei Ding\\nAlexander Lerch\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.17424\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Jun 2023 06:38:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the 24th International Society for Music Information Retrieval Conference (ISMIR 2023), 9 pages, 2 figures\\u00a7r"}']}
{title:'Ollerenshaw et al. (§72023§r)', author: 'Anna Ollerenshaw; Md Asif Jalal; Rosanna Milner; Thomas Hain', display:{Lore:['[{"text": "arXiv:2306.17500", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmpirical Interpretation of the Relationship Between Speech Acoustic Context and Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAnna Ollerenshaw\\nMd Asif Jalal\\nRosanna Milner\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.17500\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Jun 2023 09:21:48 GMT)\\u00a7r"}']}
{title:'Cui et al. (§72023§r)', author: 'Yuhao Cui; Xiongwei Wang; Zhongzhou Zhao; Wei Zhou; Haiqing Chen', display:{Lore:['[{"text": "arXiv:2307.00020", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCASEIN: Cascading Explicit and Implicit Control for Fine-grained Emotion Intensity Regulation\\u00a7r\\n\\n\\u00a78\\u00a7oYuhao Cui\\nXiongwei Wang\\nZhongzhou Zhao\\nWei Zhou\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.00020\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Jun 2023 09:45:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2023\\u00a7r"}']}
{title:'Zhao et al. (§72023§r)', author: 'Sheng Zhao; Qilong Yuan; Yibo Duan; Zhuoyue Chen', display:{Lore:['[{"text": "arXiv:2307.00729", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn End-to-End Multi-Module Audio Deepfake Generation System for ADD Challenge 2023\\u00a7r\\n\\n\\u00a78\\u00a7oSheng Zhao\\nQilong Yuan\\nYibo Duan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.00729\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 3 Jul 2023 03:21:23 GMT)\\u00a7r"}']}
{title:'Llorens et al. (§72023§r)', author: 'Ana Llorens; Federico Simonetta; Martín Serrano; Álvaro Torrente', display:{Lore:['[{"text": "arXiv:2307.01120", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lmusif: a Python package for symbolic music feature extraction\\u00a7r\\n\\n\\u00a78\\u00a7oAna Llorens\\nFederico Simonetta\\nMart\\u00edn Serrano\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.01120\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 3 Jul 2023 15:49:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at the Sound and Music Computing Conference 2023\\u00a7r"}']}
{title:'Kang et al. (§72023§r)', author: 'Chenfei Kang; Peiling Lu; Botao Yu; Xu Tan; Wei Ye; Shikun Zhang; Jiang Bian', display:{Lore:['[{"text": "arXiv:2307.01229", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmoGen: Eliminating Subjective Bias in Emotional Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oChenfei Kang\\nPeiling Lu\\nBotao Yu\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.01229\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 3 Jul 2023 05:54:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 7 pages\\u00a7r"}']}
{title:'Sahipjohn et al. (§72023§r)', author: 'Neha Sahipjohn; Neil Shah; Vishal Tambrahalli; Vineet Gandhi', display:{Lore:['[{"text": "arXiv:2307.01233", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobustL2S: Speaker-Specific Lip-to-Speech Synthesis exploiting Self-Supervised Representations\\u00a7r\\n\\n\\u00a78\\u00a7oNeha Sahipjohn\\nNeil Shah\\nVishal Tambrahalli\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.01233\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 3 Jul 2023 09:13:57 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Yijiang Chen; Chengdong Liang; Xiao-Lei Zhang', display:{Lore:['[{"text": "arXiv:2307.01386", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatial-temporal Graph Based Multi-channel Speaker Verification With Ad-hoc Microphone Arrays\\u00a7r\\n\\n\\u00a78\\u00a7oYijiang Chen\\nChengdong Liang\\nXiao-Lei Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.01386\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 3 Jul 2023 22:49:25 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Yikang Wang; Hiromitsu Nishizaki; Ming Li', display:{Lore:['[{"text": "arXiv:2307.01546", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPretraining Conformer with ASR or ASV for Anti-Spoofing Countermeasure\\u00a7r\\n\\n\\u00a78\\u00a7oYikang Wang\\nHiromitsu Nishizaki\\nMing Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.01546\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 30 Oct 2023 08:27:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 2 figures\\u00a7r"}']}
{title:'Burkhardt et al. (§72023§r)', author: 'Felix Burkhardt; Uwe Reichel; Florian Eyben; Björn Schuller', display:{Lore:['[{"text": "arXiv:2307.02132", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGoing Retro: Astonishingly Simple Yet Effective Rule-based Prosody Modelling for Speech Synthesis Simulating Emotion Dimensions\\u00a7r\\n\\n\\u00a78\\u00a7oFelix Burkhardt\\nUwe Reichel\\nFlorian Eyben\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.02132\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 5 Jul 2023 09:20:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at 34thESSV 2023, Munich 2023\\u00a7r"}']}
{title:'Dowerah et al. (§72023§r)', author: 'Sandipana Dowerah; Ajinkya Kulkarni; Romain Serizel; Denis Jouvet', display:{Lore:['[{"text": "arXiv:2307.02244", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervised learning with diffusion-based multichannel speech enhancement for speaker verification under noisy conditions\\u00a7r\\n\\n\\u00a78\\u00a7oSandipana Dowerah\\nAjinkya Kulkarni\\nRomain Serizel\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.02244\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 5 Jul 2023 12:36:39 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Zhifeng Wang; Chunyan Zeng; Surong Duan; Hongjie Ouyang; Hongmin Xu', display:{Lore:['[{"text": "arXiv:2307.02751", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDSARSR: Deep Stacked Auto-encoders Enhanced Robust Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhifeng Wang\\nChunyan Zeng\\nSurong Duan\\nHongjie Ouyang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.02751\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 6 Jul 2023 03:19:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 3 figures\\u00a7r"}']}
{title:'Kilimci et al. (§72023§r)', author: 'Zeynep Hilal Kilimci; Ulku Bayraktar; Ayhan Kucukmanisa', display:{Lore:['[{"text": "arXiv:2307.02820", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluating raw waveforms with deep learning frameworks for speech emotion recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZeynep Hilal Kilimci\\nUlku Bayraktar\\nAyhan Kucukmanisa\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.02820\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 6 Jul 2023 07:27:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages, 6 Figures, 8 Tables\\u00a7r"}']}
{title:'Gong et al. (§72023§r)', author: 'Yuan Gong; Sameer Khurana; Leonid Karlinsky; James Glass', display:{Lore:['[{"text": "arXiv:2307.03183", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers\\u00a7r\\n\\n\\u00a78\\u00a7oYuan Gong\\nSameer Khurana\\nLeonid Karlinsky\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.03183\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-2193\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of Interspeech 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 6 Jul 2023 17:58:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2023. Code at https://github.com/yuangongnd/whisper-at\\u00a7r"}']}
{title:'Leglaive et al. (§72023§r)', author: 'Simon Leglaive; Léonie Borne; Efthymios Tzinis; Mostafa Sadeghi; Matthieu Fraticelli; Scott Wisdom; Manuel Pariente; Daniel Pressnitzer; John R. Hershey', display:{Lore:['[{"text": "arXiv:2307.03533", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe CHiME-7 UDASE task: Unsupervised domain adaptation for conversational speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oSimon Leglaive\\nL\\u00e9onie Borne\\nEfthymios Tzinis\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.03533\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/CHiME.2023-2\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nThe 7th International Workshop on Speech Processing in Everyday\\n  Environments (CHiME), Dublin, Ireland, 2023\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 2 Oct 2023 07:38:18 GMT)\\u00a7r"}']}
{title:'Karystinaios et al. (§72023§r)', author: 'Emmanouil Karystinaios; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2307.03544", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRoman Numeral Analysis with Graph Neural Networks: Onset-wise Predictions from Note-wise Features\\u00a7r\\n\\n\\u00a78\\u00a7oEmmanouil Karystinaios\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.03544\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 12 Jul 2023 12:15:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of the 24th Conference of theInternational Society for Music Information Retrieval (ISMIR 2023), Milan, Italy\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Qi Wang; Shubing Zhang; Li Zhou', display:{Lore:['[{"text": "arXiv:2307.04015", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotion-Guided Music Accompaniment Generation Based on Variational Autoencoder\\u00a7r\\n\\n\\u00a78\\u00a7oQi Wang\\nShubing Zhang\\nLi Zhou\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.04015\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 8 Jul 2023 16:47:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted By International Joint Conference on Neural Networks 2023(IJCNN2023)\\u00a7r"}']}
{title:'Park et al. (§72023§r)', author: 'YeongHyeon Park; Uju Gim; Myung Jin Kim', display:{Lore:['[{"text": "arXiv:2307.04298", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEdge Storage Management Recipe with Zero-Shot Data Compression for Road Anomaly Detection\\u00a7r\\n\\n\\u00a78\\u00a7oYeongHyeon Park\\nUju Gim\\nMyung Jin Kim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.04298\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 26 Aug 2023 14:44:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 4 tables\\u00a7r"}']}
{title:'Toyama et al. (§72023§r)', author: 'Keisuke Toyama; Taketo Akama; Yukara Ikemiya; Yuhta Takida; Wei-Hsiang Liao; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2307.04305", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Piano Transcription with Hierarchical Frequency-Time Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oKeisuke Toyama\\nTaketo Akama\\nYukara Ikemiya\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.04305\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 Jul 2023 02:04:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 6 figures, to be published in ISMIR2023\\u00a7r"}']}
{title:'Kang et al. (§72023§r)', author: 'Minsung Kang; Soochul Park; Keunwoo Choi', display:{Lore:['[{"text": "arXiv:2307.04377", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHCLAS-X: Hierarchical and Cascaded Lyrics Alignment System Using Multimodal Cross-Correlation\\u00a7r\\n\\n\\u00a78\\u00a7oMinsung Kang\\nSoochul Park\\nKeunwoo Choi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.04377\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 Jul 2023 07:22:06 GMT)\\u00a7r"}']}
{title:'Choe et al. (§72023§r)', author: 'Jesse Choe; Siddhant Sood; Ryan Park', display:{Lore:['[{"text": "arXiv:2307.04604", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEchoVest: Real-Time Sound Classification and Depth Perception Expressed through Transcutaneous Electrical Nerve Stimulation\\u00a7r\\n\\n\\u00a78\\u00a7oJesse Choe\\nSiddhant Sood\\nRyan Park\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.04604\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 Jul 2023 14:43:32 GMT)\\u00a7r"}']}
{title:'Song et al. (§72023§r)', author: 'Kun Song; Yi lei; Peikun Chen; Yiqing Cao; Kun Wei; Yongmao Zhang; Lei Xie; Ning Jiang; Guoqing Zhao', display:{Lore:['[{"text": "arXiv:2307.04630", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe NPU-MSXF Speech-to-Speech Translation System for IWSLT 2023 Speech-to-Speech Translation Task\\u00a7r\\n\\n\\u00a78\\u00a7oKun Song\\nYi lei\\nPeikun Chen\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.04630\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 Jul 2023 15:15:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIWSLT@ACL 2023 system paper. Our submitted system ranks1st in the S2ST task of the IWSLT 2023 evaluation campaign\\u00a7r"}']}
{title:'Garcia et al. (§72023§r)', author: 'Hugo Flores Garcia; Prem Seetharaman; Rithesh Kumar; Bryan Pardo', display:{Lore:['[{"text": "arXiv:2307.04686", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVampNet: Music Generation via Masked Acoustic Token Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oHugo Flores Garcia\\nPrem Seetharaman\\nRithesh Kumar\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.04686\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 12 Jul 2023 17:06:41 GMT)\\u00a7r"}']}
{title:'Südholt et al. (§72023§r)', author: 'David Südholt; Mateo Cámara; Zhiyuan Xu; Joshua D. Reiss', display:{Lore:['[{"text": "arXiv:2307.04702", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVocal Tract Area Estimation by Gradient Descent\\u00a7r\\n\\n\\u00a78\\u00a7oDavid S\\u00fcdholt\\nMateo C\\u00e1mara\\nZhiyuan Xu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.04702\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 Jul 2023 16:59:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to DAFx 2023\\u00a7r"}']}
{title:'Xu et al. (§72023§r)', author: 'Siting Xu; Yunlong Tang; Feng Zheng', display:{Lore:['[{"text": "arXiv:2307.04827", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLaunchpadGPT: Language Model as Music Visualization Designer on Launchpad\\u00a7r\\n\\n\\u00a78\\u00a7oSiting Xu\\nYunlong Tang\\nFeng Zheng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.04827\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 23 Jul 2023 10:20:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by International ComputerMusic Conference (ICMC) 2023\\u00a7r"}']}
{title:'Zarkogianni et al. (§72023§r)', author: 'Konstantia Zarkogianni; Edmund Dervakos; George Filandrianos; Theofanis Ganitidis; Vasiliki Gkatzou; Aikaterini Sakagianni; Raghu Raghavendra; C. L. Max Nikias; Giorgos Stamou; Konstantina S. Nikita', display:{Lore:['[{"text": "arXiv:2307.05096", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe smarty4covid dataset and knowledge base: a framework enabling interpretable analysis of audio signals\\u00a7r\\n\\n\\u00a78\\u00a7oKonstantia Zarkogianni\\nEdmund Dervakos\\nGeorge Filandrianos\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.05096\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Jul 2023 08:10:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted for publication in Nature Scientific Data\\u00a7r"}']}
{title:'Simonetta et al. (§72023§r)', author: 'Federico Simonetta; Ana Llorens; Martín Serrano; Eduardo García-Portugués; Álvaro Torrente', display:{Lore:['[{"text": "arXiv:2307.05107", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimizing Feature Extraction for Symbolic Music\\u00a7r\\n\\n\\u00a78\\u00a7oFederico Simonetta\\nAna Llorens\\nMart\\u00edn Serrano\\nEduardo Garc\\u00eda-Portugu\\u00e9s\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.05107\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Jul 2023 08:34:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at ISMIR 2023\\u00a7r"}']}
{title:'Ma et al. (§72023§r)', author: 'Yinghao Ma; Ruibin Yuan; Yizhi Li; Ge Zhang; Xingran Chen; Hanzhi Yin; Chenghua Lin; Emmanouil Benetos; Anton Ragni; Norbert Gyenge; Ruibo Liu; Gus Xia; Roger Dannenberg; Yike Guo; Jie Fu', display:{Lore:['[{"text": "arXiv:2307.05161", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Effectiveness of Speech Self-supervised Learning for Music\\u00a7r\\n\\n\\u00a78\\u00a7oYinghao Ma\\nRuibin Yuan\\nYizhi Li\\n+ 11 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.05161\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Jul 2023 10:37:57 GMT)\\u00a7r"}']}
{title:'Sarmento et al. (§72023§r)', author: 'Pedro Sarmento; Adarsh Kumar; Dekun Xie; CJ Carr; Zack Zukowski; Mathieu Barthet', display:{Lore:['[{"text": "arXiv:2307.05324", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lShredGP: Guitarist Style-Conditioned Tablature Generation\\u00a7r\\n\\n\\u00a78\\u00a7oPedro Sarmento\\nAdarsh Kumar\\nDekun Xie\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.05324\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Jul 2023 15:11:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at CMMR 2023\\u00a7r"}']}
{title:'Loth et al. (§72023§r)', author: 'Jackson Loth; Pedro Sarmento; CJ Carr; Zack Zukowski; Mathieu Barthet', display:{Lore:['[{"text": "arXiv:2307.05328", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProgGP: From GuitarPro Tablature Neural Generation To Progressive Metal Production\\u00a7r\\n\\n\\u00a78\\u00a7oJackson Loth\\nPedro Sarmento\\nCJ Carr\\nZack Zukowski\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.05328\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Jul 2023 15:19:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPre-print accepted for publication at CMMR2023\\u00a7r"}']}
{title:'Duguay et al. (§72023§r)', author: 'Michèle Duguay; Kate Mancey; Johanna Devaney', display:{Lore:['[{"text": "arXiv:2307.05588", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCollaborative Song Dataset (CoSoD): An annotated dataset of multi-artist collaborations in popular music\\u00a7r\\n\\n\\u00a78\\u00a7oMich\\u00e8le Duguay\\nKate Mancey\\nJohanna Devaney\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.05588\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 13 Jul 2023 18:59:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in the Proceedings of the 24th International Society for Music Information Retrieval Conference (ISMIR)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Wenxuan Wang; Guodong Ma; Yuke Li; Binbin Du', display:{Lore:['[{"text": "arXiv:2307.05956", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLanguage-Routing Mixture of Experts for Multilingual and Code-Switching Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oWenxuan Wang\\nGuodong Ma\\nYuke Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.05956\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 14 Jul 2023 02:24:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proc. INTERSPEECH 2023, August 20-24, 2023, Dublin, Ireland\\u00a7r"}']}
{title:'Latif et al. (§72023§r)', author: 'Siddique Latif; Muhammad Usama; Mohammad Ibrahim Malik; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2307.06090", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCan Large Language Models Aid in Annotating Speech Emotional Data? Uncovering New Frontiers\\u00a7r\\n\\n\\u00a78\\u00a7oSiddique Latif\\nMuhammad Usama\\nMohammad Ibrahim Malik\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.06090\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 12 Jul 2023 11:27:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review\\u00a7r"}']}
{title:'Goudarzi (§72023§r)', author: 'Armin Goudarzi', display:{Lore:['[{"text": "arXiv:2307.06181", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.flu-dyn\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lB-CLEAN-SC: CLEAN-SC for broadband sources\\u00a7r\\n\\n\\u00a78\\u00a7oArmin Goudarzi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.06181\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 17 Aug 2023 10:34:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7orevision 1\\u00a7r"}']}
{title:'Ranjan et al. (§72023§r)', author: 'Rishabh Ranjan; Mayank Vatsa; Richa Singh', display:{Lore:['[{"text": "arXiv:2307.06669", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUncovering the Deceptions: An Analysis on Audio Spoofing Detection and Future Prospects\\u00a7r\\n\\n\\u00a78\\u00a7oRishabh Ranjan\\nMayank Vatsa\\nRicha Singh\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.06669\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 13 Jul 2023 10:25:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in IJCAI 2023\\u00a7r"}']}
{title:'Cañas et al. (§72023§r)', author: 'Juan Sebastián Cañas; Maria Paula Toro-Gómez; Larissa Sayuri Moreira Sugai; Hernán Darío Benítez Restrepo; Jorge Rudas; Breyner Posso Bautista; Luís Felipe Toledo; Simone Dena; Adão Henrique Rosa Domingos; Franco Leandro de Souza; Selvino Neckel-Oliveira; Anderson da Rosa; Vítor Carvalho-Rocha; José Vinícius Bernardy; José Luiz Massao Moreira Sugai; Carolina Emília dos Santos; Rogério Pereira Bastos; Diego Llusia; Juan Sebastián Ulloa', display:{Lore:['[{"text": "arXiv:2307.06860", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnuraSet: A dataset for benchmarking Neotropical anuran calls identification in passive acoustic monitoring\\u00a7r\\n\\n\\u00a78\\u00a7oJuan Sebasti\\u00e1n Ca\\u00f1as\\nMaria Paula Toro-G\\u00f3mez\\nLarissa Sayuri Moreira Sugai\\n+ 15 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.06860\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Jul 2023 22:25:21 GMT)\\u00a7r"}']}
{title:'Lau et al. (§72023§r)', author: 'Kin Wai Lau; Yasar Abbas Ur Rehman; Yuyang Xie; Lan Ma', display:{Lore:['[{"text": "arXiv:2307.07265", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudioInceptionNeXt: TCL AI LAB Submission to EPIC-SOUND Audio-Based-Interaction-Recognition Challenge 2023\\u00a7r\\n\\n\\u00a78\\u00a7oKin Wai Lau\\nYasar Abbas Ur Rehman\\nYuyang Xie\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.07265\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 Jul 2023 10:39:05 GMT)\\u00a7r"}']}
{title:'Martelloni et al. (§72023§r)', author: 'Andrea Martelloni; Andrew P McPherson; Mathieu Barthet', display:{Lore:['[{"text": "arXiv:2307.07426", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-time Percussive Technique Recognition and Embedding Learning for the Acoustic Guitar\\u00a7r\\n\\n\\u00a78\\u00a7oAndrea Martelloni\\nAndrew P McPherson\\nMathieu Barthet\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.07426\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 13 Jul 2023 10:48:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the 24th Int. Society for Music Information Retrieval Conf., Milan, Italy, 2023\\u00a7r"}']}
{title:'Barrington et al. (§72023§r)', author: 'Sarah Barrington; Romit Barua; Gautham Koorma; Hany Farid', display:{Lore:['[{"text": "arXiv:2307.07683", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSingle and Multi-Speaker Cloned Voice Detection: From Perceptual to Learned Features\\u00a7r\\n\\n\\u00a78\\u00a7oSarah Barrington\\nRomit Barua\\nGautham Koorma\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.07683\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 27 Sep 2023 16:50:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oS. Barrington, R. Barua, G. Koorma, and Hany Farid.Single and Multi-Speaker Cloned Voice Detection: From Perceptual to Learned Features. Workshop on Image Forensics and Security, Nuremberg, Germany, 2023\\u00a7r"}']}
{title:'Barahona-Ríos et al. (§72023§r)', author: 'Adrián Barahona-Ríos; Tom Collins', display:{Lore:['[{"text": "arXiv:2307.08007", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoiseBandNet: Controllable Time-Varying Neural Synthesis of Sound Effects Using Filterbanks\\u00a7r\\n\\n\\u00a78\\u00a7oAdri\\u00e1n Barahona-R\\u00edos\\nTom Collins\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.08007\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 16 Jul 2023 11:21:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\\u00a7r"}']}
{title:'Cai et al. (§72023§r)', author: 'Hanbo Cai; Pengcheng Zhang; Hai Dong; Yan Xiao; Stefanos Koffas; Yiming Li', display:{Lore:['[{"text": "arXiv:2307.08208", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Stealthy Backdoor Attacks against Speech Recognition via Elements of Sound\\u00a7r\\n\\n\\u00a78\\u00a7oHanbo Cai\\nPengcheng Zhang\\nHai Dong\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.08208\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Jul 2023 02:58:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Xiaohui Zhang; Mangui Liang; Zhengkun Tian; Jiangyan Yi; Jianhua Tao', display:{Lore:['[{"text": "arXiv:2307.08323", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTST: Time-Sparse Transducer for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXiaohui Zhang\\nMangui Liang\\nZhengkun Tian\\nJiangyan Yi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.08323\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInternational Conference on Artificial Intelligence (CICAI 2023)\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Jul 2023 08:41:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages\\u00a7r"}']}
{title:'Huh et al. (§72023§r)', author: 'Jaesung Huh; Max Bain; Andrew Zisserman', display:{Lore:['[{"text": "arXiv:2307.09006", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOxfordVGG Submission to the EGO4D AV Transcription Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oJaesung Huh\\nMax Bain\\nAndrew Zisserman\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.09006\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 Jul 2023 06:48:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTechnical Report\\u00a7r"}']}
{title:'Feng et al. (§72023§r)', author: 'Jiu Feng; Mehmet Hamza Erol; Joon Son Chung; Arda Senocak', display:{Lore:['[{"text": "arXiv:2307.09286", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFlexiAST: Flexibility is What AST Needs\\u00a7r\\n\\n\\u00a78\\u00a7oJiu Feng\\nMehmet Hamza Erol\\nJoon Son Chung\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.09286\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 Jul 2023 14:30:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023\\u00a7r"}']}
{title:'Kumar (§72023§r)', author: 'Arvind Shankar Kumar', display:{Lore:['[{"text": "arXiv:2307.09425", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.pop-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusical Excellence of Mridangam: an introductory review\\u00a7r\\n\\n\\u00a78\\u00a7oArvind Shankar Kumar\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.09425\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Jul 2023 06:24:38 GMT)\\u00a7r"}']}
{title:'Row et al. (§72023§r)', author: 'Eleanor Row; Jingjing Tang; George Fazekas', display:{Lore:['[{"text": "arXiv:2307.09670", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJAZZVAR: A Dataset of Variations found within Solo Piano Performances of Jazz Standards for Music Overpainting\\u00a7r\\n\\n\\u00a78\\u00a7oEleanor Row\\nJingjing Tang\\nGeorge Fazekas\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.09670\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 Jul 2023 22:48:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPre-print accepted for publication at CMMR2023, 12 pages, 4 figures\\u00a7r"}']}
{title:'Mu et al. (§72023§r)', author: 'Honglin Mu; Wentian Xia; Wanxiang Che', display:{Lore:['[{"text": "arXiv:2307.09723", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Domain Generalization for Sound Classification with Sparse Frequency-Regularized Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oHonglin Mu\\nWentian Xia\\nWanxiang Che\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.09723\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 19 Jul 2023 02:21:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICME 2023\\u00a7r"}']}
{title:'Papaioannou et al. (§72023§r)', author: 'Charilaos Papaioannou; Emmanouil Benetos; Alexandros Potamianos', display:{Lore:['[{"text": "arXiv:2307.09795", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrom West to East: Who can understand the music of the others better?\\u00a7r\\n\\n\\u00a78\\u00a7oCharilaos Papaioannou\\nEmmanouil Benetos\\nAlexandros Potamianos\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.09795\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 19 Jul 2023 07:29:14 GMT)\\u00a7r"}']}
{title:'Natsiou et al. (§72023§r)', author: "Anastasia Natsiou; Luca Longo; Sean O'Leary", display:{Lore:['[{"text": "arXiv:2307.10283", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInterpretable Timbre Synthesis using Variational Autoencoders Regularized on Timbre Descriptors\\u00a7r\\n\\n\\u00a78\\u00a7oAnastasia Natsiou\\nLuca Longo\\nSean O\'Leary\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.10283\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 Jul 2023 11:46:13 GMT)\\u00a7r"}']}
{title:'Min et al. (§72023§r)', author: 'Lejun Min; Junyan Jiang; Gus Xia; Jingwei Zhao', display:{Lore:['[{"text": "arXiv:2307.10304", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPolyffusion: A Diffusion Model for Polyphonic Score Generation with Internal and External Controls\\u00a7r\\n\\n\\u00a78\\u00a7oLejun Min\\nJunyan Jiang\\nGus Xia\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.10304\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 19 Jul 2023 06:36:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of the 24th Conference of theInternational Society for Music Information Retrieval (ISMIR 2023), Milan, Italy\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Daegyeom Kim; Seongho Hong; Yong-Hoon Choi', display:{Lore:['[{"text": "arXiv:2307.10550", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer\\u00a7r\\n\\n\\u00a78\\u00a7oDaegyeom Kim\\nSeongho Hong\\nYong-Hoon Choi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.10550\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 20 Jul 2023 03:28:06 GMT)\\u00a7r"}']}
{title:'Zhang (§72023§r)', author: 'Junfei Zhang', display:{Lore:['[{"text": "arXiv:2307.10773", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Genre Classification with ResNet and Bi-GRU Using Visual Spectrograms\\u00a7r\\n\\n\\u00a78\\u00a7oJunfei Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.10773\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 20 Jul 2023 11:10:06 GMT)\\u00a7r"}']}
{title:'Raj et al. (§72023§r)', author: 'Anjali Raj; Shikhar Bharadwaj; Sriram Ganapathy; Min Ma; Shikhar Vashishth', display:{Lore:['[{"text": "arXiv:2307.10982", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMASR: Multi-label Aware Speech Representation\\u00a7r\\n\\n\\u00a78\\u00a7oAnjali Raj\\nShikhar Bharadwaj\\nSriram Ganapathy\\nMin Ma\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.10982\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 25 Sep 2023 12:49:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ASRU 2023\\u00a7r"}']}
{title:'Pavlova (§72023§r)', author: 'Svetlana Pavlova', display:{Lore:['[{"text": "arXiv:2307.10994", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProgressive distillation diffusion for raw music generation\\u00a7r\\n\\n\\u00a78\\u00a7oSvetlana Pavlova\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.10994\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 20 Jul 2023 16:25:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages\\u00a7r"}']}
{title:'Taghavi et al. (§72023§r)', author: 'Zeinab Sadat Taghavi; Ali Satvaty; Hossein Sameti', display:{Lore:['[{"text": "arXiv:2307.11584", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Change of Heart: Improving Speech Emotion Recognition through Speech-to-Text Modality Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oZeinab Sadat Taghavi\\nAli Satvaty\\nHossein Sameti\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.11584\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 Jul 2023 13:48:11 GMT)\\u00a7r"}']}
{title:'Masuyama et al. (§72023§r)', author: 'Yoshiki Masuyama; Xuankai Chang; Wangyou Zhang; Samuele Cornell; Zhong-Qiu Wang; Nobutaka Ono; Yanmin Qian; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2307.12231", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation\\u00a7r\\n\\n\\u00a78\\u00a7oYoshiki Masuyama\\nXuankai Chang\\nWangyou Zhang\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.12231\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 23 Jul 2023 05:39:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEWASPAA 2023\\u00a7r"}']}
{title:'Masuyama et al. (§72023§r)', author: 'Yoshiki Masuyama; Natsuki Ueno; Nobutaka Ono', display:{Lore:['[{"text": "arXiv:2307.12232", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSignal Reconstruction from Mel-spectrogram Based on Bi-level Consistency of Full-band Magnitude and Phase\\u00a7r\\n\\n\\u00a78\\u00a7oYoshiki Masuyama\\nNatsuki Ueno\\nNobutaka Ono\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.12232\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 23 Jul 2023 05:41:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEWASPAA 2023\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Ziwei Zhu; Changhao Shan; Bihong Zhang; Jian Yu', display:{Lore:['[{"text": "arXiv:2307.12262", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA meta learning scheme for fast accent domain expansion in Mandarin speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZiwei Zhu\\nChanghao Shan\\nBihong Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.12262\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 23 Jul 2023 08:23:26 GMT)\\u00a7r"}']}
{title:'Nimitsurachat et al. (§72023§r)', author: 'Peranut Nimitsurachat; Peter Washington', display:{Lore:['[{"text": "arXiv:2307.12343", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Learning for Audio-Based Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oPeranut Nimitsurachat\\nPeter Washington\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.12343\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 23 Jul 2023 14:40:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 9 figures, submitted to IEEE Transactions on Affective Computing\\u00a7r"}']}
{title:'Qi et al. (§72023§r)', author: 'Gege Qi; Yuefeng Chen; Xiaofeng Mao; Xiaojun Jia; Ranjie Duan; Rong Zhang; Hui Xue', display:{Lore:['[{"text": "arXiv:2307.12498", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Automatic Speech Recognition via WavAugment Guided Phoneme Adversarial Training\\u00a7r\\n\\n\\u00a78\\u00a7oGege Qi\\nYuefeng Chen\\nXiaofeng Mao\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.12498\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jul 2023 03:07:40 GMT)\\u00a7r"}']}
{title:'Michieli et al. (§72023§r)', author: 'Umberto Michieli; Pablo Peso Parada; Mete Ozay', display:{Lore:['[{"text": "arXiv:2307.12660", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOnline Continual Learning in Keyword Spotting for Low-Resource Devices via Pooling High-Order Temporal Statistics\\u00a7r\\n\\n\\u00a78\\u00a7oUmberto Michieli\\nPablo Peso Parada\\nMete Ozay\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.12660\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jul 2023 10:04:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023\\u00a7r"}']}
{title:'Gusó et al. (§72023§r)', author: 'Enric Gusó; Joanna Luberadzka; Martí Baig; Umut Sayin Saraç; Xavier Serra', display:{Lore:['[{"text": "arXiv:2307.12888", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn objective evaluation of Hearing Aids and DNN-based speech enhancement in complex acoustic scenes\\u00a7r\\n\\n\\u00a78\\u00a7oEnric Gus\\u00f3\\nJoanna Luberadzka\\nMart\\u00ed Baig\\nUmut Sayin Sara\\u00e7\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.12888\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jul 2023 15:32:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to WASPAA23\\u00a7r"}']}
{title:'Lebourdais et al. (§72023§r)', author: 'Martin Lebourdais; Théo Mariotte; Marie Tahon; Anthony Larcher; Antoine Laurent; Silvio Montresor; Sylvain Meignier; Jean-Hugh Thomas', display:{Lore:['[{"text": "arXiv:2307.13012", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint speech and overlap detection: a benchmark over multiple audio setup and speech domains\\u00a7r\\n\\n\\u00a78\\u00a7oMartin Lebourdais\\nTh\\u00e9o Mariotte\\nMarie Tahon\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.13012\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jul 2023 14:29:21 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Jinxiang Liu; Chen Ju; Chaofan Ma; Yanfeng Wang; Yu Wang; Ya Zhang', display:{Lore:['[{"text": "arXiv:2307.13236", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-aware Query-enhanced Transformer for Audio-Visual Segmentation\\u00a7r\\n\\n\\u00a78\\u00a7oJinxiang Liu\\nChen Ju\\nChaofan Ma\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.13236\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Jul 2023 03:59:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2305.11019\\u00a7r"}']}
{title:'Zheng et al. (§72023§r)', author: 'Youqiang Zheng; Li Xiao; Weiping Tu; Yuhong Yang; Xinmeng Xu', display:{Lore:['[{"text": "arXiv:2307.13295", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCQNV: A combination of coarsely quantized bitstream and neural vocoder for low rate speech coding\\u00a7r\\n\\n\\u00a78\\u00a7oYouqiang Zheng\\nLi Xiao\\nWeiping Tu\\nYuhong Yang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.13295\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Jul 2023 07:21:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Xiao et al. (§72023§r)', author: 'Li Xiao; Xiuping Yang; Xinhong Li; Weiping Tu; Xiong Chen; Weiyan Yi; Jie Lin; Yuhong Yang; Yanzhen Ren', display:{Lore:['[{"text": "arXiv:2307.13346", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Snoring Sound Dataset for Body Position Recognition: Collection, Annotation, and Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oLi Xiao\\nXiuping Yang\\nXinhong Li\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.13346\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Jul 2023 09:03:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2023\\u00a7r"}']}
{title:'Close et al. (§72023§r)', author: 'George Close; Thomas Hain; Stefan Goetze', display:{Lore:['[{"text": "arXiv:2307.13423", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon Intrusive Intelligibility Predictor for Hearing Impaired Individuals using Self Supervised Speech Representations\\u00a7r\\n\\n\\u00a78\\u00a7oGeorge Close\\nThomas Hain\\nStefan Goetze\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.13423\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 7 Dec 2023 11:39:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted @ ASRU2023 SPARKS workshop\\u00a7r"}']}
{title:'Ritu et al. (§72023§r)', author: 'Jarin Ritu; Ethan Barnes; Riley Martell; Alexandra Van Dine; Joshua Peeples', display:{Lore:['[{"text": "arXiv:2307.13788", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHistogram Layer Time Delay Neural Networks for Passive Sonar Classification\\u00a7r\\n\\n\\u00a78\\u00a7oJarin Ritu\\nEthan Barnes\\nRiley Martell\\nAlexandra Van Dine\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.13788\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Jul 2023 19:47:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 Figures, Accepted to 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics\\u00a7r"}']}
{title:'Lostanlen et al. (§72023§r)', author: 'Vincent Lostanlen; Daniel Haider; Han Han; Mathieu Lagrange; Peter Balazs; Martin Ehler', display:{Lore:['[{"text": "arXiv:2307.13821", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a72math.FA\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFitting Auditory Filterbanks with Multiresolution Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oVincent Lostanlen\\nDaniel Haider\\nHan Han\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.13821\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Jul 2023 21:20:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 4 figures, 1 table, conference\\u00a7r"}']}
{title:'Gavojdian et al. (§72023§r)', author: 'Dinu Gavojdian; Teddy Lazebnik; Madalina Mincu; Ariel Oren; Ioana Nicolae; Anna Zamansky', display:{Lore:['[{"text": "arXiv:2307.13994", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBovineTalk: Machine Learning for Vocalization Analysis of Dairy Cattle under Negative Affective States\\u00a7r\\n\\n\\u00a78\\u00a7oDinu Gavojdian\\nTeddy Lazebnik\\nMadalina Mincu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.13994\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Jul 2023 07:07:03 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Tian-Hao Zhang; Dinghao Zhou; Guiping Zhong; Jiaming Zhou; Baoxiang Li', display:{Lore:['[{"text": "arXiv:2307.14132", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCIF-T: A Novel CIF-based Transducer Architecture for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oTian-Hao Zhang\\nDinghao Zhou\\nGuiping Zhong\\nJiaming Zhou\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.14132\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 15 Dec 2023 04:13:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Xubo Liu; Zhongkai Zhu; Haohe Liu; Yi Yuan; Meng Cui; Qiushi Huang; Jinhua Liang; Yin Cao; Qiuqiang Kong; Mark D. Plumbley; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2307.14335", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWavJourney: Compositional Audio Creation with Large Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oXubo Liu\\nZhongkai Zhu\\nHaohe Liu\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.14335\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 26 Nov 2023 14:12:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oGitHub:https://github.com/Audio-AGI/WavJourney\\u00a7r"}']}
{title:'Riahi et al. (§72023§r)', author: 'Abir Riahi; Éric Plourde', display:{Lore:['[{"text": "arXiv:2307.14464", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSingle Channel Speech Enhancement Using U-Net Spiking Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oAbir Riahi\\n\\u00c9ric Plourde\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.14464\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Jul 2023 19:10:29 GMT)\\u00a7r"}']}
{title:'Bralios et al. (§72023§r)', author: 'Dimitrios Bralios; Efthymios Tzinis; Paris Smaragdis', display:{Lore:['[{"text": "arXiv:2307.14609", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComplete and separate: Conditional separation with missing target source attribute completion\\u00a7r\\n\\n\\u00a78\\u00a7oDimitrios Bralios\\nEfthymios Tzinis\\nParis Smaragdis\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.14609\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/WASPAA58266.2023.10248081\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2023 IEEE Workshop on Applications of Signal Processing to Audio\\n  and Acoustics (WASPAA)\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Jul 2023 03:53:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEWorkshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)2023\\u00a7r"}']}
{title:'Cosenza et al. (§72023§r)', author: 'Emanuele Cosenza; Andrea Valenti; Davide Bacciu', display:{Lore:['[{"text": "arXiv:2307.14928", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGraph-based Polyphonic Multitrack Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oEmanuele Cosenza\\nAndrea Valenti\\nDavide Bacciu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.14928\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Jul 2023 15:18:50 GMT)\\u00a7r"}']}
{title:'Xin et al. (§72023§r)', author: 'Yifei Xin; Yuexian Zou', display:{Lore:['[{"text": "arXiv:2307.15344", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction and Auxiliary Captions\\u00a7r\\n\\n\\u00a78\\u00a7oYifei Xin\\nYuexian Zou\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.15344\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Jul 2023 06:46:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech2023\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Li Zhang; Huan Zhao; Yue Li; Bowen Pang; Yannan Wang; Hongji Wang; Wei Rao; Qing Wang; Lei Xie', display:{Lore:['[{"text": "arXiv:2307.15400", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe FlySpeech Audio-Visual Speaker Diarization System for MISP Challenge 2022\\u00a7r\\n\\n\\u00a78\\u00a7oLi Zhang\\nHuan Zhao\\nYue Li\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.15400\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Jul 2023 08:50:29 GMT)\\u00a7r"}']}
{title:'Qiang et al. (§72023§r)', author: 'Chunyu Qiang; Hao Li; Hao Ni; He Qu; Ruibo Fu; Tao Wang; Longbiao Wang; Jianwu Dang', display:{Lore:['[{"text": "arXiv:2307.15484", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMinimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding\\u00a7r\\n\\n\\u00a78\\u00a7oChunyu Qiang\\nHao Li\\nHao Ni\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.15484\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 18 Dec 2023 12:48:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Niclas et al. (§72023§r)', author: 'Angèle Niclas; Josselin Garnier', display:{Lore:['[{"text": "arXiv:2307.15491", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.class-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomated approach for source location in shallow waters\\u00a7r\\n\\n\\u00a78\\u00a7oAng\\u00e8le Niclas\\nJosselin Garnier\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.15491\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Jul 2023 11:37:33 GMT)\\u00a7r"}']}
{title:'Mari et al. (§72023§r)', author: 'Daniele Mari; Davide Salvi; Paolo Bestagini; Simone Milani', display:{Lore:['[{"text": "arXiv:2307.15555", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAll-for-One and One-For-All: Deep learning-based feature fusion for Synthetic Speech Detection\\u00a7r\\n\\n\\u00a78\\u00a7oDaniele Mari\\nDavide Salvi\\nPaolo Bestagini\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.15555\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Jul 2023 13:50:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ECML-PKDD 2023 Workshop \\"Deep Learning and MultimediaForensics. Combating fake media and misinformation\\"\\u00a7r"}']}
{title:'Fang et al. (§72023§r)', author: 'Sen Fang; Bowen Gao; Yangjian Wu; Teik Toe Teoh', display:{Lore:['[{"text": "arXiv:2307.15898", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUniBriVL: Robust Universal Representation and Generation of Audio Driven Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oSen Fang\\nBowen Gao\\nYangjian Wu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.15898\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 9 Sep 2023 11:14:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oVoice-Text fusion input; The first work of audio driven diffusion model. arXiv admin note: text overlapwith arXiv:2303.04585\\u00a7r"}']}
{title:'Pereira et al. (§72023§r)', author: 'Igor Pereira; Felipe Araújo; Filip Korzeniowski; Richard Vogl', display:{Lore:['[{"text": "arXiv:2307.15913", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMoisesdb: A dataset for source separation beyond 4-stems\\u00a7r\\n\\n\\u00a78\\u00a7oIgor Pereira\\nFelipe Ara\\u00fajo\\nFilip Korzeniowski\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.15913\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 29 Jul 2023 06:59:37 GMT)\\u00a7r"}']}
{title:'Lei et al. (§72023§r)', author: 'Shun Lei; Yixuan Zhou; Liyang Chen; Zhiyong Wu; Xixin Wu; Shiyin Kang; Helen Meng', display:{Lore:['[{"text": "arXiv:2307.16012", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMSStyleTTS: Multi-Scale Style Modeling with Hierarchical Context Information for Expressive Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oShun Lei\\nYixuan Zhou\\nLiyang Chen\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.16012\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 29 Jul 2023 15:48:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE/ACM Transactionson Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Sang-Hoon Lee; Ha-Yeong Choi; Hyung-Seok Oh; Seong-Whan Lee', display:{Lore:['[{"text": "arXiv:2307.16171", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierVST: Hierarchical Adaptive Zero-shot Voice Style Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oSang-Hoon Lee\\nHa-Yeong Choi\\nHyung-Seok Oh\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.16171\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 30 Jul 2023 08:49:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023 (Oral)\\u00a7r"}']}
{title:'Doh et al. (§72023§r)', author: 'SeungHeon Doh; Keunwoo Choi; Jongpil Lee; Juhan Nam', display:{Lore:['[{"text": "arXiv:2307.16372", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLP-MusicCaps: LLM-Based Pseudo Music Captioning\\u00a7r\\n\\n\\u00a78\\u00a7oSeungHeon Doh\\nKeunwoo Choi\\nJongpil Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.16372\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Jul 2023 02:32:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at the 24th International Society for Music Information Retrieval Conference (ISMIR 2023)\\u00a7r"}']}
{title:'Kong et al. (§72023§r)', author: 'Jungil Kong; Jihoon Park; Beomjeong Kim; Jeongmin Kim; Dohee Kong; Sangjin Kim', display:{Lore:['[{"text": "arXiv:2307.16430", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design\\u00a7r\\n\\n\\u00a78\\u00a7oJungil Kong\\nJihoon Park\\nBeomjeong Kim\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.16430\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Jul 2023 06:36:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023\\u00a7r"}']}
{title:'Quan et al. (§72023§r)', author: 'Changsheng Quan; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2307.16516", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatialNet: Extensively Learning Spatial Information for Multichannel Joint Speech Separation, Denoising and Dereverberation\\u00a7r\\n\\n\\u00a78\\u00a7oChangsheng Quan\\nXiaofei Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.16516\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 22 Dec 2023 01:52:40 GMT)\\u00a7r"}']}
{title:'Oh et al. (§72023§r)', author: 'Hyung-Seok Oh; Sang-Hoon Lee; Seong-Whan Lee', display:{Lore:['[{"text": "arXiv:2307.16549", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffProsody: Diffusion-based Latent Prosody Generation for Expressive Speech Synthesis with Prosody Conditional Adversarial Training\\u00a7r\\n\\n\\u00a78\\u00a7oHyung-Seok Oh\\nSang-Hoon Lee\\nSeong-Whan Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.16549\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Jul 2023 10:28:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 8 figures, 5 tables, under review\\u00a7r"}']}
{title:'Kefalas et al. (§72023§r)', author: 'Triantafyllos Kefalas; Yannis Panagakis; Maja Pantic', display:{Lore:['[{"text": "arXiv:2307.16584", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-visual video-to-speech synthesis with synthesized input audio\\u00a7r\\n\\n\\u00a78\\u00a7oTriantafyllos Kefalas\\nYannis Panagakis\\nMaja Pantic\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.16584\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Jul 2023 11:39:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Chen Liu; Peike Li; Xingqun Qi; Hu Zhang; Lincheng Li; Dadong Wang; Xin Yu', display:{Lore:['[{"text": "arXiv:2307.16620", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-Visual Segmentation by Exploring Cross-Modal Mutual Semantics\\u00a7r\\n\\n\\u00a78\\u00a7oChen Liu\\nPeike Li\\nXingqun Qi\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.16620\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 1 Aug 2023 01:40:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper has been received by ACM MM 23\\u00a7r"}']}
{title:'Rijal et al. (§72023§r)', author: 'S. Rijal; R. Neupane; S. P. Mainali; S. K. Regmi; S. Maharjan', display:{Lore:['[{"text": "arXiv:2308.00010", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMonaural Multi-Speaker Speech Separation Using Efficient Transformer Model\\u00a7r\\n\\n\\u00a78\\u00a7oS. Rijal\\nR. Neupane\\nS. P. Mainali\\nS. K. Regmi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.00010\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 29 Jul 2023 15:10:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 6 figures, 2 tables, study conducted as major project for B.E. (Computer Engineering), IOE Tribhuvan University 2023\\u00a7r"}']}
{title:'Barenboim et al. (§72023§r)', author: 'Gabriela Barenboim; Luigi Del Debbio; Johannes Hirn; Veronica Sanz', display:{Lore:['[{"text": "arXiv:2308.00015", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring how a Generative AI interprets music\\u00a7r\\n\\n\\u00a78\\u00a7oGabriela Barenboim\\nLuigi Del Debbio\\nJohannes Hirn\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.00015\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Jul 2023 15:35:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages, 12 figures\\u00a7r"}']}
{title:'Jeon et al. (§72023§r)', author: 'Chang-Bin Jeon; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2308.01187", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic De-limiter Networks via Sample-wise Gain Inversion\\u00a7r\\n\\n\\u00a78\\u00a7oChang-Bin Jeon\\nKyogu Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.01187\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Aug 2023 14:51:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEWorkshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)2023\\u00a7r"}']}
{title:'Wagner et al. (§72023§r)', author: 'Laurin Wagner; Mario Zusag; Theresa Bloder', display:{Lore:['[{"text": "arXiv:2308.01327", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCareful Whisper \\u2013 leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification\\u00a7r\\n\\n\\u00a78\\u00a7oLaurin Wagner\\nMario Zusag\\nTheresa Bloder\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.01327\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Aug 2023 15:53:59 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Ke Chen; Yusong Wu; Haohe Liu; Marianna Nezhurina; Taylor Berg-Kirkpatrick; Shlomo Dubnov', display:{Lore:['[{"text": "arXiv:2308.01546", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies\\u00a7r\\n\\n\\u00a78\\u00a7oKe Chen\\nYusong Wu\\nHaohe Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.01546\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 3 Aug 2023 05:35:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages, 3 figures, 2 tables, demo page: https://musicldm.github.io/\\u00a7r"}']}
{title:'Ko et al. (§72023§r)', author: 'Myeongjin Ko; Yong-Hoon Choi', display:{Lore:['[{"text": "arXiv:2308.01573", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Training of Denoising Diffusion Model Using Dual Discriminators for High-Fidelity Multi-Speaker TTS\\u00a7r\\n\\n\\u00a78\\u00a7oMyeongjin Ko\\nYong-Hoon Choi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.01573\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/OJSP.2024.3386495\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Open Journal of Signal Processing, vol. 5, pp. 577-587, 2024\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 3 Aug 2023 07:22:04 GMT)\\u00a7r"}']}
{title:'Ramesh et al. (§72023§r)', author: 'Guruprasad V Ramesh; Gopinath Chennupati; Milind Rao; Anit Kumar Sahu; Ariya Rastrow; Jasha Droppo', display:{Lore:['[{"text": "arXiv:2308.02013", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFederated Representation Learning for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGuruprasad V Ramesh\\nGopinath Chennupati\\nMilind Rao\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.02013\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 7 Aug 2023 21:34:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ISCASPSC Symposium 3rd Symposium on Security and Privacy in Speech Communication, 2023\\u00a7r"}']}
{title:'Ye et al. (§72023§r)', author: 'Jiaxin Ye; Yujie Wei; Xin-Cheng Wen; Chenglong Ma; Zhizhong Huang; Kunhong Liu; Hongming Shan', display:{Lore:['[{"text": "arXiv:2308.02190", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmo-DNA: Emotion Decoupling and Alignment Learning for Cross-Corpus Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJiaxin Ye\\nYujie Wei\\nXin-Cheng Wen\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.02190\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3581783.3611704\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Aug 2023 08:15:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ACM MM 2023\\u00a7r"}']}
{title:'Han et al. (§72023§r)', author: 'Danbinaerin Han; Rafael Caro Repetto; Dasaem Jeong', display:{Lore:['[{"text": "arXiv:2308.02249", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFinding Tori: Self-supervised Learning for Analyzing Korean Folk Song\\u00a7r\\n\\n\\u00a78\\u00a7oDanbinaerin Han\\nRafael Caro Repetto\\nDasaem Jeong\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.02249\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Aug 2023 11:13:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at 24thInternational Society for Music Information Retrieval Conference (ISMIR 2023)\\u00a7r"}']}
{title:'Long et al. (§72023§r)', author: 'Jinyu Long; Jetic Gū; Binhao Bai; Zhibo Yang; Ping Wei; Junli Li', display:{Lore:['[{"text": "arXiv:2308.02263", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Monaural Speech Enhancement using Spectrum Attention Fusion\\u00a7r\\n\\n\\u00a78\\u00a7oJinyu Long\\nJetic G\\u016b\\nBinhao Bai\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.02263\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Aug 2023 11:39:29 GMT)\\u00a7r"}']}
{title:'Roman et al. (§72023§r)', author: 'Robin San Roman; Yossi Adi; Antoine Deleforge; Romain Serizel; Gabriel Synnaeve; Alexandre Défossez', display:{Lore:['[{"text": "arXiv:2308.02560", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrom Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion\\u00a7r\\n\\n\\u00a78\\u00a7oRobin San Roman\\nYossi Adi\\nAntoine Deleforge\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.02560\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nThirty-seventh Conference on Neural Information Processing Systems\\n  (2023)\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 8 Nov 2023 10:04:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages\\u00a7r"}']}
{title:'Shao et al. (§72023§r)', author: 'Keren Shao; Ke Chen; Taylor Berg-Kirkpatrick; Shlomo Dubnov', display:{Lore:['[{"text": "arXiv:2308.02723", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Improving Harmonic Sensitivity and Prediction Stability for Singing Melody Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oKeren Shao\\nKe Chen\\nTaylor Berg-Kirkpatrick\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.02723\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Aug 2023 21:59:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 4 figures, 2 tables, Proceedings of the 24th International Society for Music Information Retrieval Conference, ISMIR 2023\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Yuning Wu; Yifeng Yu; Jiatong Shi; Tao Qian; Qin Jin', display:{Lore:['[{"text": "arXiv:2308.02867", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Systematic Exploration of Joint-training for Singing Voice Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYuning Wu\\nYifeng Yu\\nJiatong Shi\\nTao Qian\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.02867\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 5 Aug 2023 12:42:20 GMT)\\u00a7r"}']}
{title:'Gu et al. (§72023§r)', author: 'Xiangming Gu; Wei Zeng; Ye Wang', display:{Lore:['[{"text": "arXiv:2308.02898", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lElucidate Gender Fairness in Singing Voice Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oXiangming Gu\\nWei Zeng\\nYe Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.02898\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 5 Aug 2023 15:15:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCamera-ready version of ACM MM2023\\u00a7r"}']}
{title:'Vodnala et al. (§72023§r)', author: 'Naveenkumar Vodnala; Pratap Reddy Lankireddy; Padmasai Yarlagadda', display:{Lore:['[{"text": "arXiv:2308.03019", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCharacterization of cough sounds using statistical analysis\\u00a7r\\n\\n\\u00a78\\u00a7oNaveenkumar Vodnala\\nPratap Reddy Lankireddy\\nPadmasai Yarlagadda\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.03019\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 6 Aug 2023 04:26:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o19 pages, 8 figures, papersubmitted to journal Biomedical Signal Processing and Controlwhich is under review\\u00a7r"}']}
{title:'Shi et al. (§72023§r)', author: 'Xian Shi; Yexin Yang; Zerui Li; Yanni Chen; Zhifu Gao; Shiliang Zhang', display:{Lore:['[{"text": "arXiv:2308.03266", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability\\u00a7r\\n\\n\\u00a78\\u00a7oXian Shi\\nYexin Yang\\nZerui Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.03266\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 25 Dec 2023 08:44:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by ICASSP2024\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Xiaohui Zhang; Jiangyan Yi; Jianhua Tao; Chenglong Wang; Chuyuan Zhang', display:{Lore:['[{"text": "arXiv:2308.03300", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDo You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection\\u00a7r\\n\\n\\u00a78\\u00a7oXiaohui Zhang\\nJiangyan Yi\\nJianhua Tao\\nChenglong Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.03300\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Aug 2023 05:05:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o40th Internation Conference on MachineLearning (ICML 2023)\\u00a7r"}']}
{title:'Melhem et al. (§72023§r)', author: 'Rawad Melhem; Assef Jafar; Riad Hamadeh', display:{Lore:['[{"text": "arXiv:2308.03332", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Deep Attractor Network by BGRU and GMM for Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oRawad Melhem\\nAssef Jafar\\nRiad Hamadeh\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.03332\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.11916/j.issn.1005-9113.2019044\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJournal of Harbin Institute of Technology (New Series), vol. 28,\\n  no. 3, pp. 90-96, 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Aug 2023 06:26:53 GMT)\\u00a7r"}']}
{title:'Grinstein et al. (§72023§r)', author: 'Eric Grinstein; Vincent W. Neo; Patrick A. Naylor', display:{Lore:['[{"text": "arXiv:2308.04169", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDual input neural networks for positional sound source localization\\u00a7r\\n\\n\\u00a78\\u00a7oEric Grinstein\\nVincent W. Neo\\nPatrick A. Naylor\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.04169\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Aug 2023 09:59:56 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Xiaoyu Chen; Changde Du; Qiongyi Zhou; Huiguang He', display:{Lore:['[{"text": "arXiv:2308.04244", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.NC\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAuditory Attention Decoding with Task-Related Multi-View Contrastive Learning\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoyu Chen\\nChangde Du\\nQiongyi Zhou\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.04244\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Aug 2023 13:17:37 GMT)\\u00a7r"}']}
{title:'Islam et al. (§72023§r)', author: 'Samiul Islam; Md. Maksudul Haque; Abu Jobayer Md. Sadat', display:{Lore:['[{"text": "arXiv:2308.04517", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCapturing Spectral and Long-term Contextual Information for Speech Emotion Recognition Using Deep Learning Techniques\\u00a7r\\n\\n\\u00a78\\u00a7oSamiul Islam\\nMd. Maksudul Haque\\nAbu Jobayer Md. Sadat\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.04517\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Aug 2023 06:20:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7othe research paper is still in progress\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Peike Li; Boyu Chen; Yao Yao; Yikai Wang; Allen Wang; Alex Wang', display:{Lore:['[{"text": "arXiv:2308.04729", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJEN-1: Text-Guided Universal Music Generation with Omnidirectional Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oPeike Li\\nBoyu Chen\\nYao Yao\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.04729\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 9 Aug 2023 06:27:24 GMT)\\u00a7r"}']}
{title:'Luong et al. (§72023§r)', author: 'Diep Luong; Minh Tran; Shayan Gharib; Konstantinos Drossos; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2308.04960", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRepresentation Learning for Audio Privacy Preservation using Source Separation and Robust Adversarial Learning\\u00a7r\\n\\n\\u00a78\\u00a7oDiep Luong\\nMinh Tran\\nShayan Gharib\\nKonstantinos Drossos\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.04960\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 9 Aug 2023 13:50:00 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Yang Zhang; Krishna C. Puvvada; Vitaly Lavrukhin; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2308.05218", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConformer-based Target-Speaker Automatic Speech Recognition for Single-Channel Audio\\u00a7r\\n\\n\\u00a78\\u00a7oYang Zhang\\nKrishna C. Puvvada\\nVitaly Lavrukhin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.05218\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10095115\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 9 Aug 2023 20:51:54 GMT)\\u00a7r"}']}
{title:'Yin et al. (§72023§r)', author: 'Zhaohui Yin; Jingguang Tian; Xinhui Hu; Xinkang Xu; Yang Xiang', display:{Lore:['[{"text": "arXiv:2308.05987", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLarge-Scale Learning on Overlapped Speech Detection: New Benchmark and New General System\\u00a7r\\n\\n\\u00a78\\u00a7oZhaohui Yin\\nJingguang Tian\\nXinhui Hu\\nXinkang Xu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.05987\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 7 Sep 2023 07:56:10 GMT)\\u00a7r"}']}
{title:'Noel-Hirst et al. (§72023§r)', author: 'Ashley Noel-Hirst; Nick Bryan-Kinns', display:{Lore:['[{"text": "arXiv:2308.06089", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.HC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Autoethnographic Exploration of XAI in Algorithmic Composition\\u00a7r\\n\\n\\u00a78\\u00a7oAshley Noel-Hirst\\nNick Bryan-Kinns\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.06089\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Aug 2023 12:03:17 GMT)\\u00a7r"}']}
{title:'Djilali et al. (§72023§r)', author: 'Yasser Abdelaziz Dahou Djilali; Sanath Narayan; Haithem Boussaid; Ebtessam Almazrouei; Merouane Debbah', display:{Lore:['[{"text": "arXiv:2308.06112", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping\\u00a7r\\n\\n\\u00a78\\u00a7oYasser Abdelaziz Dahou Djilali\\nSanath Narayan\\nHaithem Boussaid\\nEbtessam Almazrouei\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.06112\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Aug 2023 12:59:02 GMT)\\u00a7r"}']}
{title:'Shan et al. (§72023§r)', author: 'Siyuan Shan; Yang Li; Amartya Banerjee; Junier B. Oliva', display:{Lore:['[{"text": "arXiv:2308.06382", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhoneme Hallucinator: One-shot Voice Conversion via Set Expansion\\u00a7r\\n\\n\\u00a78\\u00a7oSiyuan Shan\\nYang Li\\nAmartya Banerjee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.06382\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 30 Dec 2023 22:48:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAAAI 2024 Demo, Codes: https://phonemehallucinator.github.io/\\u00a7r"}']}
{title:'Nishu et al. (§72023§r)', author: 'Kumari Nishu; Minsik Cho; Paul Dixon; Devang Naik', display:{Lore:['[{"text": "arXiv:2308.06472", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFlexible Keyword Spotting based on Homogeneous Audio-Text Embedding\\u00a7r\\n\\n\\u00a78\\u00a7oKumari Nishu\\nMinsik Cho\\nPaul Dixon\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.06472\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 12 Aug 2023 05:41:15 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Yenan Zhang; Hiroshi Watanabe', display:{Lore:['[{"text": "arXiv:2308.06483", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBigWavGAN: A Wave-To-Wave Generative Adversarial Network for Music Super-Resolution\\u00a7r\\n\\n\\u00a78\\u00a7oYenan Zhang\\nHiroshi Watanabe\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.06483\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 29 Oct 2023 10:35:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE GCCE 2023\\u00a7r"}']}
{title:'Kaneko et al. (§72023§r)', author: 'Takuhiro Kaneko; Hirokazu Kameoka; Kou Tanaka; Shogo Seki', display:{Lore:['[{"text": "arXiv:2308.07117", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7liSTFTNet2: Faster and More Lightweight iSTFT-Based Neural Vocoder Using 1D-2D CNN\\u00a7r\\n\\n\\u00a78\\u00a7oTakuhiro Kaneko\\nHirokazu Kameoka\\nKou Tanaka\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.07117\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Aug 2023 12:56:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023. Project page: https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet2/\\u00a7r"}']}
{title:'Rauch et al. (§72023§r)', author: 'Lukas Rauch; Raphael Schwinger; Moritz Wirth; Bernhard Sick; Sven Tomforde; Christoph Scholz', display:{Lore:['[{"text": "arXiv:2308.07121", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lActive Bird2Vec: Towards End-to-End Bird Sound Monitoring with Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oLukas Rauch\\nRaphael Schwinger\\nMoritz Wirth\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.07121\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 21 Nov 2023 13:55:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted @AI4S ECAI2023. This is the author\'s version of thework\\u00a7r"}']}
{title:'Cochoy (§72023§r)', author: 'Jeremy Cochoy', display:{Lore:['[{"text": "arXiv:2308.07170", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHuman Voice Pitch Estimation: A Convolutional Network with Auto-Labeled and Synthetic Data\\u00a7r\\n\\n\\u00a78\\u00a7oJeremy Cochoy\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.07170\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 17 Dec 2023 17:46:27 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Zhaohui Li; Haitao Wang; Xinghua Jiang', display:{Lore:['[{"text": "arXiv:2308.07221", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes\\u00a7r\\n\\n\\u00a78\\u00a7oZhaohui Li\\nHaitao Wang\\nXinghua Jiang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.07221\\u00a7r\\n\\nVersion:\\u00a77v6 (Fri, 25 Aug 2023 12:33:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oNeed to supplement more detailed experiments\\u00a7r"}']}
{title:'Bhosale et al. (§72023§r)', author: 'Swapnil Bhosale; Sauradip Nag; Diptesh Kanojia; Jiankang Deng; Xiatian Zhu', display:{Lore:['[{"text": "arXiv:2308.07293", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffSED: Sound Event Detection with Denoising Diffusion\\u00a7r\\n\\n\\u00a78\\u00a7oSwapnil Bhosale\\nSauradip Nag\\nDiptesh Kanojia\\nJiankang Deng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.07293\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 16 Aug 2023 18:57:28 GMT)\\u00a7r"}']}
{title:'Choi et al. (§72023§r)', author: 'Jeongsoo Choi; Joanna Hong; Yong Man Ro', display:{Lore:['[{"text": "arXiv:2308.07787", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffV2S: Diffusion-based Video-to-Speech Synthesis with Vision-guided Speaker Embedding\\u00a7r\\n\\n\\u00a78\\u00a7oJeongsoo Choi\\nJoanna Hong\\nYong Man Ro\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.07787\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Aug 2023 14:07:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICCV 2023\\u00a7r"}']}
{title:'Zhao et al. (§72023§r)', author: 'Running Zhao; Jiangtao Yu; Hang Zhao; Edith C. H. Ngai', display:{Lore:['[{"text": "arXiv:2308.08125", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRadio2Text: Streaming Speech Recognition Using mmWave Radio Signals\\u00a7r\\n\\n\\u00a78\\u00a7oRunning Zhao\\nJiangtao Yu\\nHang Zhao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.08125\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3610873\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Aug 2023 03:31:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Proceedings of the ACMon Interactive, Mobile, Wearable and Ubiquitous Technologies (ACM IMWUT/UbiComp 2023)\\u00a7r"}']}
{title:'Du et al. (§72023§r)', author: 'Mengjie Du; Xiang Fang; Jie Li', display:{Lore:['[{"text": "arXiv:2308.08181", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChinaTelecom System Description to VoxCeleb Speaker Recognition Challenge 2023\\u00a7r\\n\\n\\u00a78\\u00a7oMengjie Du\\nXiang Fang\\nJie Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.08181\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Aug 2023 07:21:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSystem description of VoxSRC 2023\\u00a7r"}']}
{title:'Soleymanpour et al. (§72023§r)', author: 'Mohammad Soleymanpour; Michael T. Johnson; Rahim Soleymanpour; Jeffrey Berry', display:{Lore:['[{"text": "arXiv:2308.08438", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAccurate synthesis of Dysarthric Speech for ASR data augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Soleymanpour\\nMichael T. Johnson\\nRahim Soleymanpour\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.08438\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Aug 2023 15:42:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2201.11571\\u00a7r"}']}
{title:'Viswanath et al. (§72023§r)', author: 'Hrishikesh Viswanath; Aneesh Bhattacharya; Pascal Jutras-Dubé; Prerit Gupta; Mridu Prashanth; Yashvardhan Khaitan; Aniket Bera', display:{Lore:['[{"text": "arXiv:2308.08577", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAffectEcho: Speaker Independent and Language-Agnostic Emotion and Affect Transfer for Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oHrishikesh Viswanath\\nAneesh Bhattacharya\\nPascal Jutras-Dub\\u00e9\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.08577\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Aug 2023 06:28:29 GMT)\\u00a7r"}']}
{title:'Ai et al. (§72023§r)', author: 'Yang Ai; Ye-Xin Lu; Zhen-Hua Ling', display:{Lore:['[{"text": "arXiv:2308.08850", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLong-frame-shift Neural Speech Phase Prediction with Spectral Continuity Enhancement and Interpolation Error Compensation\\u00a7r\\n\\n\\u00a78\\u00a7oYang Ai\\nYe-Xin Lu\\nZhen-Hua Ling\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.08850\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Aug 2023 08:21:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at IEEESignal Processing Letters\\u00a7r"}']}
{title:'Wilkins et al. (§72023§r)', author: 'Julia Wilkins; Justin Salamon; Magdalena Fuentes; Juan Pablo Bello; Oriol Nieto', display:{Lore:['[{"text": "arXiv:2308.09089", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBridging High-Quality Audio and Video via Language for Sound Effects Retrieval from Visual Queries\\u00a7r\\n\\n\\u00a78\\u00a7oJulia Wilkins\\nJustin Salamon\\nMagdalena Fuentes\\nJuan Pablo Bello\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.09089\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Aug 2023 16:38:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWASPAA 2023. Project page: https://juliawilkins.github.io/sound-effects-retrieval-from-video/. 4 pages, 2 figures, 2 tables\\u00a7r"}']}
{title:'Wen et al. (§72023§r)', author: 'Penghui Wen; Kun Hu; Wenxi Yue; Sen Zhang; Wanlei Zhou; Zhiyong Wang', display:{Lore:['[{"text": "arXiv:2308.09302", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Audio Anti-Spoofing with Fusion-Reconstruction Learning on Multi-Order Spectrograms\\u00a7r\\n\\n\\u00a78\\u00a7oPenghui Wen\\nKun Hu\\nWenxi Yue\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.09302\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Aug 2023 04:51:15 GMT)\\u00a7r"}']}
{title:'Bjare et al. (§72023§r)', author: 'Mathias Rose Bjare; Stefan Lattner; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2308.09454", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Sampling Techniques for Generating Melodies with a Transformer Language Model\\u00a7r\\n\\n\\u00a78\\u00a7oMathias Rose Bjare\\nStefan Lattner\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.09454\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Aug 2023 10:34:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 5 figures, 1 table, accepted at the 24th Int. Society for Music Information Retrieval Conf., Milan, Italy, 2023\\u00a7r"}']}
{title:'Sarabia et al. (§72023§r)', author: 'Miguel Sarabia; Elena Menyaylenko; Alessandro Toso; Skyler Seto; Zakaria Aldeneh; Shadi Pirhosseinloo; Luca Zappella; Barry-John Theobald; Nicholas Apostoloff; Jonathan Sheaffer', display:{Lore:['[{"text": "arXiv:2308.09514", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning\\u00a7r\\n\\n\\u00a78\\u00a7oMiguel Sarabia\\nElena Menyaylenko\\nAlessandro Toso\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.09514\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-2117\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of INTERSPEECH (2023), pp. 3724-3728\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Aug 2023 12:45:32 GMT)\\u00a7r"}']}
{title:'Fan et al. (§72023§r)', author: 'Cunhang Fan; Jun Xue; Jianhua Tao; Jiangyan Yi; Chenglong Wang; Chengshi Zheng; Zhao Lv', display:{Lore:['[{"text": "arXiv:2308.09944", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatial Reconstructed Local Attention Res2Net with F0 Subband for Fake Speech Detection\\u00a7r\\n\\n\\u00a78\\u00a7oCunhang Fan\\nJun Xue\\nJianhua Tao\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.09944\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 19 Aug 2023 08:31:04 GMT)\\u00a7r"}']}
{title:'Verma (§72023§r)', author: 'Prateek Verma', display:{Lore:['[{"text": "arXiv:2308.10388", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Architectures Learning Fourier Transforms, Signal Processing and Much More....\\u00a7r\\n\\n\\u00a78\\u00a7oPrateek Verma\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.10388\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 20 Aug 2023 23:30:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 6 figures. Technical Report at Stanford University; Presented on 14th August 2023\\u00a7r"}']}
{title:'Erdogan et al. (§72023§r)', author: 'Hakan Erdogan; Scott Wisdom; Xuankai Chang; Zalán Borsos; Marco Tagliasacchi; Neil Zeghidour; John R. Hershey', display:{Lore:['[{"text": "arXiv:2308.10415", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHakan Erdogan\\nScott Wisdom\\nXuankai Chang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.10415\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Aug 2023 01:52:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023, project webpage with audio demos at https://google-research.github.io/sound-separation/papers/tokensplit\\u00a7r"}']}
{title:'Pan et al. (§72023§r)', author: 'Chao Pan; Lei Zhang; Yilong Lu; Jilu Jin; Lin Qiu; Jingdong Chen; Jacob Benesty', display:{Lore:['[{"text": "arXiv:2308.10543", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Anchor-Point Based Image-Model for Room Impulse Response Simulation with Directional Source Radiation and Sensor Directivity Patterns\\u00a7r\\n\\n\\u00a78\\u00a7oChao Pan\\nLei Zhang\\nYilong Lu\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.10543\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Aug 2023 07:54:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o19 pages, 8 figures\\u00a7r"}']}
{title:'Schmalenstroeer et al. (§72023§r)', author: 'Joerg Schmalenstroeer; Tobias Gburrek; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2308.10682", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLibriWASN: A Data Set for Meeting Separation, Diarization, and Recognition with Asynchronous Recording Devices\\u00a7r\\n\\n\\u00a78\\u00a7oJoerg Schmalenstroeer\\nTobias Gburrek\\nReinhold Haeb-Umbach\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.10682\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Aug 2023 12:33:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for presentation at the ITG conference on Speech Communication 2023\\u00a7r"}']}
{title:'Deng et al. (§72023§r)', author: 'Yimin Deng; Huaizhen Tang; Xulong Zhang; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2308.11084", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPMVC: Data Augmentation-Based Prosody Modeling for Expressive Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oYimin Deng\\nHuaizhen Tang\\nXulong Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.11084\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3581783.3613800\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Aug 2023 23:37:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the 31st ACM International Conference on Multimedia (MM2023)\\u00a7r"}']}
{title:'Kawano et al. (§72023§r)', author: 'Harunori Kawano; Sota Shimizu', display:{Lore:['[{"text": "arXiv:2308.11241", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification\\u00a7r\\n\\n\\u00a78\\u00a7oHarunori Kawano\\nSota Shimizu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.11241\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 10 Sep 2023 17:43:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Shansong Liu; Atin Sakkeer Hussain; Chenshuo Sun; Ying Shan', display:{Lore:['[{"text": "arXiv:2308.11276", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning\\u00a7r\\n\\n\\u00a78\\u00a7oShansong Liu\\nAtin Sakkeer Hussain\\nChenshuo Sun\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.11276\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Aug 2023 08:43:33 GMT)\\u00a7r"}']}
{title:'Diehl et al. (§72023§r)', author: 'Peter Udo Diehl; Hannes Zilly; Felix Sattler; Yosef Singer; Kevin Kepp; Mark Berry; Henning Hasemann; Marlene Zippel; Müge Kaya; Paul Meyer-Rachner; Annett Pudszuhn; Veit M. Hofmann; Matthias Vormann; Elias Sprengel', display:{Lore:['[{"text": "arXiv:2308.11456", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep learning-based denoising streamed from mobile phones improves speech-in-noise understanding for hearing aid users\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Udo Diehl\\nHannes Zilly\\nFelix Sattler\\n+ 10 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.11456\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Aug 2023 14:05:43 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Hualei Wang; Jianguo Mao; Zhifang Guo; Jiarui Wan; Hong Liu; Xiangdong Wang', display:{Lore:['[{"text": "arXiv:2308.11530", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFurnishing Sound Event Detection with Language Model Abilities\\u00a7r\\n\\n\\u00a78\\u00a7oHualei Wang\\nJianguo Mao\\nZhifang Guo\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.11530\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Aug 2023 15:59:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages,2 figures,published to AAAI\\u00a7r"}']}
{title:'Müller et al. (§72023§r)', author: 'Nicolas M. Müller; Philip Sperl; Konstantin Böttinger', display:{Lore:['[{"text": "arXiv:2308.11800", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComplex-valued neural networks for voice anti-spoofing\\u00a7r\\n\\n\\u00a78\\u00a7oNicolas M. M\\u00fcller\\nPhilip Sperl\\nKonstantin B\\u00f6ttinger\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.11800\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Aug 2023 21:49:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023\\u00a7r"}']}
{title:'Guo et al. (§72023§r)', author: 'Zhifang Guo; Jianguo Mao; Rui Tao; Long Yan; Kazushige Ouchi; Hong Liu; Xiangdong Wang', display:{Lore:['[{"text": "arXiv:2308.11940", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Generation with Multiple Conditional Diffusion Model\\u00a7r\\n\\n\\u00a78\\u00a7oZhifang Guo\\nJianguo Mao\\nRui Tao\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.11940\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 28 Dec 2023 10:59:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by AAAI 2024\\u00a7r"}']}
{title:'Dinkel et al. (§72023§r)', author: 'Heinrich Dinkel; Yongqing Wang; Zhiyong Yan; Junbo Zhang; Yujun Wang', display:{Lore:['[{"text": "arXiv:2308.11957", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCED: Consistent ensemble distillation for audio tagging\\u00a7r\\n\\n\\u00a78\\u00a7oHeinrich Dinkel\\nYongqing Wang\\nZhiyong Yan\\nJunbo Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.11957\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 8 Sep 2023 02:31:24 GMT)\\u00a7r"}']}
{title:"D'Hooge et al. (§72023§r)", author: "Alexandre D'Hooge; Louis Bigo; Ken Déguernel", display:{Lore:['[{"text": "arXiv:2308.12307", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModeling Bends in Popular Music Guitar Tablatures\\u00a7r\\n\\n\\u00a78\\u00a7oAlexandre D\'Hooge\\nLouis Bigo\\nKen D\\u00e9guernel\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.12307\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n24th International Society for Music Information Retrieval\\n  Conference, International Society for Music Information Retrieval, Nov 2023,\\n  Milan, Italy\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Aug 2023 07:50:58 GMT)\\u00a7r"}']}
{title:'Martel et al. (§72023§r)', author: 'Matthew Martel; Jackson Wagner', display:{Lore:['[{"text": "arXiv:2308.12408", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Initial Exploration: Learning to Generate Realistic Audio for Silent Video\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew Martel\\nJackson Wagner\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.12408\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 23 Aug 2023 20:08:56 GMT)\\u00a7r"}']}
{title:'Xu et al. (§72023§r)', author: 'Xiao Xu; Yang Wang; Xinru Wei; Fei Wang; Xizhe Zhang', display:{Lore:['[{"text": "arXiv:2308.12478", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention-Based Acoustic Feature Fusion Network for Depression Detection\\u00a7r\\n\\n\\u00a78\\u00a7oXiao Xu\\nYang Wang\\nXinru Wei\\nFei Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.12478\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Aug 2023 00:31:51 GMT)\\u00a7r"}']}
{title:'Chae et al. (§72023§r)', author: 'Yunkee Chae; Junghyun Koo; Sungho Lee; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2308.12599", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting Time-Frequency Conformers for Music Audio Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oYunkee Chae\\nJunghyun Koo\\nSungho Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.12599\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Aug 2023 06:56:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ACM Multimedia 2023\\u00a7r"}']}
{title:'Ho et al. (§72023§r)', author: 'Kuan-Hsun Ho; En-Lun Yu; Jeih-weih Hung; Berlin Chen', display:{Lore:['[{"text": "arXiv:2308.12615", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNaaloss: Rethinking the objective of speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oKuan-Hsun Ho\\nEn-Lun Yu\\nJeih-weih Hung\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.12615\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Aug 2023 07:29:31 GMT)\\u00a7r"}']}
{title:'Balvanera et al. (§72023§r)', author: 'Santiago Martinez Balvanera; Oisin Mac Aodha; Matthew J. Weldy; Holly Pringle; Ella Browning; Kate E. Jones', display:{Lore:['[{"text": "arXiv:2308.12688", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhombat: An open-source annotation tool for machine learning development in bioacoustics\\u00a7r\\n\\n\\u00a78\\u00a7oSantiago Martinez Balvanera\\nOisin Mac Aodha\\nMatthew J. Weldy\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.12688\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 7 Nov 2023 13:40:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o17 pages, 2 figures, 2 tables, to be submitted to Methods in Ecology and Evolution\\u00a7r"}']}
{title:'Bird et al. (§72023§r)', author: 'Jordan J. Bird; Ahmad Lotfi', display:{Lore:['[{"text": "arXiv:2308.12734", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-time Detection of AI-Generated Speech for DeepFake Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oJordan J. Bird\\nAhmad Lotfi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.12734\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Aug 2023 12:26:15 GMT)\\u00a7r"}']}
{title:'Latif et al. (§72023§r)', author: 'Siddique Latif; Moazzam Shoukat; Fahad Shamshad; Muhammad Usama; Yi Ren; Heriberto Cuayáhuitl; Wenwu Wang; Xulong Zhang; Roberto Togneri; Erik Cambria; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2308.12792", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSparks of Large Audio Models: A Survey and Outlook\\u00a7r\\n\\n\\u00a78\\u00a7oSiddique Latif\\nMoazzam Shoukat\\nFahad Shamshad\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.12792\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 22 Sep 2023 01:44:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review, Repo URL: https://github.com/EmulationAI/awesome-large-audio-models\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Yuheng Wang; Juan Ye; David L. Borchers', display:{Lore:['[{"text": "arXiv:2308.12859", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ME\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Automated Animal Density Estimation with Acoustic Spatial Capture-Recapture\\u00a7r\\n\\n\\u00a78\\u00a7oYuheng Wang\\nJuan Ye\\nDavid L. Borchers\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.12859\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Aug 2023 15:29:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o35 pages, 5 figures\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Yueyue Zhu; Jared Baca; Banafsheh Rekabdar; Reza Rawassizadeh', display:{Lore:['[{"text": "arXiv:2308.12982", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Survey of AI Music Generation Tools and Models\\u00a7r\\n\\n\\u00a78\\u00a7oYueyue Zhu\\nJared Baca\\nBanafsheh Rekabdar\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.12982\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Aug 2023 00:49:08 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Wenbin Wang; Yang Song; Sanjay Jha', display:{Lore:['[{"text": "arXiv:2308.13007", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeneralizable Zero-Shot Speaker Adaptive Speech Synthesis with Disentangled Representations\\u00a7r\\n\\n\\u00a78\\u00a7oWenbin Wang\\nYang Song\\nSanjay Jha\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.13007\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Aug 2023 18:13:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures. Accepted by Interspeech 2023, Oral\\u00a7r"}']}
{title:'Mohaimenuzzaman et al. (§72023§r)', author: 'Md Mohaimenuzzaman; Christoph Bergmeir; Bernd Meyer', display:{Lore:['[{"text": "arXiv:2308.13201", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Active Audio Feature Learning in Resource-Constrained Environments\\u00a7r\\n\\n\\u00a78\\u00a7oMd Mohaimenuzzaman\\nChristoph Bergmeir\\nBernd Meyer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.13201\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Aug 2023 06:45:02 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Xuyuan Li; Zengqiang Shang; Jian Liu; Hua Hua; Peiyang Shi; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2308.13365", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExpressive paragraph text-to-speech synthesis with multi-step variational autoencoder\\u00a7r\\n\\n\\u00a78\\u00a7oXuyuan Li\\nZengqiang Shang\\nJian Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.13365\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 2 Sep 2023 06:45:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, 2 tables\\u00a7r"}']}
{title:'Xiong et al. (§72023§r)', author: 'Zeyu Xiong; Weitao Wang; Jing Yu; Yue Lin; Ziyan Wang', display:{Lore:['[{"text": "arXiv:2308.13736", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comprehensive Survey for Evaluation Methodologies of AI-Generated Music\\u00a7r\\n\\n\\u00a78\\u00a7oZeyu Xiong\\nWeitao Wang\\nJing Yu\\nYue Lin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.13736\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 26 Aug 2023 02:44:33 GMT)\\u00a7r"}']}
{title:'Castillo et al. (§72023§r)', author: 'Margareth Castillo; Felipe Rubio; Dagoberto Porras; Sonia H. Contreras-Ortiz; Alexander Sepúlveda', display:{Lore:['[{"text": "arXiv:2308.13941", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA small vocabulary database of ultrasound image sequences of vocal tract dynamics\\u00a7r\\n\\n\\u00a78\\u00a7oMargareth Castillo\\nFelipe Rubio\\nDagoberto Porras\\nSonia H. Contreras-Ortiz\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.13941\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/STSIVA.2019.8730224\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nSTSIVA-2019, Bucaramanga, Colombia, 2019\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 14 Oct 2023 14:24:23 GMT)\\u00a7r"}']}
{title:'Lin et al. (§72023§r)', author: 'Guang Lin; Jianhai Zhang', display:{Lore:['[{"text": "arXiv:2308.14059", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Subdomain Adversarial Network for Cross-Subject EEG-based Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGuang Lin\\nJianhai Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.14059\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 27 Aug 2023 09:57:46 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Hejing Zhang; Jian Guan; Qiaoxi Zhu; Feiyang Xiao; Youde Liu', display:{Lore:['[{"text": "arXiv:2308.14063", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnomalous Sound Detection Using Self-Attention-Based Frequency Pattern Analysis of Machine Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oHejing Zhang\\nJian Guan\\nQiaoxi Zhu\\nFeiyang Xiao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.14063\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Sep 2023 08:42:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in INTERSPEECH 2023\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Kexin Zhu; Xulong Zhang; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2308.14317", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSymbolic     Acoustic: Multi-domain Music Emotion Modeling for Instrumental Music\\u00a7r\\n\\n\\u00a78\\u00a7oKexin Zhu\\nXulong Zhang\\nJianzong Wang\\nNing Cheng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.14317\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Aug 2023 05:47:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by 19th International Conference on Advanced Data Mining and Applications. (ADMA 2023)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Xulong Zhang; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2308.14319", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Conversion with Denoising Diffusion Probabilistic GAN Models\\u00a7r\\n\\n\\u00a78\\u00a7oXulong Zhang\\nJianzong Wang\\nNing Cheng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.14319\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Aug 2023 05:53:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by 19th International Conference on Advanced Data Mining and Applications. (ADMA 2023)\\u00a7r"}']}
{title:'Han et al. (§72023§r)', author: 'Bing Han; Junyu Dai; Weituo Hao; Xinyan He; Dong Guo; Jitong Chen; Yuxuan Wang; Yanmin Qian; Xuchen Song', display:{Lore:['[{"text": "arXiv:2308.14360", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInstructME: An Instruction Guided Music Edit And Remix Framework with Latent Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oBing Han\\nJunyu Dai\\nWeituo Hao\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.14360\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 12 Dec 2023 06:55:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDemo samples are available at https://musicedit.github.io/\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Yong Wang; Cheng Lu; Yuan Zong; Hailun Lian; Yan Zhao; Sunan Li', display:{Lore:['[{"text": "arXiv:2308.14568", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTime-Frequency Transformer: A Novel Time Frequency Joint Learning Method for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYong Wang\\nCheng Lu\\nYuan Zong\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.14568\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Aug 2023 13:34:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by International Conference on Neural Information Processing(ICONIP2023)\\u00a7r"}']}
{title:'Yoon et al. (§72023§r)', author: 'Hyungchan Yoon; Changhwan Kim; Eunwoo Song; Hyun-Wook Yoon; Hong-Goo Kang', display:{Lore:['[{"text": "arXiv:2308.14909", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPruning Self-Attention for Zero-Shot Multi-Speaker Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oHyungchan Yoon\\nChanghwan Kim\\nEunwoo Song\\nHyun-Wook Yoon\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.14909\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1301\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. INTERSPEECH 2023, 4299-4303\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Aug 2023 21:25:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023\\u00a7r"}']}
{title:'Yi et al. (§72023§r)', author: 'Jiangyan Yi; Chenglong Wang; Jianhua Tao; Xiaohui Zhang; Chu Yuan Zhang; Yan Zhao', display:{Lore:['[{"text": "arXiv:2308.14970", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Deepfake Detection: A Survey\\u00a7r\\n\\n\\u00a78\\u00a7oJiangyan Yi\\nChenglong Wang\\nJianhua Tao\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.14970\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Aug 2023 01:50:01 GMT)\\u00a7r"}']}
{title:'Hayes et al. (§72023§r)', author: 'Ben Hayes; Jordie Shier; György Fazekas; Andrew McPherson; Charalampos Saitis', display:{Lore:['[{"text": "arXiv:2308.15422", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Review of Differentiable Digital Signal Processing for Music     Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oBen Hayes\\nJordie Shier\\nGy\\u00f6rgy Fazekas\\nAndrew McPherson\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.15422\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Aug 2023 16:29:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review for Frontiers in Signal Processing\\u00a7r"}']}
{title:'Che et al. (§72023§r)', author: 'Nan Che; Chenrui Liu; Fei Yu', display:{Lore:['[{"text": "arXiv:2308.15726", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAGS: An Dataset and Taxonomy for Domestic Scene Sound Event Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oNan Che\\nChenrui Liu\\nFei Yu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.15726\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Aug 2023 03:03:47 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Yi Liu; Yuekang Li; Gelei Deng; Felix Juefei-Xu; Yao Du; Cen Zhang; Chengwei Liu; Yeting Li; Lei Ma; Yang Liu', display:{Lore:['[{"text": "arXiv:2308.15742", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lASTER: Automatic Speech Recognition System Accessibility Testing for Stutterers\\u00a7r\\n\\n\\u00a78\\u00a7oYi Liu\\nYuekang Li\\nGelei Deng\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.15742\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Aug 2023 03:46:52 GMT)\\u00a7r"}']}
{title:'Guo et al. (§72023§r)', author: 'Aoqi Guo; Sichong Qian; Baoxiang Li; Dazhi Gao', display:{Lore:['[{"text": "arXiv:2308.15990", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDual-path Transformer Based Neural Beamformer for Target Speech Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oAoqi Guo\\nSichong Qian\\nBaoxiang Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.15990\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 7 Sep 2023 06:50:18 GMT)\\u00a7r"}']}
{title:'Meng et al. (§72023§r)', author: 'Yi Meng; Xiang Li; Zhiyong Wu; Tingtian Li; Zixun Sun; Xinyu Xiao; Chi Sun; Hui Zhan; Helen Meng', display:{Lore:['[{"text": "arXiv:2308.16021", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCALM: Contrastive Cross-modal Speaking Style Modeling for Expressive Text-to-Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYi Meng\\nXiang Li\\nZhiyong Wu\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.16021\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Aug 2023 13:21:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by InterSpeech 2022\\u00a7r"}']}
{title:'Rice et al. (§72023§r)', author: 'Matthew Rice; Christian J. Steinmetz; George Fazekas; Joshua D. Reiss', display:{Lore:['[{"text": "arXiv:2308.16177", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeneral Purpose Audio Effect Removal\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew Rice\\nChristian J. Steinmetz\\nGeorge Fazekas\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.16177\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Aug 2023 17:55:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint. Accepted to IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)2023\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Haven Kim; Keunwoo Choi; Mateusz Modrzejewski; Cynthia C. S. Liem', display:{Lore:['[{"text": "arXiv:2308.16389", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CY\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Biased Journey of MSD_AUDIO.ZIP\\u00a7r\\n\\n\\u00a78\\u00a7oHaven Kim\\nKeunwoo Choi\\nMateusz Modrzejewski\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.16389\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 2 Dec 2023 02:01:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oLate-breaking/Demo ISMIR 2023\\u00a7r"}']}
{title:'Narasinh et al. (§72023§r)', author: 'Vishwaas Narasinh; Senthil Raja G', display:{Lore:['[{"text": "arXiv:2308.16421", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSequential Pitch Distributions for Raga Detection\\u00a7r\\n\\n\\u00a78\\u00a7oVishwaas Narasinh\\nSenthil Raja G\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.16421\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nAIMC 2023. Retrieved from https://aimc2023.pubpub.org/pub/j9v30p0j\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Aug 2023 03:15:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages, 6 figures, AI Music Creativity\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Jie Chen; Xingchen Song; Zhendong Peng; Binbin Zhang; Fuping Pan; Zhiyong Wu', display:{Lore:['[{"text": "arXiv:2308.16569", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLightGrad: Lightweight Diffusion Probabilistic Model for Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oJie Chen\\nXingchen Song\\nZhendong Peng\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.16569\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10096710\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Aug 2023 09:05:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Jie Chen; Changhe Song; Deyi Tuo; Xixin Wu; Shiyin Kang; Zhiyong Wu; Helen Meng', display:{Lore:['[{"text": "arXiv:2308.16577", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Mandarin Prosodic Structure Prediction with Multi-level Contextual Information\\u00a7r\\n\\n\\u00a78\\u00a7oJie Chen\\nChanghe Song\\nDeyi Tuo\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.16577\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-131\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Aug 2023 09:19:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech2022\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Weiqin Li; Shun Lei; Qiaochu Huang; Yixuan Zhou; Zhiyong Wu; Shiyin Kang; Helen Meng', display:{Lore:['[{"text": "arXiv:2308.16593", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Spontaneous Style Modeling with Semi-supervised Pre-training for Conversational Text-to-Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oWeiqin Li\\nShun Lei\\nQiaochu Huang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.16593\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Aug 2023 09:50:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Miccini et al. (§72023§r)', author: 'Riccardo Miccini; Alaa Zniber; Clément Laroche; Tobias Piechowiak; Martin Schoeberl; Luca Pezzarossa; Ouassim Karrakchou; Jens Sparsø; Mounir Ghogho', display:{Lore:['[{"text": "arXiv:2308.16678", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDynamic nsNet2: Efficient Deep Noise Suppression with Early Exiting\\u00a7r\\n\\n\\u00a78\\u00a7oRiccardo Miccini\\nAlaa Zniber\\nCl\\u00e9ment Laroche\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.16678\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/MLSP55844.2023.10285925\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Aug 2023 12:29:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the MLSP 2023\\u00a7r"}']}
{title:'Zhou et al. (§72023§r)', author: 'Shaohuan Zhou; Shun Lei; Weiya You; Deyi Tuo; Yuren You; Zhiyong Wu; Shiyin Kang; Helen Meng', display:{Lore:['[{"text": "arXiv:2308.16836", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Improving the Expressiveness of Singing Voice Synthesis with BERT Derived Semantic Information\\u00a7r\\n\\n\\u00a78\\u00a7oShaohuan Zhou\\nShun Lei\\nWeiya You\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.16836\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Aug 2023 16:12:01 GMT)\\u00a7r"}']}
{title:'Guo et al. (§72023§r)', author: 'Haohan Guo; Fenglong Xie; Jiawen Kang; Yujia Xiao; Xixin Wu; Helen Meng', display:{Lore:['[{"text": "arXiv:2309.00126", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQS-TTS: Towards Semi-Supervised Text-to-Speech Synthesis via Vector-Quantized Self-Supervised Speech Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oHaohan Guo\\nFenglong Xie\\nJiawen Kang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.00126\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Aug 2023 20:25:44 GMT)\\u00a7r"}']}
{title:'Bittar et al. (§72023§r)', author: 'Alexandre Bittar; Paul Dixon; Mohammad Samragh; Kumari Nishu; Devang Naik', display:{Lore:['[{"text": "arXiv:2309.00140", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving vision-inspired keyword spotting using dynamic module skipping in streaming conformer encoder\\u00a7r\\n\\n\\u00a78\\u00a7oAlexandre Bittar\\nPaul Dixon\\nMohammad Samragh\\nKumari Nishu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.00140\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP48485.2024.10447485\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2024 - 2024 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Aug 2023 21:25:57 GMT)\\u00a7r"}']}
{title:'Zhou et al. (§72023§r)', author: 'Shaohuan Zhou; Xu Li; Zhiyong Wu; Ying Shan; Helen Meng', display:{Lore:['[{"text": "arXiv:2309.00284", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing the vocal range of single-speaker singing voice synthesis with melody-unsupervised pre-training\\u00a7r\\n\\n\\u00a78\\u00a7oShaohuan Zhou\\nXu Li\\nZhiyong Wu\\nYing Shan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.00284\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Sep 2023 06:40:41 GMT)\\u00a7r"}']}
{title:'Wojnar et al. (§72023§r)', author: 'Tomasz Wojnar; Jaroslaw Hryszko; Adam Roman', display:{Lore:['[{"text": "arXiv:2309.00329", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMi-Go: Test Framework which uses YouTube as Data Source for Evaluating Speech Recognition Models like OpenAI\'s Whisper\\u00a7r\\n\\n\\u00a78\\u00a7oTomasz Wojnar\\nJaroslaw Hryszko\\nAdam Roman\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.00329\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Sep 2023 08:31:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o25 pages, 9 tables, 3 figures\\u00a7r"}']}
{title:'Labbé et al. (§72023§r)', author: 'Étienne Labbé; Thomas Pellegrini; Julien Pinquier', display:{Lore:['[{"text": "arXiv:2309.00454", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCoNeTTE: An efficient Audio Captioning system leveraging multiple datasets with Task Embedding\\u00a7r\\n\\n\\u00a78\\u00a7o\\u00c9tienne Labb\\u00e9\\nThomas Pellegrini\\nJulien Pinquier\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.00454\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Sep 2023 13:35:44 GMT)\\u00a7r"}']}
{title:'Moummad et al. (§72023§r)', author: 'Ilyass Moummad; Romain Serizel; Nicolas Farrugia', display:{Lore:['[{"text": "arXiv:2309.00878", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPretraining Representations for Bioacoustic Few-shot Detection using Supervised Contrastive Learning\\u00a7r\\n\\n\\u00a78\\u00a7oIlyass Moummad\\nRomain Serizel\\nNicolas Farrugia\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.00878\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Sep 2023 09:38:55 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Tao Li; Chenxu Hu; Jian Cong; Xinfa Zhu; Jingbei Li; Qiao Tian; Yuping Wang; Lei Xie', display:{Lore:['[{"text": "arXiv:2309.00883", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiCLET-TTS: Diffusion Model based Cross-lingual Emotion Transfer for Text-to-Speech \\u2013 A Study between English and Mandarin\\u00a7r\\n\\n\\u00a78\\u00a7oTao Li\\nChenxu Hu\\nJian Cong\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.00883\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Sep 2023 09:48:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by TASLP\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Qing Wang; Jixun Yao; Li Zhang; Pengcheng Guo; Lei Xie', display:{Lore:['[{"text": "arXiv:2309.00929", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTimbre-reserved Adversarial Attack in Speaker Identification\\u00a7r\\n\\n\\u00a78\\u00a7oQing Wang\\nJixun Yao\\nLi Zhang\\nPengcheng Guo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.00929\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Sep 2023 12:42:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 8 figures\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Wen Wang; Dongchao Yang; Qichen Ye; Bowen Cao; Yuexian Zou', display:{Lore:['[{"text": "arXiv:2309.01212", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNADiffuSE: Noise-aware Diffusion-based Model for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oWen Wang\\nDongchao Yang\\nQichen Ye\\nBowen Cao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.01212\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 3 Sep 2023 16:33:49 GMT)\\u00a7r"}']}
{title:'Zhou et al. (§72023§r)', author: 'Zixiang Zhou; Weiyuan Li; Baoyuan Wang', display:{Lore:['[{"text": "arXiv:2309.01340", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMDSC: Towards Evaluating the Style Consistency Between Music and Dance\\u00a7r\\n\\n\\u00a78\\u00a7oZixiang Zhou\\nWeiyuan Li\\nBaoyuan Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.01340\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 30 Nov 2023 03:12:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o19 pages, 19 figure\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Jiaxu Zhu; Changhe Song; Zhiyong Wu; Helen Meng', display:{Lore:['[{"text": "arXiv:2309.01437", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSememeASR: Boosting Performance of End-to-End Speech Recognition against Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge\\u00a7r\\n\\n\\u00a78\\u00a7oJiaxu Zhu\\nChanghe Song\\nZhiyong Wu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.01437\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1432\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 7 Oct 2023 04:30:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of Interspeech\\u00a7r"}']}
{title:'Ren et al. (§72023§r)', author: 'Ying Ren; Kailai Shen; Zhe Ye; Diqun Yan', display:{Lore:['[{"text": "arXiv:2309.01480", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBadSQA: Stealthy Backdoor Attacks Using Presence Events as Triggers in Non-Intrusive Speech Quality Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oYing Ren\\nKailai Shen\\nZhe Ye\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.01480\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Sep 2023 09:35:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 6 figures,conference\\u00a7r"}']}
{title:'Huang et al. (§72023§r)', author: 'Wen-Chin Huang; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2309.02133", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluating Methods for Ground-Truth-Free Foreign Accent Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oWen-Chin Huang\\nTomoki Toda\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.02133\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Sep 2023 11:22:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the 2023 Asia Pacific Signal and Information Processing AssociationAnnual Summit and Conference (APSIPA ASC). Demo page: https://unilight.github.io/Publication-Demos/publications/fac-evaluate. Code: https://github.co"}','{"text": "m/unilight/seq2seq-vc\\u00a7r"}']}
{title:'Xie et al. (§72023§r)', author: 'Yuankun Xie; Jingjing Zhou; Xiaolin Lu; Zhenghao Jiang; Yuxin Yang; Haonan Cheng; Long Ye', display:{Lore:['[{"text": "arXiv:2309.02232", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFSD: An Initial Chinese Dataset for Fake Song Detection\\u00a7r\\n\\n\\u00a78\\u00a7oYuankun Xie\\nJingjing Zhou\\nXiaolin Lu\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.02232\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Sep 2023 11:13:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Peeters (§72023§r)', author: 'Geoffroy Peeters', display:{Lore:['[{"text": "arXiv:2309.02243", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Similarity-Based and Novelty-based loss for music structure analysis\\u00a7r\\n\\n\\u00a78\\u00a7oGeoffroy Peeters\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.02243\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Sep 2023 13:49:29 GMT)\\u00a7r"}']}
{title:'Hu et al. (§72023§r)', author: 'Patricia Hu; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2309.02399", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.DL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Batik-plays-Mozart Corpus: Linking Performance to Score to Musicological Annotations\\u00a7r\\n\\n\\u00a78\\u00a7oPatricia Hu\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.02399\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Sep 2023 09:34:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in the Proceedings of the 24th International Society for Music Information Retrieval Conference (ISMIR 2023), Milan, Italy\\u00a7r"}']}
{title:'Pani et al. (§72023§r)', author: 'Sushanta K. Pani; Anurag Chowdhury; Morgan Sandler; Arun Ross', display:{Lore:['[{"text": "arXiv:2309.02404", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Morphing: Two Identities in One Voice\\u00a7r\\n\\n\\u00a78\\u00a7oSushanta K. Pani\\nAnurag Chowdhury\\nMorgan Sandler\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.02404\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Sep 2023 17:36:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted oral paper at BIOSIG 2023\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Jiaxu Zhu; Weinan Tong; Yaoxun Xu; Changhe Song; Zhiyong Wu; Zhao You; Dan Su; Dong Yu; Helen Meng', display:{Lore:['[{"text": "arXiv:2309.02459", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lText-Only Domain Adaptation for End-to-End Speech Recognition through Down-Sampling Acoustic Representation\\u00a7r\\n\\n\\u00a78\\u00a7oJiaxu Zhu\\nWeinan Tong\\nYaoxun Xu\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.02459\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1378\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 7 Oct 2023 04:23:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of Interspeech. arXiv admin note: text overlapwith arXiv:2309.01437\\u00a7r"}']}
{title:'Lu et al. (§72023§r)', author: 'Wei-Tsung Lu; Ju-Chiang Wang; Qiuqiang Kong; Yun-Ning Hung', display:{Lore:['[{"text": "arXiv:2309.02612", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Source Separation with Band-Split RoPE Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oWei-Tsung Lu\\nJu-Chiang Wang\\nQiuqiang Kong\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.02612\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 10 Sep 2023 00:51:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper explains the SAMI-ByteDance MSS system submitted to Sound Demixing Challenge (SDX23) Music Separation Track.Version 2 of paper fixed some typos\\u00a7r"}']}
{title:'Kawahara et al. (§72023§r)', author: 'Hideki Kawahara; Kohei Yatabe; Ken-Ichi Sakakibara; Mitsunori Mizumachi; Tatsuya Kitamura', display:{Lore:['[{"text": "arXiv:2309.02767", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSimultaneous Measurement of Multiple Acoustic Attributes Using Structured Periodic Test Signals Including Music and Other Sound Materials\\u00a7r\\n\\n\\u00a78\\u00a7oHideki Kawahara\\nKohei Yatabe\\nKen-Ichi Sakakibara\\nMitsunori Mizumachi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.02767\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Sep 2023 06:21:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 17 figures, accepted for APSIPA ASC 2023\\u00a7r"}']}
{title:'Wu (§72023§r)', author: 'Yiming Wu', display:{Lore:['[{"text": "arXiv:2309.02796", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Disentanglement of Harmonic and Rhythmic Features in Music Audio Signals\\u00a7r\\n\\n\\u00a78\\u00a7oYiming Wu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.02796\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Sep 2023 07:30:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to DAFx 2023\\u00a7r"}']}
{title:'Xie et al. (§72023§r)', author: 'Yuankun Xie; Haonan Cheng; Yutian Wang; Long Ye', display:{Lore:['[{"text": "arXiv:2309.03036", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Efficient Temporary Deepfake Location Approach Based Embeddings for Partially Spoofed Audio Detection\\u00a7r\\n\\n\\u00a78\\u00a7oYuankun Xie\\nHaonan Cheng\\nYutian Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.03036\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 21 Nov 2023 10:09:46 GMT)\\u00a7r"}']}
{title:'Arthur et al. (§72023§r)', author: 'Claire Arthur; Frank Lehman; John McNamara', display:{Lore:['[{"text": "arXiv:2309.03298", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.SC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPresenting the SWTC: A Symbolic Corpus of Themes from John Williams\' Star Wars Episodes I-IX\\u00a7r\\n\\n\\u00a78\\u00a7oClaire Arthur\\nFrank Lehman\\nJohn McNamara\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.03298\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Sep 2023 18:21:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCorpus report (5000 words)\\u00a7r"}']}
{title:'Byun et al. (§72023§r)', author: 'Kyungguen Byun; Sunkuk Moon; Erik Visser', display:{Lore:['[{"text": "arXiv:2309.03364", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHighly Controllable Diffusion-based Any-to-Any Voice Conversion Model with Frame-level Prosody Feature\\u00a7r\\n\\n\\u00a78\\u00a7oKyungguen Byun\\nSunkuk Moon\\nErik Visser\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.03364\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Sep 2023 21:16:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, submitted to ICASSP 2024\\u00a7r"}']}
{title:'Reise et al. (§72023§r)', author: 'Wojciech Reise; Ximena Fernández; Maria Dominguez; Heather A. Harrington; Mariano Beguerisse-Díaz', display:{Lore:['[{"text": "arXiv:2309.03516", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a72math.AT\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTopological fingerprints for audio identification\\u00a7r\\n\\n\\u00a78\\u00a7oWojciech Reise\\nXimena Fern\\u00e1ndez\\nMaria Dominguez\\nHeather A. Harrington\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.03516\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Sep 2023 06:56:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o26 pages\\u00a7r"}']}
{title:'Ashhad et al. (§72023§r)', author: 'Mohd Ashhad; Omar Ahmed; Sooraj K. Ambat; Zeeshan Ali Haq; Mansaf Alam', display:{Lore:['[{"text": "arXiv:2309.03544", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMVD:A Novel Methodology and Dataset for Acoustic Vehicle Type Classification\\u00a7r\\n\\n\\u00a78\\u00a7oMohd Ashhad\\nOmar Ahmed\\nSooraj K. Ambat\\nZeeshan Ali Haq\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.03544\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Sep 2023 08:02:57 GMT)\\u00a7r"}']}
{title:'Shaharabany et al. (§72023§r)', author: 'Tal Shaharabany; Ariel Shaulov; Lior Wolf', display:{Lore:['[{"text": "arXiv:2309.03884", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero-Shot Audio Captioning via Audibility Guidance\\u00a7r\\n\\n\\u00a78\\u00a7oTal Shaharabany\\nAriel Shaulov\\nLior Wolf\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.03884\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Sep 2023 17:45:58 GMT)\\u00a7r"}']}
{title:'Walsh et al. (§72023§r)', author: 'Brendan Walsh; Mark Hamilton; Greg Newby; Xi Wang; Serena Ruan; Sheng Zhao; Lei He; Shaofei Zhang; Eric Dettinger; William T. Freeman; Markus Weimer', display:{Lore:['[{"text": "arXiv:2309.03926", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.DC\\u00a7r, \\u00a7acs.DL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLarge-Scale Automatic Audiobook Creation\\u00a7r\\n\\n\\u00a78\\u00a7oBrendan Walsh\\nMark Hamilton\\nGreg Newby\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.03926\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Sep 2023 11:41:23 GMT)\\u00a7r"}']}
{title:'Huang et al. (§72023§r)', author: 'Jiayi Huang; Zeyu Yan; Wenbin Jiang; Fei Wen', display:{Lore:['[{"text": "arXiv:2309.04132", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Two-Stage Training Framework for Joint Speech Compression and Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oJiayi Huang\\nZeyu Yan\\nWenbin Jiang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.04132\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Sep 2023 05:10:42 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yang Li; Cheng Yu; Guangzhi Sun; Weiqin Zu; Zheng Tian; Ying Wen; Wei Pan; Chao Zhang; Jun Wang; Yang Yang; Fanglei Sun', display:{Lore:['[{"text": "arXiv:2309.04156", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-Utterance Conditioned VAE for Speech Generation\\u00a7r\\n\\n\\u00a78\\u00a7oYang Li\\nCheng Yu\\nGuangzhi Sun\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.04156\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Sep 2023 06:48:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages;\\u00a7r"}']}
{title:'Xiang et al. (§72023§r)', author: 'Haoran Xiang; Junyu Dai; Xuchen Song; Furao Shen', display:{Lore:['[{"text": "arXiv:2309.04182", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Long-Tail Friendly Representation Framework for Artist and Music Similarity\\u00a7r\\n\\n\\u00a78\\u00a7oHaoran Xiang\\nJunyu Dai\\nXuchen Song\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.04182\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Sep 2023 07:53:21 GMT)\\u00a7r"}']}
{title:'Jafaryani et al. (§72023§r)', author: 'Mohamadreza Jafaryani; Hamid Sheikhzadeh; Vahid Pourahmadi', display:{Lore:['[{"text": "arXiv:2309.04420", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParallel and Limited Data Voice Conversion Using Stochastic Variational Deep Kernel Learning\\u00a7r\\n\\n\\u00a78\\u00a7oMohamadreza Jafaryani\\nHamid Sheikhzadeh\\nVahid Pourahmadi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.04420\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.engappai.2022.105279\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nEngineering Applications of Artificial Intelligence.115(2022)\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Sep 2023 16:32:47 GMT)\\u00a7r"}']}
{title:'Shati et al. (§72023§r)', author: 'Asmaa Shati; Ghulam Mubashar Hassan; Amitava Datta', display:{Lore:['[{"text": "arXiv:2309.04505", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCOVID-19 Detection System: A Comparative Analysis of System Performance Based on Acoustic Features of Cough Audio Signals\\u00a7r\\n\\n\\u00a78\\u00a7oAsmaa Shati\\nGhulam Mubashar Hassan\\nAmitava Datta\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.04505\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Sep 2023 08:33:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 3 figures\\u00a7r"}']}
{title:'Jeong et al. (§72023§r)', author: 'Yujin Jeong; Wonjeong Ryoo; Seunghyun Lee; Dabin Seo; Wonmin Byeon; Sangpil Kim; Jinkyu Kim', display:{Lore:['[{"text": "arXiv:2309.04509", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Power of Sound (TPoS): Audio Reactive Video Generation with Stable Diffusion\\u00a7r\\n\\n\\u00a78\\u00a7oYujin Jeong\\nWonjeong Ryoo\\nSeunghyun Lee\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.04509\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Sep 2023 12:21:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICCV2023\\u00a7r"}']}
{title:'Pillay et al. (§72023§r)', author: 'Ashwin Pillay; Sage Betko; Ari Liloia; Hao Chen; Ankit Shah', display:{Lore:['[{"text": "arXiv:2309.04641", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Domain-Specific Enhancements for a Neural Foley Synthesizer\\u00a7r\\n\\n\\u00a78\\u00a7oAshwin Pillay\\nSage Betko\\nAri Liloia\\nHao Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.04641\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Sep 2023 23:43:57 GMT)\\u00a7r"}']}
{title:'Zhao et al. (§72023§r)', author: 'Huaibo Zhao; Yosuke Higuchi; Yusuke Kida; Tetsuji Ogawa; Tetsunori Kobayashi', display:{Lore:['[{"text": "arXiv:2309.04654", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMask-CTC-based Encoder Pre-training for Streaming End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHuaibo Zhao\\nYosuke Higuchi\\nYusuke Kida\\nTetsuji Ogawa\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.04654\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 9 Sep 2023 01:05:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to EUSIPCO 2023\\u00a7r"}']}
{title:'Kumar et al. (§72023§r)', author: 'Teerath Kumar; Muhammad Turab; Alessandra Mileo; Malika Bendechache; Takfarinas Saber', display:{Lore:['[{"text": "arXiv:2309.04762", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudRandAug: Random Image Augmentations for Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oTeerath Kumar\\nMuhammad Turab\\nAlessandra Mileo\\nMalika Bendechache\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.04762\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 9 Sep 2023 11:25:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper has accepted at 25th Irish Machine Vision andImage Processing Conference\\u00a7r"}']}
{title:'Biswas et al. (§72023§r)', author: 'Ayan Biswas; Supriya Dhabal; Palaniandavar Venkateswaran', display:{Lore:['[{"text": "arXiv:2309.04861", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Music Genre Classification: Algorithm Analysis and Deployment Architecture\\u00a7r\\n\\n\\u00a78\\u00a7oAyan Biswas\\nSupriya Dhabal\\nPalaniandavar Venkateswaran\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.04861\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Sep 2023 06:05:04 GMT)\\u00a7r"}']}
{title:'Gan et al. (§72023§r)', author: 'Yuan Gan; Zongxin Yang; Xihang Yue; Lingyun Sun; Yi Yang', display:{Lore:['[{"text": "arXiv:2309.04946", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Emotional Adaptation for Audio-Driven Talking-Head Generation\\u00a7r\\n\\n\\u00a78\\u00a7oYuan Gan\\nZongxin Yang\\nXihang Yue\\nLingyun Sun\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.04946\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 12 Oct 2023 15:04:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICCV2023. Project page: https://yuangan.github.io/eat/\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Jaechang Kim; Jeongyeon Hwang; Soheun Yi; Jaewoong Cho; Jungseul Ok', display:{Lore:['[{"text": "arXiv:2309.05287", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAddressing Feature Imbalance in Sound Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oJaechang Kim\\nJeongyeon Hwang\\nSoheun Yi\\nJaewoong Cho\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.05287\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 4 Oct 2023 05:01:10 GMT)\\u00a7r"}']}
{title:'Jovanović et al. (§72023§r)', author: 'Andrej Jovanović; Mario Mihaly; Lennon Donaldson', display:{Lore:['[{"text": "arXiv:2309.05357", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEDAC: Efficient Deployment of Audio Classification Models For COVID-19 Detection\\u00a7r\\n\\n\\u00a78\\u00a7oAndrej Jovanovi\\u0107\\nMario Mihaly\\nLennon Donaldson\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.05357\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Sep 2023 10:07:51 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Haoxu Wang; Fan Yu; Xian Shi; Yuezhang Wang; Shiliang Zhang; Ming Li', display:{Lore:['[{"text": "arXiv:2309.05396", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSlideSpeech: A Large-Scale Slide-Enriched Audio-Visual Corpus\\u00a7r\\n\\n\\u00a78\\u00a7oHaoxu Wang\\nFan Yu\\nXian Shi\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.05396\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 25 Dec 2023 13:01:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Young (§72023§r)', author: 'Halley Young', display:{Lore:['[{"text": "arXiv:2309.05595", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUndecidability Results and Their Relevance in Modern Music Making\\u00a7r\\n\\n\\u00a78\\u00a7oHalley Young\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.05595\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Sep 2023 19:39:12 GMT)\\u00a7r"}']}
{title:'Koyama et al. (§72023§r)', author: 'Shoichi Koyama; Masaki Nakada; Juliano G. C. Ribeiro; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2309.05634", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKernel Interpolation of Incident Sound Field in Region Including Scattering Objects\\u00a7r\\n\\n\\u00a78\\u00a7oShoichi Koyama\\nMasaki Nakada\\nJuliano G. C. Ribeiro\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.05634\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Sep 2023 17:26:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEWorkshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)2023\\u00a7r"}']}
{title:'Miao et al. (§72023§r)', author: 'Xiaoxiao Miao; Xin Wang; Erica Cooper; Junichi Yamagishi; Nicholas Evans; Massimiliano Todisco; Jean-François Bonastre; Mickael Rouvier', display:{Lore:['[{"text": "arXiv:2309.06141", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynVox2: Towards a privacy-friendly VoxCeleb2 dataset\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoxiao Miao\\nXin Wang\\nErica Cooper\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.06141\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Sep 2023 11:28:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oconference\\u00a7r"}']}
{title:'Shier et al. (§72023§r)', author: 'Jordie Shier; Franco Caspe; Andrew Robertson; Mark Sandler; Charalampos Saitis; Andrew McPherson', display:{Lore:['[{"text": "arXiv:2309.06649", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDifferentiable Modelling of Percussive Audio with Transient and Spectral Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oJordie Shier\\nFranco Caspe\\nAndrew Robertson\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.06649\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Sep 2023 00:21:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in The Proceedings of Forum Acusticum, Sep 2023, Turin, Italy\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Zhengyang Chen; Bing Han; Shuai Wang; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2309.06672", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention-based Encoder-Decoder End-to-End Neural Diarization with Embedding Enhancer\\u00a7r\\n\\n\\u00a78\\u00a7oZhengyang Chen\\nBing Han\\nShuai Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.06672\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Sep 2023 02:17:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE/ACM Transactions on Audio Speech and Language Processing Under Review\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Qinghua Liu; Meng Ge; Zhizheng Wu; Haizhou Li', display:{Lore:['[{"text": "arXiv:2309.06723", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPIAVE: A Pose-Invariant Audio-Visual Speaker Extraction Network\\u00a7r\\n\\n\\u00a78\\u00a7oQinghua Liu\\nMeng Ge\\nZhizheng Wu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.06723\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-889\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. INTERSPEECH 2023, 3719-3723\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Sep 2023 04:54:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Chu Yuan Zhang; Jiangyan Yi; Jianhua Tao; Chenglong Wang; Xinrui Yan', display:{Lore:['[{"text": "arXiv:2309.06780", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDistinguishing Neural Speech Synthesis Models Through Fingerprints in Speech Waveforms\\u00a7r\\n\\n\\u00a78\\u00a7oChu Yuan Zhang\\nJiangyan Yi\\nJianhua Tao\\nChenglong Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.06780\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Sep 2023 08:06:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Zhichao Wu; Qiulin Li; Sixing Liu; Qun Yang', display:{Lore:['[{"text": "arXiv:2309.06787", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDCTTS: Discrete Diffusion Model with Contrastive Learning for Text-to-speech Generation\\u00a7r\\n\\n\\u00a78\\u00a7oZhichao Wu\\nQiulin Li\\nSixing Liu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.06787\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Sep 2023 08:22:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, submitted to ICASSP\\u00a7r"}']}
{title:'Selvakumar et al. (§72023§r)', author: 'Anith Selvakumar; Homa Fashandi', display:{Lore:['[{"text": "arXiv:2309.07115", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeakly-Supervised Multi-Task Learning for Audio-Visual Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oAnith Selvakumar\\nHoma Fashandi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07115\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Sep 2023 17:45:41 GMT)\\u00a7r"}']}
{title:'Grassucci et al. (§72023§r)', author: 'Eleonora Grassucci; Christian Marinoni; Andrea Rodriguez; Danilo Comminiello', display:{Lore:['[{"text": "arXiv:2309.07195", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.ET\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffusion models for audio semantic communication\\u00a7r\\n\\n\\u00a78\\u00a7oEleonora Grassucci\\nChristian Marinoni\\nAndrea Rodriguez\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07195\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Sep 2023 13:54:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEEICASSP 2024\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Haohe Liu; Ke Chen; Qiao Tian; Wenwu Wang; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:2309.07314", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudioSR: Versatile Audio Super-resolution at Scale\\u00a7r\\n\\n\\u00a78\\u00a7oHaohe Liu\\nKe Chen\\nQiao Tian\\nWenwu Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07314\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Sep 2023 21:00:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review. Demo and code: https://audioldm.github.io/audiosr\\u00a7r"}']}
{title:'Du et al. (§72023§r)', author: 'Zhihao Du; Shiliang Zhang; Kai Hu; Siqi Zheng', display:{Lore:['[{"text": "arXiv:2309.07405", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFunCodec: A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec\\u00a7r\\n\\n\\u00a78\\u00a7oZhihao Du\\nShiliang Zhang\\nKai Hu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07405\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 7 Oct 2023 02:56:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, submitted to ICASSP 2024\\u00a7r"}']}
{title:'Ratnarajah et al. (§72023§r)', author: 'Anton Ratnarajah; Shi-Xiong Zhang; Dong Yu', display:{Lore:['[{"text": "arXiv:2309.07416", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lM3-AUDIODEC: Multi-channel multi-speaker multi-spatial audio codec\\u00a7r\\n\\n\\u00a78\\u00a7oAnton Ratnarajah\\nShi-Xiong Zhang\\nDong Yu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07416\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 23 Sep 2023 03:24:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMore results and source code are available at https://anton-jeran.github.io/MAD/\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Qingmu Liu; Yuhong Yang; Baifeng Li; Hongyang Chen; Weiping Tu; Song Lin', display:{Lore:['[{"text": "arXiv:2309.07419", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMandarin Lombard Flavor Classification\\u00a7r\\n\\n\\u00a78\\u00a7oQingmu Liu\\nYuhong Yang\\nBaifeng Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07419\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Sep 2023 04:24:07 GMT)\\u00a7r"}']}
{title:'Xu et al. (§72023§r)', author: 'Zhongweiyang Xu; Yong Xu; Vinay Kothapally; Heming Wang; Muqiao Yang; Dong Yu', display:{Lore:['[{"text": "arXiv:2309.07432", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatialCodec: Neural Spatial Speech Coding\\u00a7r\\n\\n\\u00a78\\u00a7oZhongweiyang Xu\\nYong Xu\\nVinay Kothapally\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07432\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Sep 2023 05:28:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper in Submission\\u00a7r"}']}
{title:'Yip et al. (§72023§r)', author: 'Jia Qi Yip; Dianwen Ng; Bin Ma; Chng Eng Siong', display:{Lore:['[{"text": "arXiv:2309.07458", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalysis of Speech Separation Performance Degradation on Emotional Speech Mixtures\\u00a7r\\n\\n\\u00a78\\u00a7oJia Qi Yip\\nDianwen Ng\\nBin Ma\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07458\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Sep 2023 06:35:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by APSIPA ASC 2023\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Yucong Zhang; Hongbin Suo; Yulong Wan; Ming Li', display:{Lore:['[{"text": "arXiv:2309.07500", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOutlier-aware Inlier Modeling and Multi-scale Scoring for Anomalous Sound Detection via Multitask Learning\\u00a7r\\n\\n\\u00a78\\u00a7oYucong Zhang\\nHongbin Suo\\nYulong Wan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07500\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-572\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Sep 2023 08:08:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at INTERSPEECH 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Yongqi Wang; Jionghao Bai; Rongjie Huang; Ruiqi Li; Zhiqing Hong; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2309.07566", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech-to-Speech Translation with Discrete-Unit-Based Style Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oYongqi Wang\\nJionghao Bai\\nRongjie Huang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07566\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Sep 2023 09:52:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure. submitted to ICASSP 2024\\u00a7r"}']}
{title:'Huang et al. (§72023§r)', author: 'Wen-Chin Huang; Kazuhiro Kobayashi; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2309.07598", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAAS-VC: On the Generalization Ability of Automatic Alignment Search based Non-autoregressive Sequence-to-sequence Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oWen-Chin Huang\\nKazuhiro Kobayashi\\nTomoki Toda\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07598\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 15 Sep 2023 10:47:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024. Demo: https://unilight.github.io/Publication-Demos/publications/aas-vc/index.html. Code: https://github.com/unilight/seq2seq-vc\\u00a7r"}']}
{title:'Cousin et al. (§72023§r)', author: 'Matéo Cousin; Étienne Labbé; Thomas Pellegrini', display:{Lore:['[{"text": "arXiv:2309.07615", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultilingual Audio Captioning using machine translated data\\u00a7r\\n\\n\\u00a78\\u00a7oMat\\u00e9o Cousin\\n\\u00c9tienne Labb\\u00e9\\nThomas Pellegrini\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07615\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Sep 2023 11:24:55 GMT)\\u00a7r"}']}
{title:'Jonason et al. (§72023§r)', author: 'Nicolas Jonason; Xin Wang; Erica Cooper; Lauri Juvela; Bob L. T. Sturm; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2309.07658", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDDSP-based Neural Waveform Synthesis of Polyphonic Guitar Performance from String-wise MIDI Input\\u00a7r\\n\\n\\u00a78\\u00a7oNicolas Jonason\\nXin Wang\\nErica Cooper\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07658\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Sep 2023 12:23:09 GMT)\\u00a7r"}']}
{title:'Beguš et al. (§72023§r)', author: 'Gašper Beguš; Thomas Lu; Alan Zhou; Peter Wu; Gopala K. Anumanchipalli', display:{Lore:['[{"text": "arXiv:2309.07861", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCiwaGAN: Articulatory information exchange\\u00a7r\\n\\n\\u00a78\\u00a7oGa\\u0161per Begu\\u0161\\nThomas Lu\\nAlan Zhou\\nPeter Wu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07861\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Sep 2023 17:10:39 GMT)\\u00a7r"}']}
{title:'Hsu et al. (§72023§r)', author: 'Conrad Hsu; Ross Greer', display:{Lore:['[{"text": "arXiv:2309.08027", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparative Assessment of Markov Models and Recurrent Neural Networks for Jazz Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oConrad Hsu\\nRoss Greer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08027\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Sep 2023 20:57:45 GMT)\\u00a7r"}']}
{title:'Meyer et al. (§72023§r)', author: 'Sarina Meyer; Xiaoxiao Miao; Ngoc Thang Vu', display:{Lore:['[{"text": "arXiv:2309.08049", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoicePAT: An Efficient Open-source Evaluation Toolkit for Voice Privacy Research\\u00a7r\\n\\n\\u00a78\\u00a7oSarina Meyer\\nXiaoxiao Miao\\nNgoc Thang Vu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08049\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/OJSP.2023.3344375\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 22 Dec 2023 00:36:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by OJSP-ICASSP 2024 https://ieeexplore.ieee.org/document/10365329\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Yiyuan Yang; Kaichen Zhou; Niki Trigoni; Andrew Markham', display:{Lore:['[{"text": "arXiv:2309.08072", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSSL-Net: A Synergistic Spectral and Learning-based Network for Efficient Bird Sound Classification\\u00a7r\\n\\n\\u00a78\\u00a7oYiyuan Yang\\nKaichen Zhou\\nNiki Trigoni\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08072\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 23 Dec 2023 23:17:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2024)\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Yi Zhu; Saurabh Powar; Tiago H. Falk', display:{Lore:['[{"text": "arXiv:2309.08099", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCharacterizing the temporal dynamics of universal speech representations for generalizable deepfake detection\\u00a7r\\n\\n\\u00a78\\u00a7oYi Zhu\\nSaurabh Powar\\nTiago H. Falk\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08099\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 01:37:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Feng et al. (§72023§r)', author: 'Tiantian Feng; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2309.08108", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFoundation Model Assisted Automatic Speech Emotion Recognition: Transcribing, Annotating, and Augmenting\\u00a7r\\n\\n\\u00a78\\u00a7oTiantian Feng\\nShrikanth Narayanan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08108\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 02:19:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review\\u00a7r"}']}
{title:'Seki et al. (§72023§r)', author: 'Kentaro Seki; Shinnosuke Takamichi; Takaaki Saeki; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2309.08127", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiversity-based core-set selection for text-to-speech with linguistic and acoustic features\\u00a7r\\n\\n\\u00a78\\u00a7oKentaro Seki\\nShinnosuke Takamichi\\nTakaaki Saeki\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08127\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 03:36:08 GMT)\\u00a7r"}']}
{title:'Nathoo et al. (§72023§r)', author: 'Rayan Daod Nathoo; Mikolaj Kegler; Marko Stamenovic', display:{Lore:['[{"text": "arXiv:2309.08144", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTwo-Step Knowledge Distillation for Tiny Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oRayan Daod Nathoo\\nMikolaj Kegler\\nMarko Stamenovic\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08144\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 04:19:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review ICASSP 2024\\u00a7r"}']}
{title:'Rahman et al. (§72023§r)', author: 'Md Awsafur Rahman; Bishmoy Paul; Najibul Haque Sarker; Zaber Ibn Abdul Hakim; Shaikh Anowarul Fattah; Mohammad Saquib', display:{Lore:['[{"text": "arXiv:2309.08146", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSyn-Att: Synthetic Speech Attribution via Semi-Supervised Unknown Multi-Class Ensemble of CNNs\\u00a7r\\n\\n\\u00a78\\u00a7oMd Awsafur Rahman\\nBishmoy Paul\\nNajibul Haque Sarker\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08146\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 04:26:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWinning Solution of IEEE SP Cup at ICASSP 2022\\u00a7r"}']}
{title:'Xu et al. (§72023§r)', author: 'Le Xu; Jiangyan Yi; Jianhua Tao; Tao Wang; Yong Ren; Rongxiu Zhong', display:{Lore:['[{"text": "arXiv:2309.08166", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lControllable Residual Speaker Representation for Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oLe Xu\\nJiangyan Yi\\nJianhua Tao\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08166\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 05:27:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Shin et al. (§72023§r)', author: 'Hyun-seo Shin; Jungwoo Heo; Ju-ho Kim; Chan-yeong Lim; Wonbin Kim; Ha-Jin Yu', display:{Lore:['[{"text": "arXiv:2309.08208", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHM-Conformer: A Conformer-based audio deepfake detection system with hierarchical pooling and multi-level classification token aggregation methods\\u00a7r\\n\\n\\u00a78\\u00a7oHyun-seo Shin\\nJungwoo Heo\\nJu-ho Kim\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08208\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 07:18:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to 2024 IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP 2024)\\u00a7r"}']}
{title:'Gebhard et al. (§72023§r)', author: 'Alexander Gebhard; Andreas Triantafyllopoulos; Teresa Bez; Lukas Christ; Alexander Kathan; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2309.08398", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Meta Information for Audio-based Zero-shot Bird Classification\\u00a7r\\n\\n\\u00a78\\u00a7oAlexander Gebhard\\nAndreas Triantafyllopoulos\\nTeresa Bez\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08398\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 13:50:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to the IEEE for possible publication\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Junjie Li; Ruijie Tao; Zexu Pan; Meng Ge; Shuai Wang; Haizhou Li', display:{Lore:['[{"text": "arXiv:2309.08408", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-Visual Active Speaker Extraction for Sparsely Overlapped Multi-talker Speech\\u00a7r\\n\\n\\u00a78\\u00a7oJunjie Li\\nRuijie Tao\\nZexu Pan\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08408\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 14:10:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Verma (§72023§r)', author: 'Prateek Verma', display:{Lore:['[{"text": "arXiv:2309.08751", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiverse Neural Audio Embeddings \\u2013 Bringing Features back !\\u00a7r\\n\\n\\u00a78\\u00a7oPrateek Verma\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08751\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 20:27:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 1 figure, 2 table\\u00a7r"}']}
{title:'Shi et al. (§72023§r)', author: 'Yangyang Shi; Gael Le Lan; Varun Nagaraja; Zhaoheng Ni; Xinhao Mei; Ernie Chang; Forrest Iandola; Yang Liu; Vikas Chandra', display:{Lore:['[{"text": "arXiv:2309.08773", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhance audio generation controllability through representation similarity regularization\\u00a7r\\n\\n\\u00a78\\u00a7oYangyang Shi\\nGael Le Lan\\nVarun Nagaraja\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08773\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 21:32:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Jianzong Wang; Xulong Zhang; Aolan Sun; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2309.08837", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFastGraphTTS: An Ultrafast Syntax-Aware Speech Synthesis Framework\\u00a7r\\n\\n\\u00a78\\u00a7oJianzong Wang\\nXulong Zhang\\nAolan Sun\\nNing Cheng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08837\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 16 Sep 2023 02:10:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by The 35th IEEE InternationalConference on Tools with Artificial Intelligence. (ICTAI 2023)\\u00a7r"}']}
{title:'Luo et al. (§72023§r)', author: 'Kaiyi Luo; Xulong Zhang; Jianzong Wang; Huaxiong Li; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2309.08839", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContrastive Latent Space Reconstruction Learning for Audio-Text Retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oKaiyi Luo\\nXulong Zhang\\nJianzong Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08839\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 16 Sep 2023 02:12:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by The 35th IEEE InternationalConference on Tools with Artificial Intelligence. (ICTAI 2023)\\u00a7r"}']}
{title:'Jiang et al. (§72023§r)', author: 'Ziyi Jiang; Ruoxue Wu; Zhenghan Chen; Xiaoxuan Liang', display:{Lore:['[{"text": "arXiv:2309.09075", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Generation based on Generative Adversarial Networks with Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oZiyi Jiang\\nRuoxue Wu\\nZhenghan Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09075\\u00a7r\\n\\nVersion:\\u00a77v4 (Sun, 8 Oct 2023 18:37:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: This versionhas been removed by arXiv administrators due to copyright infringement\\u00a7r"}']}
{title:'Guo et al. (§72023§r)', author: 'Haoming Guo; Seth Z. Zhao; Jiachen Lian; Gopala Anumanchipalli; Gerald Friedland', display:{Lore:['[{"text": "arXiv:2309.09088", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing GAN-Based Vocoders with Contrastive Learning Under Data-limited Condition\\u00a7r\\n\\n\\u00a78\\u00a7oHaoming Guo\\nSeth Z. Zhao\\nJiachen Lian\\nGopala Anumanchipalli\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09088\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 18 Dec 2023 17:51:33 GMT)\\u00a7r"}']}
{title:'Zhao et al. (§72023§r)', author: 'Qiuming Zhao; Guangzhi Sun; Chao Zhang; Mingxing Xu; Thomas Fang Zheng', display:{Lore:['[{"text": "arXiv:2309.09136", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Quantised End-to-End ASR Models via Personalisation\\u00a7r\\n\\n\\u00a78\\u00a7oQiuming Zhao\\nGuangzhi Sun\\nChao Zhang\\nMingxing Xu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09136\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 17 Sep 2023 02:35:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, submitted to ICASSP 2024\\u00a7r"}']}
{title:'Kushwaha et al. (§72023§r)', author: 'Saksham Singh Kushwaha; Iran R. Roman; Magdalena Fuentes; Juan Pablo Bello', display:{Lore:['[{"text": "arXiv:2309.09288", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Source Distance Estimation in Diverse and Dynamic Acoustic Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oSaksham Singh Kushwaha\\nIran R. Roman\\nMagdalena Fuentes\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09288\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 17 Sep 2023 14:49:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in WASPAA 2023\\u00a7r"}']}
{title:'Chowdary et al. (§72023§r)', author: 'Paleti Nikhil Chowdary; Vadlapudi Sai Aravind; Gorantla V N S L Vishnu Vardhan; Menta Sai Akshay; Menta Sai Aashish; Jyothish Lal. G', display:{Lore:['[{"text": "arXiv:2309.09329", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Few-Shot Approach to Dysarthric Speech Intelligibility Level Classification Using Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oPaleti Nikhil Chowdary\\nVadlapudi Sai Aravind\\nGorantla V N S L Vishnu Vardhan\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09329\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 17 Sep 2023 17:23:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper has been presented at ICCCNT 2023 and the final version will be published in IEEE Digital Library Xplore\\u00a7r"}']}
{title:'Ng et al. (§72023§r)', author: 'Dianwen Ng; Chong Zhang; Ruixi Zhang; Yukun Ma; Fabian Ritter-Gutierrez; Trung Hieu Nguyen; Chongjia Ni; Shengkui Zhao; Eng Siong Chng; Bin Ma', display:{Lore:['[{"text": "arXiv:2309.09413", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAre Soft Prompts Good Zero-shot Learners for Speech Recognition?\\u00a7r\\n\\n\\u00a78\\u00a7oDianwen Ng\\nChong Zhang\\nRuixi Zhang\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09413\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Sep 2023 01:00:40 GMT)\\u00a7r"}']}
{title:'Sheng et al. (§72023§r)', author: 'Zheng-Yan Sheng; Yang Ai; Yan-Nian Chen; Zhen-Hua Ling', display:{Lore:['[{"text": "arXiv:2309.09470", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFace-Driven Zero-Shot Voice Conversion with Memory-based Face-Voice Alignment\\u00a7r\\n\\n\\u00a78\\u00a7oZheng-Yan Sheng\\nYang Ai\\nYan-Nian Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09470\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Sep 2023 04:08:02 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Shansong Liu; Xu Li; Dian Li; Ying Shan', display:{Lore:['[{"text": "arXiv:2309.09623", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHumTrans: A Novel Open-Source Dataset for Humming Melody Transcription and Beyond\\u00a7r\\n\\n\\u00a78\\u00a7oShansong Liu\\nXu Li\\nDian Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09623\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 17 Oct 2023 05:08:47 GMT)\\u00a7r"}']}
{title:'Ochieng (§72023§r)', author: 'Peter Ochieng', display:{Lore:['[{"text": "arXiv:2309.09652", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeeding Up Speech Synthesis In Diffusion Models By Reducing Data Distribution Recovery Steps Via Content Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Ochieng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09652\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Sep 2023 10:35:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages\\u00a7r"}']}
{title:'Xiao et al. (§72023§r)', author: 'Feiyang Xiao; Qiaoxi Zhu; Jian Guan; Xubo Liu; Haohe Liu; Kejia Zhang; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2309.09705", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynth-AC: Enhancing Audio Captioning with Synthetic Supervision\\u00a7r\\n\\n\\u00a78\\u00a7oFeiyang Xiao\\nQiaoxi Zhu\\nJian Guan\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09705\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Sep 2023 12:17:16 GMT)\\u00a7r"}']}
{title:'Khan et al. (§72023§r)', author: 'Awais Khan; Khalid Mahmood Malik; Shah Nawaz', display:{Lore:['[{"text": "arXiv:2309.09837", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CY\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrame-to-Utterance Convergence: A Spectra-Temporal Approach for Unified Spoofing Detection\\u00a7r\\n\\n\\u00a78\\u00a7oAwais Khan\\nKhalid Mahmood Malik\\nShah Nawaz\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09837\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Sep 2023 14:54:42 GMT)\\u00a7r"}']}
{title:'Hossain et al. (§72023§r)', author: 'Forsad Al Hossain; Tanjid Hasan Tonmoy; Andrew A. Lover; George A. Corey; Mohammad Arif Ul Alam; Tauhidur Rahman', display:{Lore:['[{"text": "arXiv:2309.10280", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCrowdotic: A Privacy-Preserving Hospital Waiting Room Crowd Density Estimation with Non-speech Audio\\u00a7r\\n\\n\\u00a78\\u00a7oForsad Al Hossain\\nTanjid Hasan Tonmoy\\nAndrew A. Lover\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10280\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Sep 2023 23:45:05 GMT)\\u00a7r"}']}
{title:'Pan et al. (§72023§r)', author: 'Jiahui Pan; Shulin He; Tianci Wu; Hui Zhang; Xueliang Zhang', display:{Lore:['[{"text": "arXiv:2309.10379", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPDPCRN: Parallel Dual-Path CRN with Bi-directional Inter-Branch Interactions for Multi-Channel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oJiahui Pan\\nShulin He\\nTianci Wu\\nHui Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10379\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Sep 2023 07:27:38 GMT)\\u00a7r"}']}
{title:'Pan et al. (§72023§r)', author: 'Jiahui Pan; Shulin He; Hui Zhang; Xueliang Zhang', display:{Lore:['[{"text": "arXiv:2309.10393", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierarchical Modeling of Spatial Cues via Spherical Harmonics for Multi-Channel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oJiahui Pan\\nShulin He\\nHui Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10393\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Sep 2023 07:46:14 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Hongyang Chen; Yuhong Yang; Qingmu Liu; Baifeng Li; Weiping Tu; Song Lin', display:{Lore:['[{"text": "arXiv:2309.10485", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA comparative study of Grid and Natural sentences effects on Normal-to-Lombard conversion\\u00a7r\\n\\n\\u00a78\\u00a7oHongyang Chen\\nYuhong Yang\\nQingmu Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10485\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Sep 2023 09:54:36 GMT)\\u00a7r"}']}
{title:'Khan et al. (§72023§r)', author: 'Awais Khan; Khalid Mahmood Malik', display:{Lore:['[{"text": "arXiv:2309.10560", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBridging the Spoof Gap: A Unified Parallel Aggregation Network for Voice Presentation Attacks\\u00a7r\\n\\n\\u00a78\\u00a7oAwais Khan\\nKhalid Mahmood Malik\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10560\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Sep 2023 12:12:59 GMT)\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Yuxuan Wu; Roger B. Dannenberg; Gus Xia', display:{Lore:['[{"text": "arXiv:2309.10597", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMotif-Centric Representation Learning for Symbolic Music\\u00a7r\\n\\n\\u00a78\\u00a7oYuxuan Wu\\nRoger B. Dannenberg\\nGus Xia\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10597\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Sep 2023 13:09:03 GMT)\\u00a7r"}']}
{title:'Lipyanskiy (§72023§r)', author: 'Maksim Lipyanskiy', display:{Lore:['[{"text": "arXiv:2309.10719", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHarmony and Duality: An introduction to Music Theory\\u00a7r\\n\\n\\u00a78\\u00a7oMaksim Lipyanskiy\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10719\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 5 Oct 2023 13:13:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o70 pages, 72 figures\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Xinda Wu; Zhijie Huang; Kejun Zhang; Jiaxing Yu; Xu Tan; Tieyao Zhang; Zihao Wang; Lingyun Sun', display:{Lore:['[{"text": "arXiv:2309.10738", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMelodyGLM: Multi-task Pre-training for Symbolic Melody Generation\\u00a7r\\n\\n\\u00a78\\u00a7oXinda Wu\\nZhijie Huang\\nKejun Zhang\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10738\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Sep 2023 10:56:07 GMT)\\u00a7r"}']}
{title:'Bai et al. (§72023§r)', author: 'Yatong Bai; Trung Dang; Dung Tran; Kazuhito Koishida; Somayeh Sojoudi', display:{Lore:['[{"text": "arXiv:2309.10740", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAccelerating Diffusion-Based Text-to-Audio Generation with Consistency Distillation\\u00a7r\\n\\n\\u00a78\\u00a7oYatong Bai\\nTrung Dang\\nDung Tran\\nKazuhito Koishida\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10740\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Sep 2023 16:36:33 GMT)\\u00a7r"}']}
{title:'Pan et al. (§72023§r)', author: 'Jiahui Pan; Pengjie Shen; Hui Zhang; Xueliang Zhang', display:{Lore:['[{"text": "arXiv:2309.10832", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Multi-Channel Speech Enhancement with Spherical Harmonics Injection for Directional Encoding\\u00a7r\\n\\n\\u00a78\\u00a7oJiahui Pan\\nPengjie Shen\\nHui Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10832\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Sep 2023 07:50:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2309.10393\\u00a7r"}']}
{title:'Dumpala et al. (§72023§r)', author: 'Sri Harsha Dumpala; Chandramouli Sastry; Sageev Oore', display:{Lore:['[{"text": "arXiv:2309.10930", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTest-Time Training for Speech\\u00a7r\\n\\n\\u00a78\\u00a7oSri Harsha Dumpala\\nChandramouli Sastry\\nSageev Oore\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10930\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 28 Sep 2023 21:06:02 GMT)\\u00a7r"}']}
{title:'Feng et al. (§72023§r)', author: 'Tiantian Feng; Ju Lin; Yiteng Huang; Weipeng He; Kaustubh Kalgaonkar; Niko Moritz; Li Wan; Xin Lei; Ming Sun; Frank Seide', display:{Lore:['[{"text": "arXiv:2309.10993", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDirectional Source Separation for Robust Speech Recognition on Smart Glasses\\u00a7r\\n\\n\\u00a78\\u00a7oTiantian Feng\\nJu Lin\\nYiteng Huang\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10993\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Sep 2023 01:23:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Plitsis et al. (§72023§r)', author: 'Manos Plitsis; Theodoros Kouzelis; Georgios Paraskevopoulos; Vassilis Katsouros; Yannis Panagakis', display:{Lore:['[{"text": "arXiv:2309.11140", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating Personalization Methods in Text to Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oManos Plitsis\\nTheodoros Kouzelis\\nGeorgios Paraskevopoulos\\nVassilis Katsouros\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.11140\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Sep 2023 08:36:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024, Examples at https://zelaki.github.io/\\u00a7r"}']}
{title:'Sun et al. (§72023§r)', author: 'Luoyi Sun; Xuenan Xu; Mengyue Wu; Weidi Xie', display:{Lore:['[{"text": "arXiv:2309.11500", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Large-scale Dataset for Audio-Language Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oLuoyi Sun\\nXuenan Xu\\nMengyue Wu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.11500\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 3 Oct 2023 11:37:40 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Rui Liu; Jiatian Xi; Ziyue Jiang; Haizhou Li', display:{Lore:['[{"text": "arXiv:2309.11725", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFluentEditor: Text-based Speech Editing by Considering Acoustic and Prosody Consistency\\u00a7r\\n\\n\\u00a78\\u00a7oRui Liu\\nJiatian Xi\\nZiyue Jiang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.11725\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 22 Sep 2023 02:05:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP\'2024\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Meng Liu; Ke Liang; Dayu Hu; Hao Yu; Yue Liu; Lingyuan Meng; Wenxuan Tu; Sihang Zhou; Xinwang Liu', display:{Lore:['[{"text": "arXiv:2309.11845", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTMac: Temporal Multi-Modal Graph Learning for Acoustic Event Classification\\u00a7r\\n\\n\\u00a78\\u00a7oMeng Liu\\nKe Liang\\nDayu Hu\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.11845\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3581783.3611853\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 26 Sep 2023 08:03:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been accepted by ACM MM 2023 for publication\\u00a7r"}']}
{title:'Wei et al. (§72023§r)', author: 'Xianhao Wei; Jia Jia; Xiang Li; Zhiyong Wu; Ziyi Wang', display:{Lore:['[{"text": "arXiv:2309.11849", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Discourse-level Multi-scale Prosodic Model for Fine-grained Emotion Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oXianhao Wei\\nJia Jia\\nXiang Li\\nZhiyong Wu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.11849\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Sep 2023 07:45:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oChinaMM 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Yang Wang; Qibin Liang; Chenghao Xiao; Yizhi Li; Noura Al Moubayed; Chenghua Lin', display:{Lore:['[{"text": "arXiv:2309.11895", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Contrastive based Fine-tuning\\u00a7r\\n\\n\\u00a78\\u00a7oYang Wang\\nQibin Liang\\nChenghao Xiao\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.11895\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 19 Oct 2023 21:32:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review\\u00a7r"}']}
{title:'Carvalho et al. (§72023§r)', author: 'Luis Carvalho; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2309.12111", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPassage Summarization with Recurrent Models for Audio-Sheet Music Retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oLuis Carvalho\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.12111\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Sep 2023 14:30:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of the 24th Conference of theInternational Society for Music Information Retrieval (ISMIR 2023), Milan, Italy\\u00a7r"}']}
{title:'Carvalho et al. (§72023§r)', author: 'Luis Carvalho; Tobias Washüttl; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2309.12134", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Contrastive Learning for Robust Audio-Sheet Music Retrieval Systems\\u00a7r\\n\\n\\u00a78\\u00a7oLuis Carvalho\\nTobias Wash\\u00fcttl\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.12134\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3587819.3590968\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 14th ACM Multimedia Systems Conference (MMSys\\n  \'23), June 7-10, 2023, Vancouver, BC, Canada\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Sep 2023 14:54:48 GMT)\\u00a7r"}']}
{title:'Carvalho et al. (§72023§r)', author: 'Luis Carvalho; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2309.12158", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Robust and Truly Large-Scale Audio-Sheet Music Retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oLuis Carvalho\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.12158\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Sep 2023 15:11:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of the IEEE 6th International Conference on Multimedia Information Processing and Retrieval (MIPR)\\u00a7r"}']}
{title:'Kouzelis et al. (§72023§r)', author: 'Theodoros Kouzelis; Vassilis Katsouros', display:{Lore:['[{"text": "arXiv:2309.12242", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeakly-supervised Automated Audio Captioning via text only training\\u00a7r\\n\\n\\u00a78\\u00a7oTheodoros Kouzelis\\nVassilis Katsouros\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.12242\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Sep 2023 16:40:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDCASE Workshop 2023\\u00a7r"}']}
{title:'Maman et al. (§72023§r)', author: 'Ben Maman; Johannes Zeitler; Meinard Müller; Amit H. Bermano', display:{Lore:['[{"text": "arXiv:2309.12283", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerformance Conditioning for Diffusion-Based Multi-Instrument Music Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oBen Maman\\nJohannes Zeitler\\nMeinard M\\u00fcller\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.12283\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Sep 2023 17:44:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, project page available at benadar293.github.io/midipm\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Xintong Wang; Chang Zeng; Jun Chen; Chunhui Wang', display:{Lore:['[{"text": "arXiv:2309.12672", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCrossSinger: A Cross-Lingual Multi-Singer High-Fidelity Singing Voice Synthesizer Trained on Monolingual Singers\\u00a7r\\n\\n\\u00a78\\u00a7oXintong Wang\\nChang Zeng\\nJun Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.12672\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Sep 2023 07:29:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ASRU2023\\u00a7r"}']}
{title:'Ferreira et al. (§72023§r)', author: 'Alexandre R. Ferreira; Cláudio E. C. Campelo', display:{Lore:['[{"text": "arXiv:2309.12802", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeepfake audio as a data augmentation technique for training automatic speech to text transcription models\\u00a7r\\n\\n\\u00a78\\u00a7oAlexandre R. Ferreira\\nCl\\u00e1udio E. C. Campelo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.12802\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21528/CBIC2023-169\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Sep 2023 11:33:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 6 figures, 7 tables\\u00a7r"}']}
{title:'Huang et al. (§72023§r)', author: 'Jieyi Huang; Chunhao Zhang; Yufei Wang; Mengyue Wu; Kenny Zhu', display:{Lore:['[{"text": "arXiv:2309.13085", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDoes My Dog \\u201dSpeak\\u201d Like Me? The Acoustic Correlation between Pet Dogs and Their Human Owners\\u00a7r\\n\\n\\u00a78\\u00a7oJieyi Huang\\nChunhao Zhang\\nYufei Wang\\nMengyue Wu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13085\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Sep 2023 23:49:21 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Yufei Wang; Chunhao Zhang; Jieyi Huang; Mengyue Wu; Kenny Zhu', display:{Lore:['[{"text": "arXiv:2309.13086", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Lexical Analysis of Dog Vocalizations via Online Videos\\u00a7r\\n\\n\\u00a78\\u00a7oYufei Wang\\nChunhao Zhang\\nJieyi Huang\\nMengyue Wu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13086\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Sep 2023 23:53:14 GMT)\\u00a7r"}']}
{title:'Cao et al. (§72023§r)', author: 'Xirong Cao; Xiang Li; Divyesh Jadav; Yanzhao Wu; Zhehui Chen; Chen Zeng; Wenqi Wei', display:{Lore:['[{"text": "arXiv:2309.13166", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvisible Watermarking for Audio Generation Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oXirong Cao\\nXiang Li\\nDivyesh Jadav\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13166\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 31 Oct 2023 20:46:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis is an invited paper for IEEE TPS, part of the IEEE CIC/CogMI/TPS 2023 conference\\u00a7r"}']}
{title:'Wilkins et al. (§72023§r)', author: 'Julia Wilkins; Magdalena Fuentes; Luca Bondi; Shabnam Ghaffarzadegan; Ali Abavisani; Juan Pablo Bello', display:{Lore:['[{"text": "arXiv:2309.13343", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTwo vs. Four-Channel Sound Event Localization and Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJulia Wilkins\\nMagdalena Fuentes\\nLuca Bondi\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13343\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 23 Sep 2023 11:32:53 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Xiang Li; Junhao Chen; Chao Li; Hongwu Lv', display:{Lore:['[{"text": "arXiv:2309.13373", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAsca: less audio data is more insightful\\u00a7r\\n\\n\\u00a78\\u00a7oXiang Li\\nJunhao Chen\\nChao Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13373\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 23 Sep 2023 13:24:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages,3 figures\\u00a7r"}']}
{title:'Watanabe et al. (§72023§r)', author: 'Aya Watanabe; Shinnosuke Takamichi; Yuki Saito; Wataru Nakata; Detai Xin; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2309.13509", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCoco-Nut: Corpus of Japanese Utterance and Voice Characteristics Description for Prompt-based Control\\u00a7r\\n\\n\\u00a78\\u00a7oAya Watanabe\\nShinnosuke Takamichi\\nYuki Saito\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13509\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 24 Sep 2023 00:15:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ASRU2023\\u00a7r"}']}
{title:'Liang et al. (§72023§r)', author: 'Yuhao Liang; Mohan Shi; Fan Yu; Yangze Li; Shiliang Zhang; Zhihao Du; Qian Chen; Lei Xie; Yanmin Qian; Jian Wu; Zhuo Chen; Kong Aik Lee; Zhijie Yan; Hui Bu', display:{Lore:['[{"text": "arXiv:2309.13573", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe second multi-channel multi-party meeting transcription challenge (M2MeT) 2.0): A benchmark for speaker-attributed ASR\\u00a7r\\n\\n\\u00a78\\u00a7oYuhao Liang\\nMohan Shi\\nFan Yu\\n+ 10 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13573\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 5 Oct 2023 11:35:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, Accepted by ASRU2023\\u00a7r"}']}
{title:'Guo et al. (§72023§r)', author: 'Dake Guo; Xinfa Zhu; Liumeng Xue; Tao Li; Yuanjun Lv; Yuepeng Jiang; Lei Xie', display:{Lore:['[{"text": "arXiv:2309.13907", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHiGNN-TTS: Hierarchical Prosody Modeling with Graph Neural Networks for Expressive Long-form TTS\\u00a7r\\n\\n\\u00a78\\u00a7oDake Guo\\nXinfa Zhu\\nLiumeng Xue\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13907\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 7 Oct 2023 01:56:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ASRU2023\\u00a7r"}']}
{title:'Khalfaoui-Hassani et al. (§72023§r)', author: 'Ismail Khalfaoui-Hassani; Timothée Masquelier; Thomas Pellegrini', display:{Lore:['[{"text": "arXiv:2309.13972", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio classification with Dilated Convolution with Learnable Spacings\\u00a7r\\n\\n\\u00a78\\u00a7oIsmail Khalfaoui-Hassani\\nTimoth\\u00e9e Masquelier\\nThomas Pellegrini\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13972\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 22 Nov 2023 16:49:20 GMT)\\u00a7r"}']}
{title:'Shi et al. (§72023§r)', author: 'Yao Shi; Ming Li', display:{Lore:['[{"text": "arXiv:2309.14094", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoiceLens: Controllable Speaker Generation and Editing with Flow\\u00a7r\\n\\n\\u00a78\\u00a7oYao Shi\\nMing Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.14094\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Sep 2023 12:37:03 GMT)\\u00a7r"}']}
{title:'Lin et al. (§72023§r)', author: 'Wan Lin; Lantian Li; Dong Wang', display:{Lore:['[{"text": "arXiv:2309.14149", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Domain Adaptation by Self-Supervised Learning for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oWan Lin\\nLantian Li\\nDong Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.14149\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Sep 2023 14:02:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Zhou et al. (§72023§r)', author: 'Zhenyu Zhou; Junhui Chen; Namin Wang; Lantian Li; Dong Wang', display:{Lore:['[{"text": "arXiv:2309.14158", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Investigation of Distribution Alignment in Multi-Genre Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhenyu Zhou\\nJunhui Chen\\nNamin Wang\\nLantian Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.14158\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Sep 2023 14:08:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Ijaz et al. (§72023§r)', author: 'Aneeqa Ijaz; Muhammad Nabeel; Usama Masood; Tahir Mahmood; Mydah Sajid Hashmi; Iryna Posokhova; Ali Rizwan; Ali Imran', display:{Lore:['[{"text": "arXiv:2309.14383", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards using Cough for Respiratory Disease Diagnosis by leveraging Artificial Intelligence: A Survey\\u00a7r\\n\\n\\u00a78\\u00a7oAneeqa Ijaz\\nMuhammad Nabeel\\nUsama Masood\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.14383\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 24 Sep 2023 19:03:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o30 pages, 12 figures, 9 tables\\u00a7r"}']}
{title:'Gong et al. (§72023§r)', author: 'Yuan Gong; Alexander H. Liu; Hongyin Luo; Leonid Karlinsky; James Glass', display:{Lore:['[{"text": "arXiv:2309.14405", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Audio and Speech Understanding\\u00a7r\\n\\n\\u00a78\\u00a7oYuan Gong\\nAlexander H. Liu\\nHongyin Luo\\nLeonid Karlinsky\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.14405\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 10 Dec 2023 22:50:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ASRU 2023. Code, dataset, and pretrained models are at https://github.com/yuangongnd/ltu. Interactive demo at https://huggingface.co/spaces/yuangongfdu/ltu-2\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Xiaofeng Liu; Fangxu Xing; Maureen Stone; Jiachen Zhuo; Sidney Fels; Jerry L. Prince; Georges El Fakhri; Jonghye Woo', display:{Lore:['[{"text": "arXiv:2309.14586", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Audio Synthesis from Tagged MRI and Non-Negative Matrix Factorization via Plastic Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oXiaofeng Liu\\nFangxu Xing\\nMaureen Stone\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.14586\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Sep 2023 00:21:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMICCAI2023 (Oral presentation)\\u00a7r"}']}
{title:'Lin et al. (§72023§r)', author: 'Chia-Hsin Lin; Charles Jones; Björn W. Schuller; Harry Coppock', display:{Lore:['[{"text": "arXiv:2309.15024", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynthia\'s Melody: A Benchmark Framework for Unsupervised Domain Adaptation in Audio\\u00a7r\\n\\n\\u00a78\\u00a7oChia-Hsin Lin\\nCharles Jones\\nBj\\u00f6rn W. Schuller\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.15024\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Sep 2023 15:46:06 GMT)\\u00a7r"}']}
{title:'Qiang et al. (§72023§r)', author: 'Chunyu Qiang; Hao Li; Yixin Tian; Yi Zhao; Ying Zhang; Longbiao Wang; Jianwu Dang', display:{Lore:['[{"text": "arXiv:2309.15512", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHigh-Fidelity Speech Synthesis with Minimal Supervision: All Using Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oChunyu Qiang\\nHao Li\\nYixin Tian\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.15512\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 18 Dec 2023 12:52:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024. arXiv admin note: substantial text overlap with arXiv:2307.15484; text overlap with arXiv:2309.00424\\u00a7r"}']}
{title:'Hussein et al. (§72023§r)', author: 'Amir Hussein; Dorsa Zeinali; Ondřej Klejch; Matthew Wiesner; Brian Yan; Shammur Chowdhury; Ahmed Ali; Shinji Watanabe; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:2309.15674", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech collage: code-switched audio generation by collaging monolingual corpora\\u00a7r\\n\\n\\u00a78\\u00a7oAmir Hussein\\nDorsa Zeinali\\nOnd\\u0159ej Klejch\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.15674\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Sep 2023 14:17:53 GMT)\\u00a7r"}']}
{title:'Liang et al. (§72023§r)', author: 'Susan Liang; Chao Huang; Yapeng Tian; Anurag Kumar; Chenliang Xu', display:{Lore:['[{"text": "arXiv:2309.15977", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Acoustic Context Field: Rendering Realistic Room Impulse Response With Neural Fields\\u00a7r\\n\\n\\u00a78\\u00a7oSusan Liang\\nChao Huang\\nYapeng Tian\\nAnurag Kumar\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.15977\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Sep 2023 19:50:50 GMT)\\u00a7r"}']}
{title:'Ma et al. (§72023§r)', author: 'Guodong Ma; Wenxuan Wang; Yuke Li; Yuting Yang; Binbin Du; Haoran Fu', display:{Lore:['[{"text": "arXiv:2309.16178", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLAE-ST-MoE: Boosted Language-Aware Encoder Using Speech Translation Auxiliary Task for E2E Code-switching ASR\\u00a7r\\n\\n\\u00a78\\u00a7oGuodong Ma\\nWenxuan Wang\\nYuke Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.16178\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 7 Oct 2023 09:55:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEASRU 2023\\u00a7r"}']}
{title:'Ramoneda et al. (§72023§r)', author: 'Pedro Ramoneda; Jose J. Valero-Mas; Dasaem Jeong; Xavier Serra', display:{Lore:['[{"text": "arXiv:2309.16287", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.DL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPredicting performance difficulty from piano sheet music images\\u00a7r\\n\\n\\u00a78\\u00a7oPedro Ramoneda\\nJose J. Valero-Mas\\nDasaem Jeong\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.16287\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Sep 2023 09:33:47 GMT)\\u00a7r"}']}
{title:'Alonso-Jiménez et al. (§72023§r)', author: 'Pablo Alonso-Jiménez; Xavier Serra; Dmitry Bogdanov', display:{Lore:['[{"text": "arXiv:2309.16418", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Supervised Training of Audio Transformers for Music Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oPablo Alonso-Jim\\u00e9nez\\nXavier Serra\\nDmitry Bogdanov\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.16418\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Sep 2023 13:11:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the 2023 International Society for Music Information Retrieval Conference (ISMIR\'23)\\u00a7r"}']}
{title:'Praveen et al. (§72023§r)', author: 'R. Gnana Praveen; Jahangir Alam', display:{Lore:['[{"text": "arXiv:2309.16569", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-Visual Speaker Verification via Joint Cross-Attention\\u00a7r\\n\\n\\u00a78\\u00a7oR. Gnana Praveen\\nJahangir Alam\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.16569\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Sep 2023 16:25:29 GMT)\\u00a7r"}']}
{title:'Pons et al. (§72023§r)', author: 'Jordi Pons; Xiaoyu Liu; Santiago Pascual; Joan Serrà', display:{Lore:['[{"text": "arXiv:2310.00140", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGASS: Generalizing Audio Source Separation with Large-scale Data\\u00a7r\\n\\n\\u00a78\\u00a7oJordi Pons\\nXiaoyu Liu\\nSantiago Pascual\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.00140\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Sep 2023 21:02:07 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Dongyuan Li; Yusong Wang; Kotaro Funakoshi; Manabu Okumura', display:{Lore:['[{"text": "arXiv:2310.00283", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lActive Learning Based Fine-Tuning Framework for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDongyuan Li\\nYusong Wang\\nKotaro Funakoshi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.00283\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 30 Sep 2023 07:23:29 GMT)\\u00a7r"}']}
{title:'Tang et al. (§72023§r)', author: 'Jingjing Tang; Geraint Wiggins; Gyorgy Fazekas', display:{Lore:['[{"text": "arXiv:2310.00699", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPianist Identification Using Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oJingjing Tang\\nGeraint Wiggins\\nGyorgy Fazekas\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.00699\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 1 Oct 2023 15:15:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 3 figures, accepted by the 4th International Symposium on the Internet of Sounds, IS22023\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Dongchao Yang; Jinchuan Tian; Xu Tan; Rongjie Huang; Songxiang Liu; Xuankai Chang; Jiatong Shi; Sheng Zhao; Jiang Bian; Xixin Wu; Zhou Zhao; Shinji Watanabe; Helen Meng', display:{Lore:['[{"text": "arXiv:2310.00704", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUniAudio: An Audio Foundation Model Toward Universal Audio Generation\\u00a7r\\n\\n\\u00a78\\u00a7oDongchao Yang\\nJinchuan Tian\\nXu Tan\\n+ 9 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.00704\\u00a7r\\n\\nVersion:\\u00a77v5 (Mon, 11 Dec 2023 15:56:37 GMT)\\u00a7r"}']}
{title:'Roman et al. (§72023§r)', author: 'Iran R. Roman; Daniel Faronbi; Isabelle Burger-Weiser; Leila Adu-Gilmore', display:{Lore:['[{"text": "arXiv:2310.00870", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lF0 analysis of Ghanaian pop singing reveals progressive alignment with equal temperament over the past three decades: a case study\\u00a7r\\n\\n\\u00a78\\u00a7oIran R. Roman\\nDaniel Faronbi\\nIsabelle Burger-Weiser\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.00870\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5281/zenodo.8136568\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 2 Oct 2023 03:19:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPages 27-33\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Muqiao Yang; Chunlei Zhang; Yong Xu; Zhongweiyang Xu; Heming Wang; Bhiksha Raj; Dong Yu', display:{Lore:['[{"text": "arXiv:2310.00900", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7luSee: Unified Speech Enhancement and Editing with Conditional Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oMuqiao Yang\\nChunlei Zhang\\nYong Xu\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.00900\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 2 Oct 2023 04:36:39 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Ju-Chiang Wang; Wei-Tsung Lu; Minz Won', display:{Lore:['[{"text": "arXiv:2310.01809", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMel-Band RoFormer for Music Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oJu-Chiang Wang\\nWei-Tsung Lu\\nMinz Won\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.01809\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 Oct 2023 05:53:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted as an ISMIR 2023 late-breaking and demopaper\\u00a7r"}']}
{title:'Dhamyal et al. (§72023§r)', author: 'Hira Dhamyal; Benjamin Elizalde; Soham Deshmukh; Huaming Wang; Bhiksha Raj; Rita Singh', display:{Lore:['[{"text": "arXiv:2310.02298", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrompting Audios Using Acoustic Properties For Emotion Representation\\u00a7r\\n\\n\\u00a78\\u00a7oHira Dhamyal\\nBenjamin Elizalde\\nSoham Deshmukh\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.02298\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 7 Dec 2023 03:22:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2211.07737\\u00a7r"}']}
{title:'Netzorg et al. (§72023§r)', author: 'Robin Netzorg; Bohan Yu; Andrea Guzman; Peter Wu; Luna McNulty; Gopala Anumanchipalli', display:{Lore:['[{"text": "arXiv:2310.02497", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards an Interpretable Representation of Speaker Identity via Perceptual Voice Qualities\\u00a7r\\n\\n\\u00a78\\u00a7oRobin Netzorg\\nBohan Yu\\nAndrea Guzman\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.02497\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 4 Oct 2023 00:06:17 GMT)\\u00a7r"}']}
{title:'Daikoku (§72023§r)', author: 'Tatsuya Daikoku', display:{Lore:['[{"text": "arXiv:2310.02518", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lShaping the Epochal Individuality and Generality: The Temporal Dynamics of Uncertainty and Prediction Error in Musical Improvisation\\u00a7r\\n\\n\\u00a78\\u00a7oTatsuya Daikoku\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.02518\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 4 Oct 2023 01:33:26 GMT)\\u00a7r"}']}
{title:'Halpern et al. (§72023§r)', author: 'Bence Mark Halpern; Wen-Chin Huang; Lester Phillip Violeta; R. J. J. H. van Son; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2310.02570", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving severity preservation of healthy-to-pathological voice conversion with global style tokens\\u00a7r\\n\\n\\u00a78\\u00a7oBence Mark Halpern\\nWen-Chin Huang\\nLester Phillip Violeta\\nR. J. J. H. van Son\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.02570\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 4 Oct 2023 04:07:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 3 figures, 5 tables. Accepted to IEEE Automatic Speech Recognition and Understanding Workshop 2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Peikun Chen; Fan Yu; Yuhao Lian; Hongfei Xue; Xucheng Wan; Naijun Zheng; Huan Zhou; Lei Xie', display:{Lore:['[{"text": "arXiv:2310.02629", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBA-MoE: Boundary-Aware Mixture-of-Experts Adapter for Code-Switching Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oPeikun Chen\\nFan Yu\\nYuhao Lian\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.02629\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 8 Oct 2023 02:49:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ASRU2023\\u00a7r"}']}
{title:'Ettenhofer et al. (§72023§r)', author: 'Armin Ettenhofer; Jan-Philipp Schulze; Karla Pizzi', display:{Lore:['[{"text": "arXiv:2310.03349", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Integrated Algorithm for Robust and Imperceptible Audio Adversarial Examples\\u00a7r\\n\\n\\u00a78\\u00a7oArmin Ettenhofer\\nJan-Philipp Schulze\\nKarla Pizzi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.03349\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/SPSC.2023-4\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 5 Oct 2023 06:59:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProc. 3rd Symposium on Security and Privacy in Speech Communication\\u00a7r"}']}
{title:'Masclef et al. (§72023§r)', author: 'Ninon Lizé Masclef; T. Anderson Keller', display:{Lore:['[{"text": "arXiv:2310.03500", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Generative Models of Music Expectation\\u00a7r\\n\\n\\u00a78\\u00a7oNinon Liz\\u00e9 Masclef\\nT. Anderson Keller\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.03500\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 5 Oct 2023 12:25:39 GMT)\\u00a7r"}']}
{title:'Khan et al. (§72023§r)', author: 'Awais Khan; Khalid Mahmood Malik', display:{Lore:['[{"text": "arXiv:2310.03856", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSecuring Voice Biometrics: One-Shot Learning Approach for Audio Deepfake Detection\\u00a7r\\n\\n\\u00a78\\u00a7oAwais Khan\\nKhalid Mahmood Malik\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.03856\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 5 Oct 2023 19:30:22 GMT)\\u00a7r"}']}
{title:'Srivastava et al. (§72023§r)', author: 'Tejes Srivastava; Jiatong Shi; William Chen; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2310.03938", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEFFUSE: Efficient Self-Supervised Feature Fusion for E2E ASR in Multilingual and Low Resource Scenarios\\u00a7r\\n\\n\\u00a78\\u00a7oTejes Srivastava\\nJiatong Shi\\nWilliam Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.03938\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 5 Oct 2023 23:05:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 2 figures, 7 tables\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yuke Li; Xinfa Zhu; Yi Lei; Hai Li; Junhui Liu; Danming Xie; Lei Xie', display:{Lore:['[{"text": "arXiv:2310.03963", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero-Shot Emotion Transfer For Cross-Lingual Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYuke Li\\nXinfa Zhu\\nYi Lei\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.03963\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 Oct 2023 01:18:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ASRU2023\\u00a7r"}']}
{title:'Maekaku et al. (§72023§r)', author: 'Takashi Maekaku; Jiatong Shi; Xuankai Chang; Yuya Fujita; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2310.03975", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHuBERTopic: Enhancing Semantic Representation of HuBERT through Self-supervision Utilizing Topic Model\\u00a7r\\n\\n\\u00a78\\u00a7oTakashi Maekaku\\nJiatong Shi\\nXuankai Chang\\nYuya Fujita\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.03975\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 Oct 2023 02:19:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEEICASSP 2024\\u00a7r"}']}
{title:'Zhao et al. (§72023§r)', author: 'Yan Zhao; Yuan Zong; Jincen Wang; Hailun Lian; Cheng Lu; Li Zhao; Wenming Zheng', display:{Lore:['[{"text": "arXiv:2310.03992", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLayer-Adapted Implicit Distribution Alignment Networks for Cross-Corpus Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYan Zhao\\nYuan Zong\\nJincen Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.03992\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 Oct 2023 03:34:48 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Tao Li; Zhichao Wang; Xinfa Zhu; Jian Cong; Qiao Tian; Yuping Wang; Lei Xie', display:{Lore:['[{"text": "arXiv:2310.04004", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lU-Style: Cascading U-nets with Multi-level Speaker and Style Modeling for Zero-Shot Voice Cloning\\u00a7r\\n\\n\\u00a78\\u00a7oTao Li\\nZhichao Wang\\nXinfa Zhu\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.04004\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 Oct 2023 04:24:41 GMT)\\u00a7r"}']}
{title:'Xu et al. (§72023§r)', author: 'Weiming Xu; Zhouxuan Chen; Zhili Tan; Shubo Lv; Runduo Han; Wenjiang Zhou; Weifeng Zhao; Lei Xie', display:{Lore:['[{"text": "arXiv:2310.04369", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMBTFNet: Multi-Band Temporal-Frequency Neural Network For Singing Voice Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oWeiming Xu\\nZhouxuan Chen\\nZhili Tan\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.04369\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 Oct 2023 16:44:47 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Jiaming Wang; Zhihao Du; Qian Chen; Yunfei Chu; Zhifu Gao; Zerui Li; Kai Hu; Xiaohuan Zhou; Jin Xu; Ziyang Ma; Wen Wang; Siqi Zheng; Chang Zhou; Zhijie Yan; Shiliang Zhang', display:{Lore:['[{"text": "arXiv:2310.04673", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT\\u00a7r\\n\\n\\u00a78\\u00a7oJiaming Wang\\nZhihao Du\\nQian Chen\\n+ 11 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.04673\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 11 Oct 2023 02:55:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, under review\\u00a7r"}']}
{title:'He et al. (§72023§r)', author: 'Yayun He; Zuheng Kang; Jianzong Wang; Junqing Peng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2310.04681", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoiceExtender: Short-utterance Text-independent Speaker Verification with Guided Diffusion Model\\u00a7r\\n\\n\\u00a78\\u00a7oYayun He\\nZuheng Kang\\nJianzong Wang\\nJunqing Peng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.04681\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 7 Oct 2023 03:42:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the 2023 IEEE Automatic Speech Recognition andUnderstanding Workshop (ASRU 2023)\\u00a7r"}']}
{title:'Zhou et al. (§72023§r)', author: 'Monan Zhou; Shangda Wu; Shaohua Ji; Zijin Li; Wei Li', display:{Lore:['[{"text": "arXiv:2310.04722", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Holistic Evaluation of Piano Sound Quality\\u00a7r\\n\\n\\u00a78\\u00a7oMonan Zhou\\nShangda Wu\\nShaohua Ji\\nZijin Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.04722\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 7 Oct 2023 07:51:34 GMT)\\u00a7r"}']}
{title:'Caspe et al. (§72023§r)', author: 'Franco Caspe; Andrew McPherson; Mark Sandler', display:{Lore:['[{"text": "arXiv:2310.04811", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7acs.SY\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SY\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFM Tone Transfer with Envelope Learning\\u00a7r\\n\\n\\u00a78\\u00a7oFranco Caspe\\nAndrew McPherson\\nMark Sandler\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.04811\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 7 Oct 2023 14:03:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Audio Mostly 2023\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yangze Li; Fan Yu; Yuhao Liang; Pengcheng Guo; Mohan Shi; Zhihao Du; Shiliang Zhang; Lei Xie', display:{Lore:['[{"text": "arXiv:2310.04863", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSA-Paraformer: Non-autoregressive End-to-End Speaker-Attributed ASR\\u00a7r\\n\\n\\u00a78\\u00a7oYangze Li\\nFan Yu\\nYuhao Liang\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.04863\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 7 Oct 2023 16:07:42 GMT)\\u00a7r"}']}
{title:'Liu (§72023§r)', author: 'Ze Liu', display:{Lore:['[{"text": "arXiv:2310.04982", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparative Analysis of Transfer Learning in Deep Learning Text-to-Speech Models on a Few-Shot, Low-Resource, Customized Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oZe Liu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.04982\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 8 Oct 2023 03:08:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o86 pages, Bachelor\'s thesis, 34 figures\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Yongmao Zhang; Guanghou Liu; Yi Lei; Yunlin Chen; Hao Yin; Lei Xie; Zhifei Li', display:{Lore:['[{"text": "arXiv:2310.05001", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPromptSpeaker: Speaker Generation Based on Text Descriptions\\u00a7r\\n\\n\\u00a78\\u00a7oYongmao Zhang\\nGuanghou Liu\\nYi Lei\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.05001\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 8 Oct 2023 04:09:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ASRU 2023\\u00a7r"}']}
{title:'Lv et al. (§72023§r)', author: 'Yuanjun Lv; Jixun Yao; Peikun Chen; Hongbin Zhou; Heng Lu; Lei Xie', display:{Lore:['[{"text": "arXiv:2310.05051", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSALT: Distinguishable Speaker Anonymization Through Latent Space Transformation\\u00a7r\\n\\n\\u00a78\\u00a7oYuanjun Lv\\nJixun Yao\\nPeikun Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.05051\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 8 Oct 2023 07:21:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 3 figures; Accepted by ASRU2023\\u00a7r"}']}
{title:'Zhou et al. (§72023§r)', author: 'Yiquan Zhou; Meng Chen; Yi Lei; Jihua Zhu; Weifeng Zhao', display:{Lore:['[{"text": "arXiv:2310.05118", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVITS-based Singing Voice Conversion System with DSPGAN post-processing for SVCC2023\\u00a7r\\n\\n\\u00a78\\u00a7oYiquan Zhou\\nMeng Chen\\nYi Lei\\nJihua Zhu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.05118\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 8 Oct 2023 11:16:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ASRU2023\\u00a7r"}']}
{title:'Shi et al. (§72023§r)', author: 'Jiatong Shi; William Chen; Dan Berrebbi; Hsiu-Hsuan Wang; Wei-Ping Huang; En-Pei Hu; Ho-Lam Chuang; Xuankai Chang; Yuxun Tang; Shang-Wen Li; Abdelrahman Mohamed; Hung-yi Lee; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2310.05513", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFindings of the 2023 ML-SUPERB Challenge: Pre-Training and Evaluation over More Languages and Beyond\\u00a7r\\n\\n\\u00a78\\u00a7oJiatong Shi\\nWilliam Chen\\nDan Berrebbi\\n+ 9 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.05513\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 9 Oct 2023 08:30:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ASRU\\u00a7r"}']}
{title:'Shi et al. (§72023§r)', author: 'Xiangyu Shi; Yuhao Luo; Li Wang; Haorui He; Hao Li; Lei Wang; Zhizheng Wu', display:{Lore:['[{"text": "arXiv:2310.05813", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio compression-assisted feature extraction for voice replay attack detection\\u00a7r\\n\\n\\u00a78\\u00a7oXiangyu Shi\\nYuhao Luo\\nLi Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.05813\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 10 Oct 2023 09:06:42 GMT)\\u00a7r"}']}
{title:'Cabanas-Molero et al. (§72023§r)', author: 'Pablo Cabanas-Molero; Antonio J. Munoz-Montoro; Julio Carabias-Orti; Pedro Vera-Candeas', display:{Lore:['[{"text": "arXiv:2310.05821", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPre-trained Spatial Priors on Multichannel NMF for Music Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oPablo Cabanas-Molero\\nAntonio J. Munoz-Montoro\\nJulio Carabias-Orti\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.05821\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 9 Oct 2023 16:05:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at Forum Acusticum 2023\\u00a7r"}']}
{title:'Xin et al. (§72023§r)', author: 'Detai Xin; Junfeng Jiang; Shinnosuke Takamichi; Yuki Saito; Akiko Aizawa; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2310.06072", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJVNV: A Corpus of Japanese Emotional Speech with Verbal Content and Nonverbal Expressions\\u00a7r\\n\\n\\u00a78\\u00a7oDetai Xin\\nJunfeng Jiang\\nShinnosuke Takamichi\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.06072\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ACCESS.2024.3360885\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 9 Oct 2023 18:27:13 GMT)\\u00a7r"}']}
{title:'Ravenscroft et al. (§72023§r)', author: 'William Ravenscroft; Stefan Goetze; Thomas Hain', display:{Lore:['[{"text": "arXiv:2310.06125", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Time Domain Conformer Models for Monaural Speech Separation in Noisy Reverberant Acoustic Environments\\u00a7r\\n\\n\\u00a78\\u00a7oWilliam Ravenscroft\\nStefan Goetze\\nThomas Hain\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.06125\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 9 Oct 2023 20:02:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ASRU Workshop 2023\\u00a7r"}']}
{title:'Santos et al. (§72023§r)', author: 'Arthur dos Santos; Jayr Pereira; Rodrigo Nogueira; Bruno Masiero; Shiva Sander-Tavallaey; Elias Zea', display:{Lore:['[{"text": "arXiv:2310.06260", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn experiment on an automated literature survey of data-driven speech enhancement methods\\u00a7r\\n\\n\\u00a78\\u00a7oArthur dos Santos\\nJayr Pereira\\nRodrigo Nogueira\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.06260\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Oct 2023 02:07:24 GMT)\\u00a7r"}']}
{title:'Choi et al. (§72023§r)', author: 'Soonhyeon Choi; Jung-Woo Choi', display:{Lore:['[{"text": "arXiv:2310.06364", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoisy-ArcMix: Additive Noisy Angular Margin Loss Combined With Mixup Anomalous Sound Detection\\u00a7r\\n\\n\\u00a78\\u00a7oSoonhyeon Choi\\nJung-Woo Choi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.06364\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Oct 2023 07:04:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Bonafos et al. (§72023§r)', author: 'Guillem Bonafos; Jean-Marc Freyermuth; Pierre Pudlo; Samuel Tronçon; Arnaud Rey', display:{Lore:['[{"text": "arXiv:2310.06508", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.AP\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTopological data analysis of human vowels: Persistent homologies across representation spaces\\u00a7r\\n\\n\\u00a78\\u00a7oGuillem Bonafos\\nJean-Marc Freyermuth\\nPierre Pudlo\\nSamuel Tron\\u00e7on\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.06508\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Oct 2023 10:37:54 GMT)\\u00a7r"}']}
{title:'Choi et al. (§72023§r)', author: 'Haeyun Choi; Jio Gim; Yuho Lee; Youngin Kim; Young-Joo Suh', display:{Lore:['[{"text": "arXiv:2310.06546", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutoCycle-VC: Towards Bottleneck-Independent Zero-Shot Cross-Lingual Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oHaeyun Choi\\nJio Gim\\nYuho Lee\\nYoungin Kim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.06546\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Oct 2023 11:50:16 GMT)\\u00a7r"}']}
{title:'Pethe et al. (§72023§r)', author: 'Charuta Pethe; Yunting Yin; Steven Skiena', display:{Lore:['[{"text": "arXiv:2310.06930", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProsody Analysis of Audiobooks\\u00a7r\\n\\n\\u00a78\\u00a7oCharuta Pethe\\nYunting Yin\\nSteven Skiena\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.06930\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Oct 2023 18:33:47 GMT)\\u00a7r"}']}
{title:'Helwani et al. (§72023§r)', author: 'Karim Helwani; Erfan Soltanmohammadi; Michael M. Goodwin', display:{Lore:['[{"text": "arXiv:2310.07032", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SY\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SY\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Harmonium: An Interpretable Deep Structure for Nonlinear Dynamic System Identification with Application to Audio Processing\\u00a7r\\n\\n\\u00a78\\u00a7oKarim Helwani\\nErfan Soltanmohammadi\\nMichael M. Goodwin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.07032\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Oct 2023 21:32:15 GMT)\\u00a7r"}']}
{title:'Lei et al. (§72023§r)', author: 'Zhihong Lei; Mingbin Xu; Shiyi Han; Leo Liu; Zhen Huang; Tim Ng; Yuanyuan Zhang; Ernest Pusateri; Mirko Hannemann; Yaqiao Deng; Man-Hung Siu', display:{Lore:['[{"text": "arXiv:2310.07062", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic Model Fusion for End-to-end Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhihong Lei\\nMingbin Xu\\nShiyi Han\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.07062\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Oct 2023 23:00:17 GMT)\\u00a7r"}']}
{title:'Konan et al. (§72023§r)', author: 'Joseph Konan; Ojas Bhargave; Shikhar Agnihotri; Shuo Han; Yunyang Zeng; Ankit Shah; Bhiksha Raj', display:{Lore:['[{"text": "arXiv:2310.07161", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPsychoacoustic Challenges Of Speech Enhancement On VoIP Platforms\\u00a7r\\n\\n\\u00a78\\u00a7oJoseph Konan\\nOjas Bhargave\\nShikhar Agnihotri\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.07161\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 21 Nov 2023 07:54:34 GMT)\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Xinfa Zhu; Yuanjun Lv; Yi Lei; Tao Li; Wendi He; Hongbin Zhou; Heng Lu; Lei Xie', display:{Lore:['[{"text": "arXiv:2310.07246", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVec-Tok Speech: speech vectorization and tokenization for neural speech generation\\u00a7r\\n\\n\\u00a78\\u00a7oXinfa Zhu\\nYuanjun Lv\\nYi Lei\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.07246\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 12 Oct 2023 05:49:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 2 figures\\u00a7r"}']}
{title:'Duret et al. (§72023§r)', author: "Jarod Duret; Benjamin O'Brien; Yannick Estève; Titouan Parcollet", display:{Lore:['[{"text": "arXiv:2310.07279", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing expressivity transfer in textless speech-to-speech translation\\u00a7r\\n\\n\\u00a78\\u00a7oJarod Duret\\nBenjamin O\'Brien\\nYannick Est\\u00e8ve\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.07279\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nASRU, Dec 2023, Taipei, France\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 11 Oct 2023 08:07:22 GMT)\\u00a7r"}']}
{title:'Fradet et al. (§72023§r)', author: 'Nathan Fradet; Nicolas Gutowski; Fabien Chhel; Jean-Pierre Briot', display:{Lore:['[{"text": "arXiv:2310.08497", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImpact of time and note duration tokenizations on deep learning symbolic music modeling\\u00a7r\\n\\n\\u00a78\\u00a7oNathan Fradet\\nNicolas Gutowski\\nFabien Chhel\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.08497\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 12 Oct 2023 16:56:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oISMIR 2023\\u00a7r"}']}
{title:'Dhar et al. (§72023§r)', author: 'Sandipan Dhar; Anuvab Sen; Aritra Bandyopadhyay; Nanda Dulal Jana; Arjun Ghosh; Zahra Sarayloo', display:{Lore:['[{"text": "arXiv:2310.08914", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDifferential Evolution Algorithm based Hyper-Parameters Selection of Convolutional Neural Network for Speech Command Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSandipan Dhar\\nAnuvab Sen\\nAritra Bandyopadhyay\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.08914\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5220/0012251500003595\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 15th International Joint Conference on\\n  Computational Intelligence (2023)\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 13 Oct 2023 07:38:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 Pages, 7 Figures, 4 Tables, Accepted by the15th International JointConference on Computational Intelligence (IJCCI 2023), November 13-15,2023, Rome, Italy\\u00a7r"}']}
{title:'Guan et al. (§72023§r)', author: 'Jian Guan; Youde Liu; Qiuqiang Kong; Feiyang Xiao; Qiaoxi Zhu; Jiantong Tian; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2310.08950", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransformer-based Autoencoder with ID Constraint for Unsupervised Anomalous Sound Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJian Guan\\nYoude Liu\\nQiuqiang Kong\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.08950\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 13 Oct 2023 08:49:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by EURASIP Journal on Audio, Speech, and Music Processing\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Hongfu Liu; Hengguan Huang; Ye Wang', display:{Lore:['[{"text": "arXiv:2310.09505", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdvancing Test-Time Adaptation for Acoustic Foundation Models in Open-World Shifts\\u00a7r\\n\\n\\u00a78\\u00a7oHongfu Liu\\nHengguan Huang\\nYe Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.09505\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 14 Oct 2023 06:22:08 GMT)\\u00a7r"}']}
{title:'Lu et al. (§72023§r)', author: 'Jiajun Lu; Wei Huang; Hao Zhang', display:{Lore:['[{"text": "arXiv:2310.09522", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDynamic Prediction of Full-Ocean Depth SSP by Hierarchical LSTM: An Experimental Result\\u00a7r\\n\\n\\u00a78\\u00a7oJiajun Lu\\nWei Huang\\nHao Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.09522\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 14 Oct 2023 07:25:52 GMT)\\u00a7r"}']}
{title:'Zhou et al. (§72023§r)', author: 'Jiuyang Zhou; Tengfei Niu; Hong Zhu; Xingping Wang', display:{Lore:['[{"text": "arXiv:2310.09843", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCoCoFormer: A controllable feature-rich polyphonic music generation method\\u00a7r\\n\\n\\u00a78\\u00a7oJiuyang Zhou\\nTengfei Niu\\nHong Zhu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.09843\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 28 Nov 2023 03:30:44 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Dichucheng Li; Yinghao Ma; Weixing Wei; Qiuqiang Kong; Yulun Wu; Mingjin Che; Fan Xia; Emmanouil Benetos; Wei Li', display:{Lore:['[{"text": "arXiv:2310.09853", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMERTech: Instrument Playing Technique Detection Using Self-Supervised Pretrained Model With Multi-Task Finetuning\\u00a7r\\n\\n\\u00a78\\u00a7oDichucheng Li\\nYinghao Ma\\nWeixing Wei\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.09853\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 15 Oct 2023 15:00:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Du et al. (§72023§r)', author: 'Xingjian Du; Zhesong Yu; Jiaju Lin; Bilei Zhu; Qiuqiang Kong', display:{Lore:['[{"text": "arXiv:2310.10159", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Music and Language Attention Models for Zero-shot Music Tagging\\u00a7r\\n\\n\\u00a78\\u00a7oXingjian Du\\nZhesong Yu\\nJiaju Lin\\nBilei Zhu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.10159\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 16 Oct 2023 08:00:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o Music tagging, joint music and language attention models, Music Foundation Model. \\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Kaixing Yang; Xukun Zhou; Xulong Tang; Ran Diao; Hongyan Liu; Jun He; Zhaoxin Fan', display:{Lore:['[{"text": "arXiv:2310.10300", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBeatDance: A Beat-Based Model-Agnostic Contrastive Learning Framework for Music-Dance Retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oKaixing Yang\\nXukun Zhou\\nXulong Tang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.10300\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 16 Oct 2023 11:36:38 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Yu Chen; Xinyuan Qian; Zexu Pan; Kainan Chen; Haizhou Li', display:{Lore:['[{"text": "arXiv:2310.10497", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLocSelect: Target Speaker Localization with an Auditory Selective Hearing Mechanism\\u00a7r\\n\\n\\u00a78\\u00a7oYu Chen\\nXinyuan Qian\\nZexu Pan\\nKainan Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.10497\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 17 Oct 2023 13:52:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Novack et al. (§72023§r)', author: 'Zachary Novack; Nikita Srivatsan; Taylor Berg-Kirkpatrick; Julian McAuley', display:{Lore:['[{"text": "arXiv:2310.10772", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Lead Sheet Generation via Semantic Compression\\u00a7r\\n\\n\\u00a78\\u00a7oZachary Novack\\nNikita Srivatsan\\nTaylor Berg-Kirkpatrick\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.10772\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 16 Oct 2023 19:12:20 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Wenzhe Liu; Wei Xiao; Meng Wang; Shan Yang; Yupeng Shi; Yuyong Kang; Dan Su; Shidong Shang; Dong Yu', display:{Lore:['[{"text": "arXiv:2310.10992", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA High Fidelity and Low Complexity Neural Audio Coding\\u00a7r\\n\\n\\u00a78\\u00a7oWenzhe Liu\\nWei Xiao\\nMeng Wang\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.10992\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 17 Oct 2023 04:30:37 GMT)\\u00a7r"}']}
{title:'Morita et al. (§72023§r)', author: 'Mitsuki Morita; Masato Kikuchi; Tadachika Ozono', display:{Lore:['[{"text": "arXiv:2310.11035", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLyricist-Singer Entropy Affects Lyric-Lyricist Classification Performance\\u00a7r\\n\\n\\u00a78\\u00a7oMitsuki Morita\\nMasato Kikuchi\\nTadachika Ozono\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.11035\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 17 Oct 2023 07:02:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe 10th International Conference on Advanced Informatics: Concepts, Theory and Applications (ICAICTA 2023)\\u00a7r"}']}
{title:'Koops et al. (§72023§r)', author: 'Hendrik Vincent Koops; Gianluca Micchi; Ilaria Manco; Elio Quinton', display:{Lore:['[{"text": "arXiv:2310.11165", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSerenade: A Model for Human-in-the-loop Automatic Chord Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oHendrik Vincent Koops\\nGianluca Micchi\\nIlaria Manco\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.11165\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 17 Oct 2023 11:31:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at MMRP23. 7 pages, 5 figures, 2 tables\\u00a7r"}']}
{title:'Steinmetz et al. (§72023§r)', author: 'Christian J. Steinmetz; Thomas Walther; Joshua D. Reiss', display:{Lore:['[{"text": "arXiv:2310.11364", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHigh-Fidelity Noise Reduction with Differentiable Signal Processing\\u00a7r\\n\\n\\u00a78\\u00a7oChristian J. Steinmetz\\nThomas Walther\\nJoshua D. Reiss\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.11364\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 17 Oct 2023 16:02:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at the 155thConvention of the AudioEngineering Society\\u00a7r"}']}
{title:'López et al. (§72023§r)', author: 'Fernando López; Jordi Luque; Carlos Segura; Pablo Gómez', display:{Lore:['[{"text": "arXiv:2310.11379", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Wake-Up Word Detection by Two-stage Multi-resolution Ensembles\\u00a7r\\n\\n\\u00a78\\u00a7oFernando L\\u00f3pez\\nJordi Luque\\nCarlos Segura\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.11379\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 17 Oct 2023 16:22:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Noriy et al. (§72023§r)', author: 'Kari A Noriy; Xiaosong Yang; Marcin Budka; Jian Jun Zhang', display:{Lore:['[{"text": "arXiv:2310.11830", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCLARA: Multilingual Contrastive Learning for Audio Representation Acquisition\\u00a7r\\n\\n\\u00a78\\u00a7oKari A Noriy\\nXiaosong Yang\\nMarcin Budka\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.11830\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 1 Nov 2023 11:38:40 GMT)\\u00a7r"}']}
{title:'Karafiát et al. (§72023§r)', author: 'Martin Karafiát; Karel Veselý; Igor Szöke; Ladislav Mošner; Karel Beneš; Marcin Witkowski; Germán Barchi; Leonardo Pepino', display:{Lore:['[{"text": "arXiv:2310.11921", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBUT CHiME-7 system description\\u00a7r\\n\\n\\u00a78\\u00a7oMartin Karafi\\u00e1t\\nKarel Vesel\\u00fd\\nIgor Sz\\u00f6ke\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.11921\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 18 Oct 2023 12:26:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, Chime-7 challenge 2023\\u00a7r"}']}
{title:'Haberl et al. (§72023§r)', author: 'Armin Haberl; Jürgen Fleiß; Dominik Kowald; Stefan Thalmann', display:{Lore:['[{"text": "arXiv:2310.11967", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTake the aTrain. Introducing an Interface for the Accessible Transcription of Interviews\\u00a7r\\n\\n\\u00a78\\u00a7oArmin Haberl\\nJ\\u00fcrgen Flei\\u00df\\nDominik Kowald\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.11967\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 18 Oct 2023 13:45:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInstallvia Microsoft store: apps.microsoft.com/store/detail/atrain/9N15Q44SZNS2. Github: github.com/BANDAS-Center/aTrain\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Yixiao Zhang; Akira Maezawa; Gus Xia; Kazuhiko Yamamoto; Simon Dixon', display:{Lore:['[{"text": "arXiv:2310.12404", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLoop Copilot: Conducting AI Ensembles for Music Generation and Iterative Editing\\u00a7r\\n\\n\\u00a78\\u00a7oYixiao Zhang\\nAkira Maezawa\\nGus Xia\\nKazuhiko Yamamoto\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.12404\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 19 Oct 2023 01:20:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSource code anddemo video are available at <https://sites.google.com/view/loop-copilot>\\u00a7r"}']}
{title:'Sun et al. (§72023§r)', author: 'Wanli Sun; Zehai Tu; Anton Ragni', display:{Lore:['[{"text": "arXiv:2310.12765", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnergy-Based Models For Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oWanli Sun\\nZehai Tu\\nAnton Ragni\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.12765\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 19 Oct 2023 14:10:09 GMT)\\u00a7r"}']}
{title:'Hamza et al. (§72023§r)', author: 'Hanan Hamza; Fiza Gafoor; Fathima Sithara; Gayathri Anil; V. S. Anoop', display:{Lore:['[{"text": "arXiv:2310.12851", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmoDiarize: Speaker Diarization and Emotion Identification from Speech Signals using Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oHanan Hamza\\nFiza Gafoor\\nFathima Sithara\\nGayathri Anil\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.12851\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 19 Oct 2023 16:02:53 GMT)\\u00a7r"}']}
{title:'Paissan et al. (§72023§r)', author: 'Francesco Paissan; Zhepei Wang; Mirco Ravanelli; Paris Smaragdis; Cem Subakan', display:{Lore:['[{"text": "arXiv:2310.12858", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Editing with Non-Rigid Text Prompts\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesco Paissan\\nZhepei Wang\\nMirco Ravanelli\\nParis Smaragdis\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.12858\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 19 Oct 2023 16:09:44 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Han Zhang; Rayehe Karimi Mahabadi; Cynthia Rudin; Johann Guilleminot; L. Catherine Brinson', display:{Lore:['[{"text": "arXiv:2310.12869", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.app-ph\\u00a7r, \\u00a75physics.data-an\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUncertainty Quantification of Bandgaps in Acoustic Metamaterials with Stochastic Geometric Defects and Material Properties\\u00a7r\\n\\n\\u00a78\\u00a7oHan Zhang\\nRayehe Karimi Mahabadi\\nCynthia Rudin\\nJohann Guilleminot\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.12869\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 19 Oct 2023 16:18:10 GMT)\\u00a7r"}']}
{title:'Plaquet et al. (§72023§r)', author: 'Alexis Plaquet; Hervé Bredin', display:{Lore:['[{"text": "arXiv:2310.13025", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPowerset multi-class cross entropy loss for neural speaker diarization\\u00a7r\\n\\n\\u00a78\\u00a7oAlexis Plaquet\\nHerv\\u00e9 Bredin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.13025\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-205\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nINTERSPEECH 2023, Aug 2023, Dublin, Ireland. pp.3222-3226\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 19 Oct 2023 06:51:43 GMT)\\u00a7r"}']}
{title:'Akesbi et al. (§72023§r)', author: 'Kamil Akesbi; Dorian Desblancs; Benjamin Martin', display:{Lore:['[{"text": "arXiv:2310.13388", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Augmentation and Denoising For Peak-Based Audio Fingerprinting\\u00a7r\\n\\n\\u00a78\\u00a7oKamil Akesbi\\nDorian Desblancs\\nBenjamin Martin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.13388\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 29 Oct 2023 10:48:51 GMT)\\u00a7r"}']}
{title:'Jedrusiak et al. (§72023§r)', author: 'Mikel D. Jedrusiak; Thomas Harweg; Timo Haselhoff; Bryce T. Lawrence; Susanne Moebus; Frank Weichert', display:{Lore:['[{"text": "arXiv:2310.13404", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDefinition-independent Formalization of Soundscapes: Towards a Formal Methodology\\u00a7r\\n\\n\\u00a78\\u00a7oMikel D. Jedrusiak\\nThomas Harweg\\nTimo Haselhoff\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.13404\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 20 Oct 2023 10:22:15 GMT)\\u00a7r"}']}
{title:'Zeng et al. (§72023§r)', author: 'Donghuo Zeng; Kazushi Ikeda', display:{Lore:['[{"text": "arXiv:2310.13451", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTwo-Stage Triplet Loss Training with Curriculum Augmentation for Audio-Visual Retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oDonghuo Zeng\\nKazushi Ikeda\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.13451\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 20 Oct 2023 12:35:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 6 figures\\u00a7r"}']}
{title:'Sridhar et al. (§72023§r)', author: 'Sripathi Sridhar; Mark Cartwright', display:{Lore:['[{"text": "arXiv:2310.13759", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-label Open-set Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oSripathi Sridhar\\nMark Cartwright\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.13759\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 20 Oct 2023 18:43:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at the Workshop on Detection and Classification of Acoustic Scenes and Events, 2023 (DCASE 2023)\\u00a7r"}']}
{title:'Kobayashi et al. (§72023§r)', author: 'Tatsuki Kobayashi; Yoshiko Maruyama; Isao Nambu; Shohei Yano; Yasuhiro Wada', display:{Lore:['[{"text": "arXiv:2310.14018", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTemporal convolutional neural networks to generate a head-related impulse response from one direction to another\\u00a7r\\n\\n\\u00a78\\u00a7oTatsuki Kobayashi\\nYoshiko Maruyama\\nIsao Nambu\\nShohei Yano\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.14018\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 21 Oct 2023 14:02:23 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Jincheng Zhang; György Fazekas; Charalampos Saitis', display:{Lore:['[{"text": "arXiv:2310.14040", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast Diffusion GAN Model for Symbolic Music Generation Controlled by Emotions\\u00a7r\\n\\n\\u00a78\\u00a7oJincheng Zhang\\nGy\\u00f6rgy Fazekas\\nCharalampos Saitis\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.14040\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 21 Oct 2023 15:35:43 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Jincheng Zhang; Jingjing Tang; Charalampos Saitis; György Fazekas', display:{Lore:['[{"text": "arXiv:2310.14044", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComposer Style-specific Symbolic Music Generation Using Vector Quantized Discrete Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oJincheng Zhang\\nJingjing Tang\\nCharalampos Saitis\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.14044\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 21 Oct 2023 15:41:50 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Zhongliang Chen; Zhuofei Huang; Wenxiong Kang', display:{Lore:['[{"text": "arXiv:2310.14796", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Novel Transfer Learning Method Utilizing Acoustic and Vibration Signals for Rotating Machinery Fault Diagnosis\\u00a7r\\n\\n\\u00a78\\u00a7oZhongliang Chen\\nZhuofei Huang\\nWenxiong Kang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.14796\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 20 Oct 2023 10:50:14 GMT)\\u00a7r"}']}
{title:'Karystinaios et al. (§72023§r)', author: 'Emmanouil Karystinaios; Francesco Foscarin; Florent Jacquemard; Masahiko Sakai; Satoshi Tojo; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2310.14952", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l8+8=4: Formalizing Time Units to Handle Symbolic Music Durations\\u00a7r\\n\\n\\u00a78\\u00a7oEmmanouil Karystinaios\\nFrancesco Foscarin\\nFlorent Jacquemard\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.14952\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 23 Oct 2023 13:54:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of the International Symposium on Computer Music Multidisciplinary Research (CMMR 2023), Tokyo, Japan\\u00a7r"}']}
{title:'Fan et al. (§72023§r)', author: 'Peng Fan; Changhao Shan; Sining Sun; Qing Yang; Jianwei Zhang', display:{Lore:['[{"text": "arXiv:2310.14954", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKey Frame Mechanism For Efficient Conformer Based End-to-end Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oPeng Fan\\nChanghao Shan\\nSining Sun\\nQing Yang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.14954\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2023.3327585\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 28 Oct 2023 14:38:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis manuscript has been accepted by IEEE SignalProcessing Letters forpublication\\u00a7r"}']}
{title:'Ahn et al. (§72023§r)', author: 'Byeongjoo Ahn; Karren Yang; Brian Hamilton; Jonathan Sheaffer; Anurag Ranjan; Miguel Sarabia; Oncel Tuzel; Jen-Hao Rick Chang', display:{Lore:['[{"text": "arXiv:2310.15130", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNovel-View Acoustic Synthesis from 3D Reconstructed Rooms\\u00a7r\\n\\n\\u00a78\\u00a7oByeongjoo Ahn\\nKarren Yang\\nBrian Hamilton\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.15130\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 23 Oct 2023 17:34:31 GMT)\\u00a7r"}']}
{title:'Comunità et al. (§72023§r)', author: 'Marco Comunità; Riccardo F. Gramaccioni; Emilian Postolache; Emanuele Rodolà; Danilo Comminiello; Joshua D. Reiss', display:{Lore:['[{"text": "arXiv:2310.15247", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSyncFusion: Multimodal Onset-synchronized Video-to-Audio Foley Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oMarco Comunit\\u00e0\\nRiccardo F. Gramaccioni\\nEmilian Postolache\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.15247\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 23 Oct 2023 18:01:36 GMT)\\u00a7r"}']}
{title:'Krishna et al. (§72023§r)', author: 'Gautam Krishna; Sameer Dharur; Oggi Rudovic; Pranay Dighe; Saurabh Adya; Ahmed Hussen Abdelaziz; Ahmed H Tewfik', display:{Lore:['[{"text": "arXiv:2310.15261", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModality Dropout for Multimodal Device Directed Speech Detection using Verbal and Non-Verbal Features\\u00a7r\\n\\n\\u00a78\\u00a7oGautam Krishna\\nSameer Dharur\\nOggi Rudovic\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.15261\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 23 Oct 2023 18:09:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Schmid et al. (§72023§r)', author: 'Florian Schmid; Khaled Koutini; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2310.15648", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDynamic Convolutional Neural Networks as Efficient Pre-trained Audio Models\\u00a7r\\n\\n\\u00a78\\u00a7oFlorian Schmid\\nKhaled Koutini\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.15648\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 24 Oct 2023 09:08:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEE/ACM Transactionson Audio, Speech, and Language Processing. Source Code available at: https://github.com/fschmid56/EfficientAT\\u00a7r"}']}
{title:'Sun et al. (§72023§r)', author: 'Mengyi Sun; Ming Gao; Xinchen Kang; Shiru Wang; Jun Du; Dengfeng Yao; Su-Jing Wang', display:{Lore:['[{"text": "arXiv:2310.15930", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCDSD: Chinese Dysarthria Speech Database\\u00a7r\\n\\n\\u00a78\\u00a7oMengyi Sun\\nMing Gao\\nXinchen Kang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.15930\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 24 Oct 2023 15:27:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 3 figures\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Youshan Zhang; Jialu Li', display:{Lore:['[{"text": "arXiv:2310.16109", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComplex Image Generation SwinTransformer Network for Audio Denoising\\u00a7r\\n\\n\\u00a78\\u00a7oYoushan Zhang\\nJialu Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.16109\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 24 Oct 2023 18:21:03 GMT)\\u00a7r"}']}
{title:'Prabhune et al. (§72023§r)', author: 'Tejas S. Prabhune; Peter Wu; Bohan Yu; Gopala K. Anumanchipalli', display:{Lore:['[{"text": "arXiv:2310.16287", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Streaming Speech-to-Avatar Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oTejas S. Prabhune\\nPeter Wu\\nBohan Yu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.16287\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 25 Oct 2023 01:45:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Zhao et al. (§72023§r)', author: 'Jingwei Zhao; Gus Xia; Ye Wang', display:{Lore:['[{"text": "arXiv:2310.16334", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAccoMontage-3: Full-Band Accompaniment Arrangement via Sequential Style Transfer and Multi-Track Function Prior\\u00a7r\\n\\n\\u00a78\\u00a7oJingwei Zhao\\nGus Xia\\nYe Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.16334\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 25 Oct 2023 03:30:37 GMT)\\u00a7r"}']}
{title:'Drgas et al. (§72023§r)', author: 'Szymon Drgas; Lars Bramsløw; Archontis Politis; Gaurav Naithani; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2310.16550", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDynamic Processing Neural Network Architecture For Hearing Loss Compensation\\u00a7r\\n\\n\\u00a78\\u00a7oSzymon Drgas\\nLars Bramsl\\u00f8w\\nArchontis Politis\\nGaurav Naithani\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.16550\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 25 Oct 2023 11:04:32 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Jianwei Zhang; Suren Jayasuriya; Visar Berisha', display:{Lore:['[{"text": "arXiv:2310.17049", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Repeatable Speech Embeddings Using An Intra-class Correlation Regularizer\\u00a7r\\n\\n\\u00a78\\u00a7oJianwei Zhang\\nSuren Jayasuriya\\nVisar Berisha\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.17049\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 25 Oct 2023 23:21:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by NeurIPS 2023\\u00a7r"}']}
{title:'Lux et al. (§72023§r)', author: 'Florian Lux; Pascal Tilli; Sarina Meyer; Ngoc Thang Vu', display:{Lore:['[{"text": "arXiv:2310.17502", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lControllable Generation of Artificial Speaker Embeddings through Discovery of Principal Directions\\u00a7r\\n\\n\\u00a78\\u00a7oFlorian Lux\\nPascal Tilli\\nSarina Meyer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.17502\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-858\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 26 Oct 2023 15:54:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at ISCAInterspeech 2023 https://www.isca-speech.org/archive/interspeech_2023/lux23_interspeech.html\\u00a7r"}']}
{title:'Kumar et al. (§72023§r)', author: 'Neeraj Kumar; Ankur Narang; Brejesh Lall', display:{Lore:['[{"text": "arXiv:2310.18169", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStyle Description based Text-to-Speech with Conditional Prosodic Layer Normalization based Diffusion GAN\\u00a7r\\n\\n\\u00a78\\u00a7oNeeraj Kumar\\nAnkur Narang\\nBrejesh Lall\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.18169\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 27 Oct 2023 14:28:41 GMT)\\u00a7r"}']}
{title:'Dutta et al. (§72023§r)', author: 'Shruti Dutta; Shashwat Mookherjee', display:{Lore:['[{"text": "arXiv:2310.19052", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring the Emotional Landscape of Music: An Analysis of Valence Trends and Genre Variations in Spotify Music Data\\u00a7r\\n\\n\\u00a78\\u00a7oShruti Dutta\\nShashwat Mookherjee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.19052\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 29 Oct 2023 15:57:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, Accepted at the 18th International Conference for Internet Technology and Secured Transactions, 13-15 November, 2023, St Anne\'s College, Oxford, UK\\u00a7r"}']}
{title:'Puglisi et al. (§72023§r)', author: 'Valerio Francesco Puglisi; Oliver Giudice; Sebastiano Battiato', display:{Lore:['[{"text": "arXiv:2310.19081", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Audio Analyzer: a Framework to Industrialize the Research on Audio Forensics\\u00a7r\\n\\n\\u00a78\\u00a7oValerio Francesco Puglisi\\nOliver Giudice\\nSebastiano Battiato\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.19081\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 29 Oct 2023 17:04:24 GMT)\\u00a7r"}']}
{title:'Yao et al. (§72023§r)', author: 'Yao Yao; Peike Li; Boyu Chen; Alex Wang', display:{Lore:['[{"text": "arXiv:2310.19180", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oYao Yao\\nPeike Li\\nBoyu Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.19180\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 3 Nov 2023 02:27:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprints\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Junhui Li; Pu Wang; Jialu Li; Xinzhe Wang; Youshan Zhang', display:{Lore:['[{"text": "arXiv:2310.19588", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDPATD: Dual-Phase Audio Transformer for Denoising\\u00a7r\\n\\n\\u00a78\\u00a7oJunhui Li\\nPu Wang\\nJialu Li\\nXinzhe Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.19588\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 Oct 2023 14:44:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE DDP\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Jialu Li; Junhui Li; Pu Wang; Youshan Zhang', display:{Lore:['[{"text": "arXiv:2310.19602", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDCHT: Deep Complex Hybrid Transformer for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oJialu Li\\nJunhui Li\\nPu Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.19602\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 Oct 2023 14:58:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE DDP conference\\u00a7r"}']}
{title:'Atassi (§72023§r)', author: 'Lilac Atassi', display:{Lore:['[{"text": "arXiv:2310.19842", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusical Form Generation\\u00a7r\\n\\n\\u00a78\\u00a7oLilac Atassi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.19842\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 Oct 2023 08:02:08 GMT)\\u00a7r"}']}
{title:'Ye et al. (§72023§r)', author: 'Yuxin Ye; Wenming Yang; Yapeng Tian', display:{Lore:['[{"text": "arXiv:2310.20446", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLAVSS: Location-Guided Audio-Visual Spatial Audio Separation\\u00a7r\\n\\n\\u00a78\\u00a7oYuxin Ye\\nWenming Yang\\nYapeng Tian\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.20446\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 31 Oct 2023 13:30:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by WACV2024\\u00a7r"}']}
{title:'Weiying et al. (§72023§r)', author: 'Wang Weiying; Nakajima Akinori', display:{Lore:['[{"text": "arXiv:2311.00301", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetecting Syllable-Level Pronunciation Stress with A Self-Attention Model\\u00a7r\\n\\n\\u00a78\\u00a7oWang Weiying\\nNakajima Akinori\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.00301\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Nov 2023 05:05:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osource codes available at https://github.com/wangweiying303/stress-detection-model\\u00a7r"}']}
{title:'Veluri et al. (§72023§r)', author: 'Bandhav Veluri; Malek Itani; Justin Chan; Takuya Yoshioka; Shyamnath Gollakota', display:{Lore:['[{"text": "arXiv:2311.00320", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemantic Hearing: Programming Acoustic Scenes with Binaural Hearables\\u00a7r\\n\\n\\u00a78\\u00a7oBandhav Veluri\\nMalek Itani\\nJustin Chan\\nTakuya Yoshioka\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.00320\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3586183.3606779\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Nov 2023 06:07:28 GMT)\\u00a7r"}']}
{title:'Neururer et al. (§72023§r)', author: 'Daniel Neururer; Volker Dellwo; Thilo Stadelmann', display:{Lore:['[{"text": "arXiv:2311.00489", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Neural Networks for Automatic Speaker Recognition Do Not Learn Supra-Segmental Temporal Features\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Neururer\\nVolker Dellwo\\nThilo Stadelmann\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.00489\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 2 Nov 2023 06:07:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 2 figures, 3 tables\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'kai Wu; Yuanyuan Chen', display:{Lore:['[{"text": "arXiv:2311.00535", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lActive Noise Control Portable Device Design\\u00a7r\\n\\n\\u00a78\\u00a7okai Wu\\nYuanyuan Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.00535\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Nov 2023 14:13:04 GMT)\\u00a7r"}']}
{title:'Levy et al. (§72023§r)', author: 'Mark Levy; Bruno Di Giorgi; Floris Weers; Angelos Katharopoulos; Tom Nickson', display:{Lore:['[{"text": "arXiv:2311.00613", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lControllable Music Production with Diffusion Models and Guidance Gradients\\u00a7r\\n\\n\\u00a78\\u00a7oMark Levy\\nBruno Di Giorgi\\nFloris Weers\\nAngelos Katharopoulos\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.00613\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 5 Dec 2023 10:32:03 GMT)\\u00a7r"}']}
{title:'Thakkar et al. (§72023§r)', author: 'Karan Thakkar; Jiarui Hai; Mounya Elhilali', display:{Lore:['[{"text": "arXiv:2311.00814", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating Self-Supervised Deep Representations for EEG-based Auditory Attention Decoding\\u00a7r\\n\\n\\u00a78\\u00a7oKaran Thakkar\\nJiarui Hai\\nMounya Elhilali\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.00814\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 7 Nov 2023 18:53:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Sadov et al. (§72023§r)', author: 'Konstantine Sadov; Matthew Hutter; Asara Near', display:{Lore:['[{"text": "arXiv:2311.00873", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-latency Real-time Voice Conversion on CPU\\u00a7r\\n\\n\\u00a78\\u00a7oKonstantine Sadov\\nMatthew Hutter\\nAsara Near\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.00873\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Nov 2023 21:57:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 2 figures\\u00a7r"}']}
{title:'Chang et al. (§72023§r)', author: 'Ernie Chang; Pin-Jie Lin; Yang Li; Sidd Srinivasan; Gael Le Lan; David Kant; Yangyang Shi; Forrest Iandola; Vikas Chandra', display:{Lore:['[{"text": "arXiv:2311.00895", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIn-Context Prompt Editing For Conditional Audio Generation\\u00a7r\\n\\n\\u00a78\\u00a7oErnie Chang\\nPin-Jie Lin\\nYang Li\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.00895\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Nov 2023 23:31:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 2 tables\\u00a7r"}']}
{title:'Chang et al. (§72023§r)', author: 'Ernie Chang; Sidd Srinivasan; Mahi Luthra; Pin-Jie Lin; Varun Nagaraja; Forrest Iandola; Zechun Liu; Zhaoheng Ni; Changsheng Zhao; Yangyang Shi; Vikas Chandra', display:{Lore:['[{"text": "arXiv:2311.00897", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn The Open Prompt Challenge In Conditional Audio Generation\\u00a7r\\n\\n\\u00a78\\u00a7oErnie Chang\\nSidd Srinivasan\\nMahi Luthra\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.00897\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Nov 2023 23:33:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 4 tables\\u00a7r"}']}
{title:'Gao et al. (§72023§r)', author: 'Yuan Gao; Nobuyuki Morioka; Yu Zhang; Nanxin Chen', display:{Lore:['[{"text": "arXiv:2311.00945", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lE3 TTS: Easy End-to-End Diffusion-based Text to Speech\\u00a7r\\n\\n\\u00a78\\u00a7oYuan Gao\\nNobuyuki Morioka\\nYu Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.00945\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 Nov 2023 02:22:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ASRU 2023\\u00a7r"}']}
{title:'Singh et al. (§72023§r)', author: 'Shubhr Singh; Christian J. Steinmetz; Emmanouil Benetos; Huy Phan; Dan Stowell', display:{Lore:['[{"text": "arXiv:2311.01526", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lATGNN: Audio Tagging Graph Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oShubhr Singh\\nChristian J. Steinmetz\\nEmmanouil Benetos\\nHuy Phan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.01526\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 Nov 2023 18:19:26 GMT)\\u00a7r"}']}
{title:'Yeh et al. (§72023§r)', author: 'Ching-Feng Yeh; Po-Yao Huang; Vasu Sharma; Shang-Wen Li; Gargi Gosh', display:{Lore:['[{"text": "arXiv:2311.01615", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFLAP: Fast Language-Audio Pre-training\\u00a7r\\n\\n\\u00a78\\u00a7oChing-Feng Yeh\\nPo-Yao Huang\\nVasu Sharma\\nShang-Wen Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.01615\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 Nov 2023 21:58:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages\\u00a7r"}']}
{title:'Nguyen et al. (§72023§r)', author: 'Phuc Duc Nguyen; Kenji Ishikawa; Noboru Harada; Takehiro Moriya', display:{Lore:['[{"text": "arXiv:2311.01715", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcousto-optic reconstruction of exterior sound field based on concentric circle sampling with circular harmonic expansion\\u00a7r\\n\\n\\u00a78\\u00a7oPhuc Duc Nguyen\\nKenji Ishikawa\\nNoboru Harada\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.01715\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 3 Nov 2023 05:19:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\\u00a7r"}']}
{title:'Riley et al. (§72023§r)', author: 'Xavier Riley; Simon Dixon', display:{Lore:['[{"text": "arXiv:2311.02023", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFiloBass: A Dataset and Corpus Based Study of Jazz Basslines\\u00a7r\\n\\n\\u00a78\\u00a7oXavier Riley\\nSimon Dixon\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.02023\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 24th International Society for Music\\n  Information Retrieval Conference, ISMIR 2023, Milan, Italy\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 3 Nov 2023 16:36:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oISMIR 2023\\u00a7r"}']}
{title:'Sebastian et al. (§72023§r)', author: 'Abhishek Sebastian; R Pragna; K Vishal Vythianathan; Dasaraju Sohan Sai; U Shiva Sri Hari Al; R Anirudh; Apurv Choudhary', display:{Lore:['[{"text": "arXiv:2311.02087", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDesign Of Rubble Analyzer Probe Using ML For Earthquake\\u00a7r\\n\\n\\u00a78\\u00a7oAbhishek Sebastian\\nR Pragna\\nK Vishal Vythianathan\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.02087\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1063/5.0178244\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 24 Oct 2023 14:43:42 GMT)\\u00a7r"}']}
{title:'Ahmadnejad et al. (§72023§r)', author: 'Amirreza Ahmadnejad; Ahmad Mahmmodian Darviishani; Mohmmad Mehrdad Asadi; Sajjad Saffariyeh; Pedram Yousef; Emad Fatemizadeh', display:{Lore:['[{"text": "arXiv:2311.02369", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTACNET: Temporal Audio Source Counting Network\\u00a7r\\n\\n\\u00a78\\u00a7oAmirreza Ahmadnejad\\nAhmad Mahmmodian Darviishani\\nMohmmad Mehrdad Asadi\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.02369\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 4 Nov 2023 10:48:14 GMT)\\u00a7r"}']}
{title:'Elluru et al. (§72023§r)', author: 'Veera Raghavendra Elluru; Devang Kulshreshtha; Rohit Paturi; Sravan Bodapati; Srikanth Ronanki', display:{Lore:['[{"text": "arXiv:2311.02482", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeneralized zero-shot audio-to-intent classification\\u00a7r\\n\\n\\u00a78\\u00a7oVeera Raghavendra Elluru\\nDevang Kulshreshtha\\nRohit Paturi\\nSravan Bodapati\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.02482\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 4 Nov 2023 18:55:08 GMT)\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Sungho Lee; Hyeong-Seok Choi; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2311.02581", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lYet Another Generative Model For Room Impulse Response Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oSungho Lee\\nHyeong-Seok Choi\\nKyogu Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.02581\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 5 Nov 2023 07:25:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWASPAA 2023\\u00a7r"}']}
{title:'Won et al. (§72023§r)', author: 'Minz Won; Yun-Ning Hung; Duc Le', display:{Lore:['[{"text": "arXiv:2311.03318", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Foundation Model for Music Informatics\\u00a7r\\n\\n\\u00a78\\u00a7oMinz Won\\nYun-Ning Hung\\nDuc Le\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.03318\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 Nov 2023 18:12:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Krishnan et al. (§72023§r)', author: 'Karthik Sivarama Krishnan; Koushik Sivarama Krishnan', display:{Lore:['[{"text": "arXiv:2311.03509", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMFAAN: Unveiling Audio Deepfakes with a Multi-Feature Authenticity Network\\u00a7r\\n\\n\\u00a78\\u00a7oKarthik Sivarama Krishnan\\nKoushik Sivarama Krishnan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.03509\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICSC60394.2023.10441405\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 Nov 2023 20:32:39 GMT)\\u00a7r"}']}
{title:'Carvalho et al. (§72023§r)', author: 'Nádia Carvalho; Gilberto Bernardes', display:{Lore:['[{"text": "arXiv:2311.03621", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Latent Spaces of Tonal Music using Variational Autoencoders\\u00a7r\\n\\n\\u00a78\\u00a7oN\\u00e1dia Carvalho\\nGilberto Bernardes\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.03621\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Nov 2023 00:15:29 GMT)\\u00a7r"}']}
{title:'Jain et al. (§72023§r)', author: 'Rishabh Jain; Peter Corcoran', display:{Lore:['[{"text": "arXiv:2311.04313", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Child Text-to-Speech Synthesis through Fastpitch-based Transfer Learning\\u00a7r\\n\\n\\u00a78\\u00a7oRishabh Jain\\nPeter Corcoran\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.04313\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Nov 2023 19:31:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented in SpeD 23\\u00a7r"}']}
{title:'Bressler et al. (§72023§r)', author: 'Noam Bressler; Michael Faran; Amit Galor; Michael Moshe Michelashvili; Tomer Nachshon; Noa Weiss', display:{Lore:['[{"text": "arXiv:2311.04343", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSoundbay: Deep Learning Framework for Marine Mammals and Bioacoustic Research\\u00a7r\\n\\n\\u00a78\\u00a7oNoam Bressler\\nMichael Faran\\nAmit Galor\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.04343\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Nov 2023 20:55:26 GMT)\\u00a7r"}']}
{title:'Ameer et al. (§72023§r)', author: 'Huma Ameer; Seemab Latif; Rabia Latif; Sana Mukhtar', display:{Lore:['[{"text": "arXiv:2311.05203", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhisper in Focus: Enhancing Stuttered Speech Classification with Encoder Layer Optimization\\u00a7r\\n\\n\\u00a78\\u00a7oHuma Ameer\\nSeemab Latif\\nRabia Latif\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.05203\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Nov 2023 08:32:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 6 figures, 6 tables, journal paper\\u00a7r"}']}
{title:'Lin et al. (§72023§r)', author: 'David Chuan-En Lin; Nikolas Martelaro', display:{Lore:['[{"text": "arXiv:2311.05609", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhat Do I Hear? Generating Sounds for Visuals with ChatGPT\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Chuan-En Lin\\nNikolas Martelaro\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.05609\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Nov 2023 18:59:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDemo: http://soundify.cc\\u00a7r"}']}
{title:'Downward et al. (§72023§r)', author: 'Blake Downward; Jon Nordby', display:{Lore:['[{"text": "arXiv:2311.06368", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe AeroSonicDB (YPAD-0523) Dataset for Acoustic Detection and Classification of Aircraft\\u00a7r\\n\\n\\u00a78\\u00a7oBlake Downward\\nJon Nordby\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.06368\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 10 Nov 2023 19:41:10 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'June-Woo Kim; Chihyeon Yoon; Miika Toikkanen; Sangmin Bae; Ho-Young Jung', display:{Lore:['[{"text": "arXiv:2311.06480", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Fine-tuning using Generated Respiratory Sound to Address Class Imbalance\\u00a7r\\n\\n\\u00a78\\u00a7oJune-Woo Kim\\nChihyeon Yoon\\nMiika Toikkanen\\nSangmin Bae\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.06480\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 11 Nov 2023 05:02:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted in NeurIPS 2023 Workshopon Deep Generative Models for Health (DGM4H)\\u00a7r"}']}
{title:'Vhaduri et al. (§72023§r)', author: 'Sudip Vhaduri; Seungyeon Paik; Jessica E Huber', display:{Lore:['[{"text": "arXiv:2311.06707", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransfer Learning to Detect COVID-19 Coughs with Incremental Addition of Patient Coughs to Healthy People\'s Cough Detection Models\\u00a7r\\n\\n\\u00a78\\u00a7oSudip Vhaduri\\nSeungyeon Paik\\nJessica E Huber\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.06707\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 12 Nov 2023 02:01:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper has been accepted to publish at EAI International Conference on Wireless Mobile Communication and Healthcare (MobiHealth\'23)\\u00a7r"}']}
{title:'Shao et al. (§72023§r)', author: 'Qijie Shao; Pengcheng Guo; Jinghao Yan; Pengfei Hu; Lei Xie', display:{Lore:['[{"text": "arXiv:2311.07062", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDecoupling and Interacting Multi-Task Learning Network for Joint Speech and Accent Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oQijie Shao\\nPengcheng Guo\\nJinghao Yan\\nPengfei Hu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.07062\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3332542\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 17 Nov 2023 09:30:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE Transactions on Audio, Speech and Language Processing (TASLP)\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Shih-Lun Wu; Chris Donahue; Shinji Watanabe; Nicholas J. Bryan', display:{Lore:['[{"text": "arXiv:2311.07069", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic ControlNet: Multiple Time-varying Controls for Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oShih-Lun Wu\\nChris Donahue\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.07069\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Nov 2023 04:24:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 4 figure, 5 tables, Submitted to IEEE/ACM Transactionson Audio, Speech, and Language Processing (TASLP)\\u00a7r"}']}
{title:'Shi et al. (§72023§r)', author: 'Xiaohan Shi; Jiajun He; Xingfeng Li; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2311.07093", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Effectiveness of ASR Representations in Real-world Noisy Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXiaohan Shi\\nJiajun He\\nXingfeng Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.07093\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 14 Nov 2023 13:09:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Weng et al. (§72023§r)', author: 'Jinbao Weng; Yubo Qi; Yanming Yang; Hongtao Wen; Hongtao Zhou; Ruichao Xue', display:{Lore:['[{"text": "arXiv:2311.07175", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.NA\\u00a7r, \\u00a72math.NA\\u00a7r, \\u00a75physics.ao-ph\\u00a7r, \\u00a75physics.app-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lResearch and experimental verification on low-frequency long-range sound propagation characteristics under ice-covered and range-dependent marine environment in the Arctic\\u00a7r\\n\\n\\u00a78\\u00a7oJinbao Weng\\nYubo Qi\\nYanming Yang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.07175\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Nov 2023 09:08:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o46 pages, 35 figures\\u00a7r"}']}
{title:'Grumiaux et al. (§72023§r)', author: 'Pierre-Amaury Grumiaux; Mathieu Lagrange', display:{Lore:['[{"text": "arXiv:2311.07363", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient bandwidth extension of musical signals using a differentiable harmonic plus noise model\\u00a7r\\n\\n\\u00a78\\u00a7oPierre-Amaury Grumiaux\\nMathieu Lagrange\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.07363\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 27 Nov 2023 11:36:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepting for publication in EURASIP Journal on Audio, Speech, and Music Processing\\u00a7r"}']}
{title:'Gha et al. (§72023§r)', author: 'Joonsu Gha; Vincent Herrmann; Benjamin Grewe; Jürgen Schmidhuber; Anand Gopalakrishnan', display:{Lore:['[{"text": "arXiv:2311.07534", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Musical Object Discovery from Audio\\u00a7r\\n\\n\\u00a78\\u00a7oJoonsu Gha\\nVincent Herrmann\\nBenjamin Grewe\\nJ\\u00fcrgen Schmidhuber\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.07534\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 14 Nov 2023 08:15:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Machine Learning for Audio Workshop, NeurIPS 2023\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Zhaojian Li; Bin Zhao; Yuan Yuan', display:{Lore:['[{"text": "arXiv:2311.07630", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-modal Generative Model for Visual-Guided Binaural Stereo Generation\\u00a7r\\n\\n\\u00a78\\u00a7oZhaojian Li\\nBin Zhao\\nYuan Yuan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.07630\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Nov 2023 09:53:14 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Mengfei Zhang; Junqing Zhang; Jie Chen; Cédric Richard', display:{Lore:['[{"text": "arXiv:2311.07729", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.app-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDistributed pressure matching strategy using diffusion adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oMengfei Zhang\\nJunqing Zhang\\nJie Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.07729\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Nov 2023 20:24:13 GMT)\\u00a7r"}']}
{title:'Duan et al. (§72023§r)', author: 'Rui Duan; Zhe Qu; Leah Ding; Yao Liu; Zhuo Lu', display:{Lore:['[{"text": "arXiv:2311.07780", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParrot-Trained Adversarial Examples: Pushing the Practicality of Black-Box Audio Attacks against Speaker Recognition Models\\u00a7r\\n\\n\\u00a78\\u00a7oRui Duan\\nZhe Qu\\nLeah Ding\\nYao Liu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.07780\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 17 Nov 2023 21:34:33 GMT)\\u00a7r"}']}
{title:'Bargum et al. (§72023§r)', author: 'Anders R. Bargum; Stefania Serafin; Cumhur Erkut', display:{Lore:['[{"text": "arXiv:2311.08104", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReimagining Speech: A Scoping Review of Deep Learning-Powered Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oAnders R. Bargum\\nStefania Serafin\\nCumhur Erkut\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.08104\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Nov 2023 12:03:46 GMT)\\u00a7r"}']}
{title:'Bryan-Kinns et al. (§72023§r)', author: 'Nick Bryan-Kinns; Bingyuan Zhang; Songyan Zhao; Berker Banar', display:{Lore:['[{"text": "arXiv:2311.08336", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Variational Auto-Encoder Architectures, Configurations, and Datasets for Generative Music Explainable AI\\u00a7r\\n\\n\\u00a78\\u00a7oNick Bryan-Kinns\\nBingyuan Zhang\\nSongyan Zhao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.08336\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s11633-023-1457-1\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Nov 2023 17:27:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint. Springer MIR journal submission under review\\u00a7r"}']}
{title:'Narang et al. (§72023§r)', author: 'Jyoti Narang; Viviana De La Vega; Xavier Lizarraga; Oscar Mayor; Hector Parra; Jordi Janer; Xavier Serra', display:{Lore:['[{"text": "arXiv:2311.08350", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChoralSynth: Synthetic Dataset of Choral Singing\\u00a7r\\n\\n\\u00a78\\u00a7oJyoti Narang\\nViviana De La Vega\\nXavier Lizarraga\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.08350\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 21 Nov 2023 22:09:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDataset Link: https://doi.org/10.5281/zenodo.10137883\\u00a7r"}']}
{title:'Weng et al. (§72023§r)', author: 'Jinbao Weng; Yubo Qi; Yanming Yang; Hongtao Wen; Hongtao Zhou; Ruichao Xue', display:{Lore:['[{"text": "arXiv:2311.08425", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.NA\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a72math.NA\\u00a7r, \\u00a75physics.ao-ph\\u00a7r, \\u00a75physics.app-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lResearch and experimental verification on low-frequency long-range underwater sound propagation dispersion characteristics under dual-channel sound speed profiles in the Chukchi Plateau\\u00a7r\\n\\n\\u00a78\\u00a7oJinbao Weng\\nYubo Qi\\nYanming Yang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.08425\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Nov 2023 09:21:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o30 pages, 18 figures\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Ge Zhu; Yutong Wen; Marc-André Carbonneau; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2311.08667", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEDMSound: Spectrogram Based Diffusion Models for Efficient and High-Quality Audio Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oGe Zhu\\nYutong Wen\\nMarc-Andr\\u00e9 Carbonneau\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.08667\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 18 Nov 2023 15:16:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at NeurIPS Workshop: Machine Learning for Audio (Camera Ready)\\u00a7r"}']}
{title:'Deng et al. (§72023§r)', author: 'Yimin Deng; Xulong Zhang; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2311.08670", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCLN-VC: Text-Free Voice Conversion Based on Fine-Grained Style Control and Contrastive Learning with Negative Samples Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oYimin Deng\\nXulong Zhang\\nJianzong Wang\\nNing Cheng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.08670\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Nov 2023 03:29:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the 21st IEEE InternationalSymposium on Parallel and Distributed Processing with Applications (IEEE ISPA 2023)\\u00a7r"}']}
{title:'Zhou et al. (§72023§r)', author: 'Yifan Zhou; Dongxing Xu; Haoran Wei; Yanhua Long', display:{Lore:['[{"text": "arXiv:2311.08829", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutoencoder with Group-based Decoder and Multi-task Optimization for Anomalous Sound Detection\\u00a7r\\n\\n\\u00a78\\u00a7oYifan Zhou\\nDongxing Xu\\nHaoran Wei\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.08829\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Nov 2023 10:15:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to the 2024 IEEE InternationalConference on Acoustics, Speech, and Signal Processing (ICASSP 2024)\\u00a7r"}']}
{title:'Riley et al. (§72023§r)', author: 'Xavier Riley; Simon Dixon', display:{Lore:['[{"text": "arXiv:2311.08884", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCREPE Notes: A new method for segmenting pitch contours into discrete notes\\u00a7r\\n\\n\\u00a78\\u00a7oXavier Riley\\nSimon Dixon\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.08884\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5281/zenodo.8136568\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 20th Sound and Music Computing Conference. June\\n  15-17, 2023. Stockholm, Sweden\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Nov 2023 11:43:48 GMT)\\u00a7r"}']}
{title:'Kroher et al. (§72023§r)', author: 'Nadine Kroher; Helena Cuesta; Aggelos Pikrakis', display:{Lore:['[{"text": "arXiv:2311.09094", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCan MusicGen Create Training Data for MIR Tasks?\\u00a7r\\n\\n\\u00a78\\u00a7oNadine Kroher\\nHelena Cuesta\\nAggelos Pikrakis\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.09094\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Nov 2023 16:41:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis is an extended abstract presented at the Late-Breaking / Demo Session of the International Society for Music Information Retrieval Conference (ISMIR) 2023 (Milan, Italy)\\u00a7r"}']}
{title:'Lu et al. (§72023§r)', author: 'Jiajun Lu; Hao Zhang; Pengfei Wu; Sijia Li; Wei Huang', display:{Lore:['[{"text": "arXiv:2311.09537", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFuture Full-Ocean Deep SSPs Prediction based on Hierarchical Long Short-Term Memory Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oJiajun Lu\\nHao Zhang\\nPengfei Wu\\nSijia Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.09537\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Nov 2023 03:26:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2310.09522\\u00a7r"}']}
{title:'Pankov et al. (§72023§r)', author: 'Vikentii Pankov; Valeria Pronina; Alexander Kuzmin; Maksim Borisov; Nikita Usoltsev; Xingshan Zeng; Alexander Golubkov; Nikolai Ermolenko; Aleksandra Shirshova; Yulia Matveeva', display:{Lore:['[{"text": "arXiv:2311.09770", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDINO-VITS: Data-Efficient Noise-Robust Zero-Shot Voice Cloning via Multi-Tasking with Self-Supervised Speaker Verification Loss\\u00a7r\\n\\n\\u00a78\\u00a7oVikentii Pankov\\nValeria Pronina\\nAlexander Kuzmin\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.09770\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 17 Nov 2023 07:26:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Manco et al. (§72023§r)', author: 'Ilaria Manco; Benno Weck; SeungHeon Doh; Minz Won; Yixiao Zhang; Dmitry Bogdanov; Yusong Wu; Ke Chen; Philip Tovstogan; Emmanouil Benetos; Elio Quinton; György Fazekas; Juhan Nam', display:{Lore:['[{"text": "arXiv:2311.10057", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Song Describer Dataset: a Corpus of Audio Captions for Music-and-Language Evaluation\\u00a7r\\n\\n\\u00a78\\u00a7oIlaria Manco\\nBenno Weck\\nSeungHeon Doh\\n+ 9 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.10057\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 22 Nov 2023 21:22:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to NeurIPS 2023 Workshopon Machine Learning for Audio\\u00a7r"}']}
{title:'Vinay et al. (§72023§r)', author: 'Ashvala Vinay; Alexander Lerch', display:{Lore:['[{"text": "arXiv:2311.10113", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAQUATK: An Audio Quality Assessment Toolkit\\u00a7r\\n\\n\\u00a78\\u00a7oAshvala Vinay\\nAlexander Lerch\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.10113\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Nov 2023 02:55:13 GMT)\\u00a7r"}']}
{title:'Jonason et al. (§72023§r)', author: 'Nicolas Jonason; Luca Casini; Carl Thomé; Bob L. T. Sturm', display:{Lore:['[{"text": "arXiv:2311.10384", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRetrieval Augmented Generation of Symbolic Music with LLMs\\u00a7r\\n\\n\\u00a78\\u00a7oNicolas Jonason\\nLuca Casini\\nCarl Thom\\u00e9\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.10384\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 28 Dec 2023 15:08:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oLBD @ ISMIR 2023\\u00a7r"}']}
{title:'Rasouli et al. (§72023§r)', author: 'Parsa Rasouli; Azam Bastanfard', display:{Lore:['[{"text": "arXiv:2311.11074", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Persian Piano Corpus: A Collection Of Instrument-Based Feature Extracted Data Considering Dastgah\\u00a7r\\n\\n\\u00a78\\u00a7oParsa Rasouli\\nAzam Bastanfard\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.11074\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 18 Nov 2023 13:45:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oincluding 11 pages and 6 figures. wewant to inform related data PPC is submitted to Harvard Dataverse: https://doi.org/10.7910/DVN/YY7SVD\\u00a7r"}']}
{title:'Devaney et al. (§72023§r)', author: 'Johanna Devaney; Cecilia Beauchamp', display:{Lore:['[{"text": "arXiv:2311.11363", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.DL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEncoding Performance Data in MEI with the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT)\\u00a7r\\n\\n\\u00a78\\u00a7oJohanna Devaney\\nCecilia Beauchamp\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.11363\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nLate-Breaking Demo Session of the 24th International Society for\\n  Music Information Retrieval Conference (2023)\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 19 Nov 2023 16:18:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o3 pages, 2 figures\\u00a7r"}']}
{title:'Kong et al. (§72023§r)', author: 'Jungil Kong; Junmo Lee; Jeongmin Kim; Beomjeong Kim; Jihoon Park; Dohee Kong; Changheon Lee; Sangjin Kim', display:{Lore:['[{"text": "arXiv:2311.11745", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEncoding Speaker-Specific Latent Speech Feature for Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oJungil Kong\\nJunmo Lee\\nJeongmin Kim\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.11745\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Nov 2023 13:13:24 GMT)\\u00a7r"}']}
{title:'Pal et al. (§72023§r)', author: 'Neelanjana Pal; Taylor T Johnson', display:{Lore:['[{"text": "arXiv:2311.12130", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFormal Verification of Long Short-Term Memory based Audio Classifiers: A Star based Approach\\u00a7r\\n\\n\\u00a78\\u00a7oNeelanjana Pal\\nTaylor T Johnson\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.12130\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.4204/EPTCS.395.12\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nEPTCS 395, 2023, pp. 162-179\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Nov 2023 11:04:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings FMAS 2023, arXiv:2311.08987\\u00a7r"}']}
{title:'Gao et al. (§72023§r)', author: 'Chenyang Gao; Yue Gu; Ivan Marsic', display:{Lore:['[{"text": "arXiv:2311.12199", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Label Assignments Learning by Dynamic Sample Dropout Combined with Layer-wise Optimization in Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oChenyang Gao\\nYue Gu\\nIvan Marsic\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.12199\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Nov 2023 21:37:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Xu et al. (§72023§r)', author: 'Weihan Xu; Julian McAuley; Shlomo Dubnov; Hao-Wen Dong', display:{Lore:['[{"text": "arXiv:2311.12257", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEquipping Pretrained Unconditional Music Transformers with Instrument and Genre Controls\\u00a7r\\n\\n\\u00a78\\u00a7oWeihan Xu\\nJulian McAuley\\nShlomo Dubnov\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.12257\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Nov 2023 00:37:47 GMT)\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Sang-Hoon Lee; Ha-Yeong Choi; Seung-Bin Kim; Seong-Whan Lee', display:{Lore:['[{"text": "arXiv:2311.12454", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierSpeech++: Bridging the Gap between Semantic and Acoustic Representation of Speech by Hierarchical Variational Inference for Zero-shot Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oSang-Hoon Lee\\nHa-Yeong Choi\\nSeung-Bin Kim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.12454\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Nov 2023 12:26:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages, 9 figures, 12 tables\\u00a7r"}']}
{title:'Pasini et al. (§72023§r)', author: 'Marco Pasini; Stefan Lattner; George Fazekas', display:{Lore:['[{"text": "arXiv:2311.13058", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Music Source Separation Using Vector-Quantized Source Category Estimates\\u00a7r\\n\\n\\u00a78\\u00a7oMarco Pasini\\nStefan Lattner\\nGeorge Fazekas\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.13058\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Nov 2023 23:45:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 2 figures, 1 table; Accepted at the 37th Conference on Neural Information Processing Systems (2023), Machine Learning for Audio Workshop\\u00a7r"}']}
{title:'Flax (§72023§r)', author: 'Matt R. Flax', display:{Lore:['[{"text": "arXiv:2311.14239", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAllpass impulse response modelling\\u00a7r\\n\\n\\u00a78\\u00a7oMatt R. Flax\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.14239\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Nov 2023 00:48:11 GMT)\\u00a7r"}']}
{title:'Cox et al. (§72023§r)', author: 'Trevor J. Cox; Jon Barker; Will Bailey; Simone Graetzer; Michael A. Akeroyd; John F. Culling; Graham Naylor', display:{Lore:['[{"text": "arXiv:2311.14490", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOverview Of The 2023 Icassp Sp Clarity Challenge: Speech Enhancement For Hearing Aids\\u00a7r\\n\\n\\u00a78\\u00a7oTrevor J. Cox\\nJon Barker\\nWill Bailey\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.14490\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Nov 2023 14:02:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Paissan et al. (§72023§r)', author: 'Francesco Paissan; Elisabetta Farella', display:{Lore:['[{"text": "arXiv:2311.14517", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7ltinyCLAP: Distilling Constrastive Language-Audio Pretrained Models\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesco Paissan\\nElisabetta Farella\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.14517\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Nov 2023 14:45:53 GMT)\\u00a7r"}']}
{title:'Jiang et al. (§72023§r)', author: 'Jintao Jiang; Yingbo Gao; Zoltan Tuske', display:{Lore:['[{"text": "arXiv:2311.14835", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeak Alignment Supervision from Hybrid Model Improves End-to-end ASR\\u00a7r\\n\\n\\u00a78\\u00a7oJintao Jiang\\nYingbo Gao\\nZoltan Tuske\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.14835\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 30 Nov 2023 20:18:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 7 figures, and 5 tables\\u00a7r"}']}
{title:'Gu et al. (§72023§r)', author: 'Yicheng Gu; Xueyao Zhang; Liumeng Xue; Zhizheng Wu', display:{Lore:['[{"text": "arXiv:2311.14957", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Scale Sub-Band Constant-Q Transform Discriminator for High-Fidelity Vocoder\\u00a7r\\n\\n\\u00a78\\u00a7oYicheng Gu\\nXueyao Zhang\\nLiumeng Xue\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.14957\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 25 Nov 2023 07:49:09 GMT)\\u00a7r"}']}
{title:'Lin et al. (§72023§r)', author: 'Yi-Heng Lin; Wen-Hsuan Tseng; Li-Chin Chen; Ching-Ting Tan; Yu Tsao', display:{Lore:['[{"text": "arXiv:2311.15582", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLightly Weighted Automatic Audio Parameter Extraction for the Quality Assessment of Consensus Auditory-Perceptual Evaluation of Voice\\u00a7r\\n\\n\\u00a78\\u00a7oYi-Heng Lin\\nWen-Hsuan Tseng\\nLi-Chin Chen\\nChing-Ting Tan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.15582\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Nov 2023 07:19:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in IEEE42th International Conference on Consumer Electronics (ICCE 2024)\\u00a7r"}']}
{title:'Jin et al. (§72023§r)', author: 'Zezhong Jin; Youzhi Tu; Man-Wai Mak', display:{Lore:['[{"text": "arXiv:2311.15627", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhonetic-aware speaker embedding for far-field speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oZezhong Jin\\nYouzhi Tu\\nMan-Wai Mak\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.15627\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Nov 2023 08:45:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP2024\\u00a7r"}']}
{title:'Tan et al. (§72023§r)', author: 'Kaijun Tan; Benzhe Dai; Jiakui Li; Wenyu Mao', display:{Lore:['[{"text": "arXiv:2311.15959", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCheapNET: Improving Light-weight speech enhancement network by projected loss function\\u00a7r\\n\\n\\u00a78\\u00a7oKaijun Tan\\nBenzhe Dai\\nJiakui Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.15959\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Nov 2023 16:03:42 GMT)\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Chi-Chang Lee; Yu Tsao; Hsin-Min Wang; Chu-Song Chen', display:{Lore:['[{"text": "arXiv:2311.16595", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lD4AM: A General Denoising Framework for Downstream Acoustic Models\\u00a7r\\n\\n\\u00a78\\u00a7oChi-Chang Lee\\nYu Tsao\\nHsin-Min Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.16595\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Nov 2023 08:27:27 GMT)\\u00a7r"}']}
{title:'Trayford et al. (§72023§r)', author: 'James W. Trayford; Chris M. Harrison', display:{Lore:['[{"text": "arXiv:2311.16847", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a75astro-ph.IM\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntroducing STRAUSS: A flexible sonification Python package\\u00a7r\\n\\n\\u00a78\\u00a7oJames W. Trayford\\nChris M. Harrison\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.16847\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Nov 2023 14:58:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 4 figures, 28th International Conference on Auditory Display, see here for linked resources: https://data.ncl.ac.uk/articles/media/Trayford_2023_STRAUSS_ICAD_examples/22241182\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Dongning Yang; Wei Wang; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2311.17790", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFAT-HuBERT: Front-end Adaptive Training of Hidden-unit BERT for Distortion-Invariant Robust Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDongning Yang\\nWei Wang\\nYanmin Qian\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.17790\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 29 Nov 2023 16:35:13 GMT)\\u00a7r"}']}
{title:'Marmoret et al. (§72023§r)', author: 'Axel Marmoret; Jérémy E. Cohen; Frédéric Bimbot', display:{Lore:['[{"text": "arXiv:2311.18604", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBarwise Music Structure Analysis with the Correlation Block-Matching Segmentation Algorithm\\u00a7r\\n\\n\\u00a78\\u00a7oAxel Marmoret\\nJ\\u00e9r\\u00e9my E. Cohen\\nFr\\u00e9d\\u00e9ric Bimbot\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.18604\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5334/tismir.167\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nTransactions of the International Society for Music Information\\n  Retrieval, 6(1), 2023, 167--185\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Nov 2023 15:00:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o19 pages, 13 figures, 11 tables, 1 algorithm, published in Transactions of the International Society for Music Information Retrieval\\u00a7r"}']}
{title:'Ziemer (§72023§r)', author: 'Tim Ziemer', display:{Lore:['[{"text": "arXiv:2312.00091", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Terminology Describing Production and Perception of Sonification\\u00a7r\\n\\n\\u00a78\\u00a7oTim Ziemer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.00091\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.17743/jaes.2022.0133\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Nov 2023 10:49:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages, 0 figures\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Bing Yang; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2312.00476", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Learning of Spatial Acoustic Representation with Cross-Channel Signal Reconstruction and Multi-Channel Conformer\\u00a7r\\n\\n\\u00a78\\u00a7oBing Yang\\nXiaofei Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.00476\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Dec 2023 10:16:02 GMT)\\u00a7r"}']}
{title:'Yurdakul et al. (§72023§r)', author: 'Mustafa Yurdakul; Sakir Tasdemir', display:{Lore:['[{"text": "arXiv:2312.01062", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic Signal Analysis with Deep Neural Network for Detecting Fault Diagnosis in Industrial Machines\\u00a7r\\n\\n\\u00a78\\u00a7oMustafa Yurdakul\\nSakir Tasdemir\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.01062\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Dec 2023 08:09:27 GMT)\\u00a7r"}']}
{title:'Amatov et al. (§72023§r)', author: 'Amantur Amatov; Dmitry Lamanov; Maksim Titov; Ivan Vovk; Ilya Makarov; Mikhail Kudinov', display:{Lore:['[{"text": "arXiv:2312.01092", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Semi-Supervised Deep Learning Approach to Dataset Collection for Query-By-Humming Task\\u00a7r\\n\\n\\u00a78\\u00a7oAmantur Amatov\\nDmitry Lamanov\\nMaksim Titov\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.01092\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Dec 2023 09:50:00 GMT)\\u00a7r"}']}
{title:'Zhong (§72023§r)', author: 'Xuan Zhong', display:{Lore:['[{"text": "arXiv:2312.01554", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBuilding Ears for Robots: Machine Hearing in the Age of Autonomy\\u00a7r\\n\\n\\u00a78\\u00a7oXuan Zhong\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.01554\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 5 Dec 2023 06:06:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 6 figures. The materials covered in this article were presented and discussed at the Hearing Seminar at Stanford University organized by Malcolm Slaney in October, 2023\\u00a7r"}']}
{title:'Zheng et al. (§72023§r)', author: 'Litong Zheng; Feng Hong; Weijie Xu', display:{Lore:['[{"text": "arXiv:2312.01645", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA text-dependent speaker verification application framework based on Chinese numerical string corpus\\u00a7r\\n\\n\\u00a78\\u00a7oLitong Zheng\\nFeng Hong\\nWeijie Xu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.01645\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Dec 2023 05:52:59 GMT)\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Jihyun Lee; Yejin Jeon; Wonjun Lee; Yunsu Kim; Gary Geunbae Lee', display:{Lore:['[{"text": "arXiv:2312.01842", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring the Viability of Synthetic Audio Data for Audio-Based Dialogue State Tracking\\u00a7r\\n\\n\\u00a78\\u00a7oJihyun Lee\\nYejin Jeon\\nWonjun Lee\\nYunsu Kim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.01842\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Dec 2023 12:25:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ASRU2023\\u00a7r"}']}
{title:'Parsapoor (§72023§r)', author: 'Mahboobeh Parsapoor', display:{Lore:['[{"text": "arXiv:2312.02229", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynthetic Data Generation Techniques for Developing AI-based Speech Assessments for Parkinson\'s Disease (A Comparative Study)\\u00a7r\\n\\n\\u00a78\\u00a7oMahboobeh Parsapoor\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.02229\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Dec 2023 03:12:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6, 5 Tables, 5 Figures\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Ziye Yang; Wenxing Yang; Kai Xie; Jie Chen', display:{Lore:['[{"text": "arXiv:2312.02773", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntegrating Plug-and-Play Data Priors with Weighted Prediction Error for Speech Dereverberation\\u00a7r\\n\\n\\u00a78\\u00a7oZiye Yang\\nWenxing Yang\\nKai Xie\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.02773\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Dec 2023 14:03:11 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Chang Liu; Jie Zhang; Tianwei Zhang; Xi Yang; Weiming Zhang; Nenghai Yu', display:{Lore:['[{"text": "arXiv:2312.03410", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetecting Voice Cloning Attacks via Timbre Watermarking\\u00a7r\\n\\n\\u00a78\\u00a7oChang Liu\\nJie Zhang\\nTianwei Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.03410\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Dec 2023 10:48:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oNDSS 2024\\u00a7r"}']}
{title:'Namgyal et al. (§72023§r)', author: 'Tashi Namgyal; Alexander Hepburn; Raul Santos-Rodriguez; Valero Laparra; Jesus Malo', display:{Lore:['[{"text": "arXiv:2312.03455", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data\\u00a7r\\n\\n\\u00a78\\u00a7oTashi Namgyal\\nAlexander Hepburn\\nRaul Santos-Rodriguez\\nValero Laparra\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.03455\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Dec 2023 12:27:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMachine Learning for Audio Workshop, NeurIPS 2023\\u00a7r"}']}
{title:'Hollowell et al. (§72023§r)', author: 'Sven Hollowell; Tashi Namgyal; Paul Marshall', display:{Lore:['[{"text": "arXiv:2312.03479", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJAMMIN-GPT: Text-based Improvisation using LLMs in Ableton Live\\u00a7r\\n\\n\\u00a78\\u00a7oSven Hollowell\\nTashi Namgyal\\nPaul Marshall\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.03479\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Dec 2023 13:19:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oConference: 24th International Society for Music Information Retrieval. Late Breaking Demo. 2023\\u00a7r"}']}
{title:'Wagner et al. (§72023§r)', author: 'Dominik Wagner; Alexander Churchill; Siddharth Sigtia; Panayiotis Georgiou; Matt Mirsamadi; Aarshee Mishra; Erik Marchi', display:{Lore:['[{"text": "arXiv:2312.03632", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultimodal Data and Resource Efficient Device-Directed Speech Detection with Large Foundation Models\\u00a7r\\n\\n\\u00a78\\u00a7oDominik Wagner\\nAlexander Churchill\\nSiddharth Sigtia\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.03632\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Dec 2023 17:29:03 GMT)\\u00a7r"}']}
{title:'Zaugg et al. (§72023§r)', author: 'Serge Zaugg; Mike van der Schaar; Florence Erbs; Antonio Sanchez; Joan V. Castell; Emiliano Ramallo; Michel André', display:{Lore:['[{"text": "arXiv:2312.03666", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards small and accurate convolutional neural networks for acoustic biodiversity monitoring\\u00a7r\\n\\n\\u00a78\\u00a7oSerge Zaugg\\nMike van der Schaar\\nFlorence Erbs\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.03666\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Dec 2023 18:34:01 GMT)\\u00a7r"}']}
{title:'Kita et al. (§72023§r)', author: 'Shunsuke Kita; Choong Sik Park; Yoshinobu Kajikawa', display:{Lore:['[{"text": "arXiv:2312.04846", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Source Localization for a Source inside a Structure using Ac-CycleGAN\\u00a7r\\n\\n\\u00a78\\u00a7oShunsuke Kita\\nChoong Sik Park\\nYoshinobu Kajikawa\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.04846\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Dec 2023 05:50:07 GMT)\\u00a7r"}']}
{title:'Nielson et al. (§72023§r)', author: 'Via Nielson; Steven Hillis', display:{Lore:['[{"text": "arXiv:2312.05415", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Experimental Study: Assessing the Combined Framework of WavLM and BEST-RQ for Text-to-Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oVia Nielson\\nSteven Hillis\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.05415\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Dec 2023 23:59:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 1 figure, 4 tables\\u00a7r"}']}
{title:'Rai et al. (§72023§r)', author: 'Sumedha Rai; Tong Li; Bella Lyu', display:{Lore:['[{"text": "arXiv:2312.05640", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKeyword spotting \\u2013 Detecting commands in speech using deep learning\\u00a7r\\n\\n\\u00a78\\u00a7oSumedha Rai\\nTong Li\\nBella Lyu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.05640\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 9 Dec 2023 19:04:17 GMT)\\u00a7r"}']}
{title:'Ball (§72023§r)', author: 'Joshua Ball', display:{Lore:['[{"text": "arXiv:2312.05815", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Activity Detection (VAD) in Noisy Environments\\u00a7r\\n\\n\\u00a78\\u00a7oJoshua Ball\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.05815\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 10 Dec 2023 08:17:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages\\u00a7r"}']}
{title:'Plachouras et al. (§72023§r)', author: 'Christos Plachouras; Pablo Alonso-Jiménez; Dmitry Bogdanov', display:{Lore:['[{"text": "arXiv:2312.05994", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lmir_ref: A Representation Evaluation Framework for Music Information Retrieval Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oChristos Plachouras\\nPablo Alonso-Jim\\u00e9nez\\nDmitry Bogdanov\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.05994\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 12 Dec 2023 05:11:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMachine Learning for Audio Workshop, Neural Information ProcessingSystems (NeurIPS) 2023, New Orleans, LA\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Xuechen Liu; Xin Wang; Erica Cooper; Xiaoxiao Miao; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2312.06055", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker-Text Retrieval via Contrastive Learning\\u00a7r\\n\\n\\u00a78\\u00a7oXuechen Liu\\nXin Wang\\nErica Cooper\\nXiaoxiao Miao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.06055\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Dec 2023 01:23:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEESignal Processing Letters\\u00a7r"}']}
{title:'Yu et al. (§72023§r)', author: 'Xincheng Yu; Dongyue Guo; Jianwei Zhang; Yi Lin', display:{Lore:['[{"text": "arXiv:2312.06118", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lROSE: A Recognition-Oriented Speech Enhancement Framework in Air Traffic Control Using Multi-Objective Learning\\u00a7r\\n\\n\\u00a78\\u00a7oXincheng Yu\\nDongyue Guo\\nJianwei Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.06118\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Dec 2023 04:51:41 GMT)\\u00a7r"}']}
{title:'Samarakoon et al. (§72023§r)', author: 'Lahiru Samarakoon; Samuel J. Broughton; Marc Härkönen; Ivan Fung', display:{Lore:['[{"text": "arXiv:2312.06253", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransformer Attractors for Robust and Efficient End-to-End Neural Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oLahiru Samarakoon\\nSamuel J. Broughton\\nMarc H\\u00e4rk\\u00f6nen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.06253\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Dec 2023 09:49:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 1 figure, ASRU2023\\u00a7r"}']}
{title:'Meng et al. (§72023§r)', author: 'Tao Meng; Yuntao Shou; Wei Ai; Nan Yin; Keqin Li', display:{Lore:['[{"text": "arXiv:2312.06337", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Imbalanced Learning for Multimodal Emotion Recognition in Conversations\\u00a7r\\n\\n\\u00a78\\u00a7oTao Meng\\nYuntao Shou\\nWei Ai\\nNan Yin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.06337\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Dec 2023 12:35:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages, 9 figures\\u00a7r"}']}
{title:'Zhao et al. (§72023§r)', author: 'Yan Zhao; Yuan Zong; Hailun Lian; Cheng Lu; Jingang Shi; Wenming Zheng', display:{Lore:['[{"text": "arXiv:2312.06466", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Domain-Specific Cross-Corpus Speech Emotion Recognition Approach\\u00a7r\\n\\n\\u00a78\\u00a7oYan Zhao\\nYuan Zong\\nHailun Lian\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.06466\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Dec 2023 15:53:57 GMT)\\u00a7r"}']}
{title:'Damacharla et al. (§72023§r)', author: 'Praveen Damacharla; Hamid Rajabalipanah; Mohammad Hosein Fakheri', display:{Lore:['[{"text": "arXiv:2312.07059", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLSTM-CNN Network for Audio Signature Analysis in Noisy Environments\\u00a7r\\n\\n\\u00a78\\u00a7oPraveen Damacharla\\nHamid Rajabalipanah\\nMohammad Hosein Fakheri\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.07059\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Dec 2023 08:26:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10th Annual Conf. on Computational Science Computational Intelligence (CSCI\'23)\\u00a7r"}']}
{title:'Fung et al. (§72023§r)', author: 'Ivan Fung; Lahiru Samarakoon; Samuel J. Broughton', display:{Lore:['[{"text": "arXiv:2312.07136", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust End-to-End Diarization with Domain Adaptive Training and Multi-Task Learning\\u00a7r\\n\\n\\u00a78\\u00a7oIvan Fung\\nLahiru Samarakoon\\nSamuel J. Broughton\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.07136\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Dec 2023 10:15:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 2 figures, ASRU 2023\\u00a7r"}']}
{title:'Likhachov et al. (§72023§r)', author: 'Denis Likhachov; Nick Petrovsky; Elias Azarov', display:{Lore:['[{"text": "arXiv:2312.08069", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Spatial Resolution of First-order Ambisonics Using Sparse MDCT Representation\\u00a7r\\n\\n\\u00a78\\u00a7oDenis Likhachov\\nNick Petrovsky\\nElias Azarov\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.08069\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of 16-th International Conference PRIP2023, Minsk,\\n  2023, P. 122-125\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Dec 2023 11:28:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAssociated slides with audio samples https://effective-sound.com/downloads/ambisonic_upmix.pptx Audio samples with visualizations https://youtu.be/O15zvgWKa6A\\u00a7r"}']}
{title:'Netzorg et al. (§72023§r)', author: 'Robin Netzorg; Ajil Jalal; Luna McNulty; Gopala Krishna Anumanchipalli', display:{Lore:['[{"text": "arXiv:2312.08494", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerMod: Perceptually Grounded Voice Modification with Latent Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oRobin Netzorg\\nAjil Jalal\\nLuna McNulty\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.08494\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Dec 2023 20:14:27 GMT)\\u00a7r"}']}
{title:'Lei et al. (§72023§r)', author: 'Chengxi Lei; Satwinder Singh; Feng Hou; Xiaoyun Jia; Ruili Wang', display:{Lore:['[{"text": "arXiv:2312.08571", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhasePerturbation: Speech Data Augmentation via Phase Perturbation for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oChengxi Lei\\nSatwinder Singh\\nFeng Hou\\nXiaoyun Jia\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.08571\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Dec 2023 23:46:26 GMT)\\u00a7r"}']}
{title:'Tonami et al. (§72023§r)', author: 'Noriyuki Tonami; Wataru Kohno; Sakiko Mishima; Yumi Arai; Reishi Kondo; Tomoyuki Hino', display:{Lore:['[{"text": "arXiv:2312.08660", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-rank constrained multichannel signal denoising considering channel-dependent sensitivity inspired by self-supervised learning for optical fiber sensing\\u00a7r\\n\\n\\u00a78\\u00a7oNoriyuki Tonami\\nWataru Kohno\\nSakiko Mishima\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.08660\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 16 Dec 2023 08:42:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP2024\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Shuhua Liu; Chunyu Zhang; Binshuai Li; Niantong Qin; Huanting Cheng; Huayu Zhang', display:{Lore:['[{"text": "arXiv:2312.08732", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTIA: A Teaching Intonation Assessment Dataset in Real Teaching Situations\\u00a7r\\n\\n\\u00a78\\u00a7oShuhua Liu\\nChunyu Zhang\\nBinshuai Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.08732\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Dec 2023 08:20:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 3 figures, 4 tables, accepted by 2024 International Conference on Acoustics, Speech, and Signal Processing (ICASSP2024)\\u00a7r"}']}
{title:'Yu et al. (§72023§r)', author: 'Fan Yu; Haoxu Wang; Ziyang Ma; Shiliang Zhang', display:{Lore:['[{"text": "arXiv:2312.08850", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHourglass-AVSR: Down-Up Sampling-based Computational Efficiency Model for Audio-Visual Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oFan Yu\\nHaoxu Wang\\nZiyang Ma\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.08850\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Dec 2023 12:08:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Tian et al. (§72023§r)', author: 'Jinhao Tian; Zuchao Li; Jiajia Li; Ping Wang', display:{Lore:['[{"text": "arXiv:2312.08931", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lN-Gram Unsupervised Compoundation and Feature Injection for Better Symbolic Music Understanding\\u00a7r\\n\\n\\u00a78\\u00a7oJinhao Tian\\nZuchao Li\\nJiajia Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.08931\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 15 Dec 2023 03:27:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 2 figures, aaai2024\\u00a7r"}']}
{title:'Close et al. (§72023§r)', author: 'George Close; William Ravenscroft; Thomas Hain; Stefan Goetze', display:{Lore:['[{"text": "arXiv:2312.08979", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-CMGAN+/+: Leveraging Multi-Objective Speech Quality Metric Prediction for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oGeorge Close\\nWilliam Ravenscroft\\nThomas Hain\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.08979\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Dec 2023 14:27:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted @ ICASSP 2024\\u00a7r"}']}
{title:'Wilkinghoff et al. (§72023§r)', author: 'Kevin Wilkinghoff; Keisuke Imoto', display:{Lore:['[{"text": "arXiv:2312.09143", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lF1-EV Score: Measuring the Likelihood of Estimating a Good Decision Threshold for Semi-Supervised Anomaly Detection\\u00a7r\\n\\n\\u00a78\\u00a7oKevin Wilkinghoff\\nKeisuke Imoto\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.09143\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Dec 2023 17:13:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for presentation at IEEE ICASSP 2024\\u00a7r"}']}
{title:'Gauy et al. (§72023§r)', author: 'Marcelo Matheus Gauy; Marcelo Finger', display:{Lore:['[{"text": "arXiv:2312.09265", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic models of Brazilian Portuguese Speech based on Neural Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oMarcelo Matheus Gauy\\nMarcelo Finger\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.09265\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Dec 2023 14:16:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review at Journal of Brazilian Computer Society\\u00a7r"}']}
{title:'Priebe et al. (§72023§r)', author: 'Drew Priebe; Burooj Ghani; Dan Stowell', display:{Lore:['[{"text": "arXiv:2312.09269", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient speech detection in environmental audio using acoustic recognition and knowledge distillation\\u00a7r\\n\\n\\u00a78\\u00a7oDrew Priebe\\nBurooj Ghani\\nDan Stowell\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.09269\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Dec 2023 17:55:32 GMT)\\u00a7r"}']}
{title:'May et al. (§72023§r)', author: 'Avner May; Dmitriy Serdyuk; Ankit Parag Shah; Otavio Braga; Olivier Siohan', display:{Lore:['[{"text": "arXiv:2312.09369", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-visual fine-tuning of audio-only ASR models\\u00a7r\\n\\n\\u00a78\\u00a7oAvner May\\nDmitriy Serdyuk\\nAnkit Parag Shah\\nOtavio Braga\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.09369\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Dec 2023 22:05:15 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Chih-Chyau Yang; Tian-Sheuan Chang', display:{Lore:['[{"text": "arXiv:2312.09580", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA 1.6-mW Sparse Deep Learning Accelerator for Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oChih-Chyau Yang\\nTian-Sheuan Chang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.09580\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TVLSI.2023.3235760\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nin IEEE Transactions on Very Large Scale Integration (VLSI)\\n  Systems, vol. 31, no. 3, pp. 310-319, March 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Dec 2023 07:22:28 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'June-Woo Kim; Sangmin Bae; Won-Yang Cho; Byungjo Lee; Ho-Young Jung', display:{Lore:['[{"text": "arXiv:2312.09603", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStethoscope-guided Supervised Contrastive Learning for Cross-domain Adaptation on Respiratory Sound Classification\\u00a7r\\n\\n\\u00a78\\u00a7oJune-Woo Kim\\nSangmin Bae\\nWon-Yang Cho\\nByungjo Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.09603\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Dec 2023 08:34:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to ICASSP 2024\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Xiaohui Zhang; Jiangyan Yi; Chenglong Wang; Chuyuan Zhang; Siding Zeng; Jianhua Tao', display:{Lore:['[{"text": "arXiv:2312.09651", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhat to Remember: Self-Adaptive Continual Learning for Audio Deepfake Detection\\u00a7r\\n\\n\\u00a78\\u00a7oXiaohui Zhang\\nJiangyan Yi\\nChenglong Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.09651\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Dec 2023 09:52:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the main track The 38th Annual AAAI Conference on Artificial Intelligence (AAAI 2024)\\u00a7r"}']}
{title:'Mu et al. (§72023§r)', author: 'Bingshen Mu; Pengcheng Guo; Dake Guo; Pan Zhou; Wei Chen; Lei Xie', display:{Lore:['[{"text": "arXiv:2312.09746", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic channel selection and spatial feature integration for multi-channel speech recognition across various array topologies\\u00a7r\\n\\n\\u00a78\\u00a7oBingshen Mu\\nPengcheng Guo\\nDake Guo\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.09746\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Dec 2023 12:35:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Adiga et al. (§72023§r)', author: 'Nagaraj Adiga; Jinhwan Park; Chintigari Shiva Kumar; Shatrughan Singh; Kyungmin Lee; Chanwoo Kim; Dhananjaya Gowda', display:{Lore:['[{"text": "arXiv:2312.09842", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the compression of shallow non-causal ASR models using knowledge distillation and tied-and-reduced decoder for low-latency on-device speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oNagaraj Adiga\\nJinhwan Park\\nChintigari Shiva Kumar\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.09842\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Dec 2023 14:48:14 GMT)\\u00a7r"}']}
{title:'Goh et al. (§72023§r)', author: 'Sheen An Goh; Manoj Gulati; Ambuj Varshney', display:{Lore:['[{"text": "arXiv:2312.10265", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoCopilot: Voice-Activated Tracking of Everyday Interactions\\u00a7r\\n\\n\\u00a78\\u00a7oSheen An Goh\\nManoj Gulati\\nAmbuj Varshney\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.10265\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Dec 2023 23:46:52 GMT)\\u00a7r"}']}
{title:'Xu et al. (§72023§r)', author: 'Yaoxun Xu; Hangting Chen; Jianwei Yu; Qiaochu Huang; Zhiyong Wu; Shixiong Zhang; Guangzhi Li; Yi Luo; Rongzhi Gu', display:{Lore:['[{"text": "arXiv:2312.10381", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSECap: Speech Emotion Captioning with Large Language Model\\u00a7r\\n\\n\\u00a78\\u00a7oYaoxun Xu\\nHangting Chen\\nJianwei Yu\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.10381\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 23 Dec 2023 12:25:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by AAAI 2024\\u00a7r"}']}
{title:'Sato et al. (§72023§r)', author: 'Gakusei Sato; Taketo Akama', display:{Lore:['[{"text": "arXiv:2312.10402", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnnotation-free Automatic Music Transcription with Scalable Synthetic Data and Adversarial Domain Confusion\\u00a7r\\n\\n\\u00a78\\u00a7oGakusei Sato\\nTaketo Akama\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.10402\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 31 Dec 2023 02:31:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 1 figure\\u00a7r"}']}
{title:'Perez et al. (§72023§r)', author: 'Matthew Perez; Duc Le; Amrit Romana; Elise Jones; Keli Licata; Emily Mower Provost', display:{Lore:['[{"text": "arXiv:2312.10518", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSeq2seq for Automatic Paraphasia Detection in Aphasic Speech\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew Perez\\nDuc Le\\nAmrit Romana\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.10518\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 16 Dec 2023 18:22:37 GMT)\\u00a7r"}']}
{title:'Casebeer et al. (§72023§r)', author: 'Jonah Casebeer; Junkai Wu; Paris Smaragdis', display:{Lore:['[{"text": "arXiv:2312.10605", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMeta-AF Echo Cancellation for Improved Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oJonah Casebeer\\nJunkai Wu\\nParis Smaragdis\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.10605\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 17 Dec 2023 04:45:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, ICASSP 2024\\u00a7r"}']}
{title:'Kiranyaz et al. (§72023§r)', author: 'Serkan Kiranyaz; Ozer Can Devecioglu; Amir Alhams; Sadok Sassi; Turker Ince; Onur Avci; Moncef Gabbouj', display:{Lore:['[{"text": "arXiv:2312.10742", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Sound vs Vibration for Robust Fault Detection on Rotating Machinery\\u00a7r\\n\\n\\u00a78\\u00a7oSerkan Kiranyaz\\nOzer Can Devecioglu\\nAmir Alhams\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.10742\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 17 Dec 2023 15:27:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages\\u00a7r"}']}
{title:'Rudd et al. (§72023§r)', author: 'David Hason Rudd; Huan Huo; Guandong Xu', display:{Lore:['[{"text": "arXiv:2312.10937", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Extended Variational Mode Decomposition Algorithm Developed Speech Emotion Recognition Performance\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Hason Rudd\\nHuan Huo\\nGuandong Xu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.10937\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-031-33380-4_17\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nAdvances in Knowledge Discovery and Data Mining. PAKDD 2023.\\n  Lecture Notes in Computer Science(), vol 13937. Springer, Cham\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Dec 2023 05:24:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages\\u00a7r"}']}
{title:'Rudd et al. (§72023§r)', author: 'David Hason Rudd; Huan Huo; Guandong Xu', display:{Lore:['[{"text": "arXiv:2312.10949", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraged Mel spectrograms using Harmonic and Percussive Components in Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Hason Rudd\\nHuan Huo\\nGuandong Xu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.10949\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-031-05936-0_31\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nAdvances in Knowledge Discovery and Data Mining. PAKDD 2022.\\n  Lecture Notes in Computer Science(), vol 13281. Springer, Cham\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Dec 2023 05:55:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages\\u00a7r"}']}
{title:'Shen et al. (§72023§r)', author: 'Peng Shen; Xugang Lu; Hisashi Kawai', display:{Lore:['[{"text": "arXiv:2312.10959", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Mask Transformer for Multi-talker Overlapped Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oPeng Shen\\nXugang Lu\\nHisashi Kawai\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.10959\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Dec 2023 06:29:53 GMT)\\u00a7r"}']}
{title:'Arumugam et al. (§72023§r)', author: 'Guru Prakash Arumugam; Shuo-yiin Chang; Tara N. Sainath; Rohit Prabhavalkar; Quan Wang; Shaan Bijwadia', display:{Lore:['[{"text": "arXiv:2312.11123", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Long-Form Speech Recognition by Jointly Modeling the Primary and Non-primary Speakers\\u00a7r\\n\\n\\u00a78\\u00a7oGuru Prakash Arumugam\\nShuo-yiin Chang\\nTara N. Sainath\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.11123\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Dec 2023 11:47:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, ASRU 2023\\u00a7r"}']}
{title:'Dias et al. (§72023§r)', author: 'Fábio Felix Dias; Moacir Antonelli Ponti; Mílton Cezar Ribeiro; Rosane Minghim', display:{Lore:['[{"text": "arXiv:2312.11240", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluation of Barlow Twins and VICReg self-supervised learning for sound patterns of bird and anuran species\\u00a7r\\n\\n\\u00a78\\u00a7oF\\u00e1bio Felix Dias\\nMoacir Antonelli Ponti\\nM\\u00edlton Cezar Ribeiro\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.11240\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Dec 2023 14:38:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 2 figures, 3 tables\\u00a7r"}']}
{title:'Chowdhury et al. (§72023§r)', author: 'Md. Jalal Uddin Chowdhury; Ashab Hussan', display:{Lore:['[{"text": "arXiv:2312.11563", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA review-based study on different Text-to-Speech technologies\\u00a7r\\n\\n\\u00a78\\u00a7oMd. Jalal Uddin Chowdhury\\nAshab Hussan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.11563\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 17 Dec 2023 20:07:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages\\u00a7r"}']}
{title:'Zhao et al. (§72023§r)', author: 'Shengkui Zhao; Yukun Ma; Chongjia Ni; Chong Zhang; Hao Wang; Trung Hieu Nguyen; Kun Zhou; Jiaqi Yip; Dianwen Ng; Bin Ma', display:{Lore:['[{"text": "arXiv:2312.11825", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMossFormer2: Combining Transformer and RNN-Free Recurrent Network for Enhanced Time-Domain Monaural Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oShengkui Zhao\\nYukun Ma\\nChongjia Ni\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.11825\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Dec 2023 03:31:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted by ICASSP 2024\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Mengbo Li; Yuanzhong Zheng; Dichucheng Li; Yulun Wu; Yaoxuan Wang; Haojun Fei', display:{Lore:['[{"text": "arXiv:2312.11974", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMs-senet: Enhancing Speech Emotion Recognition Through Multi-scale Feature Fusion With Squeeze-and-excitation Blocks\\u00a7r\\n\\n\\u00a78\\u00a7oMengbo Li\\nYuanzhong Zheng\\nDichucheng Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.11974\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 25 Dec 2023 04:02:06 GMT)\\u00a7r"}']}
{title:'Ritter-Gutierrez et al. (§72023§r)', author: 'Fabian Ritter-Gutierrez; Kuan-Po Huang; Dianwen Ng; Jeremy H. M. Wong; Hung-yi Lee; Eng Siong Chng; Nancy F. Chen', display:{Lore:['[{"text": "arXiv:2312.12153", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoise robust distillation of self-supervised speech models via correlation metrics\\u00a7r\\n\\n\\u00a78\\u00a7oFabian Ritter-Gutierrez\\nKuan-Po Huang\\nDianwen Ng\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.12153\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Dec 2023 13:35:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Xueyuan Chen; Xi Wang; Shaofei Zhang; Lei He; Zhiyong Wu; Xixin Wu; Helen Meng', display:{Lore:['[{"text": "arXiv:2312.12181", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStyleSpeech: Self-supervised Style Enhancing with VQ-VAE-based Pre-training for Expressive Audiobook Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oXueyuan Chen\\nXi Wang\\nShaofei Zhang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.12181\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Dec 2023 14:13:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Yang Liu; Haoqin Sun; Geng Chen; Qingyue Wang; Zhen Zhao; Xugang Lu; Longbiao Wang', display:{Lore:['[{"text": "arXiv:2312.13556", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Level Knowledge Distillation for Speech Emotion Recognition in Noisy Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oYang Liu\\nHaoqin Sun\\nGeng Chen\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.13556\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Dec 2023 03:49:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Sun et al. (§72023§r)', author: 'Haoqin Sun; Shiwan Zhao; Xuechen Wang; Wenjia Zeng; Yong Chen; Yong Qin', display:{Lore:['[{"text": "arXiv:2312.13567", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFine-grained Disentangled Representation Learning for Multimodal Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHaoqin Sun\\nShiwan Zhao\\nXuechen Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.13567\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Dec 2023 04:31:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Yu et al. (§72023§r)', author: 'Guochen Yu; Xiguang Zheng; Nan Li; Runqiang Han; Chengshi Zheng; Chen Zhang; Chao Zhou; Qi Huang; Bing Yu', display:{Lore:['[{"text": "arXiv:2312.13722", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBAE-Net: A Low complexity and high fidelity Bandwidth-Adaptive neural network for speech super-resolution\\u00a7r\\n\\n\\u00a78\\u00a7oGuochen Yu\\nXiguang Zheng\\nNan Li\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.13722\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Dec 2023 10:39:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Simic et al. (§72023§r)', author: 'Christopher Simic; Tobias Bocklet', display:{Lore:['[{"text": "arXiv:2312.13873", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Adaptive AV Fusion Module for Pre-Trained ASR Models\\u00a7r\\n\\n\\u00a78\\u00a7oChristopher Simic\\nTobias Bocklet\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.13873\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Dec 2023 14:16:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2024\\u00a7r"}']}
{title:'Quelennec et al. (§72023§r)', author: 'Aurian Quelennec; Michel Olvera; Geoffroy Peeters; Slim Essid', display:{Lore:['[{"text": "arXiv:2312.14005", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the choice of the optimal temporal support for audio classification with Pre-trained embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oAurian Quelennec\\nMichel Olvera\\nGeoffroy Peeters\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.14005\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Dec 2023 16:36:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCopyright 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising "}','{"text": "or promotional purposes, creating newcollective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Walls et al. (§72023§r)', author: 'Kelvin L Walls; Iran R Roman; Bea Steers; Elena Georgieva', display:{Lore:['[{"text": "arXiv:2312.14036", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTotal variation in popular rap vocals from 2009-2023: extension of the analysis by Georgieva, Ripolles     McFee\\u00a7r\\n\\n\\u00a78\\u00a7oKelvin L Walls\\nIran R Roman\\nBea Steers\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.14036\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIsmir 2023 Hybrid Conference 2023 Nov 5\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Dec 2023 17:03:25 GMT)\\u00a7r"}']}
{title:'Gong et al. (§72023§r)', author: 'Cheng Gong; Xin Wang; Erica Cooper; Dan Wells; Longbiao Wang; Jianwu Dang; Korin Richmond; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2312.14398", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZMM-TTS: Zero-shot Multilingual and Multispeaker Speech Synthesis Conditioned on Self-supervised Discrete Speech Representations\\u00a7r\\n\\n\\u00a78\\u00a7oCheng Gong\\nXin Wang\\nErica Cooper\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.14398\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Dec 2023 02:54:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 5 figures\\u00a7r"}']}
{title:'Bilinski et al. (§72023§r)', author: 'Piotr Bilinski; Thomas Merritt; Abdelhamid Ezzerg; Kamil Pokora; Sebastian Cygert; Kayoko Yanagisawa; Roberto Barra-Chicote; Daniel Korzekwa', display:{Lore:['[{"text": "arXiv:2312.14569", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCreating New Voices using Normalizing Flows\\u00a7r\\n\\n\\u00a78\\u00a7oPiotr Bilinski\\nThomas Merritt\\nAbdelhamid Ezzerg\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.14569\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10195\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInterspeech 2022, 2958-2962\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Dec 2023 10:00:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2022\\u00a7r"}']}
{title:'Scerbo et al. (§72023§r)', author: 'Matteo Scerbo; Lauri Savioja; Enzo De Sena', display:{Lore:['[{"text": "arXiv:2312.14658", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRoom Acoustic Rendering Networks with Control of Scattering and Early Reflections\\u00a7r\\n\\n\\u00a78\\u00a7oMatteo Scerbo\\nLauri Savioja\\nEnzo De Sena\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.14658\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Dec 2023 12:47:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEE/ACM Transactionson Audio, Speech, and Language Processing. 12 pages, 12 figures, 2 tables\\u00a7r"}']}
{title:'Atkinson et al. (§72023§r)', author: 'Georgia Atkinson; Nick Wright; A. Stephen McGough; Per Berggren', display:{Lore:['[{"text": "arXiv:2312.14806", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Effects of Signal-to-Noise Ratio on Generative Adversarial Networks Applied to Marine Bioacoustic Data\\u00a7r\\n\\n\\u00a78\\u00a7oGeorgia Atkinson\\nNick Wright\\nA. Stephen McGough\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.14806\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Dec 2023 16:27:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 6 figures\\u00a7r"}']}
{title:'Zuo et al. (§72023§r)', author: 'Lingyun Zuo; Keyu An; Shiliang Zhang; Zhijie Yan', display:{Lore:['[{"text": "arXiv:2312.14860", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdvancing VAD Systems Based on Multi-Task Learning with Improved Model Structures\\u00a7r\\n\\n\\u00a78\\u00a7oLingyun Zuo\\nKeyu An\\nShiliang Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.14860\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Dec 2023 10:50:57 GMT)\\u00a7r"}']}
{title:'Cheng et al. (§72023§r)', author: 'Ming Cheng; Xingjian Diao; Shitong Cheng; Wenjun Liu', display:{Lore:['[{"text": "arXiv:2312.15190", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSAIC: Integration of Speech Anonymization and Identity Classification\\u00a7r\\n\\n\\u00a78\\u00a7oMing Cheng\\nXingjian Diao\\nShitong Cheng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.15190\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 23 Dec 2023 08:14:33 GMT)\\u00a7r"}']}
{title:'Cheng et al. (§72023§r)', author: 'Xize Cheng; Rongjie Huang; Linjun Li; Tao Jin; Zehan Wang; Aoxiong Yin; Minglei Li; Xinyu Duan; changpeng yang; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2312.15197", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransFace: Unit-Based Audio-Visual Speech Synthesizer for Talking Head Translation\\u00a7r\\n\\n\\u00a78\\u00a7oXize Cheng\\nRongjie Huang\\nLinjun Li\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.15197\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 23 Dec 2023 08:45:57 GMT)\\u00a7r"}']}
{title:'Go et al. (§72023§r)', author: 'Seonghyeon Go; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2312.15400", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCombinatorial music generation model with song structure graph analysis\\u00a7r\\n\\n\\u00a78\\u00a7oSeonghyeon Go\\nKyogu Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.15400\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 24 Dec 2023 04:09:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages(4 pages of paper and 1 references), 3 figures\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Chengxin Chen; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2312.15593", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDSNet: Disentangled Siamese Network with Neutral Calibration for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oChengxin Chen\\nPengyuan Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.15593\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Dec 2023 02:58:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 4 figures\\u00a7r"}']}
{title:'Ravuri et al. (§72023§r)', author: 'Aditya Ravuri; Erica Cooper; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2312.15616", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUncertainty as a Predictor: Leveraging Self-Supervised Learning for Zero-Shot MOS Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oAditya Ravuri\\nErica Cooper\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.15616\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Dec 2023 05:35:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, sasb draft\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Bingzhi Liu; Yin Cao; Haohe Liu; Yi Zhou', display:{Lore:['[{"text": "arXiv:2312.15628", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBalanced SNR-Aware Distillation for Guided Text-to-Audio Generation\\u00a7r\\n\\n\\u00a78\\u00a7oBingzhi Liu\\nYin Cao\\nHaohe Liu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.15628\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Dec 2023 06:46:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Vyas et al. (§72023§r)', author: 'Apoorv Vyas; Bowen Shi; Matthew Le; Andros Tjandra; Yi-Chiao Wu; Baishan Guo; Jiemin Zhang; Xinyue Zhang; Robert Adkins; William Ngan; Jeff Wang; Ivan Cruz; Bapi Akula; Akinniyi Akinyemi; Brian Ellis; Rashel Moritz; Yael Yungster; Alice Rakotoarison; Liang Tan; Chris Summers; Carleigh Wood; Joshua Lane; Mary Williamson; Wei-Ning Hsu', display:{Lore:['[{"text": "arXiv:2312.15821", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudiobox: Unified Audio Generation with Natural Language Prompts\\u00a7r\\n\\n\\u00a78\\u00a7oApoorv Vyas\\nBowen Shi\\nMatthew Le\\n+ 20 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.15821\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Dec 2023 22:24:49 GMT)\\u00a7r"}']}
{title:'Han et al. (§72023§r)', author: 'Bo Han; Yi Ren; Hao Peng; Teng Zhang; Zeyu Ling; Xiang Yin; Feilin Han', display:{Lore:['[{"text": "arXiv:2312.15946", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnchantDance: Unveiling the Potential of Music-Driven Dance Movement\\u00a7r\\n\\n\\u00a78\\u00a7oBo Han\\nYi Ren\\nHao Peng\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.15946\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Dec 2023 08:19:10 GMT)\\u00a7r"}']}
{title:'He et al. (§72023§r)', author: 'Yuhang He; Zhuangzhuang Dai; Long Chen; Niki Trigoni; Andrew Markham', display:{Lore:['[{"text": "arXiv:2312.16149", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSoundCount: Sound Counting from Raw Audio with Dyadic Decomposition Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oYuhang He\\nZhuangzhuang Dai\\nLong Chen\\nNiki Trigoni\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.16149\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Dec 2023 18:18:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAAAI2024 Paper\\u00a7r"}']}
{title:'Mitra et al. (§72023§r)', author: 'Vikramjit Mitra; Jingping Nie; Erdrin Azemi', display:{Lore:['[{"text": "arXiv:2312.16180", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating salient representations and label Variance in Dimensional Speech Emotion Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oVikramjit Mitra\\nJingping Nie\\nErdrin Azemi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.16180\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2024\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 17 Dec 2023 04:54:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Qifei Li; Yingming Gao; Cong Wang; Yayue Deng; Jinlong Xue; Yichen Han; Ya Li', display:{Lore:['[{"text": "arXiv:2312.16383", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrame-level emotional state alignment method for speech emotion recognition\\u00a7r\\n\\n\\u00a78\\u00a7oQifei Li\\nYingming Gao\\nCong Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.16383\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Dec 2023 03:07:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Mosiński et al. (§72023§r)', author: 'Jakub Mosiński; Piotr Biliński; Thomas Merritt; Abdelhamid Ezzerg; Daniel Korzekwa', display:{Lore:['[{"text": "arXiv:2312.16552", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAE-Flow: AutoEncoder Normalizing Flow\\u00a7r\\n\\n\\u00a78\\u00a7oJakub Mosi\\u0144ski\\nPiotr Bili\\u0144ski\\nThomas Merritt\\nAbdelhamid Ezzerg\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.16552\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Dec 2023 12:29:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Li Li; Shogo Seki', display:{Lore:['[{"text": "arXiv:2312.16836", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRemixed2Remixed: Domain adaptation for speech enhancement by Noise2Noise learning with Remixing\\u00a7r\\n\\n\\u00a78\\u00a7oLi Li\\nShogo Seki\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.16836\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Dec 2023 05:45:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2024\\u00a7r"}']}
{title:'Ma et al. (§72023§r)', author: 'Linhan Ma; Yongmao Zhang; Xinfa Zhu; Yi Lei; Ziqian Ning; Pengcheng Zhu; Lei Xie', display:{Lore:['[{"text": "arXiv:2312.16850", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAccent-VITS:accent transfer for end-to-end TTS\\u00a7r\\n\\n\\u00a78\\u00a7oLinhan Ma\\nYongmao Zhang\\nXinfa Zhu\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.16850\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 29 Dec 2023 05:12:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by NCMMSC2023\\u00a7r"}']}
{title:'Bousquet et al. (§72023§r)', author: 'Pierre-Michel Bousquet; Mickael Rouvier', display:{Lore:['[{"text": "arXiv:2312.16885", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJeffreys divergence-based regularization of neural network output distribution applied to speaker recognition\\u00a7r\\n\\n\\u00a78\\u00a7oPierre-Michel Bousquet\\nMickael Rouvier\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.16885\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Dec 2023 08:18:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ICASSP 2023\\u00a7r"}']}
{title:'Shi (§72023§r)', author: 'Fan Shi', display:{Lore:['[{"text": "arXiv:2312.17281", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRevolutionizing Personalized Voice Synthesis: The Journey towards Emotional and Individual Authenticity with DIVSE (Dynamic Individual Voice Synthesis Engine)\\u00a7r\\n\\n\\u00a78\\u00a7oFan Shi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.17281\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Dec 2023 00:59:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages\\u00a7r"}']}
{title:'Xu (§72023§r)', author: 'Qi Xu', display:{Lore:['[{"text": "arXiv:2312.17633", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Arrow of Time in Music \\u2013 Revisiting the Temporal Structure of Music with Distinguishability and Unique Orientability as the Anchor Point\\u00a7r\\n\\n\\u00a78\\u00a7oQi Xu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.17633\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Dec 2023 10:14:00 GMT)\\u00a7r"}']}
{title:'Luck (§72023§r)', author: 'Geoff Luck', display:{Lore:['[{"text": "arXiv:2401.00209", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAI and Tempo Estimation: A Review\\u00a7r\\n\\n\\u00a78\\u00a7oGeoff Luck\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.00209\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 30 Dec 2023 11:42:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages\\u00a7r"}']}
{title:'Peter (§72023§r)', author: 'Silvan David Peter', display:{Lore:['[{"text": "arXiv:2401.00466", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOnline Symbolic Music Alignment with Offline Reinforcement Learning\\u00a7r\\n\\n\\u00a78\\u00a7oSilvan David Peter\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.00466\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5281/zenodo.10265367\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 24th International Society for Music\\n  Information Retrieval Conference, {ISMIR} 2023, Milan, Italy, November 5-9,\\n  2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 31 Dec 2023 11:42:42 GMT)\\u00a7r"}']}
{title:'Peter et al. (§72023§r)', author: 'Silvan David Peter; Carlos Eduardo Cancino-Chacón; Emmanouil Karystinaios; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2401.00471", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSounding Out Reconstruction Error-Based Evaluation of Generative Models of Expressive Performance\\u00a7r\\n\\n\\u00a78\\u00a7oSilvan David Peter\\nCarlos Eduardo Cancino-Chac\\u00f3n\\nEmmanouil Karystinaios\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.00471\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3625135.3625141\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n10th International Conference on Digital Libraries for Musicology,\\n  November 10, 2023, Milan, Italy\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 31 Dec 2023 11:59:20 GMT)\\u00a7r"}']}
{title:'Yan (§72023§r)', author: 'Nicholas Yan', display:{Lore:['[{"text": "arXiv:2401.01997", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerating Rhythm Game Music with Jukebox\\u00a7r\\n\\n\\u00a78\\u00a7oNicholas Yan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.01997\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Dec 2023 00:57:31 GMT)\\u00a7r"}']}
{title:'Labrador et al. (§72023§r)', author: 'Beltrán Labrador; Manuel Otero-Gonzalez; Alicia Lozano-Diez; Daniel Ramos; Doroteo T. Toledano; Joaquin Gonzalez-Rodriguez', display:{Lore:['[{"text": "arXiv:2401.09441", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoxceleb-ESP: preliminary experiments detecting Spanish celebrities from their voices\\u00a7r\\n\\n\\u00a78\\u00a7oBeltr\\u00e1n Labrador\\nManuel Otero-Gonzalez\\nAlicia Lozano-Diez\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.09441\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Dec 2023 11:55:06 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Weide Liu; Huijing Zhan; Hao Chen; Fengmao Lv', display:{Lore:['[{"text": "arXiv:2401.10747", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach\\u00a7r\\n\\n\\u00a78\\u00a7oWeide Liu\\nHuijing Zhan\\nHao Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.10747\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Dec 2023 06:47:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Ali et al. (§72023§r)', author: 'Ratul Ali; Aktarul Islam; Md. Shohel Rana; Saila Nasrin; Sohel Afzal Shajol; Professor Dr. A. H. M. Saifullah Sadi', display:{Lore:['[{"text": "arXiv:2402.10005", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lML-ASPA: A Contemplation of Machine Learning-based Acoustic Signal Processing Analysis for Sounds,     Strains Emerging Technology\\u00a7r\\n\\n\\u00a78\\u00a7oRatul Ali\\nAktarul Islam\\nMd. Shohel Rana\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.10005\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Dec 2023 03:04:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 5 figures, Article\\u00a7r"}']}
