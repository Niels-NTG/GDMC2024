{title:'Madgazin (§72024§r)', author: 'Vadim R. Madgazin', display:{Lore:['[{"text": "arXiv:1005.2465", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDichotic harmony for the musical practice\\u00a7r\\n\\n\\u00a78\\u00a7oVadim R. Madgazin\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1005.2465\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 8 May 2024 12:02:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages, in Russian, links added\\u00a7r"}']}
{title:'Kameoka et al. (§72024§r)', author: 'Hirokazu Kameoka; Takuhiro Kaneko; Kou Tanaka; Nobukatsu Hojo; Shogo Seki', display:{Lore:['[{"text": "arXiv:2010.02977", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoiceGrad: Non-Parallel Any-to-Many Voice Conversion with Annealed Langevin Dynamics\\u00a7r\\n\\n\\u00a78\\u00a7oHirokazu Kameoka\\nTakuhiro Kaneko\\nKou Tanaka\\nNobukatsu Hojo\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.02977\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 9 Mar 2024 16:30:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oFor more details on the baseline method used for comparison, please refer to our article in arXiv:2008.12604\\u00a7r"}']}
{title:'Giraudo (§72024§r)', author: 'Samuele Giraudo', display:{Lore:['[{"text": "arXiv:2104.13040", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a72math.CO\\u00a7r, \\u00a72math.QA\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe music box operad: Random generation of musical phrases from patterns\\u00a7r\\n\\n\\u00a78\\u00a7oSamuele Giraudo\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.13040\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5920/jcms.1255\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJournal of Creative Music Systems 8, Issue 1, 2024\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 28 Apr 2024 15:29:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o42 pages. Extended version of arXiv:2104.12432\\u00a7r"}']}
{title:'Chou et al. (§72024§r)', author: 'Yi-Hui Chou; I-Chun Chen; Chin-Jui Chang; Joann Ching; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2107.05223", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBERT-like Pre-training for Symbolic Piano Music Classification Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oYi-Hui Chou\\nI-Chun Chen\\nChin-Jui Chang\\nJoann Ching\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.05223\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 14 Apr 2024 03:40:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Journal of Creative Music Systems\\u00a7r"}']}
{title:'Sobhdel et al. (§72024§r)', author: 'Amirreza Sobhdel; Roozbeh Razavi-Far', display:{Lore:['[{"text": "arXiv:2109.10561", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Few-Shot Learning Approach for Sound Source Distance Estimation Using Relation Networks\\u00a7r\\n\\n\\u00a78\\u00a7oAmirreza Sobhdel\\nRoozbeh Razavi-Far\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.10561\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 1 May 2024 10:12:52 GMT)\\u00a7r"}']}
{title:'von Rütte et al. (§72024§r)', author: 'Dimitri von Rütte; Luca Biggio; Yannic Kilcher; Thomas Hofmann', display:{Lore:['[{"text": "arXiv:2201.10936", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFIGARO: Generating Symbolic Music with Fine-Grained Artistic Control\\u00a7r\\n\\n\\u00a78\\u00a7oDimitri von R\\u00fctte\\nLuca Biggio\\nYannic Kilcher\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.10936\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 22 Feb 2024 10:34:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in ICLR2023\\u00a7r"}']}
{title:'Cao et al. (§72024§r)', author: 'Ruizhe Cao; Sherif Abdulatif; Bin Yang', display:{Lore:['[{"text": "arXiv:2203.15149", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCMGAN: Conformer-based Metric GAN for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oRuizhe Cao\\nSherif Abdulatif\\nBin Yang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15149\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-517\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of INTERSPEECH, 2022, pp. 936--940\\u00a7r\\n\\nVersion:\\u00a77v4 (Sun, 3 Mar 2024 15:32:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, 2 tables, published in INTERSPEECH 2022\\u00a7r"}']}
{title:'Romero et al. (§72024§r)', author: 'José Luis Romero; Michael Speckbacher', display:{Lore:['[{"text": "arXiv:2205.10205", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a72math.FA\\u00a7r, \\u00a72math.ST\\u00a7r, \\u00a7cstat.TH\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEstimation of binary time-frequency masks from ambient noise\\u00a7r\\n\\n\\u00a78\\u00a7oJos\\u00e9 Luis Romero\\nMichael Speckbacher\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.10205\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 2 Feb 2024 14:17:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o30 pages, 2 figures\\u00a7r"}']}
{title:'Sun et al. (§72024§r)', author: 'Qinggang Sun; Kejun Wang', display:{Lore:['[{"text": "arXiv:2207.11749", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSimultaneous source separation of unknown numbers of single-channel underwater acoustic signals based on deep neural networks with separator-decoder structure\\u00a7r\\n\\n\\u00a78\\u00a7oQinggang Sun\\nKejun Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.11749\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 28 May 2024 09:13:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages, 5 figures, 4 tables. For codes, see https://github.com/QinggangSUN/unknown_number_source_separation\\u00a7r"}']}
{title:'Moussa et al. (§72024§r)', author: 'Denise Moussa; Germans Hirsch; Christian Riess', display:{Lore:['[{"text": "arXiv:2207.14682", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Unconstrained Audio Splicing Detection and Localization with Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oDenise Moussa\\nGermans Hirsch\\nChristian Riess\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.14682\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-031-37742-6_22\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 3 May 2024 14:52:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at MMFORWILD 2022, ICPR Workshops - Code: https://faui1-gitlab.cs.fau.de/denise.moussa/audio-splicing-localization .International Conference on PatternRecognition. Cham: Springer Nature Switzerland, 2022\\u00a7r"}']}
{title:'Raza et al. (§72024§r)', author: 'Mohsin Raza; Leandro A. Passos; Ahmed Khubaib; Ahsan Adeel', display:{Lore:['[{"text": "arXiv:2209.03275", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultimodal Speech Enhancement Using Burst Propagation\\u00a7r\\n\\n\\u00a78\\u00a7oMohsin Raza\\nLeandro A. Passos\\nAhmed Khubaib\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.03275\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 5 Feb 2024 17:54:04 GMT)\\u00a7r"}']}
{title:'Abdulatif et al. (§72024§r)', author: 'Sherif Abdulatif; Ruizhe Cao; Bin Yang', display:{Lore:['[{"text": "arXiv:2209.11112", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCMGAN: Conformer-Based Metric-GAN for Monaural Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oSherif Abdulatif\\nRuizhe Cao\\nBin Yang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.11112\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2024.3393718\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol. 32, pp. 2477-2493, 2024\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 3 May 2024 21:38:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o17 pages, 11 figures, and 6tables. arXiv admin note:text overlap with arXiv:2203.15149\\u00a7r"}']}
{title:'Ajmi et al. (§72024§r)', author: 'Sahar Al Ajmi; Khizar Hayat; Alaa M. Al Obaidi; Naresh Kumar; Munaf Najmuldeen; Baptiste Magnier', display:{Lore:['[{"text": "arXiv:2209.12573", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFaked Speech Detection with Zero Prior Knowledge\\u00a7r\\n\\n\\u00a78\\u00a7oSahar Al Ajmi\\nKhizar Hayat\\nAlaa M. Al Obaidi\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.12573\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s42452-024-05893-3\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nDiscover Applied Sciences, vol. 6 (288), May 2024\\u00a7r\\n\\nVersion:\\u00a77v6 (Tue, 2 Apr 2024 07:58:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages, 4 figures (6 if you count subfigures), 2 tables\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Haohe Liu; Xubo Liu; Qiuqiang Kong; Wenwu Wang; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:2210.01719", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Temporal Resolution in Spectrogram for Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oHaohe Liu\\nXubo Liu\\nQiuqiang Kong\\nWenwu Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.01719\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 12 Jan 2024 18:35:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the 38th Annual AAAI Conference on Artificial Intelligence\\u00a7r"}']}
{title:'He et al. (§72024§r)', author: 'Shulin He; Huaiwen Zhang; Wei Rao; Kanghao Zhang; Yukai Ju; Yang Yang; Xueliang Zhang', display:{Lore:['[{"text": "arXiv:2210.15849", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierarchical speaker representation for target speaker extraction\\u00a7r\\n\\n\\u00a78\\u00a7oShulin He\\nHuaiwen Zhang\\nWei Rao\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15849\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 5 Jan 2024 03:06:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Ellinas et al. (§72024§r)', author: 'Nikolaos Ellinas; Georgios Vamvoukakis; Konstantinos Markopoulos; Georgia Maniati; Panos Kakoulidis; June Sig Sung; Inchul Hwang; Spyros Raptis; Aimilios Chalamandaris; Pirros Tsiakoulis', display:{Lore:['[{"text": "arXiv:2210.17264", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-lingual Text-To-Speech with Flow-based Voice Conversion for Improved Pronunciation\\u00a7r\\n\\n\\u00a78\\u00a7oNikolaos Ellinas\\nGeorgios Vamvoukakis\\nKonstantinos Markopoulos\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.17264\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 27 Feb 2024 13:14:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oFundamental changes to the model described and experimental procedure\\u00a7r"}']}
{title:'Szatkowski et al. (§72024§r)', author: 'Filip Szatkowski; Karol J. Piczak; Przemysław Spurek; Jacek Tabor; Tomasz Trzciński', display:{Lore:['[{"text": "arXiv:2211.01839", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHyperSound: Generating Implicit Neural Representations of Audio Signals with Hypernetworks\\u00a7r\\n\\n\\u00a78\\u00a7oFilip Szatkowski\\nKarol J. Piczak\\nPrzemys\\u0142aw Spurek\\nJacek Tabor\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01839\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 Jan 2024 16:49:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oNeurIPS 2022 MetaLearn workshop\\u00a7r"}']}
{title:'Yi et al. (§72024§r)', author: 'Jiangyan Yi; Chenglong Wang; Jianhua Tao; Chu Yuan Zhang; Cunhang Fan; Zhengkun Tian; Haoxin Ma; Ruibo Fu', display:{Lore:['[{"text": "arXiv:2211.06073", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSceneFake: An Initial Dataset and Benchmarks for Scene Fake Audio Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJiangyan Yi\\nChenglong Wang\\nJianhua Tao\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.06073\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 4 Apr 2024 09:58:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Pattern Recognition, 1 April 2024\\u00a7r"}']}
{title:'Wu et al. (§72024§r)', author: 'Yusong Wu; Ke Chen; Tianyu Zhang; Yuchen Hui; Marianna Nezhurina; Taylor Berg-Kirkpatrick; Shlomo Dubnov', display:{Lore:['[{"text": "arXiv:2211.06687", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLarge-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oYusong Wu\\nKe Chen\\nTianyu Zhang\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.06687\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 21 Mar 2024 21:35:04 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Fangyuan Wang; Bo Xu; Bo Xu', display:{Lore:['[{"text": "arXiv:2211.11419", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSSCFormer: Push the Limit of Chunk-wise Conformer for Streaming ASR Using Sequentially Sampled Chunks and Chunked Causal Convolution\\u00a7r\\n\\n\\u00a78\\u00a7oFangyuan Wang\\nBo Xu\\nBo Xu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.11419\\u00a7r\\n\\nVersion:\\u00a77v4 (Sun, 4 Feb 2024 08:03:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis manuscript has been accepted by SPL\\u00a7r"}']}
{title:'Atito et al. (§72024§r)', author: 'Sara Atito; Muhammad Awais; Wenwu Wang; Mark D Plumbley; Josef Kittler', display:{Lore:['[{"text": "arXiv:2211.13189", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lASiT: Local-Global Audio Spectrogram vIsion Transformer for Event Classification\\u00a7r\\n\\n\\u00a78\\u00a7oSara Atito\\nMuhammad Awais\\nWenwu Wang\\nMark D Plumbley\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.13189\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 10 Mar 2024 19:56:17 GMT)\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Kai Li; Fenghua Xie; Hang Chen; Kexin Yuan; Xiaolin Hu', display:{Lore:['[{"text": "arXiv:2212.10744", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Audio-Visual Speech Separation Model Inspired by Cortico-Thalamo-Cortical Circuits\\u00a7r\\n\\n\\u00a78\\u00a7oKai Li\\nFenghua Xie\\nHang Chen\\nKexin Yuan\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.10744\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 22 Mar 2024 14:33:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by TPAMI 2024\\u00a7r"}']}
{title:'Ren et al. (§72024§r)', author: 'Zhao Ren; Yi Chang; Thanh Tam Nguyen; Yang Tan; Kun Qian; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2301.09362", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comprehensive Survey on Heart Sound Analysis in the Deep Learning Era\\u00a7r\\n\\n\\u00a78\\u00a7oZhao Ren\\nYi Chang\\nThanh Tam Nguyen\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.09362\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 11 May 2024 16:37:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE Computational Intelligence Magazine\\u00a7r"}']}
{title:'Mariani et al. (§72024§r)', author: 'Giorgio Mariani; Irene Tallini; Emilian Postolache; Michele Mancusi; Luca Cosmo; Emanuele Rodolà', display:{Lore:['[{"text": "arXiv:2302.02257", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Source Diffusion Models for Simultaneous Music Generation and Separation\\u00a7r\\n\\n\\u00a78\\u00a7oGiorgio Mariani\\nIrene Tallini\\nEmilian Postolache\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.02257\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 18 Mar 2024 11:39:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICLR 2024 oral presentation.Demo page: https://gladia-research-group.github.io/multi-source-diffusion-models/\\u00a7r"}']}
{title:'Ma et al. (§72024§r)', author: 'Jianbo Ma; Siqi Pan; Deepak Chandran; Andrea Fanelli; Richard Cartwright', display:{Lore:['[{"text": "arXiv:2302.13451", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA low latency attention module for streaming self-supervised speech representation learning\\u00a7r\\n\\n\\u00a78\\u00a7oJianbo Ma\\nSiqi Pan\\nDeepak Chandran\\nAndrea Fanelli\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.13451\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 18 Mar 2024 01:09:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o19 pages, 4 figures\\u00a7r"}']}
{title:'Selvaraj et al. (§72024§r)', author: 'Nithish Muthuchamy Selvaraj; Xiaobao Guo; Adams Kong; Bingquan Shen; Alex Kot', display:{Lore:['[{"text": "arXiv:2302.14314", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdapter Incremental Continual Learning of Efficient Audio Spectrogram Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oNithish Muthuchamy Selvaraj\\nXiaobao Guo\\nAdams Kong\\nBingquan Shen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.14314\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 2 Jan 2024 07:26:22 GMT)\\u00a7r"}']}
{title:'Huh et al. (§72024§r)', author: 'Mina Huh; Ruchira Ray; Corey Karnei', display:{Lore:['[{"text": "arXiv:2303.00510", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparison of Speech Data Augmentation Methods Using S3PRL Toolkit\\u00a7r\\n\\n\\u00a78\\u00a7oMina Huh\\nRuchira Ray\\nCorey Karnei\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.00510\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 29 Mar 2024 08:01:32 GMT)\\u00a7r"}']}
{title:'Scott et al. (§72024§r)', author: 'K. Jack Scott; Lucinda J. Speers; David K. Bilkey', display:{Lore:['[{"text": "arXiv:2303.03183", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUtilizing synthetic training data for the supervised classification of rat ultrasonic vocalizations\\u00a7r\\n\\n\\u00a78\\u00a7oK. Jack Scott\\nLucinda J. Speers\\nDavid K. Bilkey\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.03183\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0024340\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJ Acoust Soc Am 1 January 2024 155 (1)\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 19 Jan 2024 02:31:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o25 pages, 5 main figures, 2 tables\\u00a7r"}']}
{title:'Bhattacharya et al. (§72024§r)', author: 'Aneesh Bhattacharya; Manas Paranjape; Uttaran Bhattacharya; Aniket Bera', display:{Lore:['[{"text": "arXiv:2303.03870", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDanceAnyWay: Synthesizing Beat-Guided 3D Dances with Randomized Temporal Contrastive Learning\\u00a7r\\n\\n\\u00a78\\u00a7oAneesh Bhattacharya\\nManas Paranjape\\nUttaran Bhattacharya\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.03870\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 23 Feb 2024 06:20:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 7 figures, 3 tables. To appear as part of the proceedings of the 38th Annual AAAI Conference on Artificial Intelligence, 2024\\u00a7r"}']}
{title:'Yadav et al. (§72024§r)', author: 'Hemant Yadav; Sunayana Sitaram; Rajiv Ratn Shah', display:{Lore:['[{"text": "arXiv:2303.06982", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalysing the Masked predictive coding training criterion for pre-training a Speech Representation Model\\u00a7r\\n\\n\\u00a78\\u00a7oHemant Yadav\\nSunayana Sitaram\\nRajiv Ratn Shah\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.06982\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 11 Jan 2024 11:15:38 GMT)\\u00a7r"}']}
{title:'Xu et al. (§72024§r)', author: 'Xuenan Xu; Zhiling Zhang; Zelin Zhou; Pingyue Zhang; Zeyu Xie; Mengyue Wu; Kenny Q. Zhu', display:{Lore:['[{"text": "arXiv:2303.07902", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBLAT: Bootstrapping Language-Audio Pre-training based on AudioSet Tag-guided Synthetic Data\\u00a7r\\n\\n\\u00a78\\u00a7oXuenan Xu\\nZhiling Zhang\\nZelin Zhou\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07902\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 5 Mar 2024 15:29:57 GMT)\\u00a7r"}']}
{title:'Tsai et al. (§72024§r)', author: 'Ping-Rui Tsai; Yen-Ting Chou; Nathan-Christopher Wang; Hui-Ling Chen; Hong-Yue Huang; Zih-Jia Luo; Tzay-Ming Hong', display:{Lore:['[{"text": "arXiv:2303.13631", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIn-depth analysis of music structure as a text network\\u00a7r\\n\\n\\u00a78\\u00a7oPing-Rui Tsai\\nYen-Ting Chou\\nNathan-Christopher Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.13631\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 2 Jan 2024 09:35:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 8 figures\\u00a7r"}']}
{title:'Khan et al. (§72024§r)', author: 'Ibrahim Khan; Thai Van Nguyen; Chollakorn Nimpattanavong; Ruck Thawonmas', display:{Lore:['[{"text": "arXiv:2303.15734", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptive Background Music for a Fighting Game: A Multi-Instrument Volume Modulation Approach\\u00a7r\\n\\n\\u00a78\\u00a7oIbrahim Khan\\nThai Van Nguyen\\nChollakorn Nimpattanavong\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.15734\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 5 Mar 2024 06:33:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn the updated version, the description of the association between the distance between the two players (PD) and the instrument\'s volume on page 3 has been revised\\u00a7r"}']}
{title:'Gharib et al. (§72024§r)', author: 'Shayan Gharib; Minh Tran; Diep Luong; Konstantinos Drossos; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2305.00011", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Representation Learning for Robust Privacy Preservation in Audio\\u00a7r\\n\\n\\u00a78\\u00a7oShayan Gharib\\nMinh Tran\\nDiep Luong\\nKonstantinos Drossos\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.00011\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/OJSP.2023.3349113\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 3 Jan 2024 13:51:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in IEEEOpen Journal of Signal Processing\\u00a7r"}']}
{title:'Budaghyan et al. (§72024§r)', author: 'David Budaghyan; Charles C. Onu; Arsenii Gorin; Cem Subakan; Doina Precup', display:{Lore:['[{"text": "arXiv:2305.00969", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Budaghyan\\nCharles C. Onu\\nArsenii Gorin\\nCem Subakan\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.00969\\u00a7r\\n\\nVersion:\\u00a77v7 (Thu, 21 Mar 2024 17:52:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2024\\u00a7r"}']}
{title:'Sadok et al. (§72024§r)', author: 'Samir Sadok; Simon Leglaive; Renaud Séguier', display:{Lore:['[{"text": "arXiv:2305.03568", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA vector quantized masked autoencoder for audiovisual speech emotion recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSamir Sadok\\nSimon Leglaive\\nRenaud S\\u00e9guier\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.03568\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 15 May 2024 13:54:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 5 figures, https://samsad35.github.io/VQ-MAE-AudioVisual/\\u00a7r"}']}
{title:'Sadok et al. (§72024§r)', author: 'Samir Sadok; Simon Leglaive; Laurent Girin; Xavier Alameda-Pineda; Renaud Séguier', display:{Lore:['[{"text": "arXiv:2305.03582", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA multimodal dynamical variational autoencoder for audiovisual speech representation learning\\u00a7r\\n\\n\\u00a78\\u00a7oSamir Sadok\\nSimon Leglaive\\nLaurent Girin\\nXavier Alameda-Pineda\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.03582\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.neunet.2024.106120\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 20 Feb 2024 16:18:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 figures, https://samsad35.github.io/site-mdvae/\\u00a7r"}']}
{title:'Su et al. (§72024§r)', author: 'Kun Su; Judith Yue Li; Qingqing Huang; Dima Kuzmin; Joonseok Lee; Chris Donahue; Fei Sha; Aren Jansen; Yu Wang; Mauro Verzetti; Timo I. Denk', display:{Lore:['[{"text": "arXiv:2305.06594", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lV2Meow: Meowing to the Visual Beat via Video-to-Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oKun Su\\nJudith Yue Li\\nQingqing Huang\\n+ 7 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.06594\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 22 Feb 2024 05:58:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at AAAI2024, music samples available at https://tinyurl.com/v2meow\\u00a7r"}']}
{title:'Solovyev et al. (§72024§r)', author: 'Roman Solovyev; Alexander Stempkovskiy; Tatiana Habruseva', display:{Lore:['[{"text": "arXiv:2305.07489", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBenchmarks and leaderboards for sound demixing tasks\\u00a7r\\n\\n\\u00a78\\u00a7oRoman Solovyev\\nAlexander Stempkovskiy\\nTatiana Habruseva\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.07489\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 7 May 2024 10:35:10 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Zihao Wang; Le Ma; Chen Zhang; Bo Han; Yunfei Xu; Yikai Wang; Xinyi Chen; HaoRong Hong; Wenbo Liu; Xinda Wu; Kejun Zhang', display:{Lore:['[{"text": "arXiv:2305.08029", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lREMAST: Real-time Emotion-based Music Arrangement with Soft Transition\\u00a7r\\n\\n\\u00a78\\u00a7oZihao Wang\\nLe Ma\\nChen Zhang\\n+ 7 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.08029\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 5 Feb 2024 08:35:33 GMT)\\u00a7r"}']}
{title:'Shi et al. (§72024§r)', author: 'Hao Shi; Kazuki Shimada; Masato Hirano; Takashi Shibuya; Yuichiro Koyama; Zhi Zhong; Shusuke Takahashi; Tatsuya Kawahara; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2305.10734", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffusion-Based Speech Enhancement with Joint Generative and Predictive Decoders\\u00a7r\\n\\n\\u00a78\\u00a7oHao Shi\\nKazuki Shimada\\nMasato Hirano\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10734\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 28 Feb 2024 12:10:19 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Zizheng Zhang; Chen Chen; Hsin-Hung Chen; Xiang Liu; Yuchen Hu; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2305.10761", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoise-Aware Speech Separation with Contrastive Learning\\u00a7r\\n\\n\\u00a78\\u00a7oZizheng Zhang\\nChen Chen\\nHsin-Hung Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10761\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 8 Jan 2024 05:24:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, ICASSP 2024\\u00a7r"}']}
{title:'Han et al. (§72024§r)', author: 'Cong Han; Kevin Wilson; Scott Wisdom; John R. Hershey', display:{Lore:['[{"text": "arXiv:2305.11151", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Multi-channel Separation and Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oCong Han\\nKevin Wilson\\nScott Wisdom\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11151\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 22 Mar 2024 17:21:20 GMT)\\u00a7r"}']}
{title:'Zhong et al. (§72024§r)', author: 'Yi Zhong; Chen Zhang; Xule Liu; Chenxi Sun; Weishan Deng; Haifeng Hu; Zhongqian Sun', display:{Lore:['[{"text": "arXiv:2305.12107", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEE-TTS: Emphatic Expressive TTS with Linguistic Information\\u00a7r\\n\\n\\u00a78\\u00a7oYi Zhong\\nChen Zhang\\nXule Liu\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12107\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 14 Apr 2024 12:33:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2023, fix some typos\\u00a7r"}']}
{title:'Gasenzer et al. (§72024§r)', author: 'Konstantin Gasenzer; Moritz Wolter', display:{Lore:['[{"text": "arXiv:2305.13033", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards generalizing deep-audio fake detection networks\\u00a7r\\n\\n\\u00a78\\u00a7oKonstantin Gasenzer\\nMoritz Wolter\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13033\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPublished in Transactions on Machine Learning Research (04/2024)\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 9 Apr 2024 16:22:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCode available at: https://github.com/gan-police/audiodeepfake-detection\\u00a7r"}']}
{title:'Rajapakshe et al. (§72024§r)', author: 'Thejan Rajapakshe; Rajib Rana; Sara Khalifa; Berrak Sisman; Björn Schuller', display:{Lore:['[{"text": "arXiv:2305.14402", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Speech Emotion Recognition Through Differentiable Architecture Search\\u00a7r\\n\\n\\u00a78\\u00a7oThejan Rajapakshe\\nRajib Rana\\nSara Khalifa\\nBerrak Sisman\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.14402\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 19 Jan 2024 00:16:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures\\u00a7r"}']}
{title:'B. et al. (§72024§r)', author: 'Matías P. Pizarro B.; Dorothea Kolossa; Asja Fischer', display:{Lore:['[{"text": "arXiv:2305.17000", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDistriBlock: Identifying adversarial audio samples by leveraging characteristics of the output distribution\\u00a7r\\n\\n\\u00a78\\u00a7oMat\\u00edas P. Pizarro B.\\nDorothea Kolossa\\nAsja Fischer\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.17000\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 22 May 2024 15:04:36 GMT)\\u00a7r"}']}
{title:'Xie et al. (§72024§r)', author: 'Yuan Xie; Jiawei Ren; Ji Xu', display:{Lore:['[{"text": "arXiv:2305.19612", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnderwater-Art: Expanding Information Perspectives With Text Templates For Underwater Acoustic Target Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYuan Xie\\nJiawei Ren\\nJi Xu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19612\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0015053\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nThe Journal of the Acoustical Society of America, 2022, 152(5):\\n  2641-2651\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 19 Feb 2024 13:32:56 GMT)\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Yizhi Li; Ruibin Yuan; Ge Zhang; Yinghao Ma; Xingran Chen; Hanzhi Yin; Chenghao Xiao; Chenghua Lin; Anton Ragni; Emmanouil Benetos; Norbert Gyenge; Roger Dannenberg; Ruibo Liu; Wenhu Chen; Gus Xia; Yemin Shi; Wenhao Huang; Zili Wang; Yike Guo; Jie Fu', display:{Lore:['[{"text": "arXiv:2306.00107", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training\\u00a7r\\n\\n\\u00a78\\u00a7oYizhi Li\\nRuibin Yuan\\nGe Zhang\\n+ 16 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00107\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 22 Apr 2024 21:52:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by ICLR 2024\\u00a7r"}']}
{title:'Siuzdak (§72024§r)', author: 'Hubert Siuzdak', display:{Lore:['[{"text": "arXiv:2306.00814", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oHubert Siuzdak\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00814\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 29 May 2024 14:21:47 GMT)\\u00a7r"}']}
{title:'Copet et al. (§72024§r)', author: 'Jade Copet; Felix Kreuk; Itai Gat; Tal Remez; David Kant; Gabriel Synnaeve; Yossi Adi; Alexandre Défossez', display:{Lore:['[{"text": "arXiv:2306.05284", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSimple and Controllable Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oJade Copet\\nFelix Kreuk\\nItai Gat\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.05284\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 30 Jan 2024 04:49:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at Neurips 2023\\u00a7r"}']}
{title:'Feng et al. (§72024§r)', author: 'Tiantian Feng; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2306.05350", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPEFT-SER: On the Use of Parameter Efficient Transfer Learning Approaches For Speech Emotion Recognition Using Pre-trained Speech Models\\u00a7r\\n\\n\\u00a78\\u00a7oTiantian Feng\\nShrikanth Narayanan\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.05350\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ACII59096.2023.10388152\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 14 Feb 2024 09:26:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work was accepted to the 11th International Conference on Affective Computing and Intelligent Interaction (ACII), 2023\\u00a7r"}']}
{title:'Xu et al. (§72024§r)', author: 'Ji Xu; Yuan Xie; Wenchao Wang', display:{Lore:['[{"text": "arXiv:2306.06945", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnderwater Acoustic Target Recognition based on Smoothness-inducing Regularization and Spectrogram-based Data Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oJi Xu\\nYuan Xie\\nWenchao Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.06945\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.oceaneng.2023.114926\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nOcean Engineering, 2023, 281: 114926\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 30 Apr 2024 06:59:02 GMT)\\u00a7r"}']}
{title:'Du et al. (§72024§r)', author: 'Chenpeng Du; Yiwei Guo; Feiyu Shen; Zhijun Liu; Zheng Liang; Xie Chen; Shuai Wang; Hui Zhang; Kai Yu', display:{Lore:['[{"text": "arXiv:2306.07547", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding\\u00a7r\\n\\n\\u00a78\\u00a7oChenpeng Du\\nYiwei Guo\\nFeiyu Shen\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.07547\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1609/aaai.v38i16.29747\\u00a7r\\n\\nVersion:\\u00a77v6 (Thu, 28 Mar 2024 13:56:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to AAAI2024\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Yenan Zhang; Guilly Kolkman; Hiroshi Watanabe', display:{Lore:['[{"text": "arXiv:2306.11282", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhase Repair for Time-Domain Convolutional Neural Networks in Music Super-Resolution\\u00a7r\\n\\n\\u00a78\\u00a7oYenan Zhang\\nGuilly Kolkman\\nHiroshi Watanabe\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.11282\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 18 Feb 2024 06:40:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Yin Li; Rohan Reddy; Cheng Zhang; Rajalakshmi Nandakumar', display:{Lore:['[{"text": "arXiv:2306.17477", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBeyond-Voice: Towards Continuous 3D Hand Pose Tracking on Commercial Home Assistant Devices\\u00a7r\\n\\n\\u00a78\\u00a7oYin Li\\nRohan Reddy\\nCheng Zhang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.17477\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 8 Mar 2024 23:45:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IPSN 2024\\u00a7r"}']}
{title:'Chen et al. (§72024§r)', author: 'Weidong Chen; Xiaofen Xing; Peihao Chen; Xiangmin Xu', display:{Lore:['[{"text": "arXiv:2307.10757", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVesper: A Compact and Effective Pretrained Model for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oWeidong Chen\\nXiaofen Xing\\nPeihao Chen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.10757\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 18 Apr 2024 13:08:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper was accepted by IEEE Transactions on Affective Computing 2024\\u00a7r"}']}
{title:'Vallés-Pérez et al. (§72024§r)', author: 'Ivan Vallés-Pérez; Grzegorz Beringer; Piotr Bilinski; Gary Cook; Roberto Barra-Chicote', display:{Lore:['[{"text": "arXiv:2307.12445", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSCRAPS: Speech Contrastive Representations of Acoustic and Phonetic Spaces\\u00a7r\\n\\n\\u00a78\\u00a7oIvan Vall\\u00e9s-P\\u00e9rez\\nGrzegorz Beringer\\nPiotr Bilinski\\nGary Cook\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.12445\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 Jan 2024 23:09:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn proceedings of the 26th European Conference on Artificial Intelligence ECAI 2023. 8 pages + 1 appendix page\\u00a7r"}']}
{title:'Fish et al. (§72024§r)', author: 'Edward Fish; Umberto Michieli; Mete Ozay', display:{Lore:['[{"text": "arXiv:2307.12659", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Model for Every User and Budget: Label-Free and Personalized Mixed-Precision Quantization\\u00a7r\\n\\n\\u00a78\\u00a7oEdward Fish\\nUmberto Michieli\\nMete Ozay\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.12659\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 11 Feb 2024 12:07:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023. Code is available at https://github.com/SamsungLabs/myQASR\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Hongkuan Zhang; Qiyuan Wang; Mathias Fink; Guancong Ma', display:{Lore:['[{"text": "arXiv:2308.01531", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.app-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimizing multi-user indoor sound communications with acoustic reconfigurable metasurfaces\\u00a7r\\n\\n\\u00a78\\u00a7oHongkuan Zhang\\nQiyuan Wang\\nMathias Fink\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.01531\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1038/s41467-024-45435-4\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nNature Communications (2024)\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 10 Feb 2024 13:01:29 GMT)\\u00a7r"}']}
{title:'Pan et al. (§72024§r)', author: 'Yu Pan; Yuguang Yang; Yuheng Huang; Jixun Yao; Jingjing Yin; Yanni Hu; Heng Lu; Lei Ma; Jianjun Zhao', display:{Lore:['[{"text": "arXiv:2308.04025", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMSAC: Multiple Speech Attribute Control Method for Reliable Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYu Pan\\nYuguang Yang\\nYuheng Huang\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.04025\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 22 Mar 2024 14:49:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages\\u00a7r"}']}
{title:'Ge et al. (§72024§r)', author: 'Zirui Ge; Xinzhou Xu; Haiyan Guo; Tingting Wang; Zhen Yang', display:{Lore:['[{"text": "arXiv:2308.04666", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Recognition Using Isomorphic Graph Attention Network Based Pooling on Self-Supervised Representation\\u00a7r\\n\\n\\u00a78\\u00a7oZirui Ge\\nXinzhou Xu\\nHaiyan Guo\\nTingting Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.04666\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 24 Feb 2024 03:06:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 4 figures\\u00a7r"}']}
{title:'Borrel-Jensen et al. (§72024§r)', author: 'Nikolas Borrel-Jensen; Somdatta Goswami; Allan P. Engsig-Karup; George Em Karniadakis; Cheol-Ho Jeong', display:{Lore:['[{"text": "arXiv:2308.05141", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound propagation in realistic interactive 3D scenes with parameterized sources using deep neural operators\\u00a7r\\n\\n\\u00a78\\u00a7oNikolas Borrel-Jensen\\nSomdatta Goswami\\nAllan P. Engsig-Karup\\nGeorge Em Karniadakis\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.05141\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 13 Jan 2024 11:40:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o25 pages, 10 figures, 4 tables\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Haohe Liu; Yi Yuan; Xubo Liu; Xinhao Mei; Qiuqiang Kong; Qiao Tian; Yuping Wang; Wenwu Wang; Yuxuan Wang; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:2308.05734", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining\\u00a7r\\n\\n\\u00a78\\u00a7oHaohe Liu\\nYi Yuan\\nXubo Liu\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.05734\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 11 May 2024 11:24:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE/ACM Transactionson Audio, Speech and Language Processing. Project page is https://audioldm.github.io/audioldm2\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Fan Zhang; Naye Ji; Fuxing Gao; Siyuan Zhao; Zhaohan Wang; Shunman Li', display:{Lore:['[{"text": "arXiv:2308.05995", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio is all in one: speech-driven gesture synthetics using WavLM pre-trained model\\u00a7r\\n\\n\\u00a78\\u00a7oFan Zhang\\nNaye Ji\\nFuxing Gao\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.05995\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 13 Apr 2024 15:22:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis article needs major revision\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Kai Li; Runxuan Yang; Fuchun Sun; Xiaolin Hu', display:{Lore:['[{"text": "arXiv:2308.08143", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIIANet: An Intra- and Inter-Modality Attention Network for Audio-Visual Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oKai Li\\nRunxuan Yang\\nFuchun Sun\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.08143\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 2 Feb 2024 09:37:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages, 6 figures\\u00a7r"}']}
{title:'Nguyen et al. (§72024§r)', author: 'Thai-Binh Nguyen; Alexander Waibel', display:{Lore:['[{"text": "arXiv:2308.11380", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConvoifilter: A case study of doing cocktail party speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oThai-Binh Nguyen\\nAlexander Waibel\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.11380\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 7 Apr 2024 13:27:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at HSCMA 2024\\u00a7r"}']}
{title:'Chen et al. (§72024§r)', author: 'Guangyu Chen; Yu Wu; Shujie Liu; Tao Liu; Xiaoyong Du; Furu Wei', display:{Lore:['[{"text": "arXiv:2308.12770", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWavMark: Watermarking for Audio Generation\\u00a7r\\n\\n\\u00a78\\u00a7oGuangyu Chen\\nYu Wu\\nShujie Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.12770\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 7 Jan 2024 07:05:37 GMT)\\u00a7r"}']}
{title:'Dibbo et al. (§72024§r)', author: 'Sayanton V. Dibbo; Juston S. Moore; Garrett T. Kenyon; Michael A. Teti', display:{Lore:['[{"text": "arXiv:2308.12882", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLCANets++: Robust Audio Classification using Multi-layer Neural Networks with Lateral Competition\\u00a7r\\n\\n\\u00a78\\u00a7oSayanton V. Dibbo\\nJuston S. Moore\\nGarrett T. Kenyon\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.12882\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 27 Mar 2024 14:47:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at 2024 IEEE InternationalConference on Acoustics, Speech and Signal Processing Workshops (ICASSPW)\\u00a7r"}']}
{title:'Shibuya et al. (§72024§r)', author: 'Takashi Shibuya; Yuhta Takida; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2309.02836", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBigVSAN: Enhancing GAN-based Neural Vocoders with Slicing Adversarial Network\\u00a7r\\n\\n\\u00a78\\u00a7oTakashi Shibuya\\nYuhta Takida\\nYuki Mitsufuji\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.02836\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 25 Mar 2024 03:17:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2024. Equation (5) in the previous version is wrong. We modified it\\u00a7r"}']}
{title:'Park et al. (§72024§r)', author: 'Jeongsoo Park; Dong-Gyun Han; Hyoung Sul La; Sangmin Lee; Yoonchang Han; Eun-Jin Yang', display:{Lore:['[{"text": "arXiv:2309.03451", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-domain Sound Recognition for Efficient Underwater Data Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oJeongsoo Park\\nDong-Gyun Han\\nHyoung Sul La\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.03451\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 21 Feb 2024 05:02:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to APSIPA 2023\\u00a7r"}']}
{title:'Brima et al. (§72024§r)', author: 'Yusuf Brima; Ulf Krumnack; Simone Pika; Gunther Heidemann', display:{Lore:['[{"text": "arXiv:2309.03619", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnderstanding Self-Supervised Learning of Speech Representation via Invariance and Redundancy Reduction\\u00a7r\\n\\n\\u00a78\\u00a7oYusuf Brima\\nUlf Krumnack\\nSimone Pika\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.03619\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 24 Jan 2024 13:37:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 5 figures, in submission to MDPI Information\\u00a7r"}']}
{title:'Du et al. (§72024§r)', author: 'Yu Du; Xu Liu; Yansong Chua', display:{Lore:['[{"text": "arXiv:2309.03641", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpiking Structured State Space Model for Monaural Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oYu Du\\nXu Liu\\nYansong Chua\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.03641\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 21 Apr 2024 03:39:55 GMT)\\u00a7r"}']}
{title:'Cui et al. (§72024§r)', author: 'Meng Cui; Xubo Liu; Haohe Liu; Zhuangzhuang Du; Tao Chen; Guoping Lian; Daoliang Li; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2309.05058", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultimodal Fish Feeding Intensity Assessment in Aquaculture\\u00a7r\\n\\n\\u00a78\\u00a7oMeng Cui\\nXubo Liu\\nHaohe Liu\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.05058\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 19 May 2024 22:54:32 GMT)\\u00a7r"}']}
{title:'Elizalde et al. (§72024§r)', author: 'Benjamin Elizalde; Soham Deshmukh; Huaming Wang', display:{Lore:['[{"text": "arXiv:2309.05767", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNatural Language Supervision for General-Purpose Audio Representations\\u00a7r\\n\\n\\u00a78\\u00a7oBenjamin Elizalde\\nSoham Deshmukh\\nHuaming Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.05767\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 6 Feb 2024 21:42:03 GMT)\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Baifeng Li; Qingmu Liu; Yuhong Yang; Hongyang Chen; Weiping Tu; Song Lin', display:{Lore:['[{"text": "arXiv:2309.06858", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEMALG: An Enhanced Mandarin Lombard Grid Corpus with Meaningful Sentences\\u00a7r\\n\\n\\u00a78\\u00a7oBaifeng Li\\nQingmu Liu\\nYuhong Yang\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.06858\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 Jan 2024 13:21:57 GMT)\\u00a7r"}']}
{title:'Pepino et al. (§72024§r)', author: 'Leonardo Pepino; Pablo Riera; Luciana Ferrer', display:{Lore:['[{"text": "arXiv:2309.07391", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnCodecMAE: Leveraging neural codecs for universal audio representation learning\\u00a7r\\n\\n\\u00a78\\u00a7oLeonardo Pepino\\nPablo Riera\\nLuciana Ferrer\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07391\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 21 May 2024 00:39:47 GMT)\\u00a7r"}']}
{title:'Zang et al. (§72024§r)', author: 'Yongyi Zang; You Zhang; Mojtaba Heydari; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2309.07525", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSingFake: Singing Voice Deepfake Detection\\u00a7r\\n\\n\\u00a78\\u00a7oYongyi Zang\\nYou Zhang\\nMojtaba Heydari\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07525\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 21 Jan 2024 08:57:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2024\\u00a7r"}']}
{title:'Chen et al. (§72024§r)', author: 'Sizhou Chen; Songyang Gao; Sen Fang', display:{Lore:['[{"text": "arXiv:2309.07765", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEchotune: A Modular Extractor Leveraging the Variable-Length Nature of Speech in ASR Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oSizhou Chen\\nSongyang Gao\\nSen Fang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07765\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Apr 2024 03:30:34 GMT)\\u00a7r"}']}
{title:'Yuan et al. (§72024§r)', author: 'Yi Yuan; Haohe Liu; Xubo Liu; Qiushi Huang; Mark D. Plumbley; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2309.08051", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRetrieval-Augmented Text-to-Audio Generation\\u00a7r\\n\\n\\u00a78\\u00a7oYi Yuan\\nHaohe Liu\\nXubo Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08051\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 5 Jan 2024 14:10:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Cai et al. (§72024§r)', author: 'Yiqiang Cai; Peihong Zhang; Shengchen Li', display:{Lore:['[{"text": "arXiv:2309.08200", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTF-SepNet: An Efficient 1D Kernel Design in CNNs for Low-Complexity Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oYiqiang Cai\\nPeihong Zhang\\nShengchen Li\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08200\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP48485.2024.10447999\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 29 May 2024 09:35:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the 2024 IEEE InternationalConference on Acoustics, Speech, and Signal Processing (ICASSP 2024)\\u00a7r"}']}
{title:'Moummad et al. (§72024§r)', author: 'Ilyass Moummad; Romain Serizel; Nicolas Farrugia', display:{Lore:['[{"text": "arXiv:2309.08971", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRegularized Contrastive Pre-training for Few-shot Bioacoustic Sound Detection\\u00a7r\\n\\n\\u00a78\\u00a7oIlyass Moummad\\nRomain Serizel\\nNicolas Farrugia\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08971\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 17 Jan 2024 11:35:33 GMT)\\u00a7r"}']}
{title:'Zang et al. (§72024§r)', author: 'Yongyi Zang; Yi Zhong; Frank Cwitkowitz; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2309.09085", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynthTab: Leveraging Synthesized Data for Guitar Tablature Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oYongyi Zang\\nYi Zhong\\nFrank Cwitkowitz\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09085\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 25 Jan 2024 03:00:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Shimada et al. (§72024§r)', author: 'Kazuki Shimada; Kengo Uchida; Yuichiro Koyama; Takashi Shibuya; Shusuke Takahashi; Yuki Mitsufuji; Tatsuya Kawahara', display:{Lore:['[{"text": "arXiv:2309.09223", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero- and Few-shot Sound Event Localization and Detection\\u00a7r\\n\\n\\u00a78\\u00a7oKazuki Shimada\\nKengo Uchida\\nYuichiro Koyama\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09223\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 18 Jan 2024 00:16:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, accepted for publication in IEEE ICASSP 2024\\u00a7r"}']}
{title:'Song et al. (§72024§r)', author: 'Zeyang Song; Jibin Wu; Malu Zhang; Mike Zheng Shou; Haizhou Li', display:{Lore:['[{"text": "arXiv:2309.09469", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpiking-LEAF: A Learnable Auditory front-end for Spiking Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oZeyang Song\\nJibin Wu\\nMalu Zhang\\nMike Zheng Shou\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09469\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 23 Mar 2024 04:41:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2024\\u00a7r"}']}
{title:'Violeta et al. (§72024§r)', author: 'Lester Phillip Violeta; Wen-Chin Huang; Ding Ma; Ryuichi Yamamoto; Kazuhiro Kobayashi; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2309.09627", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lElectrolaryngeal Speech Intelligibility Enhancement Through Robust Linguistic Encoders\\u00a7r\\n\\n\\u00a78\\u00a7oLester Phillip Violeta\\nWen-Chin Huang\\nDing Ma\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09627\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 20 Jan 2024 05:20:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024. Demo page: lesterphillip.github.io/icassp2024_el_sie\\u00a7r"}']}
{title:'Cheng et al. (§72024§r)', author: 'Luyao Cheng; Siqi Zheng; Qinglin Zhang; Hui Wang; Yafeng Chen; Qian Chen; Shiliang Zhang', display:{Lore:['[{"text": "arXiv:2309.10456", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Speaker Diarization using Semantic Information: Joint Pairwise Constraints Propagation\\u00a7r\\n\\n\\u00a78\\u00a7oLuyao Cheng\\nSiqi Zheng\\nQinglin Zhang\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10456\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 4 Feb 2024 06:05:06 GMT)\\u00a7r"}']}
{title:'Ao et al. (§72024§r)', author: 'Junyi Ao; Mehmet Sinan Yıldırım; Ruijie Tao; Meng Ge; Shuai Wang; Yanmin Qian; Haizhou Li', display:{Lore:['[{"text": "arXiv:2309.10674", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUSED: Universal Speaker Extraction and Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oJunyi Ao\\nMehmet Sinan Y\\u0131ld\\u0131r\\u0131m\\nRuijie Tao\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10674\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 9 May 2024 08:54:51 GMT)\\u00a7r"}']}
{title:'Lei et al. (§72024§r)', author: 'Shun Lei; Yixuan Zhou; Liyang Chen; Dan Luo; Zhiyong Wu; Xixin Wu; Shiyin Kang; Tao Jiang; Yahui Zhou; Yuxing Han; Helen Meng', display:{Lore:['[{"text": "arXiv:2309.11977", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts\\u00a7r\\n\\n\\u00a78\\u00a7oShun Lei\\nYixuan Zhou\\nLiyang Chen\\n+ 7 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.11977\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 9 Apr 2024 08:39:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted bt ICASSP 2024\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Dongmei Wang; Xiong Xiao; Naoyuki Kanda; Midia Yousefi; Takuya Yoshioka; Jian Wu', display:{Lore:['[{"text": "arXiv:2309.12521", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProfile-Error-Tolerant Target-Speaker Voice Activity Detection\\u00a7r\\n\\n\\u00a78\\u00a7oDongmei Wang\\nXiong Xiao\\nNaoyuki Kanda\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.12521\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 3 Apr 2024 21:39:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmission for ICASSP 2024\\u00a7r"}']}
{title:'Pacheco-Gonzalez et al. (§72024§r)', author: 'Alberto Pacheco-Gonzalez; Raymundo Torres; Raul Chacon; Isidro Robledo', display:{Lore:['[{"text": "arXiv:2309.13920", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.FL\\u00a7r, \\u00a7acs.SC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-Time Emergency Vehicle Detection using Mel Spectrograms and Regular Expressions\\u00a7r\\n\\n\\u00a78\\u00a7oAlberto Pacheco-Gonzalez\\nRaymundo Torres\\nRaul Chacon\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13920\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 27 Jan 2024 02:09:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oin Spanish language\\u00a7r"}']}
{title:'Yang et al. (§72024§r)', author: 'Zijian Yang; Wei Zhou; Ralf Schlüter; Hermann Ney', display:{Lore:['[{"text": "arXiv:2309.14130", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Relation between Internal Language Model and Sequence Discriminative Training for Neural Transducers\\u00a7r\\n\\n\\u00a78\\u00a7oZijian Yang\\nWei Zhou\\nRalf Schl\\u00fcter\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.14130\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 13 Apr 2024 08:06:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at ICASSP 2024\\u00a7r"}']}
{title:'Truong et al. (§72024§r)', author: 'Duc-Tuan Truong; Ruijie Tao; Jia Qi Yip; Kong Aik Lee; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2309.14838", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmphasized Non-Target Speaker Knowledge in Knowledge Distillation for Automatic Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oDuc-Tuan Truong\\nRuijie Tao\\nJia Qi Yip\\nKong Aik Lee\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.14838\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 15 Jan 2024 02:45:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Wuyang Liu; Yanzhen Ren', display:{Lore:['[{"text": "arXiv:2309.16265", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemantic Proximity Alignment: Towards Human Perception-consistent Audio Tagging by Aligning with Label Text Description\\u00a7r\\n\\n\\u00a78\\u00a7oWuyang Liu\\nYanzhen Ren\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.16265\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 Jan 2024 07:00:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures. Accepted by ICASSP 2024\\u00a7r"}']}
{title:'Ragano et al. (§72024§r)', author: 'Alessandro Ragano; Jan Skoglund; Andrew Hines', display:{Lore:['[{"text": "arXiv:2309.16284", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNOMAD: Unsupervised Learning of Perceptual Embeddings for Speech Enhancement and Non-matching Reference Audio Quality Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oAlessandro Ragano\\nJan Skoglund\\nAndrew Hines\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.16284\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 19 Jan 2024 14:45:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP 2024\\u00a7r"}']}
{title:'Milling et al. (§72024§r)', author: 'Manuel Milling; Andreas Triantafyllopoulos; Iosif Tsangko; Simon David Noel Rampp; Björn Wolfgang Schuller', display:{Lore:['[{"text": "arXiv:2309.16369", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBringing the Discussion of Minima Sharpness to the Audio Domain: a Filter-Normalised Evaluation for Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oManuel Milling\\nAndreas Triantafyllopoulos\\nIosif Tsangko\\nSimon David Noel Rampp\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.16369\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 15 Jan 2024 14:40:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to the IEEE for possible publication\\u00a7r"}']}
{title:'Guan et al. (§72024§r)', author: 'Wenhao Guan; Qi Su; Haodong Zhou; Shiyu Miao; Xingjia Xie; Lin Li; Qingyang Hong', display:{Lore:['[{"text": "arXiv:2309.17056", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReFlow-TTS: A Rectified Flow Model for High-fidelity Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oWenhao Guan\\nQi Su\\nHaodong Zhou\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.17056\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 31 Jan 2024 10:13:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP2024\\u00a7r"}']}
{title:'Pegg et al. (§72024§r)', author: 'Samuel Pegg; Kai Li; Xiaolin Hu', display:{Lore:['[{"text": "arXiv:2309.17189", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRTFS-Net: Recurrent Time-Frequency Modelling for Efficient Audio-Visual Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oSamuel Pegg\\nKai Li\\nXiaolin Hu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.17189\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 21 Mar 2024 09:19:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by The Twelfth International Conference on Learning Representations (ICLR)2024, see https://openreview.net/forum?id=PEuDO2EiDr\\u00a7r"}']}
{title:'Wu et al. (§72024§r)', author: 'Shih-Lun Wu; Xuankai Chang; Gordon Wichern; Jee-weon Jung; François Germain; Jonathan Le Roux; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2309.17352", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Audio Captioning Models with Fine-grained Audio Features, Text Embedding Supervision, and LLM Mix-up Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oShih-Lun Wu\\nXuankai Chang\\nGordon Wichern\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.17352\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 10 Jan 2024 00:14:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2024 camera-ready paper. Winner of the DCASE 2023 Challenge Task 6A:Automated Audio Captioning (AAC)\\u00a7r"}']}
{title:'Ren et al. (§72024§r)', author: 'Yong Ren; Tao Wang; Jiangyan Yi; Le Xu; Jianhua Tao; Chuyuan Zhang; Junzuo Zhou', display:{Lore:['[{"text": "arXiv:2310.00014", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFewer-token Neural Speech Codec with Time-invariant Codes\\u00a7r\\n\\n\\u00a78\\u00a7oYong Ren\\nTao Wang\\nJiangyan Yi\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.00014\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 11 Mar 2024 02:09:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Benita et al. (§72024§r)', author: 'Roi Benita; Michael Elad; Joseph Keshet', display:{Lore:['[{"text": "arXiv:2310.01381", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffAR: Denoising Diffusion Autoregressive Model for Raw Speech Waveform Generation\\u00a7r\\n\\n\\u00a78\\u00a7oRoi Benita\\nMichael Elad\\nJoseph Keshet\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.01381\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 10 Mar 2024 22:31:45 GMT)\\u00a7r"}']}
{title:'Shi et al. (§72024§r)', author: 'Jiatong Shi; Hirofumi Inaguma; Xutai Ma; Ilia Kulikov; Anna Sun', display:{Lore:['[{"text": "arXiv:2310.02720", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-resolution HuBERT: Multi-resolution Speech Self-Supervised Learning with Masked Unit Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oJiatong Shi\\nHirofumi Inaguma\\nXutai Ma\\nIlia Kulikov\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.02720\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 Jan 2024 08:52:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICLR2024 as spotlight\\u00a7r"}']}
{title:'Nunez et al. (§72024§r)', author: 'Elvis Nunez; Yanzi Jin; Mohammad Rastegari; Sachin Mehta; Maxwell Horton', display:{Lore:['[{"text": "arXiv:2310.03937", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffusion Models as Masked Audio-Video Learners\\u00a7r\\n\\n\\u00a78\\u00a7oElvis Nunez\\nYanzi Jin\\nMohammad Rastegari\\nSachin Mehta\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.03937\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 4 Jan 2024 21:39:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCamera-ready version for the MachineLearning for Audio Workshop at NeurIPS 2023\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Jiawei Li; Chunxu Guo; Li Fu; Lu Fan; Edward F. Chang; Yuanning Li', display:{Lore:['[{"text": "arXiv:2310.04644", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural2Speech: A Transfer Learning Framework for Neural-Driven Speech Reconstruction\\u00a7r\\n\\n\\u00a78\\u00a7oJiawei Li\\nChunxu Guo\\nLi Fu\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.04644\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 31 Jan 2024 09:58:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in 2024 IEEE International Conference on Acoustics, Speech and Signal Processing\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Jiaqi Li; Li Wang; Liumeng Xue; Lei Wang; Zhizheng Wu', display:{Lore:['[{"text": "arXiv:2310.05354", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Initial Investigation of Neural Replay Simulator for Over-the-Air Adversarial Perturbations to Automatic Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oJiaqi Li\\nLi Wang\\nLiumeng Xue\\nLei Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.05354\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 3 Jan 2024 07:33:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ICASSP 2024\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Li Wang; Jiaqi Li; Yuhao Luo; Jiahao Zheng; Lei Wang; Hao Li; Ke Xu; Chengfang Fang; Jie Shi; Zhizheng Wu', display:{Lore:['[{"text": "arXiv:2310.05369", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdvSV: An Over-the-Air Adversarial Attack Dataset for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oLi Wang\\nJiaqi Li\\nYuhao Luo\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.05369\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 Jan 2024 08:18:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2024\\u00a7r"}']}
{title:'Gardner et al. (§72024§r)', author: 'Josh Gardner; Simon Durand; Daniel Stoller; Rachel M. Bittner', display:{Lore:['[{"text": "arXiv:2310.07160", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLLark: A Multimodal Instruction-Following Language Model for Music\\u00a7r\\n\\n\\u00a78\\u00a7oJosh Gardner\\nSimon Durand\\nDaniel Stoller\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.07160\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 8 Feb 2024 21:23:40 GMT)\\u00a7r"}']}
{title:'Ghosh et al. (§72024§r)', author: 'Sreyan Ghosh; Ashish Seth; Sonal Kumar; Utkarsh Tyagi; Chandra Kiran Evuru; S. Ramaneswaran; S. Sakshi; Oriol Nieto; Ramani Duraiswami; Dinesh Manocha', display:{Lore:['[{"text": "arXiv:2310.08753", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oSreyan Ghosh\\nAshish Seth\\nSonal Kumar\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.08753\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 22 May 2024 20:52:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICLR 2024\\u00a7r"}']}
{title:'Fan et al. (§72024§r)', author: 'Cunhang Fan; Mingming Ding; Jianhua Tao; Ruibo Fu; Jiangyan Yi; Zhengqi Wen; Zhao Lv', display:{Lore:['[{"text": "arXiv:2310.08869", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDual-Branch Knowledge Distillation for Noise-Robust Synthetic Speech Detection\\u00a7r\\n\\n\\u00a78\\u00a7oCunhang Fan\\nMingming Ding\\nJianhua Tao\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.08869\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2024.3389643\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 Apr 2024 10:18:50 GMT)\\u00a7r"}']}
{title:'Xue et al. (§72024§r)', author: 'Huaying Xue; Xiulian Peng; Yan Lu', display:{Lore:['[{"text": "arXiv:2310.08981", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-latency Speech Enhancement via Speech Token Generation\\u00a7r\\n\\n\\u00a78\\u00a7oHuaying Xue\\nXiulian Peng\\nYan Lu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.08981\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 23 Jan 2024 06:13:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, ICASSP2024(accepted)\\u00a7r"}']}
{title:'Neekhara et al. (§72024§r)', author: 'Paarth Neekhara; Shehzeen Hussain; Rafael Valle; Boris Ginsburg; Rishabh Ranjan; Shlomo Dubnov; Farinaz Koushanfar; Julian McAuley', display:{Lore:['[{"text": "arXiv:2310.09653", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelfVC: Voice Conversion With Iterative Refinement using Self Transformations\\u00a7r\\n\\n\\u00a78\\u00a7oPaarth Neekhara\\nShehzeen Hussain\\nRafael Valle\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.09653\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 3 May 2024 16:45:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICML2024\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Xueyao Zhang; Yicheng Gu; Haopeng Chen; Zihao Fang; Lexiao Zou; Junan Zhang; Liumeng Xue; Jinchao Zhang; Jie Zhou; Zhizheng Wu', display:{Lore:['[{"text": "arXiv:2310.11160", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Diverse Semantic-based Audio Pretrained Models for Singing Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oXueyao Zhang\\nYicheng Gu\\nHaopeng Chen\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.11160\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 May 2024 21:38:08 GMT)\\u00a7r"}']}
{title:'Yeon et al. (§72024§r)', author: 'Inmo Yeon; Iljoo Jeong; Seungchul Lee; Jung-Woo Choi', display:{Lore:['[{"text": "arXiv:2310.11728", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEchoScan: Scanning Complex Indoor Geometries via Acoustic Echoes\\u00a7r\\n\\n\\u00a78\\u00a7oInmo Yeon\\nIljoo Jeong\\nSeungchul Lee\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.11728\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 16 Apr 2024 04:56:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 8 figures, 2 tables\\u00a7r"}']}
{title:'Peladeau et al. (§72024§r)', author: 'Côme Peladeau; Geoffroy Peeters', display:{Lore:['[{"text": "arXiv:2310.11781", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBlind estimation of audio effects using an auto-encoder approach and differentiable digital signal processing\\u00a7r\\n\\n\\u00a78\\u00a7oC\\u00f4me Peladeau\\nGeoffroy Peeters\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.11781\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 9 Feb 2024 18:40:26 GMT)\\u00a7r"}']}
{title:'Yokota et al. (§72024§r)', author: 'Kazuya Yokota; Takahiko Kurahashi; Masajiro Abe', display:{Lore:['[{"text": "arXiv:2310.11804", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhysics-informed Neural Network for Acoustic Resonance Analysis in a One-Dimensional Acoustic Tube\\u00a7r\\n\\n\\u00a78\\u00a7oKazuya Yokota\\nTakahiko Kurahashi\\nMasajiro Abe\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.11804\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 16 Apr 2024 08:26:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 18 figures. The following article has been submitted to the Journal of the Acoustical Society of America. After it is published, it will be found at https://pubs.aip.org/asa/jasa\\u00a7r"}']}
{title:'Tang et al. (§72024§r)', author: 'Changli Tang; Wenyi Yu; Guangzhi Sun; Xianzhao Chen; Tian Tan; Wei Li; Lu Lu; Zejun Ma; Chao Zhang', display:{Lore:['[{"text": "arXiv:2310.13289", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSALMONN: Towards Generic Hearing Abilities for Large Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oChangli Tang\\nWenyi Yu\\nGuangzhi Sun\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.13289\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Apr 2024 06:12:52 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Hejing Zhang; Qiaoxi Zhu; Jian Guan; Haohe Liu; Feiyang Xiao; Jiantong Tian; Xinhao Mei; Xubo Liu; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2310.14173", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFirst-Shot Unsupervised Anomalous Sound Detection With Unknown Anomalies Estimated by Metadata-Assisted Audio Generation\\u00a7r\\n\\n\\u00a78\\u00a7oHejing Zhang\\nQiaoxi Zhu\\nJian Guan\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.14173\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 11 Mar 2024 08:02:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2024\\u00a7r"}']}
{title:'Wei et al. (§72024§r)', author: 'Kun Wei; Bei Li; Hang Lv; Quan Lu; Ning Jiang; Lei Xie', display:{Lore:['[{"text": "arXiv:2310.14278", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConversational Speech Recognition by Learning Audio-textual Cross-modal Contextual Representation\\u00a7r\\n\\n\\u00a78\\u00a7oKun Wei\\nBei Li\\nHang Lv\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.14278\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2024.3389630\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  2024\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 28 Apr 2024 03:50:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTASLP\\u00a7r"}']}
{title:'Shen et al. (§72024§r)', author: 'Feiyu Shen; Yiwei Guo; Chenpeng Du; Xie Chen; Kai Yu', display:{Lore:['[{"text": "arXiv:2310.14580", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic BPE for Speech Generation with Discrete Tokens\\u00a7r\\n\\n\\u00a78\\u00a7oFeiyu Shen\\nYiwei Guo\\nChenpeng Du\\nXie Chen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.14580\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 15 Jan 2024 05:53:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures; accepted to ICASSP 2024\\u00a7r"}']}
{title:'Xie et al. (§72024§r)', author: 'Peng Xie; Zihao Xin; Yang Wang; Shengjun Huang; Tsz Wai Chan; Kani Chen', display:{Lore:['[{"text": "arXiv:2310.17953", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMCE: Mixed Cantonese and English Audio Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oPeng Xie\\nZihao Xin\\nYang Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.17953\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 18 Feb 2024 08:24:56 GMT)\\u00a7r"}']}
{title:'Healy et al. (§72024§r)', author: 'Brendan Healy; Patrick McNamee; Zahra Nili Ahmadabadi', display:{Lore:['[{"text": "arXiv:2310.19063", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFeature Aggregation in Joint Sound Classification and Localization Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oBrendan Healy\\nPatrick McNamee\\nZahra Nili Ahmadabadi\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.19063\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 27 Jan 2024 20:45:13 GMT)\\u00a7r"}']}
{title:'Kang et al. (§72024§r)', author: 'Jaeyong Kang; Soujanya Poria; Dorien Herremans', display:{Lore:['[{"text": "arXiv:2311.00968", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVideo2Music: Suitable Music Generation from Videos using an Affective Multimodal Transformer model\\u00a7r\\n\\n\\u00a78\\u00a7oJaeyong Kang\\nSoujanya Poria\\nDorien Herremans\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.00968\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.eswa.2024.123640\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 4 Mar 2024 07:54:31 GMT)\\u00a7r"}']}
{title:'Jeon et al. (§72024§r)', author: 'Sungho Jeon; Ching-Feng Yeh; Hakan Inan; Wei-Ning Hsu; Rashi Rungta; Yashar Mehdad; Daniel Bikel', display:{Lore:['[{"text": "arXiv:2311.02772", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention or Convolution: Transformer Encoders in Audio Language Models for Inference Efficiency\\u00a7r\\n\\n\\u00a78\\u00a7oSungho Jeon\\nChing-Feng Yeh\\nHakan Inan\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.02772\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 8 Feb 2024 10:09:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages; accepted to Self-supervision in Audio, Speech and Beyond (SASB) workshop in ICASSP24\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Mason Wang; Samuel Clarke; Jui-Hsien Wang; Ruohan Gao; Jiajun Wu', display:{Lore:['[{"text": "arXiv:2311.03517", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSoundCam: A Dataset for Finding Humans Using Room Acoustics\\u00a7r\\n\\n\\u00a78\\u00a7oMason Wang\\nSamuel Clarke\\nJui-Hsien Wang\\nRuohan Gao\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.03517\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 15 Jan 2024 08:15:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn NeurIPS 2023 Datasets and Benchmarks Track. Project page: https://masonlwang.com/soundcam/. Wang and Clarke contributed equally to this work\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Hanzhao Li; Xinfa Zhu; Liumeng Xue; Yang Song; Yunlin Chen; Lei Xie', display:{Lore:['[{"text": "arXiv:2311.07179", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSponTTS: modeling and transferring spontaneous style for TTS\\u00a7r\\n\\n\\u00a78\\u00a7oHanzhao Li\\nXinfa Zhu\\nLiumeng Xue\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.07179\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Jan 2024 11:01:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, Accepted by ICASSP2024\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Jianzong Wang; Pengcheng Li; Xulong Zhang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2311.07965", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDQR-TTS: Semi-supervised Text-to-speech Synthesis with Dynamic Quantized Representation\\u00a7r\\n\\n\\u00a78\\u00a7oJianzong Wang\\nPengcheng Li\\nXulong Zhang\\nNing Cheng\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.07965\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 2 Feb 2024 08:32:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the 13th IEEE InternationalConference on Big Dataand Cloud Computing (IEEE BDCloud 2023)\\u00a7r"}']}
{title:'He et al. (§72024§r)', author: 'Wentao He; Yuchen Yan; Jianfeng Ren; Ruibin Bai; Xudong Jiang', display:{Lore:['[{"text": "arXiv:2311.09655", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-View Spectrogram Transformer for Respiratory Sound Classification\\u00a7r\\n\\n\\u00a78\\u00a7oWentao He\\nYuchen Yan\\nJianfeng Ren\\nRuibin Bai\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.09655\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 30 May 2024 05:42:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe paper was published at ICASSP 2024\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Shansong Liu; Atin Sakkeer Hussain; Chenshuo Sun; Ying Shan', display:{Lore:['[{"text": "arXiv:2311.11255", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lM^2UGen: Multi-modal Music Understanding and Generation with the Power of Large Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oShansong Liu\\nAtin Sakkeer Hussain\\nChenshuo Sun\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.11255\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 5 Mar 2024 03:12:03 GMT)\\u00a7r"}']}
{title:'Han et al. (§72024§r)', author: 'Han Han; Vincent Lostanlen; Mathieu Lagrange', display:{Lore:['[{"text": "arXiv:2311.14213", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning to Solve Inverse Problems for Perceptual Sound Matching\\u00a7r\\n\\n\\u00a78\\u00a7oHan Han\\nVincent Lostanlen\\nMathieu Lagrange\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.14213\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2024.3393738\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 6 May 2024 15:02:05 GMT)\\u00a7r"}']}
{title:'Kamble et al. (§72024§r)', author: 'Anand Kamble; Aniket Tathe; Suyash Kumbharkar; Atharva Bhandare; Anirban C. Mitra', display:{Lore:['[{"text": "arXiv:2311.14836", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCustom Data Augmentation for low resource ASR using Bark and Retrieval-Based Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oAnand Kamble\\nAniket Tathe\\nSuyash Kumbharkar\\nAtharva Bhandare\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.14836\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 9 Jan 2024 21:37:45 GMT)\\u00a7r"}']}
{title:'Fei et al. (§72024§r)', author: 'Zhengcong Fei; Mingyuan Fan; Junshi Huang', display:{Lore:['[{"text": "arXiv:2311.15830", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA-JEPA: Joint-Embedding Predictive Architecture Can Listen\\u00a7r\\n\\n\\u00a78\\u00a7oZhengcong Fei\\nMingyuan Fan\\nJunshi Huang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.15830\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 11 Jan 2024 13:16:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2207.06405 by other authors\\u00a7r"}']}
{title:'Lee et al. (§72024§r)', author: 'Jin Woo Lee; Min Jun Choi; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2311.18505", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lString Sound Synthesizer on GPU-accelerated Finite Difference Scheme\\u00a7r\\n\\n\\u00a78\\u00a7oJin Woo Lee\\nMin Jun Choi\\nKyogu Lee\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.18505\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Jan 2024 12:01:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be appeared in ICASSP 2024\\u00a7r"}']}
{title:'Ratnarajah et al. (§72024§r)', author: 'Anton Ratnarajah; Sreyan Ghosh; Sonal Kumar; Purva Chiniya; Dinesh Manocha', display:{Lore:['[{"text": "arXiv:2312.00834", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAV-RIR: Audio-Visual Room Impulse Response Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oAnton Ratnarajah\\nSreyan Ghosh\\nSonal Kumar\\nPurva Chiniya\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.00834\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 23 Apr 2024 19:36:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to CVPR 2024\\u00a7r"}']}
{title:'Qin et al. (§72024§r)', author: 'Zengyi Qin; Wenliang Zhao; Xumin Yu; Xin Sun', display:{Lore:['[{"text": "arXiv:2312.01479", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOpenVoice: Versatile Instant Voice Cloning\\u00a7r\\n\\n\\u00a78\\u00a7oZengyi Qin\\nWenliang Zhao\\nXumin Yu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.01479\\u00a7r\\n\\nVersion:\\u00a77v5 (Tue, 2 Jan 2024 17:45:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTechnical Report\\u00a7r"}']}
{title:'Sha et al. (§72024§r)', author: 'Binzhu Sha; Xu Li; Zhiyong Wu; Ying Shan; Helen Meng', display:{Lore:['[{"text": "arXiv:2312.04919", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Concatenative Singing Voice Conversion: Rethinking Concatenation-Based Approach for One-Shot Singing Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oBinzhu Sha\\nXu Li\\nZhiyong Wu\\nYing Shan\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.04919\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Jan 2024 13:36:36 GMT)\\u00a7r"}']}
{title:'Yao et al. (§72024§r)', author: 'Dong Yao; Jieming Zhu; Jiahao Xun; Shengyu Zhang; Zhou Zhao; Liqun Deng; Wenqiao Zhang; Zhenhua Dong; Xin Jiang', display:{Lore:['[{"text": "arXiv:2312.06197", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMART: Learning Hierarchical Music Audio Representations with Part-Whole Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oDong Yao\\nJieming Zhu\\nJiahao Xun\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.06197\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 19 Apr 2024 13:41:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oShort paper accepted by WWW 2024. This is revised and condensed based on the previous version titled \\"Music-PAW: Learning Music Representations via Hierarchical Part-whole Interactionand Contrast\\". For more experimental"}','{"text": "details and discussions,please refer to the original long paper at arXiv:2312.06197v1\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Junjie Li; Yiwei Guo; Xie Chen; Kai Yu', display:{Lore:['[{"text": "arXiv:2312.08676", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention\\u00a7r\\n\\n\\u00a78\\u00a7oJunjie Li\\nYiwei Guo\\nXie Chen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.08676\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 Jan 2024 14:11:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, accepted to ICASSP 2024\\u00a7r"}']}
{title:'Parker et al. (§72024§r)', author: 'Julian D. Parker; Janne Spijkervet; Katerina Kosta; Furkan Yesiler; Boris Kuznetsov; Ju-Chiang Wang; Matt Avent; Jitong Chen; Duc Le', display:{Lore:['[{"text": "arXiv:2312.08723", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStemGen: A music generation model that listens\\u00a7r\\n\\n\\u00a78\\u00a7oJulian D. Parker\\nJanne Spijkervet\\nKaterina Kosta\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.08723\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 Jan 2024 09:15:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at ICASSP 2024\\u00a7r"}']}
{title:'Jang et al. (§72024§r)', author: 'Kangwook Jang; Sungnyun Kim; Hoirin Kim', display:{Lore:['[{"text": "arXiv:2312.09040", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSTaR: Distilling Speech Temporal Relation for Lightweight Speech Self-Supervised Learning Models\\u00a7r\\n\\n\\u00a78\\u00a7oKangwook Jang\\nSungnyun Kim\\nHoirin Kim\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.09040\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 Apr 2024 16:08:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2024 Best Student Paper Awarded. Code URL: https://github.com/sungnyun/ARMHuBERT\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Xueyao Zhang; Liumeng Xue; Yicheng Gu; Yuancheng Wang; Haorui He; Chaoren Wang; Xi Chen; Zihao Fang; Haopeng Chen; Junan Zhang; Tze Ying Tang; Lexiao Zou; Mingxuan Wang; Jun Han; Kai Chen; Haizhou Li; Zhizheng Wu', display:{Lore:['[{"text": "arXiv:2312.09911", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAmphion: An Open-Source Audio, Music and Speech Generation Toolkit\\u00a7r\\n\\n\\u00a78\\u00a7oXueyao Zhang\\nLiumeng Xue\\nYicheng Gu\\n+ 13 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.09911\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 22 Feb 2024 06:58:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAmphion Website: https://github.com/open-mmlab/Amphion\\u00a7r"}']}
{title:'Mu et al. (§72024§r)', author: 'Zhaoxi Mu; Xinyu Yang; Sining Sun; Qing Yang', display:{Lore:['[{"text": "arXiv:2312.10305", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Disentangled Representation Learning for Robust Target Speech Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oZhaoxi Mu\\nXinyu Yang\\nSining Sun\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.10305\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 20 Jan 2024 02:10:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by AAAI2024\\u00a7r"}']}
{title:'Ji et al. (§72024§r)', author: 'Shulei Ji; Xinyu Yang', display:{Lore:['[{"text": "arXiv:2312.10307", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusER: Musical Element-Based Regularization for Generating Symbolic Music with Emotion\\u00a7r\\n\\n\\u00a78\\u00a7oShulei Ji\\nXinyu Yang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.10307\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 2 Jan 2024 02:36:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by AAAI 2024\\u00a7r"}']}
{title:'He et al. (§72024§r)', author: 'Shulin He; Jinjiang liu; Hao Li; Yang Yang; Fei Chen; Xueliang Zhang', display:{Lore:['[{"text": "arXiv:2312.10979", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l3S-TSE: Efficient Three-Stage Target Speaker Extraction for Real-Time and Low-Resource Applications\\u00a7r\\n\\n\\u00a78\\u00a7oShulin He\\nJinjiang liu\\nHao Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.10979\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 5 Jan 2024 03:04:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Lyberatos et al. (§72024§r)', author: 'Vassilis Lyberatos; Spyridon Kantarelis; Edmund Dervakos; Giorgos Stamou', display:{Lore:['[{"text": "arXiv:2312.11234", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerceptual Musical Features for Interpretable Audio Tagging\\u00a7r\\n\\n\\u00a78\\u00a7oVassilis Lyberatos\\nSpyridon Kantarelis\\nEdmund Dervakos\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.11234\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 23 Feb 2024 13:41:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oGithub Repository: https://github.com/vaslyb/perceptible-music-tagging\\u00a7r"}']}
{title:'Chen (§72024§r)', author: 'Minghao Chen', display:{Lore:['[{"text": "arXiv:2312.13143", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnderwater Acoustic Signal Recognition Based on Salient Feature\\u00a7r\\n\\n\\u00a78\\u00a7oMinghao Chen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.13143\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 5 Jan 2024 02:14:13 GMT)\\u00a7r"}']}
{title:'Zhou et al. (§72024§r)', author: 'Jiaming Zhou; Shiwan Zhao; Yaqi Liu; Wenjia Zeng; Yong Chen; Yong Qin', display:{Lore:['[{"text": "arXiv:2312.13560", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lkNN-CTC: Enhancing ASR via Retrieval of CTC Pseudo Labels\\u00a7r\\n\\n\\u00a78\\u00a7oJiaming Zhou\\nShiwan Zhao\\nYaqi Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.13560\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 3 Feb 2024 01:57:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Torres et al. (§72024§r)', author: 'Bernardo Torres; Geoffroy Peeters; Gaël Richard', display:{Lore:['[{"text": "arXiv:2312.14507", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Harmonic Parameter Estimation Using Differentiable DSP and Spectral Optimal Transport\\u00a7r\\n\\n\\u00a78\\u00a7oBernardo Torres\\nGeoffroy Peeters\\nGa\\u00ebl Richard\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.14507\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE International Conference on Acoustics, Speech and Signal\\n  Processing, Apr 2024, Seoul, South Korea\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 15 Jan 2024 10:41:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ICASSP 2024\\u00a7r"}']}
{title:'Moummad et al. (§72024§r)', author: 'Ilyass Moummad; Romain Serizel; Nicolas Farrugia', display:{Lore:['[{"text": "arXiv:2312.15824", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Learning for Few-Shot Bird Sound Classification\\u00a7r\\n\\n\\u00a78\\u00a7oIlyass Moummad\\nRomain Serizel\\nNicolas Farrugia\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.15824\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 9 Feb 2024 12:00:56 GMT)\\u00a7r"}']}
{title:'Zhu (§72024§r)', author: 'Wentao Zhu', display:{Lore:['[{"text": "arXiv:2312.16228", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeformable Audio Transformer for Audio Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oWentao Zhu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.16228\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Jan 2024 03:52:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2024. arXiv admin note: substantial text overlap with arXiv:2201.00520 by other authors\\u00a7r"}']}
{title:'Bovbjerg et al. (§72024§r)', author: 'Holger Severin Bovbjerg; Jesper Jensen; Jan Østergaard; Zheng-Hua Tan', display:{Lore:['[{"text": "arXiv:2312.16613", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervised Pretraining for Robust Personalized Voice Activity Detection in Adverse Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oHolger Severin Bovbjerg\\nJesper Jensen\\nJan \\u00d8stergaard\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.16613\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 23 Jan 2024 10:04:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published at ICASSP2024, 14th of April 2024, Seoul, South Korea. Copyright (c) 2023 IEEE. 5 pages, 2, figures, 5 tables\\u00a7r"}']}
{title:'Chang et al. (§72024§r)', author: 'Chih-Cheng Chang; Li Su', display:{Lore:['[{"text": "arXiv:2312.17156", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBEAST: Online Joint Beat and Downbeat Tracking Based on Streaming Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oChih-Cheng Chang\\nLi Su\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.17156\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 23 Apr 2024 09:48:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Xue et al. (§72024§r)', author: 'Hongfei Xue; Yuhao Liang; Bingshen Mu; Shiliang Zhang; Mengzhe Chen; Qian Chen; Lei Xie', display:{Lore:['[{"text": "arXiv:2401.00475", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lE-chat: Emotion-sensitive Spoken Dialogue System with Large Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oHongfei Xue\\nYuhao Liang\\nBingshen Mu\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.00475\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 6 Jan 2024 14:37:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 3 figures\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Huimeng Wang; Zengrui Jin; Mengzhe Geng; Shujie Hu; Guinan Li; Tianzi Wang; Haoning Xu; Xunying Liu', display:{Lore:['[{"text": "arXiv:2401.00662", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Pre-trained ASR System Fine-tuning for Dysarthric Speech Recognition using Adversarial Data Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oHuimeng Wang\\nZengrui Jin\\nMengzhe Geng\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.00662\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Jan 2024 04:21:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at IEEE ICASSP 2024\\u00a7r"}']}
{title:'Xue et al. (§72024§r)', author: 'Jinlong Xue; Yayue Deng; Yingming Gao; Ya Li', display:{Lore:['[{"text": "arXiv:2401.01044", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAuffusion: Leveraging the Power of Diffusion and Large Language Models for Text-to-Audio Generation\\u00a7r\\n\\n\\u00a78\\u00a7oJinlong Xue\\nYayue Deng\\nYingming Gao\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.01044\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Jan 2024 05:42:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDemo and implementation at https://auffusion.github.io\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Ronghui Li; Yuqin Dai; Yachao Zhang; Jun Li; Jian Yang; Jie Guo; Xiu Li', display:{Lore:['[{"text": "arXiv:2401.01382", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Multi-Modal Control in Music-Driven Dance Generation\\u00a7r\\n\\n\\u00a78\\u00a7oRonghui Li\\nYuqin Dai\\nYachao Zhang\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.01382\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP48485.2024.10447825\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Jan 2024 09:25:20 GMT)\\u00a7r"}']}
{title:'Du et al. (§72024§r)', author: 'Muyang Du; Chuan Liu; Junjie Lai', display:{Lore:['[{"text": "arXiv:2401.01755", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIncremental FastPitch: Chunk-based High Quality Text to Speech\\u00a7r\\n\\n\\u00a78\\u00a7oMuyang Du\\nChuan Liu\\nJunjie Lai\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.01755\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Jan 2024 14:17:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, 1 table\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Jianyu Wang; Shanzheng Guan; Jingdong Chen; Jacob Benesty', display:{Lore:['[{"text": "arXiv:2401.01762", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIndependent low-rank matrix analysis based on the Sinkhorn divergence source model for blind source separation\\u00a7r\\n\\n\\u00a78\\u00a7oJianyu Wang\\nShanzheng Guan\\nJingdong Chen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.01762\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Jan 2024 14:32:38 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Jianyu Wang; Shanzheng Guan', display:{Lore:['[{"text": "arXiv:2401.01763", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultichannel blind speech source separation with a disjoint constraint source model\\u00a7r\\n\\n\\u00a78\\u00a7oJianyu Wang\\nShanzheng Guan\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.01763\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Jan 2024 14:32:47 GMT)\\u00a7r"}']}
{title:'Jeon et al. (§72024§r)', author: 'Yejin Jeon; Yunsu Kim; Gary Geunbae Lee', display:{Lore:['[{"text": "arXiv:2401.02014", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Zero-Shot Multi-Speaker TTS with Negated Speaker Representations\\u00a7r\\n\\n\\u00a78\\u00a7oYejin Jeon\\nYunsu Kim\\nGary Geunbae Lee\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.02014\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 5 Mar 2024 14:08:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to AAAI2024\\u00a7r"}']}
{title:'Gokul et al. (§72024§r)', author: 'Vignesh Gokul; Shlomo Dubnov', display:{Lore:['[{"text": "arXiv:2401.02135", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPosCUDA: Position based Convolution for Unlearnable Audio Datasets\\u00a7r\\n\\n\\u00a78\\u00a7oVignesh Gokul\\nShlomo Dubnov\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.02135\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Jan 2024 08:39:49 GMT)\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Xiaoquan Li; Stephan Weiss; Yijun Yan; Yinhe Li; Jinchang Ren; John Soraghan; Ming Gong', display:{Lore:['[{"text": "arXiv:2401.02566", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSiamese Residual Neural Network for Musical Shape Evaluation in Piano Performance Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoquan Li\\nStephan Weiss\\nYijun Yan\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.02566\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Jan 2024 22:51:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oX.Li, S.Weiss, Y.Yan, Y.Li, J.Ren, J.Soraghan, M.Gong,\\"Siamese residual neural network for musical shape evaluation in piano performance assessment\\" in Proc. ofthe 31st European Signal Processing Conference, Helsinki, "}','{"text": "Finland\\u00a7r"}']}
{title:'Xu et al. (§72024§r)', author: 'Xuenan Xu; Ziyang Ma; Mengyue Wu; Kai Yu', display:{Lore:['[{"text": "arXiv:2401.02584", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Weakly Supervised Text-to-Audio Grounding\\u00a7r\\n\\n\\u00a78\\u00a7oXuenan Xu\\nZiyang Ma\\nMengyue Wu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.02584\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Jan 2024 00:27:32 GMT)\\u00a7r"}']}
{title:'Ma et al. (§72024§r)', author: 'Yi Ma; Kong Aik Lee; Ville Hautamäki; Meng Ge; Haizhou Li', display:{Lore:['[{"text": "arXiv:2401.02626", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGradient weighting for speaker verification in extremely low Signal-to-Noise Ratio\\u00a7r\\n\\n\\u00a78\\u00a7oYi Ma\\nKong Aik Lee\\nVille Hautam\\u00e4ki\\nMeng Ge\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.02626\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Jan 2024 04:09:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Qian et al. (§72024§r)', author: 'Yikai Qian; Tianle Wang; Xinyi Tong; Xin Jin; Duo Xu; Bo Zheng; Tiezheng Ge; Feng Yu; Song-Chun Zhu', display:{Lore:['[{"text": "arXiv:2401.02678", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusicAOG: an Energy-Based Model for Learning and Sampling a Hierarchical Representation of Symbolic Music\\u00a7r\\n\\n\\u00a78\\u00a7oYikai Qian\\nTianle Wang\\nXinyi Tong\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.02678\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Jan 2024 07:24:07 GMT)\\u00a7r"}']}
{title:'Saeed et al. (§72024§r)', author: 'Tabish Saeed; Aneeqa Ijaz; Ismail Sadiq; Haneya N. Qureshi; Ali Rizwan; Ali Imran', display:{Lore:['[{"text": "arXiv:2401.02996", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn AI-enabled Bias-Free Respiratory Disease Diagnosis Model using Cough Audio: A Case Study for COVID-19\\u00a7r\\n\\n\\u00a78\\u00a7oTabish Saeed\\nAneeqa Ijaz\\nIsmail Sadiq\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.02996\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Jan 2024 13:09:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 7 figures, 5 tables\\u00a7r"}']}
{title:'Muaz et al. (§72024§r)', author: 'Muhammad Muaz; Nathan Paull; Jahnavi Malagavalli', display:{Lore:['[{"text": "arXiv:2401.03000", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBridging Modalities: Knowledge Distillation and Masked Training for Translating Multi-Modal Emotion Recognition to Uni-Modal, Speech-Only Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMuhammad Muaz\\nNathan Paull\\nJahnavi Malagavalli\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03000\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Jan 2024 22:42:14 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'He Wang; Pengcheng Guo; Pan Zhou; Lei Xie', display:{Lore:['[{"text": "arXiv:2401.03424", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMLCA-AVSR: Multi-Layer Cross Attention Fusion based Audio-Visual Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHe Wang\\nPengcheng Guo\\nPan Zhou\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03424\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP48485.2024.10446769\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 8 Apr 2024 12:50:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures Accepted at ICASSP 2024\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'He Wang; Pengcheng Guo; Yue Li; Ao Zhang; Jiayao Sun; Lei Xie; Wei Chen; Pan Zhou; Hui Bu; Xin Xu; Binbin Zhang; Zhuo Chen; Jian Wu; Longbiao Wang; Eng Siong Chng; Sun Li', display:{Lore:['[{"text": "arXiv:2401.03473", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lICMC-ASR: The ICASSP 2024 In-Car Multi-Channel Automatic Speech Recognition Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oHe Wang\\nPengcheng Guo\\nYue Li\\n+ 12 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03473\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 21 Feb 2024 03:39:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2024\\u00a7r"}']}
{title:'Han et al. (§72024§r)', author: 'Runduo Han; Xiaopeng Yan; Weiming Xu; Pengcheng Guo; Jiayao Sun; He Wang; Quan Lu; Ning Jiang; Lei Xie', display:{Lore:['[{"text": "arXiv:2401.03697", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn audio-quality-based multi-strategy approach for target speaker extraction in the MISP 2023 Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oRunduo Han\\nXiaopeng Yan\\nWeiming Xu\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03697\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 7 Mar 2024 02:59:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Wei et al. (§72024§r)', author: 'Haojie Wei; Xueke Cao; Wenbo Xu; Tangpeng Dan; Yueguo Chen', display:{Lore:['[{"text": "arXiv:2401.03856", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDJCM: A Deep Joint Cascade Model for Singing Voice Separation and Vocal Pitch Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oHaojie Wei\\nXueke Cao\\nWenbo Xu\\nTangpeng Dan\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03856\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP48485.2024.10446951\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Jan 2024 12:37:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper has been accepted by ICASSP 2024\\u00a7r"}']}
{title:'Kang et al. (§72024§r)', author: 'Jiawen Kang; Lingwei Meng; Mingyu Cui; Haohan Guo; Xixin Wu; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2401.04152", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-Speaker Encoding Network for Multi-Talker Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJiawen Kang\\nLingwei Meng\\nMingyu Cui\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.04152\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Jan 2024 16:37:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2024\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Mingshuai Liu; Zhuangqi Chen; Xiaopeng Yan; Yuanjun Lv; Xianjun Xia; Chuanzeng Huang; Yijian Xiao; Lei Xie', display:{Lore:['[{"text": "arXiv:2401.04389", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRaD-Net: A Repairing and Denoising Network for Speech Signal Improvement\\u00a7r\\n\\n\\u00a78\\u00a7oMingshuai Liu\\nZhuangqi Chen\\nXiaopeng Yan\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.04389\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Jan 2024 07:21:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Zhe Zhang; Taketo Akama', display:{Lore:['[{"text": "arXiv:2401.04558", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHyperGANStrument: Instrument Sound Synthesis and Editing with Pitch-Invariant Hypernetworks\\u00a7r\\n\\n\\u00a78\\u00a7oZhe Zhang\\nTaketo Akama\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.04558\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Jan 2024 13:54:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, Accepted for 2024 IEEEInternational Conference on Acoustics, Speech, and Signal Processing (ICASSP), Audio examples: https://noto.li/MLIuBC\\u00a7r"}']}
{title:'Ziv et al. (§72024§r)', author: 'Alon Ziv; Itai Gat; Gael Le Lan; Tal Remez; Felix Kreuk; Alexandre Défossez; Jade Copet; Gabriel Synnaeve; Yossi Adi', display:{Lore:['[{"text": "arXiv:2401.04577", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMasked Audio Generation using a Single Non-Autoregressive Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oAlon Ziv\\nItai Gat\\nGael Le Lan\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.04577\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 5 Mar 2024 09:12:35 GMT)\\u00a7r"}']}
{title:'Meng (§72024§r)', author: 'Yigang Meng', display:{Lore:['[{"text": "arXiv:2401.04737", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Genre Classification: A Comparative Analysis of CNN and XGBoost Approaches with Mel-frequency cepstral coefficients and Mel Spectrograms\\u00a7r\\n\\n\\u00a78\\u00a7oYigang Meng\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.04737\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Jan 2024 01:50:31 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72024§r)', author: 'Junming Chen; Yunfei Liu; Jianan Wang; Ailing Zeng; Yu Li; Qifeng Chen', display:{Lore:['[{"text": "arXiv:2401.04747", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation\\u00a7r\\n\\n\\u00a78\\u00a7oJunming Chen\\nYunfei Liu\\nJianan Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.04747\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 6 Apr 2024 14:53:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by CVPR 2024. Project page: https://jeremycjm.github.io/proj/DiffSHEG\\u00a7r"}']}
{title:'Costa-jussà et al. (§72024§r)', author: 'Marta R. Costa-jussà; Mariano Coria Meglioli; Pierre Andrews; David Dale; Prangthip Hansanti; Elahe Kalbassi; Alex Mourachko; Christophe Ropers; Carleigh Wood', display:{Lore:['[{"text": "arXiv:2401.05060", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMuTox: Universal MUltilingual Audio-based TOXicity Dataset and Zero-shot Detector\\u00a7r\\n\\n\\u00a78\\u00a7oMarta R. Costa-juss\\u00e0\\nMariano Coria Meglioli\\nPierre Andrews\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.05060\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Jan 2024 10:37:45 GMT)\\u00a7r"}']}
{title:'Torres et al. (§72024§r)', author: 'Bernardo Torres; Stefan Lattner; Gaël Richard', display:{Lore:['[{"text": "arXiv:2401.05064", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSinger Identity Representation Learning using Self-Supervised Techniques\\u00a7r\\n\\n\\u00a78\\u00a7oBernardo Torres\\nStefan Lattner\\nGa\\u00ebl Richard\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.05064\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5281/zenodo.10265323\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 24th International Society for Music\\n  Information Retrieval Conference (ISMIR 2023), Milan, Italy\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Jan 2024 10:41:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the ISMIR conference, Milan, Italy, 2023\\u00a7r"}']}
{title:'Fujita et al. (§72024§r)', author: 'Kenichi Fujita; Hiroshi Sato; Takanori Ashihara; Hiroki Kanagawa; Marc Delcroix; Takafumi Moriya; Yusuke Ijima', display:{Lore:['[{"text": "arXiv:2401.05111", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoise-robust zero-shot text-to-speech synthesis conditioned on self-supervised speech-representation model with adapters\\u00a7r\\n\\n\\u00a78\\u00a7oKenichi Fujita\\nHiroshi Sato\\nTakanori Ashihara\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.05111\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Jan 2024 12:21:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages,3 figures, Accepted to IEEE ICASSP 2024\\u00a7r"}']}
{title:'Huang et al. (§72024§r)', author: 'Lian Huang; Chi-Man Pun', display:{Lore:['[{"text": "arXiv:2401.05614", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Attention and Hybrid Features for Replay and Deep-Fake Audio Detection\\u00a7r\\n\\n\\u00a78\\u00a7oLian Huang\\nChi-Man Pun\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.05614\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Jan 2024 01:41:16 GMT)\\u00a7r"}']}
{title:'Aramaki et al. (§72024§r)', author: 'Mitsuko Aramaki; Corentin Bernard; Richard Kronland-Martinet; Samuel Poirot; Sølvi Ystad', display:{Lore:['[{"text": "arXiv:2401.05757", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.class-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntuitive Control of Scraping and Rubbing Through Audio-tactile Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oMitsuko Aramaki\\nCorentin Bernard\\nRichard Kronland-Martinet\\nSamuel Poirot\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.05757\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n16th International Symposium on CMMR, Future University Hakodate,\\n  Japan; Asia University, Japan; Nihon University, Japan; Laboratoire PRISM,\\n  Marseille, France, Nov 2023, Tokyo, France\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Jan 2024 09:07:44 GMT)\\u00a7r"}']}
{title:'Guan et al. (§72024§r)', author: 'Yadong Guan; Jiqing Han; Hongwei Song; Wenjie Song; Guibin Zheng; Tieran Zheng; Yongjun He', display:{Lore:['[{"text": "arXiv:2401.05850", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContrastive Loss Based Frame-wise Feature disentanglement for Polyphonic Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oYadong Guan\\nJiqing Han\\nHongwei Song\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.05850\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Jan 2024 11:39:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by icassp2024\\u00a7r"}']}
{title:'Yu et al. (§72024§r)', author: 'Fan Yu; Haoxu Wang; Xian Shi; Shiliang Zhang', display:{Lore:['[{"text": "arXiv:2401.06390", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLCB-net: Long-Context Biasing for Audio-Visual Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oFan Yu\\nHaoxu Wang\\nXian Shi\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.06390\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Jan 2024 06:03:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASPP 2024\\u00a7r"}']}
{title:'Ryu et al. (§72024§r)', author: 'Myeonghoon Ryu; Hongseok Oh; Suji Lee; Han Park', display:{Lore:['[{"text": "arXiv:2401.06913", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMicrophone Conversion: Mitigating Device Variability in Sound Event Classification\\u00a7r\\n\\n\\u00a78\\u00a7oMyeonghoon Ryu\\nHongseok Oh\\nSuji Lee\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.06913\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Jan 2024 21:59:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Lin et al. (§72024§r)', author: 'Zhiwei Lin; Jun Chen; Boshi Tang; Binzhu Sha; Jing Yang; Yaolong Ju; Fan Fan; Shiyin Kang; Zhiyong Wu; Helen Meng', display:{Lore:['[{"text": "arXiv:2401.07532", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-view MidiVAE: Fusing Track- and Bar-view Representations for Long Multi-track Symbolic Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oZhiwei Lin\\nJun Chen\\nBoshi Tang\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.07532\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Jan 2024 08:41:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Pandey et al. (§72024§r)', author: 'Ashutosh Pandey; Buye Xu', display:{Lore:['[{"text": "arXiv:2401.07879", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDecoupled Spatial and Temporal Processing for Resource Efficient Multichannel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oAshutosh Pandey\\nBuye Xu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.07879\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Jan 2024 18:15:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at ICASSP\\u00a7r"}']}
{title:'Hsieh et al. (§72024§r)', author: 'Tsun-An Hsieh; Jacob Donley; Daniel Wong; Buye Xu; Ashutosh Pandey', display:{Lore:['[{"text": "arXiv:2401.07882", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Importance of Neural Wiener Filter for Resource Efficient Multichannel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oTsun-An Hsieh\\nJacob Donley\\nDaniel Wong\\nBuye Xu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.07882\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Jan 2024 18:23:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at ICASSP\\u00a7r"}']}
{title:'Kimelman (§72024§r)', author: 'Robert G. Kimelman', display:{Lore:['[{"text": "arXiv:2401.07967", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMCMChaos: Improvising Rap Music with MCMC Methods and Chaos Theory\\u00a7r\\n\\n\\u00a78\\u00a7oRobert G. Kimelman\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.07967\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Jan 2024 21:10:19 GMT)\\u00a7r"}']}
{title:'Oh et al. (§72024§r)', author: 'Hyung-Seok Oh; Sang-Hoon Lee; Deok-Hyeon Cho; Seong-Whan Lee', display:{Lore:['[{"text": "arXiv:2401.08095", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDurFlex-EVC: Duration-Flexible Emotional Voice Conversion with Parallel Generation\\u00a7r\\n\\n\\u00a78\\u00a7oHyung-Seok Oh\\nSang-Hoon Lee\\nDeok-Hyeon Cho\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.08095\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 7 Mar 2024 08:40:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 9 figures, 8 tables\\u00a7r"}']}
{title:'Deng et al. (§72024§r)', author: 'Yimin Deng; Huaizhen Tang; Xulong Zhang; Ning Cheng; Jing Xiao; Jianzong Wang', display:{Lore:['[{"text": "arXiv:2401.08096", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Disentangled Speech Representations with Contrastive Learning and Time-Invariant Retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oYimin Deng\\nHuaizhen Tang\\nXulong Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.08096\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 18 Jan 2024 02:36:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by 2024 IEEE InternationalConference on Acoustics, Speech, and Signal Processing (ICASSP2024)\\u00a7r"}']}
{title:'Im et al. (§72024§r)', author: 'Jaekwon Im; Juhan Nam', display:{Lore:['[{"text": "arXiv:2401.08102", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDIFFRENT: A Diffusion Model for Recording Environment Transfer of Speech\\u00a7r\\n\\n\\u00a78\\u00a7oJaekwon Im\\nJuhan Nam\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.08102\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Jan 2024 04:10:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 2 figures\\u00a7r"}']}
{title:'Rixte (§72024§r)', author: 'Alice Rixte', display:{Lore:['[{"text": "arXiv:2401.08181", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLiveScaler: Live control of the harmony of an electronic music track\\u00a7r\\n\\n\\u00a78\\u00a7oAlice Rixte\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.08181\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJourn{\\\\\'e}es d\'Informatique Musicale (JIM), Association\\n  Francophone d\'Informatique Musicale (AFIM), May 2023, Saint-Denis (93),\\n  France\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Jan 2024 07:54:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oin French language\\u00a7r"}']}
{title:'Feng et al. (§72024§r)', author: 'Jiu Feng; Mehmet Hamza Erol; Joon Son Chung; Arda Senocak', display:{Lore:['[{"text": "arXiv:2401.08415", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrom Coarse to Fine: Efficient Training for Audio Spectrogram Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oJiu Feng\\nMehmet Hamza Erol\\nJoon Son Chung\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.08415\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Jan 2024 14:59:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2024\\u00a7r"}']}
{title:'Roman et al. (§72024§r)', author: 'Adrian S. Roman; Iran R. Roman; Juan P. Bello', display:{Lore:['[{"text": "arXiv:2401.08717", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust DOA estimation using deep acoustic imaging\\u00a7r\\n\\n\\u00a78\\u00a7oAdrian S. Roman\\nIran R. Roman\\nJuan P. Bello\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.08717\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Jan 2024 04:40:39 GMT)\\u00a7r"}']}
{title:'Vinnikov et al. (§72024§r)', author: 'Alon Vinnikov; Amir Ivry; Aviv Hurvitz; Igor Abramovski; Sharon Koubi; Ilya Gurvich; Shai Pe`er; Xiong Xiao; Benjamin Martinez Elizalde; Naoyuki Kanda; Xiaofei Wang; Shalev Shaer; Stav Yagev; Yossi Asher; Sunit Sivasankaran; Yifan Gong; Min Tang; Huaming Wang; Eyal Krupka', display:{Lore:['[{"text": "arXiv:2401.08887", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNOTSOFAR-1 Challenge: New Datasets, Baseline, and Tasks for Distant Meeting Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oAlon Vinnikov\\nAmir Ivry\\nAviv Hurvitz\\n+ 15 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.08887\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Jan 2024 23:50:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7opreprint\\u00a7r"}']}
{title:'McCallum et al. (§72024§r)', author: 'Matthew C. McCallum; Matthew E. P. Davies; Florian Henkel; Jaehun Kim; Samuel E. Sandberg', display:{Lore:['[{"text": "arXiv:2401.08889", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Effect of Data-Augmentation on Local Embedding Properties in the Contrastive Learning of Music Audio Representations\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew C. McCallum\\nMatthew E. P. Davies\\nFlorian Henkel\\nJaehun Kim\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.08889\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Jan 2024 00:12:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2024\\u00a7r"}']}
{title:'Henkel et al. (§72024§r)', author: 'Florian Henkel; Jaehun Kim; Matthew C. McCallum; Samuel E. Sandberg; Matthew E. P. Davies', display:{Lore:['[{"text": "arXiv:2401.08891", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTempo estimation as fully self-supervised binary classification\\u00a7r\\n\\n\\u00a78\\u00a7oFlorian Henkel\\nJaehun Kim\\nMatthew C. McCallum\\nSamuel E. Sandberg\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.08891\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Jan 2024 00:15:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2024\\u00a7r"}']}
{title:'McCallum et al. (§72024§r)', author: 'Matthew C. McCallum; Florian Henkel; Jaehun Kim; Samuel E. Sandberg; Matthew E. P. Davies', display:{Lore:['[{"text": "arXiv:2401.08902", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.DL\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSimilar but Faster: Manipulation of Tempo in Music Audio Embeddings for Tempo Prediction and Search\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew C. McCallum\\nFlorian Henkel\\nJaehun Kim\\nSamuel E. Sandberg\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.08902\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Jan 2024 01:06:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2024\\u00a7r"}']}
{title:'Park et al. (§72024§r)', author: 'Jiyun Park; Sangeon Yong; Taegyun Kwon; Juhan Nam', display:{Lore:['[{"text": "arXiv:2401.09200", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Real-Time Lyrics Alignment System Using Chroma And Phonetic Features For Classical Vocal Performance\\u00a7r\\n\\n\\u00a78\\u00a7oJiyun Park\\nSangeon Yong\\nTaegyun Kwon\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.09200\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Jan 2024 13:25:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo Appear IEEE ICASSP 2024\\u00a7r"}']}
{title:'Chung et al. (§72024§r)', author: 'Yoonjin Chung; Junwon Lee; Juhan Nam', display:{Lore:['[{"text": "arXiv:2401.09294", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lT-FOLEY: A Controllable Waveform-Domain Diffusion Model for Temporal-Event-Guided Foley Sound Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYoonjin Chung\\nJunwon Lee\\nJuhan Nam\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.09294\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Jan 2024 15:54:36 GMT)\\u00a7r"}']}
{title:'Müller et al. (§72024§r)', author: 'Nicolas M. Müller; Piotr Kawa; Wei Herng Choong; Edresson Casanova; Eren Gölge; Thorsten Müller; Piotr Syga; Philip Sperl; Konstantin Böttinger', display:{Lore:['[{"text": "arXiv:2401.09512", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMLAAD: The Multi-Language Audio Anti-Spoofing Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oNicolas M. M\\u00fcller\\nPiotr Kawa\\nWei Herng Choong\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.09512\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 16 Apr 2024 11:25:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIJCNN 2024\\u00a7r"}']}
{title:'Lu et al. (§72024§r)', author: 'Cheng Lu; Yuan Zong; Hailun Lian; Yan Zhao; Björn Schuller; Wenming Zheng', display:{Lore:['[{"text": "arXiv:2401.09752", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Speaker-independent Speech Emotion Recognition Using Dynamic Joint Distribution Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oCheng Lu\\nYuan Zong\\nHailun Lian\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.09752\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Jan 2024 06:52:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Laleye et al. (§72024§r)', author: 'Fréjus A. A. Laleye; Mikaël A. Mousse', display:{Lore:['[{"text": "arXiv:2401.09880", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention-Based Recurrent Neural Network For Automatic Behavior Laying Hen Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oFr\\u00e9jus A. A. Laleye\\nMika\\u00ebl A. Mousse\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.09880\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Jan 2024 10:52:46 GMT)\\u00a7r"}']}
{title:'Agrawal et al. (§72024§r)', author: 'Prabhav Agrawal; Thilo Koehler; Zhiping Xiu; Prashant Serai; Qing He', display:{Lore:['[{"text": "arXiv:2401.10460", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUltra-lightweight Neural Differential DSP Vocoder For High Quality Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oPrabhav Agrawal\\nThilo Koehler\\nZhiping Xiu\\nPrashant Serai\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.10460\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Jan 2024 02:51:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP 2024\\u00a7r"}']}
{title:'Liang et al. (§72024§r)', author: 'Yun Liang; Hai Lin; Shaojian Qiu; Yihang Zhang', display:{Lore:['[{"text": "arXiv:2401.10544", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAAT: Adapting Audio Transformer for Various Acoustics Recognition Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oYun Liang\\nHai Lin\\nShaojian Qiu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.10544\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Jan 2024 08:07:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint versionfor ICASSP 2024, Korea\\u00a7r"}']}
{title:'Ji et al. (§72024§r)', author: 'Qingfeng Ji; Jicun Zhang; Yuxin Wang', display:{Lore:['[{"text": "arXiv:2401.11102", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lASM: Audio Spectrogram Mixer\\u00a7r\\n\\n\\u00a78\\u00a7oQingfeng Ji\\nJicun Zhang\\nYuxin Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.11102\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 20 Jan 2024 03:33:18 GMT)\\u00a7r"}']}
{title:'Cai (§72024§r)', author: 'Dongqi Cai', display:{Lore:['[{"text": "arXiv:2401.11983", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLightweight Protection for Privacy in Offloaded Speech Understanding\\u00a7r\\n\\n\\u00a78\\u00a7oDongqi Cai\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.11983\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Jan 2024 14:36:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin comment: This version has been removed by arXiv administrators as the submitter did not have the rights to agree to the license at the time of submission\\u00a7r"}']}
{title:'Borrelli et al. (§72024§r)', author: 'Clara Borrelli; James Rae; Dogac Basaran; Matt McVicar; Mehrez Souden; Matthias Mauch', display:{Lore:['[{"text": "arXiv:2401.12068", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lResource-constrained stereo singing voice cancellation\\u00a7r\\n\\n\\u00a78\\u00a7oClara Borrelli\\nJames Rae\\nDogac Basaran\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.12068\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Jan 2024 16:05:30 GMT)\\u00a7r"}']}
{title:'Novack et al. (§72024§r)', author: 'Zachary Novack; Julian McAuley; Taylor Berg-Kirkpatrick; Nicholas J. Bryan', display:{Lore:['[{"text": "arXiv:2401.12179", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDITTO: Diffusion Inference-Time T-Optimization for Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oZachary Novack\\nJulian McAuley\\nTaylor Berg-Kirkpatrick\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.12179\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Jan 2024 18:10:10 GMT)\\u00a7r"}']}
{title:'Zhang (§72024§r)', author: 'Yawen Zhang', display:{Lore:['[{"text": "arXiv:2401.12266", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Exploratory Study of Multimodal Physiological Data in Jazz Improvisation Using Basic Machine Learning Techniques\\u00a7r\\n\\n\\u00a78\\u00a7oYawen Zhang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.12266\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Jan 2024 10:32:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMaster\'s thesis\\u00a7r"}']}
{title:'Härkönen et al. (§72024§r)', author: 'Marc Härkönen; Samuel J. Broughton; Lahiru Samarakoon', display:{Lore:['[{"text": "arXiv:2401.12600", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEEND-M2F: Masked-attention mask transformers for speaker diarization\\u00a7r\\n\\n\\u00a78\\u00a7oMarc H\\u00e4rk\\u00f6nen\\nSamuel J. Broughton\\nLahiru Samarakoon\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.12600\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Jan 2024 09:56:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages, 2 figures\\u00a7r"}']}
{title:'Cui et al. (§72024§r)', author: 'Wenqian Cui; Pedro Sarmento; Mathieu Barthet', display:{Lore:['[{"text": "arXiv:2401.12656", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMoodLoopGP: Generating Emotion-Conditioned Loop Tablature Music with Multi-Granular Features\\u00a7r\\n\\n\\u00a78\\u00a7oWenqian Cui\\nPedro Sarmento\\nMathieu Barthet\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.12656\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 Jan 2024 12:02:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis preprint is licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). The Version of Record of this contribution is published in Proceedings of EvoMUSART: International Conference on "}','{"text": "Computational Intelligence in Music, Sound, Art and Design (Part of EvoStar) 2024\\u00a7r"}']}
{title:'Zhao et al. (§72024§r)', author: 'Yan Zhao; Jincen Wang; Cheng Lu; Sunan Li; Björn Schuller; Yuan Zong; Wenming Zheng', display:{Lore:['[{"text": "arXiv:2401.12925", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotion-Aware Contrastive Adaptation Network for Source-Free Cross-Corpus Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYan Zhao\\nJincen Wang\\nCheng Lu\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.12925\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Jan 2024 17:21:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Kim et al. (§72024§r)', author: 'Hounsu Kim; Soonbeom Choi; Juhan Nam', display:{Lore:['[{"text": "arXiv:2401.13498", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExpressive Acoustic Guitar Sound Synthesis with an Instrument-Specific Input Representation and Diffusion Outpainting\\u00a7r\\n\\n\\u00a78\\u00a7oHounsu Kim\\nSoonbeom Choi\\nJuhan Nam\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.13498\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Jan 2024 14:44:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Monir et al. (§72024§r)', author: 'Nasser-Eddine Monir; Paul Magron; Romain Serizel', display:{Lore:['[{"text": "arXiv:2401.13548", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Phoneme-Scale Assessment of Multichannel Speech Enhancement Algorithms\\u00a7r\\n\\n\\u00a78\\u00a7oNasser-Eddine Monir\\nPaul Magron\\nRomain Serizel\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.13548\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Jan 2024 16:08:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis is the preprint of the paper that we submittedto the Trends in Hearing Journal\\u00a7r"}']}
{title:'Mogridge et al. (§72024§r)', author: 'Rhiannon Mogridge; George Close; Robert Sutherland; Thomas Hain; Jon Barker; Stefan Goetze; Anton Ragni', display:{Lore:['[{"text": "arXiv:2401.13611", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-Intrusive Speech Intelligibility Prediction for Hearing-Impaired Users using Intermediate ASR Features and Human Memory Models\\u00a7r\\n\\n\\u00a78\\u00a7oRhiannon Mogridge\\nGeorge Close\\nRobert Sutherland\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.13611\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Jan 2024 17:31:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted paper. IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), Seoul, Korea, April 2024\\u00a7r"}']}
{title:'Arora et al. (§72024§r)', author: 'Akshit Arora; Rohan Badlani; Sungwon Kim; Rafael Valle; Bryan Catanzaro', display:{Lore:['[{"text": "arXiv:2401.13851", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScaling NVIDIA\'s Multi-speaker Multi-lingual TTS Systems with Zero-Shot TTS to Indic Languages\\u00a7r\\n\\n\\u00a78\\u00a7oAkshit Arora\\nRohan Badlani\\nSungwon Kim\\nRafael Valle\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.13851\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 29 Jan 2024 18:51:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresentation accepted at ICASSP 2024\\u00a7r"}']}
{title:'Pegg et al. (§72024§r)', author: 'Samuel Pegg; Kai Li; Xiaolin Hu', display:{Lore:['[{"text": "arXiv:2401.14185", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTDFNet: An Efficient Audio-Visual Speech Separation Model with Top-down Fusion\\u00a7r\\n\\n\\u00a78\\u00a7oSamuel Pegg\\nKai Li\\nXiaolin Hu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.14185\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICIST59754.2023.10367130\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2023 13th International Conference on Information Science and\\n  Technology (ICIST), Cairo, Egypt, 2023, pp. 243-252\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 Jan 2024 13:47:22 GMT)\\u00a7r"}']}
{title:'Cuervo et al. (§72024§r)', author: 'Santiago Cuervo; Ricard Marxer', display:{Lore:['[{"text": "arXiv:2401.14289", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech foundation models on intelligibility prediction for hearing-impaired listeners\\u00a7r\\n\\n\\u00a78\\u00a7oSantiago Cuervo\\nRicard Marxer\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.14289\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Jan 2024 18:26:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be presented in ICASSP 2024\\u00a7r"}']}
{title:'Ristea et al. (§72024§r)', author: 'Nicolae Catalin Ristea; Ando Saabas; Ross Cutler; Babak Naderi; Sebastian Braun; Solomiya Branets', display:{Lore:['[{"text": "arXiv:2401.14444", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lICASSP 2024 Speech Signal Improvement Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oNicolae Catalin Ristea\\nAndo Saabas\\nRoss Cutler\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.14444\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 Jan 2024 18:08:00 GMT)\\u00a7r"}']}
{title:'Barnett et al. (§72024§r)', author: 'Julia Barnett; Hugo Flores Garcia; Bryan Pardo', display:{Lore:['[{"text": "arXiv:2401.14542", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Musical Roots: Applying Audio Embeddings to Empower Influence Attribution for a Generative Music Model\\u00a7r\\n\\n\\u00a78\\u00a7oJulia Barnett\\nHugo Flores Garcia\\nBryan Pardo\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.14542\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 Jan 2024 22:20:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages + references. Under conference review\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Yuejiao Wang; Xixin Wu; Disong Wang; Lingwei Meng; Helen Meng', display:{Lore:['[{"text": "arXiv:2401.14664", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUNIT-DSR: Dysarthric Speech Reconstruction System Using Speech Unit Normalization\\u00a7r\\n\\n\\u00a78\\u00a7oYuejiao Wang\\nXixin Wu\\nDisong Wang\\nLingwei Meng\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.14664\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Jan 2024 06:08:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Chowdhury et al. (§72024§r)', author: 'Shreyan Chowdhury; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2401.14826", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExpressivity-aware Music Performance Retrieval using Mid-level Perceptual Features and Emotion Word Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oShreyan Chowdhury\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.14826\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Jan 2024 12:52:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at FIRE 2023 (Forum for Information Retrieval Evaluation) conference, Goa, India\\u00a7r"}']}
{title:'Fedoseev et al. (§72024§r)', author: 'V. I. Fedoseev; A. A. Konev; A. Yu. Yakimuk', display:{Lore:['[{"text": "arXiv:2401.14890", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparison of parameters of vowel sounds of russian and english languages\\u00a7r\\n\\n\\u00a78\\u00a7oV. I. Fedoseev\\nA. A. Konev\\nA. Yu. Yakimuk\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.14890\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Jan 2024 14:15:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 1 figures, 3 tables\\u00a7r"}']}
{title:'Devulapally et al. (§72024§r)', author: 'Naresh Kumar Devulapally; Sidharth Anand; Sreyasee Das Bhattacharjee; Junsong Yuan; Yu-Ping Chang', display:{Lore:['[{"text": "arXiv:2401.15164", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAMuSE: Adaptive Multimodal Analysis for Speaker Emotion Recognition in Group Conversations\\u00a7r\\n\\n\\u00a78\\u00a7oNaresh Kumar Devulapally\\nSidharth Anand\\nSreyasee Das Bhattacharjee\\nJunsong Yuan\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.15164\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Jan 2024 19:17:05 GMT)\\u00a7r"}']}
{title:'Joung et al. (§72024§r)', author: 'Haesun Joung; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2401.15323", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Auto-Tagging with Robust Music Representation Learned via Domain Adversarial Training\\u00a7r\\n\\n\\u00a78\\u00a7oHaesun Joung\\nKyogu Lee\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.15323\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 27 Jan 2024 06:56:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted to ICASSP 2024\\u00a7r"}']}
{title:'Xin et al. (§72024§r)', author: 'Yifei Xin; Xiulian Peng; Yan Lu', display:{Lore:['[{"text": "arXiv:2401.15953", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMasked Audio Modeling with CLAP and Multi-Objective Learning\\u00a7r\\n\\n\\u00a78\\u00a7oYifei Xin\\nXiulian Peng\\nYan Lu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.15953\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 Jan 2024 08:35:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech2023\\u00a7r"}']}
{title:'Zhao et al. (§72024§r)', author: 'He Zhao; Hangting Chen; Jianwei Yu; Yuehai Wang', display:{Lore:['[{"text": "arXiv:2401.15993", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContinuous Target Speech Extraction: Enhancing Personalized Diarization and Extraction on Complex Recordings\\u00a7r\\n\\n\\u00a78\\u00a7oHe Zhao\\nHangting Chen\\nJianwei Yu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.15993\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 Jan 2024 09:23:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 6 figures\\u00a7r"}']}
{title:'Jain et al. (§72024§r)', author: 'Arhan Jain; Alec Bunn; Austin Pham; TJ Tsai', display:{Lore:['[{"text": "arXiv:2401.16803", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPBSCSR: The Piano Bootleg Score Composer Style Recognition Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oArhan Jain\\nAlec Bunn\\nAustin Pham\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.16803\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 7 Feb 2024 06:48:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 4 figures\\u00a7r"}']}
{title:'Saeki et al. (§72024§r)', author: 'Takaaki Saeki; Soumi Maiti; Shinnosuke Takamichi; Shinji Watanabe; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2401.16812", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeechBERTScore: Reference-Aware Automatic Evaluation of Speech Generation Leveraging NLP Evaluation Metrics\\u00a7r\\n\\n\\u00a78\\u00a7oTakaaki Saeki\\nSoumi Maiti\\nShinnosuke Takamichi\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.16812\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Jan 2024 08:26:28 GMT)\\u00a7r"}']}
{title:'Roman et al. (§72024§r)', author: 'Adrian S. Roman; Baladithya Balamurugan; Rithik Pothuganti', display:{Lore:['[{"text": "arXiv:2401.17129", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhanced Sound Event Localization and Detection in Real 360-degree audio-visual soundscapes\\u00a7r\\n\\n\\u00a78\\u00a7oAdrian S. Roman\\nBaladithya Balamurugan\\nRithik Pothuganti\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.17129\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 Jan 2024 06:05:23 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72024§r)', author: 'Guangke Chen; Yedi Zhang; Fu Song; Ting Wang; Xiaoning Du; Yang Liu', display:{Lore:['[{"text": "arXiv:2401.17133", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Proactive and Dual Prevention Mechanism against Illegal Song Covers empowered by Singing Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oGuangke Chen\\nYedi Zhang\\nFu Song\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.17133\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Jan 2024 16:07:44 GMT)\\u00a7r"}']}
{title:'Jung et al. (§72024§r)', author: 'Jee-weon Jung; Wangyou Zhang; Jiatong Shi; Zakaria Aldeneh; Takuya Higuchi; Barry-John Theobald; Ahmed Hussen Abdelaziz; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2401.17230", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lESPnet-SPK: full pipeline speaker embedding toolkit with reproducible recipes, self-supervised front-ends, and off-the-shelf models\\u00a7r\\n\\n\\u00a78\\u00a7oJee-weon Jung\\nWangyou Zhang\\nJiatong Shi\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.17230\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Jan 2024 18:18:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 7 tables\\u00a7r"}']}
{title:'Roman et al. (§72024§r)', author: 'Robin San Roman; Pierre Fernandez; Alexandre Défossez; Teddy Furon; Tuan Tran; Hady Elsahar', display:{Lore:['[{"text": "arXiv:2401.17264", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CR\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProactive Detection of Voice Cloning with Localized Watermarking\\u00a7r\\n\\n\\u00a78\\u00a7oRobin San Roman\\nPierre Fernandez\\nAlexandre D\\u00e9fossez\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.17264\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Jan 2024 18:56:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCode at https://github.com/facebookresearch/audioseal\\u00a7r"}']}
{title:'Shi et al. (§72024§r)', author: 'Jiatong Shi; Yueqian Lin; Xinyi Bai; Keyi Zhang; Yuning Wu; Yuxun Tang; Yifeng Yu; Qin Jin; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2401.17619", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSinging Voice Data Scaling-up: An Introduction to ACE-Opencpop and KiSing-v2\\u00a7r\\n\\n\\u00a78\\u00a7oJiatong Shi\\nYueqian Lin\\nXinyi Bai\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.17619\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 Jan 2024 06:17:51 GMT)\\u00a7r"}']}
{title:'Jaiswal et al. (§72024§r)', author: 'Pranay Jaiswal; Haroon R. Lone', display:{Lore:['[{"text": "arXiv:2401.17738", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHarnessing Smartwatch Microphone Sensors for Cough Detection and Classification\\u00a7r\\n\\n\\u00a78\\u00a7oPranay Jaiswal\\nHaroon R. Lone\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.17738\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 20 Apr 2024 07:50:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages\\u00a7r"}']}
{title:'Chen et al. (§72024§r)', author: 'Xueyuan Chen; Yuejiao Wang; Xixin Wu; Disong Wang; Zhiyong Wu; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2401.17796", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting Audio-Visual Features with Pretrained AV-HuBERT for Multi-Modal Dysarthric Speech Reconstruction\\u00a7r\\n\\n\\u00a78\\u00a7oXueyuan Chen\\nYuejiao Wang\\nXixin Wu\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.17796\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 Jan 2024 12:45:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Sifei Li; Weiming Dong; Yuxin Zhang; Fan Tang; Chongyang Ma; Oliver Deussen; Tong-Yee Lee; Changsheng Xu', display:{Lore:['[{"text": "arXiv:2401.17800", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDance-to-Music Generation with Encoder-based Textual Inversion of Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oSifei Li\\nWeiming Dong\\nYuxin Zhang\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.17800\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 Jan 2024 12:51:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 3 figures\\u00a7r"}']}
{title:'Aldeneh et al. (§72024§r)', author: 'Zakaria Aldeneh; Takuya Higuchi; Jee-weon Jung; Skyler Seto; Tatiana Likhomanenko; Stephen Shum; Ahmed Hussen Abdelaziz; Shinji Watanabe; Barry-John Theobald', display:{Lore:['[{"text": "arXiv:2402.00340", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCan you Remove the Downstream Model for Speaker Recognition with Self-Supervised Speech Features?\\u00a7r\\n\\n\\u00a78\\u00a7oZakaria Aldeneh\\nTakuya Higuchi\\nJee-weon Jung\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.00340\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Feb 2024 05:03:05 GMT)\\u00a7r"}']}
{title:'Liao et al. (§72024§r)', author: 'Huan Liao; Haonan Han; Kai Yang; Tianjiao Du; Rui Yang; Zunnan Xu; Qinmei Xu; Jingquan Liu; Jiasheng Lu; Xiu Li', display:{Lore:['[{"text": "arXiv:2402.00744", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBATON: Aligning Text-to-Audio Model with Human Preference Feedback\\u00a7r\\n\\n\\u00a78\\u00a7oHuan Liao\\nHaonan Han\\nKai Yang\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.00744\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Feb 2024 16:39:47 GMT)\\u00a7r"}']}
{title:'Liao et al. (§72024§r)', author: 'Shijia Liao; Shiyi Lan; Arun George Zachariah', display:{Lore:['[{"text": "arXiv:2402.00892", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks\\u00a7r\\n\\n\\u00a78\\u00a7oShijia Liao\\nShiyi Lan\\nArun George Zachariah\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.00892\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 Jan 2024 03:31:03 GMT)\\u00a7r"}']}
{title:'Pluta et al. (§72024§r)', author: 'Adam Pluta; Zbigniew Pioch; Jędrzej Kardach; Piotr Zioło; Tomasz Kręcicki; Elżbieta Trypka', display:{Lore:['[{"text": "arXiv:2402.00897", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScreening method for early dementia using sound objects as voice biomarkers\\u00a7r\\n\\n\\u00a78\\u00a7oAdam Pluta\\nZbigniew Pioch\\nJ\\u0119drzej Kardach\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.00897\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 Jan 2024 19:20:08 GMT)\\u00a7r"}']}
{title:'Chang et al. (§72024§r)', author: 'Yi Chang; Zhao Ren; Zixing Zhang; Xin Jing; Kun Qian; Xi Shao; Bin Hu; Tanja Schultz; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2402.01227", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSTAA-Net: A Sparse and Transferable Adversarial Attack for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYi Chang\\nZhao Ren\\nZixing Zhang\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.01227\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Feb 2024 08:46:57 GMT)\\u00a7r"}']}
{title:'Heggan et al. (§72024§r)', author: 'Calum Heggan; Sam Budgett; Timothy Hospedales; Mehrdad Yaghoobi', display:{Lore:['[{"text": "arXiv:2402.01274", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Transferability of Large-Scale Self-Supervision to Few-Shot Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oCalum Heggan\\nSam Budgett\\nTimothy Hospedales\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.01274\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 13 Feb 2024 20:21:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCamera Ready version as submitted to ICASSP SASB Workshop 2024. 5 pages, 2 figures, 3 tables\\u00a7r"}']}
{title:'Pasini et al. (§72024§r)', author: 'Marco Pasini; Maarten Grachten; Stefan Lattner', display:{Lore:['[{"text": "arXiv:2402.01412", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBass Accompaniment Generation via Latent Diffusion\\u00a7r\\n\\n\\u00a78\\u00a7oMarco Pasini\\nMaarten Grachten\\nStefan Lattner\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.01412\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Feb 2024 13:44:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2024\\u00a7r"}']}
{title:'Leglaive et al. (§72024§r)', author: 'Simon Leglaive; Matthieu Fraticelli; Hend ElGhazaly; Léonie Borne; Mostafa Sadeghi; Scott Wisdom; Manuel Pariente; John R. Hershey; Daniel Pressnitzer; Jon P. Barker', display:{Lore:['[{"text": "arXiv:2402.01413", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lObjective and subjective evaluation of speech enhancement methods in the UDASE task of the 7th CHiME challenge\\u00a7r\\n\\n\\u00a78\\u00a7oSimon Leglaive\\nMatthieu Fraticelli\\nHend ElGhazaly\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.01413\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Feb 2024 13:45:42 GMT)\\u00a7r"}']}
{title:'Edwards et al. (§72024§r)', author: 'Drew Edwards; Simon Dixon; Emmanouil Benetos; Akira Maezawa; Yuta Kusaka', display:{Lore:['[{"text": "arXiv:2402.01424", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Data-Driven Analysis of Robust Automatic Piano Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oDrew Edwards\\nSimon Dixon\\nEmmanouil Benetos\\nAkira Maezawa\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.01424\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Feb 2024 14:11:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in IEEE Signal Processing Letters on 31 Janurary, 2024\\u00a7r"}']}
{title:'Kakoulidis et al. (§72024§r)', author: 'Panos Kakoulidis; Nikolaos Ellinas; Georgios Vamvoukakis; Myrsini Christidou; Alexandra Vioni; Georgia Maniati; Junkwang Oh; Gunu Jho; Inchul Hwang; Pirros Tsiakoulis; Aimilios Chalamandaris', display:{Lore:['[{"text": "arXiv:2402.01520", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Resource Cross-Domain Singing Voice Synthesis via Reduced Self-Supervised Speech Representations\\u00a7r\\n\\n\\u00a78\\u00a7oPanos Kakoulidis\\nNikolaos Ellinas\\nGeorgios Vamvoukakis\\n+ 7 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.01520\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Feb 2024 16:06:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEICASSP SASB 2024\\u00a7r"}']}
{title:'Lisboa et al. (§72024§r)', author: 'Martim Lisboa; Guillaume Bellec', display:{Lore:['[{"text": "arXiv:2402.01571", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpiking Music: Audio Compression with Event Based Auto-encoders\\u00a7r\\n\\n\\u00a78\\u00a7oMartim Lisboa\\nGuillaume Bellec\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.01571\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Feb 2024 17:07:39 GMT)\\u00a7r"}']}
{title:'Baoueb et al. (§72024§r)', author: 'Teysir Baoueb; Haocheng Liu; Mathieu Fontaine; Jonathan Le Roux; Gael Richard', display:{Lore:['[{"text": "arXiv:2402.01753", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpecDiff-GAN: A Spectrally-Shaped Noise Diffusion GAN for Speech and Music Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oTeysir Baoueb\\nHaocheng Liu\\nMathieu Fontaine\\nJonathan Le Roux\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.01753\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE International Conference on Acoustics, Speech and Signal\\n  Processing, Apr 2024, Seoul (Korea), South Korea\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Jan 2024 09:17:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2024\\u00a7r"}']}
{title:'Freye et al. (§72024§r)', author: 'Arthur Freye; Jannis Müller', display:{Lore:['[{"text": "arXiv:2402.01773", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75quant-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCreating a Synthesizer from Schr\\u00f6dinger\'s Equation\\u00a7r\\n\\n\\u00a78\\u00a7oArthur Freye\\nJannis M\\u00fcller\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.01773\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 28th International Conference on Auditory\\n  Display (ICAD 2023), 2023, pp. 179-182\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Feb 2024 10:56:08 GMT)\\u00a7r"}']}
{title:'Yu et al. (§72024§r)', author: 'Guochen Yu; Runqiang Han; Chenglin Xu; Haoran Zhao; Nan Li; Chen Zhang; Xiguang Zheng; Chao Zhou; Qi Huang; Bing Yu', display:{Lore:['[{"text": "arXiv:2402.01808", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKS-Net: Multi-band joint speech restoration and enhancement network for 2024 ICASSP SSI Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oGuochen Yu\\nRunqiang Han\\nChenglin Xu\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.01808\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Feb 2024 11:28:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024; Rank 1st inICASSP 2024 Speech Signal Improvement (SSI) Challenge\\u00a7r"}']}
{title:'Niemelä et al. (§72024§r)', author: 'Marko Niemelä; Mikaela von Bonsdorff; Sami Äyrämö; Tommi Kärkkäinen', display:{Lore:['[{"text": "arXiv:2402.01824", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIdentification of Cognitive Decline from Spoken Language through Feature Selection and the Bag of Acoustic Words Model\\u00a7r\\n\\n\\u00a78\\u00a7oMarko Niemel\\u00e4\\nMikaela von Bonsdorff\\nSami \\u00c4yr\\u00e4m\\u00f6\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.01824\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Feb 2024 17:06:03 GMT)\\u00a7r"}']}
{title:'Kong et al. (§72024§r)', author: 'Zhifeng Kong; Arushi Goel; Rohan Badlani; Wei Ping; Rafael Valle; Bryan Catanzaro', display:{Lore:['[{"text": "arXiv:2402.01831", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities\\u00a7r\\n\\n\\u00a78\\u00a7oZhifeng Kong\\nArushi Goel\\nRohan Badlani\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.01831\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 28 May 2024 05:44:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICML 2024\\u00a7r"}']}
{title:'Lyth et al. (§72024§r)', author: 'Dan Lyth; Simon King', display:{Lore:['[{"text": "arXiv:2402.01912", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNatural language guidance of high-fidelity text-to-speech with synthetic annotations\\u00a7r\\n\\n\\u00a78\\u00a7oDan Lyth\\nSimon King\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.01912\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Feb 2024 21:29:34 GMT)\\u00a7r"}']}
{title:'García-Ordás et al. (§72024§r)', author: 'María Teresa García-Ordás; Héctor Alaiz-Moretón; José Alberto Benítez-Andrades; Isaías García-Rodríguez; Oscar García-Olalla; Carmen Benavides', display:{Lore:['[{"text": "arXiv:2402.02184", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSentiment analysis in non-fixed length audios using a Fully Convolutional Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oMar\\u00eda Teresa Garc\\u00eda-Ord\\u00e1s\\nH\\u00e9ctor Alaiz-Moret\\u00f3n\\nJos\\u00e9 Alberto Ben\\u00edtez-Andrades\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.02184\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.bspc.2021.102946\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nBiomedical Signal Processing and Control, Volume 69, August 2021,\\n  ID 102946\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 3 Feb 2024 15:26:28 GMT)\\u00a7r"}']}
{title:'Zhou et al. (§72024§r)', author: 'Zhenyu Zhou; Junhui Chen; Namin Wang; Lantian Li; Dong Wang', display:{Lore:['[{"text": "arXiv:2402.02699", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Data Augmentation for Robust Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oZhenyu Zhou\\nJunhui Chen\\nNamin Wang\\nLantian Li\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.02699\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Feb 2024 03:23:34 GMT)\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Pengqi Li; Tianhao Wang; Lantian Li; Askar Hamdulla; Dong Wang', display:{Lore:['[{"text": "arXiv:2402.02730", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHow phonemes contribute to deep speaker models?\\u00a7r\\n\\n\\u00a78\\u00a7oPengqi Li\\nTianhao Wang\\nLantian Li\\nAskar Hamdulla\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.02730\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Feb 2024 05:06:26 GMT)\\u00a7r"}']}
{title:'Della Libera et al. (§72024§r)', author: 'Luca Della Libera; Cem Subakan; Mirco Ravanelli', display:{Lore:['[{"text": "arXiv:2402.02754", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFocal Modulation Networks for Interpretable Sound Classification\\u00a7r\\n\\n\\u00a78\\u00a7oLuca Della Libera\\nCem Subakan\\nMirco Ravanelli\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.02754\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Feb 2024 06:20:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024 XAI-SA Workshop\\u00a7r"}']}
{title:'Xiao et al. (§72024§r)', author: 'Yang Xiao; Rohan Kumar Das', display:{Lore:['[{"text": "arXiv:2402.02781", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDual Knowledge Distillation for Efficient Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oYang Xiao\\nRohan Kumar Das\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.02781\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Feb 2024 07:30:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024 (Deep Neural Network Model Compression Workshop)\\u00a7r"}']}
{title:'Rehman et al. (§72024§r)', author: 'Yasar Abbas Ur Rehman; Kin Wai Lau; Yuyang Xie; Lan Ma; Jiajun Shen', display:{Lore:['[{"text": "arXiv:2402.02889", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Federated Self-Supervised Learning for General Purpose Audio Understanding\\u00a7r\\n\\n\\u00a78\\u00a7oYasar Abbas Ur Rehman\\nKin Wai Lau\\nYuyang Xie\\nLan Ma\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.02889\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Feb 2024 10:57:48 GMT)\\u00a7r"}']}
{title:'Ghimire et al. (§72024§r)', author: 'Rupak Raj Ghimire; Bal Krishna Bal; Prakash Poudyal', display:{Lore:['[{"text": "arXiv:2402.03050", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comprehensive Study of the Current State-of-the-Art in Nepali Automatic Speech Recognition Systems\\u00a7r\\n\\n\\u00a78\\u00a7oRupak Raj Ghimire\\nBal Krishna Bal\\nPrakash Poudyal\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.03050\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Feb 2024 14:34:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in International Conference on Technologies for Computer, Electrical, Electronics Communication (ICT-CEEL 2023)\\u00a7r"}']}
{title:'Hagiwara et al. (§72024§r)', author: 'Masato Hagiwara; Marius Miron; Jen-Yu Liu', display:{Lore:['[{"text": "arXiv:2402.03269", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lISPA: Inter-Species Phonetic Alphabet for Transcribing Animal Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oMasato Hagiwara\\nMarius Miron\\nJen-Yu Liu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.03269\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Feb 2024 18:27:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at XAI-AI Workshop (IEEEXplore track) @ ICASSP 2024\\u00a7r"}']}
{title:'Geva et al. (§72024§r)', author: 'Gil Geva; Olivier Warusfel; Shlomo Dubnov; Tammuz Dubnov; Amir Amedi; Yacov Hel-Or', display:{Lore:['[{"text": "arXiv:2402.03867", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBinaural sound source localization using a hybrid time and frequency domain model\\u00a7r\\n\\n\\u00a78\\u00a7oGil Geva\\nOlivier Warusfel\\nShlomo Dubnov\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.03867\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Feb 2024 10:28:07 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Canyu Zhang; Youbao Tang; Ning Zhang; Ruei-Sung Lin; Mei Han; Jing Xiao; Song Wang', display:{Lore:['[{"text": "arXiv:2402.04356", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBidirectional Autoregressive Diffusion Model for Dance Generation\\u00a7r\\n\\n\\u00a78\\u00a7oCanyu Zhang\\nYoubao Tang\\nNing Zhang\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.04356\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Feb 2024 19:42:18 GMT)\\u00a7r"}']}
{title:'Gracic et al. (§72024§r)', author: 'Mak Gracic; Guy Gubnisky; Roee Diamant', display:{Lore:['[{"text": "arXiv:2402.04735", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReview of Cetacean\'s click detection algorithms\\u00a7r\\n\\n\\u00a78\\u00a7oMak Gracic\\nGuy Gubnisky\\nRoee Diamant\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.04735\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Feb 2024 10:41:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o23 pages, 6 tables, 4 figures\\u00a7r"}']}
{title:'Evans et al. (§72024§r)', author: 'Zach Evans; CJ Carr; Josiah Taylor; Scott H. Hawley; Jordi Pons', display:{Lore:['[{"text": "arXiv:2402.04825", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast Timing-Conditioned Latent Audio Diffusion\\u00a7r\\n\\n\\u00a78\\u00a7oZach Evans\\nCJ Carr\\nJosiah Taylor\\nScott H. Hawley\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.04825\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 13 May 2024 14:05:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICML2024. Code: https://github.com/Stability-AI/stable-audio-tools. Metrics: https://github.com/Stability-AI/stable-audio-metrics. Demo: https://stability-ai.github.io/stable-audio-demo\\u00a7r"}']}
{title:'García-Ordás et al. (§72024§r)', author: 'María Teresa García-Ordás; Sergio Rubio-Martín; José Alberto Benítez-Andrades; Hector Alaiz-Moretón; Isaías García-Rodríguez', display:{Lore:['[{"text": "arXiv:2402.05489", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultispecies bird sound recognition using a fully convolutional neural network\\u00a7r\\n\\n\\u00a78\\u00a7oMar\\u00eda Teresa Garc\\u00eda-Ord\\u00e1s\\nSergio Rubio-Mart\\u00edn\\nJos\\u00e9 Alberto Ben\\u00edtez-Andrades\\nHector Alaiz-Moret\\u00f3n\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.05489\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s10489-023-04704-3\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nApplied Intelligence, Volume 53, July 2023, pp. 23287 - 23300\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Feb 2024 08:46:50 GMT)\\u00a7r"}']}
{title:'Salvi et al. (§72024§r)', author: 'Davide Salvi; Temesgen Semu Balcha; Paolo Bestagini; Stefano Tubaro', display:{Lore:['[{"text": "arXiv:2402.05567", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lListening Between the Lines: Synthetic Speech Detection Disregarding Verbal Content\\u00a7r\\n\\n\\u00a78\\u00a7oDavide Salvi\\nTemesgen Semu Balcha\\nPaolo Bestagini\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.05567\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Feb 2024 11:05:49 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Yixiao Zhang; Yukara Ikemiya; Gus Xia; Naoki Murata; Marco A. Martínez-Ramírez; Wei-Hsiang Liao; Yuki Mitsufuji; Simon Dixon', display:{Lore:['[{"text": "arXiv:2402.06178", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusicMagus: Zero-Shot Text-to-Music Editing via Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oYixiao Zhang\\nYukara Ikemiya\\nGus Xia\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.06178\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 28 May 2024 16:47:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IJCAI 2024\\u00a7r"}']}
{title:'Müller et al. (§72024§r)', author: 'Nicolas M. Müller; Piotr Kawa; Shen Hu; Matthias Neu; Jennifer Williams; Philip Sperl; Konstantin Böttinger', display:{Lore:['[{"text": "arXiv:2402.06304", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA New Approach to Voice Authenticity\\u00a7r\\n\\n\\u00a78\\u00a7oNicolas M. M\\u00fcller\\nPiotr Kawa\\nShen Hu\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.06304\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Feb 2024 10:34:01 GMT)\\u00a7r"}']}
{title:'Garcia-Barrios et al. (§72024§r)', author: 'Guillermo Garcia-Barrios; Eduardo Latorre Iglesias; Juana M. Gutierrez-Arriola; Ruben Fraile; Nicolas Saenz-Lechon; Victor Jose Osma-Ruiz', display:{Lore:['[{"text": "arXiv:2402.06411", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting spatial diversity for increasing the robustness of sound source localization systems against reverberation\\u00a7r\\n\\n\\u00a78\\u00a7oGuillermo Garcia-Barrios\\nEduardo Latorre Iglesias\\nJuana M. Gutierrez-Arriola\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.06411\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.apacoust.2022.109138\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Feb 2024 13:57:02 GMT)\\u00a7r"}']}
{title:'Garcia-Barrios et al. (§72024§r)', author: 'Guillermo Garcia-Barrios; Juana M. Gutierrez-Arriola; Nicolas Saenz-Lechon; Victor Jose Osma-Ruiz; Ruben Fraile', display:{Lore:['[{"text": "arXiv:2402.06586", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalytical model for the relation between signal bandwidth and spatial resolution in Steered-Response Power Phase Transform (SRP-PHAT) maps\\u00a7r\\n\\n\\u00a78\\u00a7oGuillermo Garcia-Barrios\\nJuana M. Gutierrez-Arriola\\nNicolas Saenz-Lechon\\nVictor Jose Osma-Ruiz\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.06586\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ACCESS.2021.3105650\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Feb 2024 18:05:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAny paper that cite this one has to thank IEEE for easing the open accessof the article\\u00a7r"}']}
{title:'Gokul et al. (§72024§r)', author: 'Vignesh Gokul; Chris Francis; Shlomo Dubnov', display:{Lore:['[{"text": "arXiv:2402.06810", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.IT\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a72math.IT\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluating Co-Creativity using Total Information Flow\\u00a7r\\n\\n\\u00a78\\u00a7oVignesh Gokul\\nChris Francis\\nShlomo Dubnov\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.06810\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Feb 2024 22:15:39 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Xiaofeng Liu; Fangxu Xing; Jiachen Zhuo; Maureen Stone; Jerry L. Prince; Georges El Fakhri; Jonghye Woo', display:{Lore:['[{"text": "arXiv:2402.06984", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech motion anomaly detection via cross-modal translation of 4D motion fields from tagged MRI\\u00a7r\\n\\n\\u00a78\\u00a7oXiaofeng Liu\\nFangxu Xing\\nJiachen Zhuo\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.06984\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 10 Feb 2024 16:16:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSPIE Medical Imaging 2024: Image Processing\\u00a7r"}']}
{title:'Zhu et al. (§72024§r)', author: 'Ge Zhu; Jordan Darefsky; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2402.06986", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCacophony: An Improved Contrastive Audio-Text Model\\u00a7r\\n\\n\\u00a78\\u00a7oGe Zhu\\nJordan Darefsky\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.06986\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 29 Apr 2024 05:46:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWork inProgress\\u00a7r"}']}
{title:'Fujita et al. (§72024§r)', author: 'Kenichi Fujita; Atsushi Ando; Yusuke Ijima', display:{Lore:['[{"text": "arXiv:2402.07085", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Rhythm-Based Speaker Embeddings Extraction from Phonemes and Phoneme Duration for Multi-Speaker Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oKenichi Fujita\\nAtsushi Ando\\nYusuke Ijima\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.07085\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1587/transinf.2023EDP7039\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEICE TRANSACTIONS on Information and Systems 107.1 (2024): 93-104\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 11 Feb 2024 02:26:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages,9 figures, Accepted to IEICE TRANSACTIONS on Information and Systems\\u00a7r"}']}
{title:'Zhao et al. (§72024§r)', author: 'Hang Zhao; Yifei Xin; Zhesong Yu; Bilei Zhu; Lu Lu; Zejun Ma', display:{Lore:['[{"text": "arXiv:2402.07485", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMINT: Boosting Audio-Language Model via Multi-Target Pre-Training and Instruction Tuning\\u00a7r\\n\\n\\u00a78\\u00a7oHang Zhao\\nYifei Xin\\nZhesong Yu\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.07485\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 14 Mar 2024 13:39:45 GMT)\\u00a7r"}']}
{title:'Yan et al. (§72024§r)', author: 'Yuyang Yan; Wafaa Aljbawi; Sami O. Simons; Visara Urovi', display:{Lore:['[{"text": "arXiv:2402.07619", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeveloping a Multi-variate Prediction Model For COVID-19 From Crowd-sourced Respiratory Voice Data\\u00a7r\\n\\n\\u00a78\\u00a7oYuyang Yan\\nWafaa Aljbawi\\nSami O. Simons\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.07619\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 12 Feb 2024 12:52:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2209.03727\\u00a7r"}']}
{title:'Alonso-Jiménez et al. (§72024§r)', author: 'Pablo Alonso-Jiménez; Leonardo Pepino; Roser Batlle-Roca; Pablo Zinemanas; Dmitry Bogdanov; Xavier Serra; Martín Rocamora', display:{Lore:['[{"text": "arXiv:2402.09318", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Pre-Trained Autoencoders for Interpretable Prototype Learning of Music Audio\\u00a7r\\n\\n\\u00a78\\u00a7oPablo Alonso-Jim\\u00e9nez\\nLeonardo Pepino\\nRoser Batlle-Roca\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.09318\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Feb 2024 17:13:36 GMT)\\u00a7r"}']}
{title:'Lin et al. (§72024§r)', author: 'Liwei Lin; Gus Xia; Yixiao Zhang; Junyan Jiang', display:{Lore:['[{"text": "arXiv:2402.09508", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lArrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation and Editing via Content-based Controls\\u00a7r\\n\\n\\u00a78\\u00a7oLiwei Lin\\nGus Xia\\nYixiao Zhang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.09508\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Feb 2024 19:00:01 GMT)\\u00a7r"}']}
{title:'Deshmukh et al. (§72024§r)', author: 'Soham Deshmukh; Rita Singh; Bhiksha Raj', display:{Lore:['[{"text": "arXiv:2402.09585", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDomain Adaptation for Contrastive Audio-Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oSoham Deshmukh\\nRita Singh\\nBhiksha Raj\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.09585\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Feb 2024 21:25:06 GMT)\\u00a7r"}']}
{title:'Han et al. (§72024§r)', author: 'Hyewon Han; Naveen Kumar', display:{Lore:['[{"text": "arXiv:2402.09797", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA cross-talk robust multichannel VAD model for multiparty agent interactions trained using synthetic re-recordings\\u00a7r\\n\\n\\u00a78\\u00a7oHyewon Han\\nNaveen Kumar\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.09797\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Feb 2024 08:52:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for presentation at the Hands-free Speech Communication and Microphone Arrays (HSCMA 2024)\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Zihao Wang; Shuyu Li; Tao Zhang; Qi Wang; Pengfei Yu; Jinyang Luo; Yan Liu; Ming Xi; Kejun Zhang', display:{Lore:['[{"text": "arXiv:2402.09871", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMuChin: A Chinese Colloquial Description Benchmark for Evaluating Language Models in the Field of Music\\u00a7r\\n\\n\\u00a78\\u00a7oZihao Wang\\nShuyu Li\\nTao Zhang\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.09871\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 24 Apr 2024 10:16:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by International Joint Conference on Artificial Intelligence 2024 (IJCAI 2024)\\u00a7r"}']}
{title:'Manor et al. (§72024§r)', author: 'Hila Manor; Tomer Michaeli', display:{Lore:['[{"text": "arXiv:2402.10009", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion\\u00a7r\\n\\n\\u00a78\\u00a7oHila Manor\\nTomer Michaeli\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.10009\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 29 May 2024 11:27:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICML 2024; Examples and code available in https://hilamanor.github.io/AudioEditing/\\u00a7r"}']}
{title:'Mahdi et al. (§72024§r)', author: 'Hamza Mahdi; Eptehal Nashnoush; Rami Saab; Arjun Balachandar; Rishit Dagli; Lucas X. Perri; Houman Khosravani', display:{Lore:['[{"text": "arXiv:2402.10100", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTuning In: Analysis of Audio Classifier Performance in Clinical Settings with Limited Data\\u00a7r\\n\\n\\u00a78\\u00a7oHamza Mahdi\\nEptehal Nashnoush\\nRami Saab\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.10100\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 5 Apr 2024 21:40:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCHIL 2024\\u00a7r"}']}
{title:'Madhusudhan et al. (§72024§r)', author: 'Sathwik Tejaswi Madhusudhan; Girish Chowdhary', display:{Lore:['[{"text": "arXiv:2402.10168", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeepSRGM \\u2013 Sequence Classification and Ranking in Indian Classical Music with Deep Learning\\u00a7r\\n\\n\\u00a78\\u00a7oSathwik Tejaswi Madhusudhan\\nGirish Chowdhary\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.10168\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Feb 2024 18:11:02 GMT)\\u00a7r"}']}
{title:'Togootogtokh et al. (§72024§r)', author: 'Enkhtogtokh Togootogtokh; Christian Klasen', display:{Lore:['[{"text": "arXiv:2402.10218", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAntiDeepFake: AI for Deep Fake Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oEnkhtogtokh Togootogtokh\\nChristian Klasen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.10218\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Jan 2024 08:11:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2308.12734 by other authors\\u00a7r"}']}
{title:'Bouquillard et al. (§72024§r)', author: 'Augustin Bouquillard; Florent Jacquemard', display:{Lore:['[{"text": "arXiv:2402.10247", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEngraving Oriented Joint Estimation of Pitch Spelling and Local and Global Keys\\u00a7r\\n\\n\\u00a78\\u00a7oAugustin Bouquillard\\nFlorent Jacquemard\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.10247\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Feb 2024 10:28:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInternational Conference on Technologies for Music Notation and Representation (TENOR), Apr 2024, Zurich (CH), Switzerland\\u00a7r"}']}
{title:'Ai et al. (§72024§r)', author: 'Yang Ai; Xiao-Hang Jiang; Ye-Xin Lu; Hui-Peng Du; Zhen-Hua Ling', display:{Lore:['[{"text": "arXiv:2402.10533", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAPCodec: A Neural Audio Codec with Parallel Amplitude and Phase Spectrum Encoding and Decoding\\u00a7r\\n\\n\\u00a78\\u00a7oYang Ai\\nXiao-Hang Jiang\\nYe-Xin Lu\\nHui-Peng Du\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.10533\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 16 Feb 2024 09:38:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEE/ACM Transactionson Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Brima et al. (§72024§r)', author: 'Yusuf Brima; Ulf Krumnack; Simone Pika; Gunther Heidemann', display:{Lore:['[{"text": "arXiv:2402.10547", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Disentangled Audio Representations through Controlled Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYusuf Brima\\nUlf Krumnack\\nSimone Pika\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.10547\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 16 Feb 2024 10:20:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 12 figures, accepted as a Tiny paper at ICLR 2024\\u00a7r"}']}
{title:'Haghighatshoar et al. (§72024§r)', author: 'Saeid Haghighatshoar; Dylan R Muir', display:{Lore:['[{"text": "arXiv:2402.11748", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-power SNN-based audio source localisation using a Hilbert Transform spike encoding scheme\\u00a7r\\n\\n\\u00a78\\u00a7oSaeid Haghighatshoar\\nDylan R Muir\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.11748\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 25 Feb 2024 08:12:18 GMT)\\u00a7r"}']}
{title:'Xie et al. (§72024§r)', author: 'Yuan Xie; Jiawei Ren; Ji Xu', display:{Lore:['[{"text": "arXiv:2402.11919", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnraveling Complex Data Diversity in Underwater Acoustic Target Recognition through Convolution-based Mixture of Experts\\u00a7r\\n\\n\\u00a78\\u00a7oYuan Xie\\nJiawei Ren\\nJi Xu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.11919\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.eswa.2024.123431\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nExpert Systems with Applications (2024): 123431\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 Apr 2024 06:35:12 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Xiaohui Zhang; Wenjie Fu; Mangui Liang', display:{Lore:['[{"text": "arXiv:2402.11931", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSoft-Weighted CrossEntropy Loss for Continous Alzheimer\'s Disease Detection\\u00a7r\\n\\n\\u00a78\\u00a7oXiaohui Zhang\\nWenjie Fu\\nMangui Liang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.11931\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Feb 2024 08:18:52 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Xiaohui Zhang; Wenjie Fu; Mangui Liang', display:{Lore:['[{"text": "arXiv:2402.11954", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultimodal Emotion Recognition from Raw Audio with Sinc-convolution\\u00a7r\\n\\n\\u00a78\\u00a7oXiaohui Zhang\\nWenjie Fu\\nMangui Liang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.11954\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Feb 2024 08:49:09 GMT)\\u00a7r"}']}
{title:'Varshavsky-Hassid et al. (§72024§r)', author: 'Miri Varshavsky-Hassid; Roy Hirsch; Regev Cohen; Tomer Golany; Daniel Freedman; Ehud Rivlin', display:{Lore:['[{"text": "arXiv:2402.12423", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Semantic Latent Space of Diffusion-Based Text-to-Speech Models\\u00a7r\\n\\n\\u00a78\\u00a7oMiri Varshavsky-Hassid\\nRoy Hirsch\\nRegev Cohen\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.12423\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Feb 2024 16:22:21 GMT)\\u00a7r"}']}
{title:'Sabra et al. (§72024§r)', author: 'Adam Sabra; Cyprian Wronka; Michelle Mao; Samer Hijazi', display:{Lore:['[{"text": "arXiv:2402.12482", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSECP: A Speech Enhancement-Based Curation Pipeline For Scalable Acquisition Of Clean Speech\\u00a7r\\n\\n\\u00a78\\u00a7oAdam Sabra\\nCyprian Wronka\\nMichelle Mao\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.12482\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Feb 2024 19:38:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2024\\u00a7r"}']}
{title:'Xie et al. (§72024§r)', author: 'Yuan Xie; Jiawei Ren; Ji Xu', display:{Lore:['[{"text": "arXiv:2402.12658", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGuiding the underwater acoustic target recognition with interpretable contrastive learning\\u00a7r\\n\\n\\u00a78\\u00a7oYuan Xie\\nJiawei Ren\\nJi Xu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.12658\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/OCEANSLimerick52467.2023.10244447\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nOCEANS 2023-Limerick. IEEE, 2023: 1-6\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Feb 2024 02:14:45 GMT)\\u00a7r"}']}
{title:'Xue et al. (§72024§r)', author: 'Liumeng Xue; Chaoren Wang; Mingxuan Wang; Xueyao Zhang; Jun Han; Zhizheng Wu', display:{Lore:['[{"text": "arXiv:2402.12660", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSingVisio: Visual Analytics of Diffusion Model for Singing Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oLiumeng Xue\\nChaoren Wang\\nMingxuan Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.12660\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Feb 2024 02:16:24 GMT)\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Yang Li; Yuan Shangguan; Yuhao Wang; Liangzhen Lai; Ernie Chang; Changsheng Zhao; Yangyang Shi; Vikas Chandra', display:{Lore:['[{"text": "arXiv:2402.13076", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNot All Weights Are Created Equal: Enhancing Energy Efficiency in On-Device Streaming Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYang Li\\nYuan Shangguan\\nYuhao Wang\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.13076\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Feb 2024 15:22:25 GMT)\\u00a7r"}']}
{title:'Agarwal et al. (§72024§r)', author: 'Manvi Agarwal; Changhong Wang; Gaël Richard', display:{Lore:['[{"text": "arXiv:2402.13301", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStructure-informed Positional Encoding for Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oManvi Agarwal\\nChanghong Wang\\nGa\\u00ebl Richard\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.13301\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE International Conference on Acoustics, Speech and Signal\\n  Processing (ICASSP), Apr 2024, Seoul, South Korea\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 28 Feb 2024 12:37:34 GMT)\\u00a7r"}']}
{title:'Vaessen et al. (§72024§r)', author: 'Nik Vaessen; David A. van Leeuwen', display:{Lore:['[{"text": "arXiv:2402.13723", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Effect of Batch Size on Contrastive Self-Supervised Speech Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oNik Vaessen\\nDavid A. van Leeuwen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.13723\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Feb 2024 11:35:19 GMT)\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Sifei Li; Yuxin Zhang; Fan Tang; Chongyang Ma; Weiming dong; Changsheng Xu', display:{Lore:['[{"text": "arXiv:2402.13763", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Style Transfer with Time-Varying Inversion of Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oSifei Li\\nYuxin Zhang\\nFan Tang\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.13763\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Feb 2024 12:38:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 4 figures, AAAI 2024\\u00a7r"}']}
{title:'Kamuni et al. (§72024§r)', author: 'Navin Kamuni; Sathishkumar Chintala; Naveen Kunchakuri; Jyothi Swaroop Arlagadda Narasimharaju; Venkat Kumar', display:{Lore:['[{"text": "arXiv:2402.13957", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdvancing Audio Fingerprinting Accuracy Addressing Background Noise and Distortion Challenges\\u00a7r\\n\\n\\u00a78\\u00a7oNavin Kamuni\\nSathishkumar Chintala\\nNaveen Kunchakuri\\nJyothi Swaroop Arlagadda Narasimharaju\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.13957\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Feb 2024 17:37:30 GMT)\\u00a7r"}']}
{title:'Yadav et al. (§72024§r)', author: 'Amit Kumar Singh Yadav; Ziyue Xiang; Kratika Bhagtani; Paolo Bestagini; Stefano Tubaro; Edward J. Delp', display:{Lore:['[{"text": "arXiv:2402.14205", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCompression Robust Synthetic Speech Detection Using Patched Spectrogram Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oAmit Kumar Singh Yadav\\nZiyue Xiang\\nKratika Bhagtani\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.14205\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Feb 2024 01:18:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted as longoral paper at ICMLA 2023\\u00a7r"}']}
{title:'Huang et al. (§72024§r)', author: 'Yujia Huang; Adishree Ghatare; Yuanzhe Liu; Ziniu Hu; Qinsheng Zhang; Chandramouli S Sastry; Siddharth Gururani; Sageev Oore; Yisong Yue', display:{Lore:['[{"text": "arXiv:2402.14285", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSymbolic Music Generation with Non-Differentiable Rule Guided Diffusion\\u00a7r\\n\\n\\u00a78\\u00a7oYujia Huang\\nAdishree Ghatare\\nYuanzhe Liu\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.14285\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 23 Feb 2024 02:15:32 GMT)\\u00a7r"}']}
{title:'Salehi et al. (§72024§r)', author: 'Mahsa Salehi; Kalin Stefanov; Ehsan Shareghi', display:{Lore:['[{"text": "arXiv:2402.14982", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHuman Brain Exhibits Distinct Patterns When Listening to Fake Versus Real Audio: Preliminary Evidence\\u00a7r\\n\\n\\u00a78\\u00a7oMahsa Salehi\\nKalin Stefanov\\nEhsan Shareghi\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.14982\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Mar 2024 22:43:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 4 figures, 3 tables\\u00a7r"}']}
{title:'Agchar et al. (§72024§r)', author: 'Ismael Agchar; Ilja Baumann; Franziska Braun; Paula Andrea Perez-Toro; Korbinian Riedhammer; Sebastian Trump; Martin Ullrich', display:{Lore:['[{"text": "arXiv:2402.15294", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Survey of Music Generation in the Context of Interaction\\u00a7r\\n\\n\\u00a78\\u00a7oIsmael Agchar\\nIlja Baumann\\nFranziska Braun\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.15294\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Feb 2024 12:41:44 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Haocheng Liu; Teysir Baoueb; Mathieu Fontaine; Jonathan Le Roux; Gael Richard', display:{Lore:['[{"text": "arXiv:2402.15516", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGLA-Grad: A Griffin-Lim Extended Waveform Generation Diffusion Model\\u00a7r\\n\\n\\u00a78\\u00a7oHaocheng Liu\\nTeysir Baoueb\\nMathieu Fontaine\\nJonathan Le Roux\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.15516\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE International Conference on Acoustics, Speech and Signal\\n  Processing, Apr 2024, Seoul (Korea), South Korea\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Feb 2024 12:12:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2024\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Xingyuan Li; Sinong Wang; Zeyu Xie; Mengyue Wu; Kenny Q. Zhu', display:{Lore:['[{"text": "arXiv:2402.15985", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhonetic and Lexical Discovery of a Canine Language using HuBERT\\u00a7r\\n\\n\\u00a78\\u00a7oXingyuan Li\\nSinong Wang\\nZeyu Xie\\nMengyue Wu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.15985\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 25 Feb 2024 04:35:45 GMT)\\u00a7r"}']}
{title:'Yuan et al. (§72024§r)', author: 'Ruibin Yuan; Hanfeng Lin; Yi Wang; Zeyue Tian; Shangda Wu; Tianhao Shen; Ge Zhang; Yuhang Wu; Cong Liu; Ziya Zhou; Ziyang Ma; Liumeng Xue; Ziyu Wang; Qin Liu; Tianyu Zheng; Yizhi Li; Yinghao Ma; Yiming Liang; Xiaowei Chi; Ruibo Liu; Zili Wang; Pengfei Li; Jingcheng Wu; Chenghua Lin; Qifeng Liu; Tao Jiang; Wenhao Huang; Wenhu Chen; Emmanouil Benetos; Jie Fu; Gus Xia; Roger Dannenberg; Wei Xue; Shiyin Kang; Yike Guo', display:{Lore:['[{"text": "arXiv:2402.16153", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChatMusician: Understanding and Generating Music Intrinsically with LLM\\u00a7r\\n\\n\\u00a78\\u00a7oRuibin Yuan\\nHanfeng Lin\\nYi Wang\\n+ 31 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.16153\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 25 Feb 2024 17:19:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oGitHub:https://shanghaicannon.github.io/ChatMusician/\\u00a7r"}']}
{title:'Fu et al. (§72024§r)', author: 'Szu-Wei Fu; Kuo-Hsuan Hung; Yu Tsao; Yu-Chiang Frank Wang', display:{Lore:['[{"text": "arXiv:2402.16321", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Speech Quality Estimation and Enhancement Using Only Clean Speech\\u00a7r\\n\\n\\u00a78\\u00a7oSzu-Wei Fu\\nKuo-Hsuan Hung\\nYu Tsao\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.16321\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Feb 2024 06:01:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished as a conference paper at ICLR 2024\\u00a7r"}']}
{title:'Kirton-Wingate et al. (§72024§r)', author: 'Jasper Kirton-Wingate; Shafique Ahmed; Adeel Hussain; Mandar Gogate; Kia Dashtipour; Jen-Cheng Hou; Tassadaq Hussain; Yu Tsao; Amir Hussain', display:{Lore:['[{"text": "arXiv:2402.16757", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Environmental Preference Based Speech Enhancement For Individualised Multi-Modal Hearing Aids\\u00a7r\\n\\n\\u00a78\\u00a7oJasper Kirton-Wingate\\nShafique Ahmed\\nAdeel Hussain\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.16757\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Feb 2024 17:21:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis has been submitted to the Trends in Hearing journal\\u00a7r"}']}
{title:'Diener et al. (§72024§r)', author: 'Lorenz Diener; Solomiya Branets; Ando Saabas; Ross Cutler', display:{Lore:['[{"text": "arXiv:2402.16927", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe ICASSP 2024 Audio Deep Packet Loss Concealment Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oLorenz Diener\\nSolomiya Branets\\nAndo Saabas\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.16927\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Feb 2024 13:26:20 GMT)\\u00a7r"}']}
{title:'Kang et al. (§72024§r)', author: 'Taein Kang; Soyul Han; Sunmook Choi; Jaejin Seo; Sanghyeok Chung; Seungeun Lee; Seungsang Oh; Il-Youp Kwak', display:{Lore:['[{"text": "arXiv:2402.17127", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExperimental Study: Enhancing Voice Spoofing Detection Models with wav2vec 2.0\\u00a7r\\n\\n\\u00a78\\u00a7oTaein Kang\\nSoyul Han\\nSunmook Choi\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.17127\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Feb 2024 01:45:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Tan et al. (§72024§r)', author: 'Liwen Tan; Yin Cao; Yi Zhou', display:{Lore:['[{"text": "arXiv:2402.17259", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEDTC: enhance depth of text comprehension in automated audio captioning\\u00a7r\\n\\n\\u00a78\\u00a7oLiwen Tan\\nYin Cao\\nYi Zhou\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.17259\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Feb 2024 07:05:22 GMT)\\u00a7r"}']}
{title:'Ani et al. (§72024§r)', author: 'Saja Al Ani; Joanne Cleland; Ahmed Zoha', display:{Lore:['[{"text": "arXiv:2402.17482", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomated Classification of Phonetic Segments in Child Speech Using Raw Ultrasound Imaging\\u00a7r\\n\\n\\u00a78\\u00a7oSaja Al Ani\\nJoanne Cleland\\nAhmed Zoha\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.17482\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5220/0012592700003657\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 17th International Joint Conference on\\n  Biomedical Engineering Systems and Technologies - Volume 1: BIOIMAGING, 2024,\\n  pages 326-331\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Feb 2024 13:08:34 GMT)\\u00a7r"}']}
{title:'Zaragozá et al. (§72024§r)', author: 'Lucía Gómez Zaragozá; Rocío del Amor; Elena Parra Vargas; Valery Naranjo; Mariano Alcañiz Raya; Javier Marín-Morales', display:{Lore:['[{"text": "arXiv:2402.17496", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages\\u00a7r\\n\\n\\u00a78\\u00a7oLuc\\u00eda G\\u00f3mez Zaragoz\\u00e1\\nRoc\\u00edo del Amor\\nElena Parra Vargas\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.17496\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Feb 2024 13:22:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 6 figures, submitted to Scientific Data\\u00a7r"}']}
{title:'Ding et al. (§72024§r)', author: 'Shuangrui Ding; Zihan Liu; Xiaoyi Dong; Pan Zhang; Rui Qian; Conghui He; Dahua Lin; Jiaqi Wang', display:{Lore:['[{"text": "arXiv:2402.17645", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSongComposer: A Large Language Model for Lyric and Melody Composition in Song Generation\\u00a7r\\n\\n\\u00a78\\u00a7oShuangrui Ding\\nZihan Liu\\nXiaoyi Dong\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.17645\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Feb 2024 16:15:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oproject page: https://pjlab-songcomposer.github.io/ code: https://github.com/pjlab-songcomposer/songcomposer\\u00a7r"}']}
{title:'Liang et al. (§72024§r)', author: 'Xia Liang; Xingjian Du; Jiaju Lin; Pei Zou; Yuan Wan; Bilei Zhu', display:{Lore:['[{"text": "arXiv:2402.17785", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lByteComposer: a Human-like Melody Composition Method based on Language Model Agent\\u00a7r\\n\\n\\u00a78\\u00a7oXia Liang\\nXingjian Du\\nJiaju Lin\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.17785\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 7 Mar 2024 00:32:27 GMT)\\u00a7r"}']}
{title:'Mittal et al. (§72024§r)', author: 'Govind Mittal; Arthur Jakobsson; Kelly O. Marshall; Chinmay Hegde; Nasir Memon', display:{Lore:['[{"text": "arXiv:2402.18085", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAI-assisted Tagging of Deepfake Audio Calls using Challenge-Response\\u00a7r\\n\\n\\u00a78\\u00a7oGovind Mittal\\nArthur Jakobsson\\nKelly O. Marshall\\nChinmay Hegde\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.18085\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Feb 2024 06:17:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDataset will be made public by end of March 2024\\u00a7r"}']}
{title:'López-Chilet et al. (§72024§r)', author: 'Álvaro López-Chilet; Zhaoyi Liu; Jon Ander Gómez; Carlos Alvarez; Marivi Alonso Ortiz; Andres Orejuela Mesa; David Newton; Friedrich Wolf-Monheim; Sam Michiels; Danny Hughes', display:{Lore:['[{"text": "arXiv:2402.18204", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConvDTW-ACS: Audio Segmentation for Track Type Detection During Car Manufacturing\\u00a7r\\n\\n\\u00a78\\u00a7o\\u00c1lvaro L\\u00f3pez-Chilet\\nZhaoyi Liu\\nJon Ander G\\u00f3mez\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.18204\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Feb 2024 09:48:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 2 figures\\u00a7r"}']}
{title:'Shi et al. (§72024§r)', author: 'Hao Shi; Tatsuya Kawahara', display:{Lore:['[{"text": "arXiv:2402.18275", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigation of Adapter for Automatic Speech Recognition in Noisy Environment\\u00a7r\\n\\n\\u00a78\\u00a7oHao Shi\\nTatsuya Kawahara\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.18275\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 29 Feb 2024 05:28:18 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Lin Zhang; Themos Stafylakis; Federico Landini; Mireia Diez; Anna Silnova; Lukáš Burget', display:{Lore:['[{"text": "arXiv:2402.19325", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDo End-to-End Neural Diarization Attractors Need to Encode Speaker Characteristic Information?\\u00a7r\\n\\n\\u00a78\\u00a7oLin Zhang\\nThemos Stafylakis\\nFederico Landini\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.19325\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 29 Feb 2024 16:28:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Odyssey 2024\\u00a7r"}']}
{title:'Joshi et al. (§72024§r)', author: 'Sonal Joshi; Thomas Thebaud; Jesús Villalba; Najim Dehak', display:{Lore:['[{"text": "arXiv:2402.19355", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnraveling Adversarial Examples against Speaker Identification \\u2013 Techniques for Attack Detection and Victim Model Classification\\u00a7r\\n\\n\\u00a78\\u00a7oSonal Joshi\\nThomas Thebaud\\nJes\\u00fas Villalba\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.19355\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 29 Feb 2024 17:06:52 GMT)\\u00a7r"}']}
{title:'Raymondaud et al. (§72024§r)', author: 'Quentin Raymondaud; Mickael Rouvier; Richard Dufour', display:{Lore:['[{"text": "arXiv:2402.19443", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProbing the Information Encoded in Neural-based Acoustic Models of Automatic Speech Recognition Systems\\u00a7r\\n\\n\\u00a78\\u00a7oQuentin Raymondaud\\nMickael Rouvier\\nRichard Dufour\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.19443\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 29 Feb 2024 18:43:53 GMT)\\u00a7r"}']}
{title:'Lin et al. (§72024§r)', author: 'Weiwei Lin; Chenhang He; Man-Wai Mak; Jiachen Lian; Kong Aik Lee', display:{Lore:['[{"text": "arXiv:2403.00529", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoxGenesis: Unsupervised Discovery of Latent Speaker Manifold for Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oWeiwei Lin\\nChenhang He\\nMan-Wai Mak\\nJiachen Lian\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.00529\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Mar 2024 13:39:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7opreprint\\u00a7r"}']}
{title:'Moyo (§72024§r)', author: 'Tofara Moyo', display:{Lore:['[{"text": "arXiv:2403.00790", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStructuring Concept Space with the Musical Circle of Fifths by Utilizing Music Grammar Based Activations\\u00a7r\\n\\n\\u00a78\\u00a7oTofara Moyo\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.00790\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Feb 2024 03:28:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o3 pages\\u00a7r"}']}
{title:'Casebeer et al. (§72024§r)', author: 'Jonah Casebeer; Nicholas J. Bryan; Paris Smaragdis', display:{Lore:['[{"text": "arXiv:2403.00977", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScaling Up Adaptive Filter Optimizers\\u00a7r\\n\\n\\u00a78\\u00a7oJonah Casebeer\\nNicholas J. Bryan\\nParis Smaragdis\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.00977\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Mar 2024 20:58:43 GMT)\\u00a7r"}']}
{title:'Kheddar et al. (§72024§r)', author: 'Hamza Kheddar; Mustapha Hemis; Yassine Himeur', display:{Lore:['[{"text": "arXiv:2403.01255", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Speech Recognition using Advanced Deep Learning Approaches: A survey\\u00a7r\\n\\n\\u00a78\\u00a7oHamza Kheddar\\nMustapha Hemis\\nYassine Himeur\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.01255\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.inffus.2024.102422\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInformation Fusion, Elsevier, 2024\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 18 Apr 2024 17:29:29 GMT)\\u00a7r"}']}
{title:'Xie et al. (§72024§r)', author: 'Zeyu Xie; Baihan Li; Xuenan Xu; Mengyue Wu; Kai Yu', display:{Lore:['[{"text": "arXiv:2403.01278", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Audio Generation Diversity with Visual Information\\u00a7r\\n\\n\\u00a78\\u00a7oZeyu Xie\\nBaihan Li\\nXuenan Xu\\nMengyue Wu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.01278\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Mar 2024 17:59:57 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Haoxu Wang; Ming Cheng; Qiang Fu; Ming Li', display:{Lore:['[{"text": "arXiv:2403.01700", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Wake Word Spotting With Frame-Level Cross-Modal Attention Based Audio-Visual Conformer\\u00a7r\\n\\n\\u00a78\\u00a7oHaoxu Wang\\nMing Cheng\\nQiang Fu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.01700\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Mar 2024 03:25:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Ho et al. (§72024§r)', author: 'Kuan-Hsun Ho; Jeih-weih Hung; Berlin Chen', display:{Lore:['[{"text": "arXiv:2403.01785", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhat do neural networks listen to? Exploring the crucial bands in Speech Enhancement using Sinc-convolution\\u00a7r\\n\\n\\u00a78\\u00a7oKuan-Hsun Ho\\nJeih-weih Hung\\nBerlin Chen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.01785\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Mar 2024 07:27:25 GMT)\\u00a7r"}']}
{title:'Ho et al. (§72024§r)', author: 'Kuan-Hsun Ho; Jeih-weih Hung; Berlin Chen', display:{Lore:['[{"text": "arXiv:2403.01792", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConSep: a Noise- and Reverberation-Robust Speech Separation Framework by Magnitude Conditioning\\u00a7r\\n\\n\\u00a78\\u00a7oKuan-Hsun Ho\\nJeih-weih Hung\\nBerlin Chen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.01792\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Mar 2024 07:34:24 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72024§r)', author: 'Yujie Yang; Haochen Qin; Hang Zhou; Chengcheng Wang; Tianyu Guo; Kai Han; Yunhe Wang', display:{Lore:['[{"text": "arXiv:2403.01960", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA robust audio deepfake detection system via multi-view feature\\u00a7r\\n\\n\\u00a78\\u00a7oYujie Yang\\nHaochen Qin\\nHang Zhou\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.01960\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Mar 2024 11:57:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures\\u00a7r"}']}
{title:'Inoue et al. (§72024§r)', author: 'Sho Inoue; Kun Zhou; Shuai Wang; Haizhou Li', display:{Lore:['[{"text": "arXiv:2403.02002", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFine-Grained Quantitative Emotion Editing for Speech Generation\\u00a7r\\n\\n\\u00a78\\u00a7oSho Inoue\\nKun Zhou\\nShuai Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.02002\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Mar 2024 12:53:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper is submitted to IEEE Signal Processing Letters\\u00a7r"}']}
{title:'Fan et al. (§72024§r)', author: 'Zhiyun Fan; Linhao Dong; Jun Zhang; Lu Lu; Zejun Ma', display:{Lore:['[{"text": "arXiv:2403.02010", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSA-SOT: Speaker-Aware Serialized Output Training for Multi-Talker ASR\\u00a7r\\n\\n\\u00a78\\u00a7oZhiyun Fan\\nLinhao Dong\\nJun Zhang\\nLu Lu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.02010\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Mar 2024 13:10:40 GMT)\\u00a7r"}']}
{title:'Khan et al. (§72024§r)', author: 'Ibrahim Khan; Thai Van Nguyen; Chollakorn Nimpattanavong; Ruck Thawonmas', display:{Lore:['[{"text": "arXiv:2403.02701", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFighting Game Adaptive Background Music for Improved Gameplay\\u00a7r\\n\\n\\u00a78\\u00a7oIbrahim Khan\\nThai Van Nguyen\\nChollakorn Nimpattanavong\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.02701\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Mar 2024 06:46:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis is an updated version of our IEEE CoG 2023 paper (https://ieeexplore.ieee.org/document/10333245). This version has revised the description of the association between the distance between the two players (PD) and "}','{"text": "the instrument\'s volumeon page 2. arXiv admin note: substantial text overlap with arXiv:2303.15734\\u00a7r"}']}
{title:'Hirawata et al. (§72024§r)', author: 'So Hirawata; Noriko Otani', display:{Lore:['[{"text": "arXiv:2403.03395", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInteractive Melody Generation System for Enhancing the Creativity of Musicians\\u00a7r\\n\\n\\u00a78\\u00a7oSo Hirawata\\nNoriko Otani\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.03395\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Mar 2024 01:33:48 GMT)\\u00a7r"}']}
{title:'Kalkhorani et al. (§72024§r)', author: 'Vahid Ahmadi Kalkhorani; DeLiang Wang', display:{Lore:['[{"text": "arXiv:2403.03411", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCrossNet: Leveraging Global, Cross-Band, Narrow-Band, and Positional Encoding for Single- and Multi-Channel Speaker Separation\\u00a7r\\n\\n\\u00a78\\u00a7oVahid Ahmadi Kalkhorani\\nDeLiang Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.03411\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Mar 2024 02:39:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages\\u00a7r"}']}
{title:'Schoder et al. (§72024§r)', author: 'Stefan Schoder; Paul Maurerlehner', display:{Lore:['[{"text": "arXiv:2403.03510", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.class-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMETAMAT 01: A semi-analytic Solution for Benchmarking Wave Propagation Simulations of homogeneous Absorbers in 1D/3D and 2D\\u00a7r\\n\\n\\u00a78\\u00a7oStefan Schoder\\nPaul Maurerlehner\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.03510\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Mar 2024 07:37:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4\\u00a7r"}']}
{title:'Biron et al. (§72024§r)', author: 'Tirza Biron; Moshe Barboy; Eran Ben-Artzy; Alona Golubchik; Yanir Marmor; Smadar Szekely; Yaron Winter; David Harel', display:{Lore:['[{"text": "arXiv:2403.03522", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-verbal information in spontaneous speech \\u2013 towards a new framework of analysis\\u00a7r\\n\\n\\u00a78\\u00a7oTirza Biron\\nMoshe Barboy\\nEran Ben-Artzy\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.03522\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 13 Mar 2024 09:50:40 GMT)\\u00a7r"}']}
{title:'Álvarez et al. (§72024§r)', author: 'Jorge Álvarez; Juan Carlos Armenteros; Camilo Torrón; Miguel Ortega-Martín; Alfonso Ardoiz; Óscar García; Ignacio Arranz; Íñigo Galdeano; Ignacio Garrido; Adrián Alonso; Fernando Bayón; Oleg Vorontsov', display:{Lore:['[{"text": "arXiv:2403.03538", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRADIA \\u2013 Radio Advertisement Detection with Intelligent Analytics\\u00a7r\\n\\n\\u00a78\\u00a7oJorge \\u00c1lvarez\\nJuan Carlos Armenteros\\nCamilo Torr\\u00f3n\\n+ 8 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.03538\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Mar 2024 08:34:28 GMT)\\u00a7r"}']}
{title:'Ramoneda et al. (§72024§r)', author: 'Pedro Ramoneda; Minhee Lee; Dasaem Jeong; J. J. Valero-Mas; Xavier Serra', display:{Lore:['[{"text": "arXiv:2403.03947", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCan Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oPedro Ramoneda\\nMinhee Lee\\nDasaem Jeong\\nJ. J. Valero-Mas\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.03947\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Mar 2024 18:54:13 GMT)\\u00a7r"}']}
{title:'Jeon et al. (§72024§r)', author: 'Yejin Jeon; Gary Geunbae Lee', display:{Lore:['[{"text": "arXiv:2403.04111", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Level Attention Aggregation for Language-Agnostic Speaker Replication\\u00a7r\\n\\n\\u00a78\\u00a7oYejin Jeon\\nGary Geunbae Lee\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.04111\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 3 Apr 2024 07:40:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to EACL Main 2024\\u00a7r"}']}
{title:'Dai et al. (§72024§r)', author: 'Yusheng Dai; Hang Chen; Jun Du; Ruoyu Wang; Shihao Chen; Jiefeng Ma; Haotian Wang; Chin-Hui Lee', display:{Lore:['[{"text": "arXiv:2403.04245", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Study of Dropout-Induced Modality Bias on Robustness to Missing Video Frames for Audio-Visual Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYusheng Dai\\nHang Chen\\nJun Du\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.04245\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Mar 2024 06:06:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7othe paper is accepted by CVPR2024\\u00a7r"}']}
{title:'Xu et al. (§72024§r)', author: 'Xuenan Xu; Xiaohang Xu; Zeyu Xie; Pingyue Zhang; Mengyue Wu; Kai Yu', display:{Lore:['[{"text": "arXiv:2403.04594", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Detailed Audio-Text Data Simulation Pipeline using Single-Event Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oXuenan Xu\\nXiaohang Xu\\nZeyu Xie\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.04594\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Mar 2024 15:40:01 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Peng Liu; Dongyang Dai', display:{Lore:['[{"text": "arXiv:2403.05010", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction\\u00a7r\\n\\n\\u00a78\\u00a7oPeng Liu\\nDongyang Dai\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.05010\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Mar 2024 03:16:47 GMT)\\u00a7r"}']}
{title:'Gohari et al. (§72024§r)', author: 'Mahyar Gohari; Paolo Bestagini; Sergio Benini; Nicola Adami', display:{Lore:['[{"text": "arXiv:2403.05380", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpectrogram-Based Detection of Auto-Tuned Vocals in Music Recordings\\u00a7r\\n\\n\\u00a78\\u00a7oMahyar Gohari\\nPaolo Bestagini\\nSergio Benini\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.05380\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Mar 2024 15:19:26 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72024§r)', author: 'Qu Yang; Qianhui Liu; Nan Li; Meng Ge; Zeyang Song; Haizhou Li', display:{Lore:['[{"text": "arXiv:2403.05772", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lsVAD: A Robust, Low-Power, and Light-Weight Voice Activity Detection with Spiking Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oQu Yang\\nQianhui Liu\\nNan Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.05772\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 9 Mar 2024 02:55:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Yang et al. (§72024§r)', author: 'Yudong Yang; Rongfeng Su; Xiaokang Liu; Nan Yan; Lan Wang', display:{Lore:['[{"text": "arXiv:2403.05820", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Audio-textual Diffusion Model For Converting Speech Signals Into Ultrasound Tongue Imaging Data\\u00a7r\\n\\n\\u00a78\\u00a7oYudong Yang\\nRongfeng Su\\nXiaokang Liu\\nNan Yan\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.05820\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 12 Mar 2024 11:26:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP2024 Accept\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Chunhui Wang; Chang Zeng; Bowen Zhang; Ziyang Ma; Yefan Zhu; Zifeng Cai; Jian Zhao; Zhonglin Jiang; Yong Chen', display:{Lore:['[{"text": "arXiv:2403.05989", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHAM-TTS: Hierarchical Acoustic Modeling for Token-Based Zero-Shot Text-to-Speech with Model and Data Scaling\\u00a7r\\n\\n\\u00a78\\u00a7oChunhui Wang\\nChang Zeng\\nBowen Zhang\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.05989\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 9 Mar 2024 19:05:48 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72024§r)', author: 'Yufeng Yang; Ashutosh Pandey; DeLiang Wang', display:{Lore:['[{"text": "arXiv:2403.06387", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Decoupling Frontend Enhancement and Backend Recognition in Monaural Robust ASR\\u00a7r\\n\\n\\u00a78\\u00a7oYufeng Yang\\nAshutosh Pandey\\nDeLiang Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.06387\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Mar 2024 02:45:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEE/ACM Transactionson Audio, Speech and Language Processing. arXiv admin note: text overlap with arXiv:2210.13318\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Qiongqiong Wang; Kong Aik Lee', display:{Lore:['[{"text": "arXiv:2403.06404", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCosine Scoring with Uncertainty for Neural Speaker Embedding\\u00a7r\\n\\n\\u00a78\\u00a7oQiongqiong Wang\\nKong Aik Lee\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.06404\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2024.3375080\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Signal Processing Letters 2024\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Mar 2024 03:31:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures\\u00a7r"}']}
{title:'Quan et al. (§72024§r)', author: 'Changsheng Quan; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2403.07675", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultichannel Long-Term Streaming Neural Speech Enhancement for Static and Moving Speakers\\u00a7r\\n\\n\\u00a78\\u00a7oChangsheng Quan\\nXiaofei Li\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.07675\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Mar 2024 14:11:29 GMT)\\u00a7r"}']}
{title:'Cioflan et al. (§72024§r)', author: 'Cristian Cioflan; Lukas Cavigelli; Luca Benini', display:{Lore:['[{"text": "arXiv:2403.07802", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBoosting keyword spotting through on-device learnable user speech characteristics\\u00a7r\\n\\n\\u00a78\\u00a7oCristian Cioflan\\nLukas Cavigelli\\nLuca Benini\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.07802\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Mar 2024 16:41:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 tables, 2 figures. Accepted as a full paper by the tinyMLResearch Symposium 2024\\u00a7r"}']}
{title:'Mo et al. (§72024§r)', author: 'Shentong Mo; Jing Shi; Yapeng Tian', display:{Lore:['[{"text": "arXiv:2403.07938", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lText-to-Audio Generation Synchronized with Videos\\u00a7r\\n\\n\\u00a78\\u00a7oShentong Mo\\nJing Shi\\nYapeng Tian\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.07938\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Mar 2024 22:27:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2305.12903\\u00a7r"}']}
{title:'Bhandari et al. (§72024§r)', author: 'Keshav Bhandari; Simon Colton', display:{Lore:['[{"text": "arXiv:2403.07995", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMotifs, Phrases, and Beyond: The Modelling of Structure in Symbolic Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oKeshav Bhandari\\nSimon Colton\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.07995\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Mar 2024 18:03:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to 13thInternational Conference on Artificial Intelligence in Music, Sound, Art and Design (EvoMUSART) 2024\\u00a7r"}']}
{title:'Liang et al. (§72024§r)', author: 'Ziqi Liang; Haoxiang Shi; Jiawei Wang; Keda Lu', display:{Lore:['[{"text": "arXiv:2403.08164", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oZiqi Liang\\nHaoxiang Shi\\nJiawei Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.08164\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 17 Mar 2024 10:06:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the 27th IEEE InternationalConference on Computer Supported Cooperative Work in Design (IEEE CSCWD 2024). arXiv admin note: substantial text overlap with arXiv:2211.01948\\u00a7r"}']}
{title:'Martinsson et al. (§72024§r)', author: 'John Martinsson; Olof Mogren; Maria Sandsten; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2403.08525", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrom Weak to Strong Sound Event Labels using Adaptive Change-Point Detection and Active Learning\\u00a7r\\n\\n\\u00a78\\u00a7oJohn Martinsson\\nOlof Mogren\\nMaria Sandsten\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.08525\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Mar 2024 13:33:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review at EUSIPCO 2024\\u00a7r"}']}
{title:'Juvela et al. (§72024§r)', author: 'Lauri Juvela; Eero-Pekka Damskägg; Aleksi Peussa; Jaakko Mäkinen; Thomas Sherson; Stylianos I. Mimilakis; Athanasios Gotsopoulos', display:{Lore:['[{"text": "arXiv:2403.08559", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Amp Modeling: From Data to Controllable Guitar Amplifier Models\\u00a7r\\n\\n\\u00a78\\u00a7oLauri Juvela\\nEero-Pekka Damsk\\u00e4gg\\nAleksi Peussa\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.08559\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Mar 2024 14:10:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Zhao Wang; Xiaomeng Li; Na Li; Longlong Shu', display:{Lore:['[{"text": "arXiv:2403.09030", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn AI-Driven Approach to Wind Turbine Bearing Fault Diagnosis from Acoustic Signals\\u00a7r\\n\\n\\u00a78\\u00a7oZhao Wang\\nXiaomeng Li\\nNa Li\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.09030\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Mar 2024 01:46:30 GMT)\\u00a7r"}']}
{title:'Kruspe (§72024§r)', author: 'Anna Kruspe', display:{Lore:['[{"text": "arXiv:2403.09298", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMore than words: Advancements and challenges in speech recognition for singing\\u00a7r\\n\\n\\u00a78\\u00a7oAnna Kruspe\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.09298\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Mar 2024 11:37:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oConference on Electronic Speech Signal Processing (ESSV) 2024, Keynote\\u00a7r"}']}
{title:'Khodzhaev (§72024§r)', author: 'Zulfidin Khodzhaev', display:{Lore:['[{"text": "arXiv:2403.09321", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Practical Guide to Spectrogram Analysis for Audio Signal Processing\\u00a7r\\n\\n\\u00a78\\u00a7oZulfidin Khodzhaev\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.09321\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Mar 2024 12:10:47 GMT)\\u00a7r"}']}
{title:'Yin et al. (§72024§r)', author: 'Wenjie Yin; Xuejiao Zhao; Yi Yu; Hang Yin; Danica Kragic; Mårten Björkman', display:{Lore:['[{"text": "arXiv:2403.09407", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLM2D: Lyrics- and Music-Driven Dance Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oWenjie Yin\\nXuejiao Zhao\\nYi Yu\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.09407\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Mar 2024 13:59:04 GMT)\\u00a7r"}']}
{title:'Grinstein et al. (§72024§r)', author: 'Eric Grinstein; Toon van Waterschoot; Mike Brookes; Patrick A. Naylor', display:{Lore:['[{"text": "arXiv:2403.09455", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Neural-SRP method for positional sound source localization\\u00a7r\\n\\n\\u00a78\\u00a7oEric Grinstein\\nToon van Waterschoot\\nMike Brookes\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.09455\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Mar 2024 14:53:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at Asilomar Conference on Signals, Systems, andComputers\\u00a7r"}']}
{title:'Tabassum et al. (§72024§r)', author: 'Afrina Tabassum; Dung Tran; Trung Dang; Ismini Lourentzou; Kazuhito Koishida', display:{Lore:['[{"text": "arXiv:2403.09579", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7luaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with Unsupervised Audio Mixtures\\u00a7r\\n\\n\\u00a78\\u00a7oAfrina Tabassum\\nDung Tran\\nTrung Dang\\nIsmini Lourentzou\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.09579\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Mar 2024 17:13:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 6 figures, 4 tables. To appear in ICASSP\'2024\\u00a7r"}']}
{title:'Moummad et al. (§72024§r)', author: 'Ilyass Moummad; Nicolas Farrugia; Romain Serizel; Jeremy Froidevaux; Vincent Lostanlen', display:{Lore:['[{"text": "arXiv:2403.09598", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMixture of Mixups for Multi-label Classification of Rare Anuran Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oIlyass Moummad\\nNicolas Farrugia\\nRomain Serizel\\nJeremy Froidevaux\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.09598\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Mar 2024 17:39:14 GMT)\\u00a7r"}']}
{title:'Groh et al. (§72024§r)', author: 'René Groh; Nina Goes; Andreas M. Kist', display:{Lore:['[{"text": "arXiv:2403.09753", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpokeN-100: A Cross-Lingual Benchmarking Dataset for The Classification of Spoken Numbers in Different Languages\\u00a7r\\n\\n\\u00a78\\u00a7oRen\\u00e9 Groh\\nNina Goes\\nAndreas M. Kist\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.09753\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Mar 2024 12:07:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted as a full paper by the tinyMLResearch Symposium 2024\\u00a7r"}']}
{title:'Tan et al. (§72024§r)', author: 'Hao Hao Tan; Kin Wai Cheuk; Taemin Cho; Wei-Hsiang Liao; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2403.10024", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMR-MT3: Memory Retaining Multi-Track Music Transcription to Mitigate Instrument Leakage\\u00a7r\\n\\n\\u00a78\\u00a7oHao Hao Tan\\nKin Wai Cheuk\\nTaemin Cho\\nWei-Hsiang Liao\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.10024\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Mar 2024 05:13:38 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Qian Wang; Jia-Chen Gu; Zhen-Hua Ling', display:{Lore:['[{"text": "arXiv:2403.10146", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultiscale Matching Driven by Cross-Modal Similarity Consistency for Audio-Text Retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oQian Wang\\nJia-Chen Gu\\nZhen-Hua Ling\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.10146\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Mar 2024 09:47:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted to ICASSP2024\\u00a7r"}']}
{title:'Rauch et al. (§72024§r)', author: 'Lukas Rauch; Raphael Schwinger; Moritz Wirth; René Heinrich; Jonas Lange; Stefan Kahl; Bernhard Sick; Sven Tomforde; Christoph Scholz', display:{Lore:['[{"text": "arXiv:2403.10380", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBirdSet: A Multi-Task Benchmark for Classification in Computational Avian Bioacoustics\\u00a7r\\n\\n\\u00a78\\u00a7oLukas Rauch\\nRaphael Schwinger\\nMoritz Wirth\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.10380\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Apr 2024 20:58:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWork inprogress, to be submitted @DMLR next month\\u00a7r"}']}
{title:'Zhu et al. (§72024§r)', author: 'Ge Zhu; Juan-Pablo Caceres; Zhiyao Duan; Nicholas J. Bryan', display:{Lore:['[{"text": "arXiv:2403.10493", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusicHiFi: Fast High-Fidelity Stereo Vocoding\\u00a7r\\n\\n\\u00a78\\u00a7oGe Zhu\\nJuan-Pablo Caceres\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.10493\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Mar 2024 06:05:00 GMT)\\u00a7r"}']}
{title:'Cioflan et al. (§72024§r)', author: 'Cristian Cioflan; Lukas Cavigelli; Manuele Rusci; Miguel de Prado; Luca Benini', display:{Lore:['[{"text": "arXiv:2403.10549", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn-Device Domain Learning for Keyword Spotting on Low-Power Extreme Edge Embedded Systems\\u00a7r\\n\\n\\u00a78\\u00a7oCristian Cioflan\\nLukas Cavigelli\\nManuele Rusci\\nMiguel de Prado\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.10549\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Mar 2024 19:54:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 tables, 2 figures. Accepted at IEEE AICAS 2024\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Yin Li; Rajalakshmi Nanadakumar', display:{Lore:['[{"text": "arXiv:2403.10796", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCoPlay: Audio-agnostic Cognitive Scaling for Acoustic Sensing\\u00a7r\\n\\n\\u00a78\\u00a7oYin Li\\nRajalakshmi Nanadakumar\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.10796\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 16 Mar 2024 03:59:33 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Fan Zhang; Zhaohan Wang; Xin Lyu; Siyuan Zhao; Mengjian Li; Weidong Geng; Naye Ji; Hui Du; Fuxing Gao; Hao Wu; Shunman Li', display:{Lore:['[{"text": "arXiv:2403.10805", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech-driven Personalized Gesture Synthetics: Harnessing Automatic Fuzzy Feature Inference\\u00a7r\\n\\n\\u00a78\\u00a7oFan Zhang\\nZhaohan Wang\\nXin Lyu\\n+ 7 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.10805\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 16 Mar 2024 04:40:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages,\\u00a7r"}']}
{title:'Spitznagel et al. (§72024§r)', author: 'Martin Spitznagel; Janis Keuper', display:{Lore:['[{"text": "arXiv:2403.10904", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUrban Sound Propagation: a Benchmark for 1-Step Generative Modeling of Complex Physical Systems\\u00a7r\\n\\n\\u00a78\\u00a7oMartin Spitznagel\\nJanis Keuper\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.10904\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 19 Mar 2024 11:37:28 GMT)\\u00a7r"}']}
{title:'Zou et al. (§72024§r)', author: 'Liang Zou; Genwei Yan; Ruoyu Wang; Jun Du; Meng Lei; Tian Gao; Xin Fang', display:{Lore:['[{"text": "arXiv:2403.11091", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultitask frame-level learning for few-shot sound event detection\\u00a7r\\n\\n\\u00a78\\u00a7oLiang Zou\\nGenwei Yan\\nRuoyu Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.11091\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 17 Mar 2024 05:00:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 4 figures, conference\\u00a7r"}']}
{title:'Postolache et al. (§72024§r)', author: 'Emilian Postolache; Giorgio Mariani; Luca Cosmo; Emmanouil Benetos; Emanuele Rodolà', display:{Lore:['[{"text": "arXiv:2403.11706", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeneralized Multi-Source Inference for Text Conditioned Music Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oEmilian Postolache\\nGiorgio Mariani\\nLuca Cosmo\\nEmmanouil Benetos\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.11706\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Mar 2024 12:08:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2024\\u00a7r"}']}
{title:'Close et al. (§72024§r)', author: 'George Close; Thomas Hain; Stefan Goetze', display:{Lore:['[{"text": "arXiv:2403.11732", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHallucination in Perceptual Metric-Driven Speech Enhancement Networks\\u00a7r\\n\\n\\u00a78\\u00a7oGeorge Close\\nThomas Hain\\nStefan Goetze\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.11732\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 24 May 2024 07:48:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for EUSIPCO 2024\\u00a7r"}']}
{title:'Mathew et al. (§72024§r)', author: 'Jonat John Mathew; Rakin Ahsan; Sae Furukawa; Jagdish Gautham Krishna Kumar; Huzaifa Pallan; Agamjeet Singh Padda; Sara Adamski; Madhu Reddiboina; Arjun Pankajakshan', display:{Lore:['[{"text": "arXiv:2403.11778", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards the Development of a Real-Time Deepfake Audio Detection System in Communication Platforms\\u00a7r\\n\\n\\u00a78\\u00a7oJonat John Mathew\\nRakin Ahsan\\nSae Furukawa\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.11778\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Mar 2024 13:35:10 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Yongqi Wang; Ruofan Hu; Rongjie Huang; Zhiqing Hong; Ruiqi Li; Wenrui Liu; Fuming You; Tao Jin; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2403.11780", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt\\u00a7r\\n\\n\\u00a78\\u00a7oYongqi Wang\\nRuofan Hu\\nRongjie Huang\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.11780\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Mar 2024 13:39:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by NAACL 2024 (main conference)\\u00a7r"}']}
{title:'Krause et al. (§72024§r)', author: 'Daniel Aleksander Krause; Archontis Politis; Annamaria Mesaros', display:{Lore:['[{"text": "arXiv:2403.11827", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Event Detection and Localization with Distance Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Aleksander Krause\\nArchontis Politis\\nAnnamaria Mesaros\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.11827\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Mar 2024 14:34:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper has been submitted for the 32nd European Signal Processing Conference EUSIPCO 2024 in Lyon\\u00a7r"}']}
{title:'Hallmen et al. (§72024§r)', author: 'Tobias Hallmen; Fabian Deuser; Norbert Oswald; Elisabeth André', display:{Lore:['[{"text": "arXiv:2403.11879", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnimodal Multi-Task Fusion for Emotional Mimicry Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oTobias Hallmen\\nFabian Deuser\\nNorbert Oswald\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.11879\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 22 Mar 2024 10:08:51 GMT)\\u00a7r"}']}
{title:'Shepardson et al. (§72024§r)', author: 'Victor Shepardson; Jack Armitage; Thor Magnusson', display:{Lore:['[{"text": "arXiv:2403.12000", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNotochord: a Flexible Probabilistic Model for Real-Time MIDI Performance\\u00a7r\\n\\n\\u00a78\\u00a7oVictor Shepardson\\nJack Armitage\\nThor Magnusson\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.12000\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5281/zenodo.7088404\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Mar 2024 17:35:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 6 figures. Proceedings of the 3rdConference on AI MusicCreativity (2022, September 17)\\u00a7r"}']}
{title:'Ishikawa et al. (§72024§r)', author: 'Yuto Ishikawa; Kohei Konaka; Tomohiko Nakamura; Norihiro Takamune; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2403.12477", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-time Speech Extraction Using Spatially Regularized Independent Low-rank Matrix Analysis and Rank-constrained Spatial Covariance Matrix Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oYuto Ishikawa\\nKohei Konaka\\nTomohiko Nakamura\\nNorihiro Takamune\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.12477\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Mar 2024 06:27:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted at HSCMA 2024\\u00a7r"}']}
{title:'Paissan et al. (§72024§r)', author: 'Francesco Paissan; Mirco Ravanelli; Cem Subakan', display:{Lore:['[{"text": "arXiv:2403.13086", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lListenable Maps for Audio Classifiers\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesco Paissan\\nMirco Ravanelli\\nCem Subakan\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.13086\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Mar 2024 18:32:48 GMT)\\u00a7r"}']}
{title:'Song (§72024§r)', author: 'Tao Song', display:{Lore:['[{"text": "arXiv:2403.13252", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrequency-aware convolution for sound event detection\\u00a7r\\n\\n\\u00a78\\u00a7oTao Song\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.13252\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Mar 2024 02:32:13 GMT)\\u00a7r"}']}
{title:'Song (§72024§r)', author: 'Tao Song', display:{Lore:['[{"text": "arXiv:2403.13254", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOnset and offset weighted loss function for sound event detection\\u00a7r\\n\\n\\u00a78\\u00a7oTao Song\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.13254\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Mar 2024 02:32:41 GMT)\\u00a7r"}']}
{title:'Watanabe et al. (§72024§r)', author: 'Aya Watanabe; Shinnosuke Takamichi; Yuki Saito; Wataru Nakata; Detai Xin; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2403.13353", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBuilding speech corpus with diverse voice characteristics for its prompt-based representation\\u00a7r\\n\\n\\u00a78\\u00a7oAya Watanabe\\nShinnosuke Takamichi\\nYuki Saito\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.13353\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Mar 2024 07:31:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEE/ACM Transactionson Audio, Speech, and Language Processing. arXiv admin note: text overlap with arXiv:2309.13509\\u00a7r"}']}
{title:'Gong et al. (§72024§r)', author: 'Xun Gong; Yu Wu; Jinyu Li; Shujie Liu; Rui Zhao; Xie Chen; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2403.13423", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdvanced Long-Content Speech Recognition With Factorized Neural Transducer\\u00a7r\\n\\n\\u00a78\\u00a7oXun Gong\\nYu Wu\\nJinyu Li\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.13423\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2024.3350893\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol. 32, pp. 1803-1815, 2024\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Mar 2024 09:09:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by TASLP 2024\\u00a7r"}']}
{title:'Nakata et al. (§72024§r)', author: 'Wataru Nakata; Kazuki Yamauchi; Dong Yang; Hiroaki Hyodo; Yuki Saito', display:{Lore:['[{"text": "arXiv:2403.13720", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUTDUSS: UTokyo-SaruLab System for Interspeech2024 Speech Processing Using Discrete Speech Unit Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oWataru Nakata\\nKazuki Yamauchi\\nDong Yang\\nHiroaki Hyodo\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.13720\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Mar 2024 16:27:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Baird et al. (§72024§r)', author: 'Alice Baird; Rachel Manzelli; Panagiotis Tzirakis; Chris Gagne; Haoqi Li; Sadie Allen; Sander Dieleman; Brian Kulis; Shrikanth S. Narayanan; Alan Cowen', display:{Lore:['[{"text": "arXiv:2403.14048", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio Benchmarks and Novel Data\\u00a7r\\n\\n\\u00a78\\u00a7oAlice Baird\\nRachel Manzelli\\nPanagiotis Tzirakis\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.14048\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Mar 2024 00:13:59 GMT)\\u00a7r"}']}
{title:'Rajapakshe et al. (§72024§r)', author: 'Thejan Rajapakshe; Rajib Rana; Sara Khalifa; Berrak Sisman; Bjorn W. Schuller; Carlos Busso', display:{Lore:['[{"text": "arXiv:2403.14083", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lemoDARTS: Joint Optimisation of CNN     Sequential Neural Network Architectures for Superior Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oThejan Rajapakshe\\nRajib Rana\\nSara Khalifa\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.14083\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Mar 2024 02:26:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEETransactions on Affective Computing on February 19, 2024. arXiv admin note: text overlap with arXiv:2305.14402\\u00a7r"}']}
{title:'Raghav et al. (§72024§r)', author: 'Nikhil Raghav; Md Sahidullah', display:{Lore:['[{"text": "arXiv:2403.14286", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAssessing the Robustness of Spectral Clustering for Deep Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oNikhil Raghav\\nMd Sahidullah\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.14286\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Mar 2024 10:49:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oManuscript Under Review\\u00a7r"}']}
{title:'Saha et al. (§72024§r)', author: 'Subhajit Saha; Md Sahidullah; Swagatam Das', display:{Lore:['[{"text": "arXiv:2403.14290", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Green AI for Audio Deepfake Detection\\u00a7r\\n\\n\\u00a78\\u00a7oSubhajit Saha\\nMd Sahidullah\\nSwagatam Das\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.14290\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Mar 2024 10:54:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis manuscript is under review in a conference\\u00a7r"}']}
{title:'Han et al. (§72024§r)', author: 'HyoJung Han; Mohamed Anwar; Juan Pino; Wei-Ning Hsu; Marine Carpuat; Bowen Shi; Changhan Wang', display:{Lore:['[{"text": "arXiv:2403.14402", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lXLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for Noise-Robust Speech Perception\\u00a7r\\n\\n\\u00a78\\u00a7oHyoJung Han\\nMohamed Anwar\\nJuan Pino\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.14402\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Mar 2024 13:52:17 GMT)\\u00a7r"}']}
{title:'Correia et al. (§72024§r)', author: 'André Correia; Luís A. Alexandre', display:{Lore:['[{"text": "arXiv:2403.15569", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic to Dance as Language Translation using Sequence Models\\u00a7r\\n\\n\\u00a78\\u00a7oAndr\\u00e9 Correia\\nLu\\u00eds A. Alexandre\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.15569\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Mar 2024 18:47:54 GMT)\\u00a7r"}']}
{title:'Wu et al. (§72024§r)', author: 'Wenxuan Wu; Xueyuan Chen; Xixin Wu; Haizhou Li; Helen Meng', display:{Lore:['[{"text": "arXiv:2403.16078", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTarget Speech Extraction with Pre-trained AV-HuBERT and Mask-And-Recover Strategy\\u00a7r\\n\\n\\u00a78\\u00a7oWenxuan Wu\\nXueyuan Chen\\nXixin Wu\\nHaizhou Li\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.16078\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 24 Mar 2024 09:42:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IJCNN 2024\\u00a7r"}']}
{title:'Yin et al. (§72024§r)', author: 'Hanzhi Yin; Gang Cheng; Christian J. Steinmetz; Ruibin Yuan; Richard M. Stern; Roger B. Dannenberg', display:{Lore:['[{"text": "arXiv:2403.16331", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModeling Analog Dynamic Range Compressors using Deep Learning and State-space Models\\u00a7r\\n\\n\\u00a78\\u00a7oHanzhi Yin\\nGang Cheng\\nChristian J. Steinmetz\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.16331\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 24 Mar 2024 23:50:15 GMT)\\u00a7r"}']}
{title:'Kaneko et al. (§72024§r)', author: 'Takuhiro Kaneko; Hirokazu Kameoka; Kou Tanaka', display:{Lore:['[{"text": "arXiv:2403.16464", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTraining Generative Adversarial Network-Based Vocoder with Limited Data Using Augmentation-Conditional Discriminator\\u00a7r\\n\\n\\u00a78\\u00a7oTakuhiro Kaneko\\nHirokazu Kameoka\\nKou Tanaka\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.16464\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Mar 2024 06:46:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024. Project page: https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/\\u00a7r"}']}
{title:'Kim et al. (§72024§r)', author: 'Jeong-Yoon Kim; Seung-Ho Lee', display:{Lore:['[{"text": "arXiv:2403.17327", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAccuracy enhancement method for speech emotion recognition from spectrogram using temporal frequency correlation and positional information learning through knowledge transfer\\u00a7r\\n\\n\\u00a78\\u00a7oJeong-Yoon Kim\\nSeung-Ho Lee\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.17327\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Mar 2024 02:21:36 GMT)\\u00a7r"}']}
{title:'Khatri et al. (§72024§r)', author: 'Dheepak Khatri; Kenneth Granlund', display:{Lore:['[{"text": "arXiv:2403.17376", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTheoretical Analysis of Quality of Conventional Beamforming for Phased Microphone Arrays\\u00a7r\\n\\n\\u00a78\\u00a7oDheepak Khatri\\nKenneth Granlund\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.17376\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 24 Mar 2024 10:18:04 GMT)\\u00a7r"}']}
{title:'Ai et al. (§72024§r)', author: 'Yang Ai; Zhen-Hua Ling', display:{Lore:['[{"text": "arXiv:2403.17378", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Latency Neural Speech Phase Prediction based on Parallel Estimation Architecture and Anti-Wrapping Losses for Speech Generation Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oYang Ai\\nZhen-Hua Ling\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.17378\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Mar 2024 04:53:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE Transactions on Audio, Speech and Language Processing. arXiv admin note: substantial text overlap with arXiv:2211.15974\\u00a7r"}']}
{title:'Jhanji (§72024§r)', author: 'Etash Jhanji', display:{Lore:['[{"text": "arXiv:2403.17379", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring and Applying Audio-Based Sentiment Analysis in Music\\u00a7r\\n\\n\\u00a78\\u00a7oEtash Jhanji\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.17379\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Feb 2024 22:34:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 7 figures, 2 tables. For source code, see https://github.com/etashj/Exploring-and-Applying-Audio-Based-Sentiment-Analysis\\u00a7r"}']}
{title:'Tailleur et al. (§72024§r)', author: 'Modan Tailleur; Junwon Lee; Mathieu Lagrange; Keunwoo Choi; Laurie M. Heller; Keisuke Imoto; Yuki Okamoto', display:{Lore:['[{"text": "arXiv:2403.17508", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCorrelation of Fr\\u00e9chet Audio Distance With Human Perception of Environmental Audio Is Embedding Dependant\\u00a7r\\n\\n\\u00a78\\u00a7oModan Tailleur\\nJunwon Lee\\nMathieu Lagrange\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.17508\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Mar 2024 09:09:59 GMT)\\u00a7r"}']}
{title:'Ouajdi et al. (§72024§r)', author: 'Hafsa Ouajdi; Oussama Hadder; Modan Tailleur; Mathieu Lagrange; Laurie M. Heller', display:{Lore:['[{"text": "arXiv:2403.17529", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetection of Deepfake Environmental Audio\\u00a7r\\n\\n\\u00a78\\u00a7oHafsa Ouajdi\\nOussama Hadder\\nModan Tailleur\\nMathieu Lagrange\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.17529\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Mar 2024 09:35:16 GMT)\\u00a7r"}']}
{title:'Saumard et al. (§72024§r)', author: 'Matthieu Saumard; Abir El Haj; Thibault Napoleon', display:{Lore:['[{"text": "arXiv:2403.17562", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.AP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep functional multiple index models with an application to SER\\u00a7r\\n\\n\\u00a78\\u00a7oMatthieu Saumard\\nAbir El Haj\\nThibault Napoleon\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.17562\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Mar 2024 10:10:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure\\u00a7r"}']}
{title:'Wijngaard et al. (§72024§r)', author: 'Gijs Wijngaard; Elia Formisano; Bruno L. Giordano; Michel Dumontier', display:{Lore:['[{"text": "arXiv:2403.18572", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lACES: Evaluating Automated Audio Captioning Models on the Semantics of Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oGijs Wijngaard\\nElia Formisano\\nBruno L. Giordano\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.18572\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Mar 2024 13:54:17 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72024§r)', author: 'Ziyang Chen; Israel D. Gebru; Christian Richardt; Anurag Kumar; William Laney; Andrew Owens; Alexander Richard', display:{Lore:['[{"text": "arXiv:2403.18821", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark\\u00a7r\\n\\n\\u00a78\\u00a7oZiyang Chen\\nIsrael D. Gebru\\nChristian Richardt\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.18821\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Mar 2024 17:59:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to CVPR 2024. Project site: https://facebookresearch.github.io/real-acoustic-fields/\\u00a7r"}']}
{title:'Shen et al. (§72024§r)', author: 'Siyuan Shen; Yu Gao; Feng Liu; Hanyang Wang; Aimin Zhou', display:{Lore:['[{"text": "arXiv:2403.19224", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotion Neural Transducer for Fine-Grained Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSiyuan Shen\\nYu Gao\\nFeng Liu\\nHanyang Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.19224\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP48485.2024.10446974\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Mar 2024 08:38:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by 49th IEEE InternationalConference on Acoustics, Speech, and Signal Processing (ICASSP 2024)\\u00a7r"}']}
{title:'Dia et al. (§72024§r)', author: 'Mamadou Dia; Ghazaleh Khodabandelou; Alice Othmani', display:{Lore:['[{"text": "arXiv:2403.19441", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Novel Stochastic Transformer-based Approach for Post-Traumatic Stress Disorder Detection using Audio Recording of Clinical Interviews\\u00a7r\\n\\n\\u00a78\\u00a7oMamadou Dia\\nGhazaleh Khodabandelou\\nAlice Othmani\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.19441\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/CBMS58004.2023.00303\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2023 IEEE 36th International Symposium on Computer-Based Medical\\n  Systems (2023) 700-705\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Mar 2024 14:11:40 GMT)\\u00a7r"}']}
{title:'Bousquet et al. (§72024§r)', author: 'Pierre-Michel Bousquet; Mickael Rouvier', display:{Lore:['[{"text": "arXiv:2403.19634", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAsymmetric and trial-dependent modeling: the contribution of LIA to SdSV Challenge Task 2\\u00a7r\\n\\n\\u00a78\\u00a7oPierre-Michel Bousquet\\nMickael Rouvier\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.19634\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Mar 2024 17:49:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oLIA system description for the Short Duration Speaker Verification (SdSv) challenge 2020 Task 2\\u00a7r"}']}
{title:'Peng et al. (§72024§r)', author: 'Tristan Peng; Hongchan Choi; Jonathan Berger', display:{Lore:['[{"text": "arXiv:2403.19763", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCreating Aesthetic Sonifications on the Web with SIREN\\u00a7r\\n\\n\\u00a78\\u00a7oTristan Peng\\nHongchan Choi\\nJonathan Berger\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.19763\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Mar 2024 18:24:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 1 figure, 5 listings, submitted to theWeb Audio Conference 2024\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Dongzhe Zhang; Jianfeng Chen; Jisheng Bai; Mou Wang', display:{Lore:['[{"text": "arXiv:2403.20130", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound event localization and classification using WASN in Outdoor Environment\\u00a7r\\n\\n\\u00a78\\u00a7oDongzhe Zhang\\nJianfeng Chen\\nJisheng Bai\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.20130\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Mar 2024 11:44:14 GMT)\\u00a7r"}']}
{title:'Ganchev (§72024§r)', author: 'Radan Ganchev', display:{Lore:['[{"text": "arXiv:2403.20202", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Signal Processing for Machine Learning. The Case of Speaker Isolation\\u00a7r\\n\\n\\u00a78\\u00a7oRadan Ganchev\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.20202\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Mar 2024 14:31:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMSc. thesis. for associated source code, see https://github.com/rganchev/speech-signal-processing-for-ml\\u00a7r"}']}
{title:'Hassanuzzaman et al. (§72024§r)', author: 'Md Hassanuzzaman; Nurul Akhtar Hasan; Mohammad Abdullah Al Mamun; Khawza I Ahmed; Ahsan H Khandoker; Raqibul Mostafa', display:{Lore:['[{"text": "arXiv:2404.00470", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lClassification of Short Segment Pediatric Heart Sounds Based on a Transformer-Based Convolutional Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oMd Hassanuzzaman\\nNurul Akhtar Hasan\\nMohammad Abdullah Al Mamun\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.00470\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 30 Mar 2024 20:32:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages,11 Figures\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Xiang Li; Fan Bu; Ambuj Mehrish; Yingting Li; Jiale Han; Bo Cheng; Soujanya Poria', display:{Lore:['[{"text": "arXiv:2404.00569", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCM-TTS: Enhancing Real Time Text-to-Speech Synthesis Efficiency through Weighted Samplers and Consistency Models\\u00a7r\\n\\n\\u00a78\\u00a7oXiang Li\\nFan Bu\\nAmbuj Mehrish\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.00569\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 31 Mar 2024 05:38:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Findings of NAACL 2024.Code is available at https://github.com/XiangLi2022/CM-TTS\\u00a7r"}']}
{title:'Grachten (§72024§r)', author: 'Maarten Grachten', display:{Lore:['[{"text": "arXiv:2404.00775", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMeasuring audio prompt adherence with distribution-based embedding distances\\u00a7r\\n\\n\\u00a78\\u00a7oMaarten Grachten\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.00775\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 4 Apr 2024 11:36:50 GMT)\\u00a7r"}']}
{title:'Choi et al. (§72024§r)', author: 'Kahyun Choi; Minje Kim', display:{Lore:['[{"text": "arXiv:2404.00789", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparative Analysis of Poetry Reading Audio: Singing, Narrating, or Somewhere In Between?\\u00a7r\\n\\n\\u00a78\\u00a7oKahyun Choi\\nMinje Kim\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.00789\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP48485.2024.10447582\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE International Conference on Acoustics, Speech and Signal\\n  Processing (ICASSP), 2024, pp. 1296-1300\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 31 Mar 2024 20:28:16 GMT)\\u00a7r"}']}
{title:'Jang et al. (§72024§r)', author: 'Inseon Jang; Haici Yang; Wootaek Lim; Seungkwon Beack; Minje Kim', display:{Lore:['[{"text": "arXiv:2404.00791", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPersonalized Neural Speech Codec\\u00a7r\\n\\n\\u00a78\\u00a7oInseon Jang\\nHaici Yang\\nWootaek Lim\\nSeungkwon Beack\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.00791\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP48485.2024.10446067\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE International Conference on Acoustics, Speech and Signal\\n  Processing (ICASSP), 2024, pp. 991-995\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 31 Mar 2024 20:32:40 GMT)\\u00a7r"}']}
{title:'Hwang et al. (§72024§r)', author: 'Injune Hwang; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2404.00856", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRemoving Speaker Information from Speech Representation using Variable-Length Soft Pooling\\u00a7r\\n\\n\\u00a78\\u00a7oInjune Hwang\\nKyogu Lee\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.00856\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Apr 2024 01:49:09 GMT)\\u00a7r"}']}
{title:'Kamuni et al. (§72024§r)', author: 'Navin Kamuni; Mayank Jindal; Arpita Soni; Sukender Reddy Mallreddy; Sharath Chandra Macha', display:{Lore:['[{"text": "arXiv:2404.01058", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Novel Audio Representation for Music Genre Identification in MIR\\u00a7r\\n\\n\\u00a78\\u00a7oNavin Kamuni\\nMayank Jindal\\nArpita Soni\\nSukender Reddy Mallreddy\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.01058\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Apr 2024 11:40:09 GMT)\\u00a7r"}']}
{title:'Anibal et al. (§72024§r)', author: 'James Anibal; Hannah Huth; Ming Li; Lindsey Hazen; Yen Minh Lam; Nguyen Thi Thu Hang; Michael Kleinman; Shelley Ost; Christopher Jackson; Laura Sprabery; Cheran Elangovan; Balaji Krishnaiah; Lee Akst; Ioan Lina; Iqbal Elyazar; Lenny Ekwati; Stefan Jansen; Richard Nduwayezu; Charisse Garcia; Jeffrey Plum; Jacqueline Brenner; Miranda Song; Emily Ricotta; David Clifton; C. Louise Thwaites; Yael Bensoussan; Bradford Wood', display:{Lore:['[{"text": "arXiv:2404.01620", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CY\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice EHR: Introducing Multimodal Audio Data for Health\\u00a7r\\n\\n\\u00a78\\u00a7oJames Anibal\\nHannah Huth\\nMing Li\\n+ 23 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.01620\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Apr 2024 04:07:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages, 2 figures, 7 tables\\u00a7r"}']}
{title:'Mahmud et al. (§72024§r)', author: 'Tanvir Mahmud; Saeed Amizadeh; Kazuhito Koishida; Diana Marculescu', display:{Lore:['[{"text": "arXiv:2404.01740", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeakly-supervised Audio Separation via Bi-modal Semantic Similarity\\u00a7r\\n\\n\\u00a78\\u00a7oTanvir Mahmud\\nSaeed Amizadeh\\nKazuhito Koishida\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.01740\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Apr 2024 08:59:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTech report. Accepted in ICLR-2024\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Kai Li; Guo Chen', display:{Lore:['[{"text": "arXiv:2404.02063", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSPMamba: State-space model is all you need in speech separation\\u00a7r\\n\\n\\u00a78\\u00a7oKai Li\\nGuo Chen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.02063\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Apr 2024 16:04:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTechnical Report. Work in progress. Code is available at https://github.com/JusperLee/SPMamba\\u00a7r"}']}
{title:'Koo et al. (§72024§r)', author: 'Junghyun Koo; Gordon Wichern; Francois G. Germain; Sameer Khurana; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2404.02252", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSMITIN: Self-Monitored Inference-Time INtervention for Generative Music Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oJunghyun Koo\\nGordon Wichern\\nFrancois G. Germain\\nSameer Khurana\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.02252\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Apr 2024 19:18:42 GMT)\\u00a7r"}']}
{title:'Pan et al. (§72024§r)', author: 'Yu Pan; Lei Ma; Jianjun Zhao', display:{Lore:['[{"text": "arXiv:2404.02702", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPromptCodec: High-Fidelity Neural Speech Codec using Disentangled Representation Learning based Adaptive Feature-aware Prompt Encoders\\u00a7r\\n\\n\\u00a78\\u00a7oYu Pan\\nLei Ma\\nJianjun Zhao\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.02702\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 13 Apr 2024 11:48:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7\\u00a7r"}']}
{title:'Hasan et al. (§72024§r)', author: 'S M Rakib Hasan; Aakar Dhakal; Ms. Ayesha Siddiqua; Mohammad Mominur Rahman; Md Maidul Islam; Mohammed Arfat Raihan Chowdhury; S M Masfequier Rahman Swapno; SM Nuruzzaman Nobel', display:{Lore:['[{"text": "arXiv:2404.03606", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalyzing Musical Characteristics of National Anthems in Relation to Global Indices\\u00a7r\\n\\n\\u00a78\\u00a7oS M Rakib Hasan\\nAakar Dhakal\\nMs. Ayesha Siddiqua\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.03606\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Apr 2024 17:25:31 GMT)\\u00a7r"}']}
{title:'McCormack et al. (§72024§r)', author: 'Jon McCormack; Elliott Wilson', display:{Lore:['[{"text": "arXiv:2404.03894", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MA\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHolon: a cybernetic interface for bio-semiotics\\u00a7r\\n\\n\\u00a78\\u00a7oJon McCormack\\nElliott Wilson\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.03894\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Apr 2024 05:03:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper accepted at ISEA 24, The 29th International Symposium on Electronic Art, Brisbane, Australia, 21-29 June 2024\\u00a7r"}']}
{title:'Wu et al. (§72024§r)', author: 'Yushu Wu; Xiao Quan; Mohammad Rasool Izadi; Chuan-Che Huang', display:{Lore:['[{"text": "arXiv:2404.04386", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l\\"It is okay to be uncommon\\": Quantizing Sound Event Detection Networks on Hardware Accelerators with Uncommon Sub-Byte Support\\u00a7r\\n\\n\\u00a78\\u00a7oYushu Wu\\nXiao Quan\\nMohammad Rasool Izadi\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.04386\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Apr 2024 20:08:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, Accepted to ICASSP 2024\\u00a7r"}']}
{title:'Cardoso et al. (§72024§r)', author: 'Igor Cardoso; Rubens O. Moraes; Lucas N. Ferreira', display:{Lore:['[{"text": "arXiv:2404.04420", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe NES Video-Music Database: A Dataset of Symbolic Video Game Music Paired with Gameplay Videos\\u00a7r\\n\\n\\u00a78\\u00a7oIgor Cardoso\\nRubens O. Moraes\\nLucas N. Ferreira\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.04420\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3649921.3650011\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Apr 2024 21:41:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at the 19th International Conference on the Foundations of Digital Games\\u00a7r"}']}
{title:'Schneider et al. (§72024§r)', author: 'Maxwell Schneider; Cody McCarthy; Michael G. Maxwell; Joshua Pfeffer; Robert Schneider; Andrew V. Sills', display:{Lore:['[{"text": "arXiv:2404.04739", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a72math.HO\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMathematics of the MML functional quantizer modules for VCV Rack software synthesizer\\u00a7r\\n\\n\\u00a78\\u00a7oMaxwell Schneider\\nCody McCarthy\\nMichael G. Maxwell\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.04739\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 28 Apr 2024 04:56:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, published in Infinite Loop: an online journal for undergraduate research and applied computing projects (2024)\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Yuang Li; Min Zhang; Mengxin Ren; Miaomiao Ma; Daimeng Wei; Hao Yang', display:{Lore:['[{"text": "arXiv:2404.04904", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-Domain Audio Deepfake Detection: Dataset and Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oYuang Li\\nMin Zhang\\nMengxin Ren\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.04904\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 7 Apr 2024 10:10:15 GMT)\\u00a7r"}']}
{title:'Mayya et al. (§72024§r)', author: 'Roopa Mayya; Vivekanand Venkataraman; Anwesh P R; Narayana Darapaneni', display:{Lore:['[{"text": "arXiv:2404.05765", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Novel Bi-LSTM And Transformer Architecture For Generating Tabla Music\\u00a7r\\n\\n\\u00a78\\u00a7oRoopa Mayya\\nVivekanand Venkataraman\\nAnwesh P R\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.05765\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 6 Apr 2024 16:15:02 GMT)\\u00a7r"}']}
{title:'Cai et al. (§72024§r)', author: 'Le Cai; Sam Ferguson; Gengfa Fang; Hani Alshamrani', display:{Lore:['[{"text": "arXiv:2404.06103", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Diverse Sounds: Identifying Outliers in a Music Corpus\\u00a7r\\n\\n\\u00a78\\u00a7oLe Cai\\nSam Ferguson\\nGengfa Fang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.06103\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5281/zenodo.10114235\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nThe 16th International Symposium on Computer Music\\n  Multidisciplinary Research,2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Apr 2024 08:10:04 GMT)\\u00a7r"}']}
{title:'Qu et al. (§72024§r)', author: 'Xingwei Qu; Yuelin Bai; Yinghao Ma; Ziya Zhou; Ka Man Lo; Jiaheng Liu; Ruibin Yuan; Lejun Min; Xueling Liu; Tianyu Zhang; Xinrun Du; Shuyue Guo; Yiming Liang; Yizhi Li; Shangda Wu; Junting Zhou; Tianyu Zheng; Ziyang Ma; Fengze Han; Wei Xue; Gus Xia; Emmanouil Benetos; Xiang Yue; Chenghua Lin; Xu Tan; Stephen W. Huang; Wenhu Chen; Jie Fu; Ge Zhang', display:{Lore:['[{"text": "arXiv:2404.06393", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMuPT: A Generative Symbolic Music Pretrained Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oXingwei Qu\\nYuelin Bai\\nYinghao Ma\\n+ 25 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.06393\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 10 Apr 2024 15:09:52 GMT)\\u00a7r"}']}
{title:'Anastassiou et al. (§72024§r)', author: 'Philip Anastassiou; Zhenyu Tang; Kainan Peng; Dongya Jia; Jiaxin Li; Ming Tu; Yuping Wang; Yuxuan Wang; Mingbo Ma', display:{Lore:['[{"text": "arXiv:2404.06674", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoiceShop: A Unified Speech-to-Speech Framework for Identity-Preserving Zero-Shot Voice Editing\\u00a7r\\n\\n\\u00a78\\u00a7oPhilip Anastassiou\\nZhenyu Tang\\nKainan Peng\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.06674\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 11 Apr 2024 17:52:15 GMT)\\u00a7r"}']}
{title:'Hashizume et al. (§72024§r)', author: 'Yuka Hashizume; Li Li; Atsushi Miyashita; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2404.06682", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Multidimensional Disentangled Representations of Instrumental Sounds for Musical Similarity Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oYuka Hashizume\\nLi Li\\nAtsushi Miyashita\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.06682\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Apr 2024 02:02:51 GMT)\\u00a7r"}']}
{title:'Lo et al. (§72024§r)', author: 'Tien-Hong Lo; Fu-An Chao; Tzu-I Wu; Yao-Ting Sung; Berlin Chen', display:{Lore:['[{"text": "arXiv:2404.07575", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Effective Automated Speaking Assessment Approach to Mitigating Data Scarcity and Imbalanced Distribution\\u00a7r\\n\\n\\u00a78\\u00a7oTien-Hong Lo\\nFu-An Chao\\nTzu-I Wu\\nYao-Ting Sung\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.07575\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 12 Apr 2024 01:22:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to NAACL 2024 Findings\\u00a7r"}']}
{title:'Serre et al. (§72024§r)', author: 'Thomas Serre; Mathieu Fontaine; Éric Benhaim; Geoffroy Dutour; Slim Essid', display:{Lore:['[{"text": "arXiv:2404.08022", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA lightweight dual-stage framework for personalized speech enhancement based on DeepFilterNet2\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Serre\\nMathieu Fontaine\\n\\u00c9ric Benhaim\\nGeoffroy Dutour\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.08022\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP, Apr 2024, Seoul (Korea), South Korea\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Apr 2024 08:09:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at HSCMA24, Satellite workshop of ICASSP24\\u00a7r"}']}
{title:'Sheng et al. (§72024§r)', author: 'Zhengyan Sheng; Yang Ai; Li-Juan Liu; Jia Pan; Zhen-Hua Ling', display:{Lore:['[{"text": "arXiv:2404.08857", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Attribute Editing with Text Prompt\\u00a7r\\n\\n\\u00a78\\u00a7oZhengyan Sheng\\nYang Ai\\nLi-Juan Liu\\nJia Pan\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.08857\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 13 Apr 2024 00:07:40 GMT)\\u00a7r"}']}
{title:'Meseguer-Brocal et al. (§72024§r)', author: 'Gabriel Meseguer-Brocal; Dorian Desblancs; Romain Hennequin', display:{Lore:['[{"text": "arXiv:2404.09177", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Experimental Comparison Of Multi-view Self-supervised Methods For Music Tagging\\u00a7r\\n\\n\\u00a78\\u00a7oGabriel Meseguer-Brocal\\nDorian Desblancs\\nRomain Hennequin\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.09177\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 14 Apr 2024 07:56:08 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Quanxiu Wang; Hui Huang; Mingjie Wang; Yong Dai; Jinzuomu Zhong; Benlai Tang', display:{Lore:['[{"text": "arXiv:2404.09192", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrior-agnostic Multi-scale Contrastive Text-Audio Pre-training for Parallelized TTS Frontend Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oQuanxiu Wang\\nHui Huang\\nMingjie Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.09192\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 14 Apr 2024 08:56:19 GMT)\\u00a7r"}']}
{title:'Yan et al. (§72024§r)', author: 'Yujia Yan; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2404.09466", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScoring Intervals using Non-Hierarchical Transformer For Automatic Piano Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oYujia Yan\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.09466\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 24 May 2024 02:20:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oFixed Typos\\u00a7r"}']}
{title:'Majumder et al. (§72024§r)', author: 'Navonil Majumder; Chia-Yu Hung; Deepanway Ghosal; Wei-Ning Hsu; Rada Mihalcea; Soujanya Poria', display:{Lore:['[{"text": "arXiv:2404.09956", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTango 2: Aligning Diffusion-based Text-to-Audio Generations through Direct Preference Optimization\\u00a7r\\n\\n\\u00a78\\u00a7oNavonil Majumder\\nChia-Yu Hung\\nDeepanway Ghosal\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.09956\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 Apr 2024 12:12:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ohttps://github.com/declare-lab/tango\\u00a7r"}']}
{title:'Evans et al. (§72024§r)', author: 'Zach Evans; Julian D. Parker; CJ Carr; Zack Zukowski; Josiah Taylor; Jordi Pons', display:{Lore:['[{"text": "arXiv:2404.10301", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLong-form music generation with latent diffusion\\u00a7r\\n\\n\\u00a78\\u00a7oZach Evans\\nJulian D. Parker\\nCJ Carr\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.10301\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Apr 2024 06:09:33 GMT)\\u00a7r"}']}
{title:'Abu et al. (§72024§r)', author: 'Avi Abu; Nikola Miskovic; Oleg Chebotar; Neven Cukrov; Roee Diamant', display:{Lore:['[{"text": "arXiv:2404.10316", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultiple Mobile Target Detection and Tracking in Active Sonar Array Using a Track-Before-Detect Approach\\u00a7r\\n\\n\\u00a78\\u00a7oAvi Abu\\nNikola Miskovic\\nOleg Chebotar\\nNeven Cukrov\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.10316\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Apr 2024 06:48:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 10 figures\\u00a7r"}']}
{title:'Fayet (§72024§r)', author: 'Mateo Fayet', display:{Lore:['[{"text": "arXiv:2404.10578", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVivo : une approche multimodale de la synthese concatenative par corpus dans le cadre d\'une oeuvre audiovisuelle immersive\\u00a7r\\n\\n\\u00a78\\u00a7oMateo Fayet\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.10578\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Apr 2024 14:02:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oin French language\\u00a7r"}']}
{title:'Bhuyan et al. (§72024§r)', author: 'Amit Kumar Bhuyan; Hrishikesh Dutta; Subir Biswas', display:{Lore:['[{"text": "arXiv:2404.10842", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Speaker Diarization in Distributed IoT Networks Using Federated Learning\\u00a7r\\n\\n\\u00a78\\u00a7oAmit Kumar Bhuyan\\nHrishikesh Dutta\\nSubir Biswas\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.10842\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Apr 2024 18:40:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 7 figures, 1 table\\u00a7r"}']}
{title:'Shao et al. (§72024§r)', author: 'Keren Shao; Ke Chen; Shlomo Dubnov', display:{Lore:['[{"text": "arXiv:2404.11116", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Enhancement with Deep Filters: A Technical Report for The ICASSP 2024 Cadenza Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oKeren Shao\\nKe Chen\\nShlomo Dubnov\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.11116\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Apr 2024 07:01:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2 pages, 2 figures, 1 tables, Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2024\\u00a7r"}']}
{title:'Bai et al. (§72024§r)', author: 'Ye Bai; Chenxing Li; Hao Li; Yuanyuan Zhao; Xiaorui Wang', display:{Lore:['[{"text": "arXiv:2404.11275", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJointly Recognizing Speech and Singing Voices Based on Multi-Task Audio Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oYe Bai\\nChenxing Li\\nHao Li\\nYuanyuan Zhao\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.11275\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Apr 2024 11:31:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICME 2024\\u00a7r"}']}
{title:'Atassi (§72024§r)', author: 'Lilac Atassi', display:{Lore:['[{"text": "arXiv:2404.11976", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLarge Language Models: From Notes to Musical Form\\u00a7r\\n\\n\\u00a78\\u00a7oLilac Atassi\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.11976\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Apr 2024 08:07:42 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Jinwu Wang; Wei Mao; Miaomiao Liu', display:{Lore:['[{"text": "arXiv:2404.12062", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMIDGET: Music Conditioned 3D Dance Generation\\u00a7r\\n\\n\\u00a78\\u00a7oJinwu Wang\\nWei Mao\\nMiaomiao Liu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.12062\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-981-99-8388-9_23\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIn Australasian Joint Conference on Artificial Intelligence (pp.\\n  277-288). Singapore: Springer Nature Singapore 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Apr 2024 10:20:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 6 figures Published in AI 2023: Advances in Artificial Intelligence\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Rong Wang; Kun Sun', display:{Lore:['[{"text": "arXiv:2404.12077", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTIMIT Speaker Profiling: A Comparison of Multi-task learning and Single-task learning Approaches\\u00a7r\\n\\n\\u00a78\\u00a7oRong Wang\\nKun Sun\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.12077\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Apr 2024 10:59:54 GMT)\\u00a7r"}']}
{title:'Amiriparian et al. (§72024§r)', author: 'Shahin Amiriparian; Maurice Gerczuk; Justina Lutz; Wolfgang Strube; Irina Papazova; Alkomiet Hasan; Alexander Kathan; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2404.12132", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Suicide Risk Assessment: A Speech-Based Automated Approach in Emergency Medicine\\u00a7r\\n\\n\\u00a78\\u00a7oShahin Amiriparian\\nMaurice Gerczuk\\nJustina Lutz\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.12132\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Apr 2024 12:33:57 GMT)\\u00a7r"}']}
{title:'Mu et al. (§72024§r)', author: 'Zhaoxi Mu; Xinyu Yang', display:{Lore:['[{"text": "arXiv:2404.12725", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSeparate in the Speech Chain: Cross-Modal Conditional Audio-Visual Target Speech Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oZhaoxi Mu\\nXinyu Yang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.12725\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 5 May 2024 08:00:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IJCAI 2024\\u00a7r"}']}
{title:'Chen et al. (§72024§r)', author: 'Chengxin Chen; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2404.12979", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTRNet: Two-level Refinement Network leveraging Speech Enhancement for Noise Robust Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oChengxin Chen\\nPengyuan Zhang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.12979\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Apr 2024 16:09:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 3 figures\\u00a7r"}']}
{title:'Yousif et al. (§72024§r)', author: 'Mohammed Yousif; Jonat John Mathew; Huzaifa Pallan; Agamjeet Singh Padda; Syed Daniyal Shah; Sara Adamski; Madhu Reddiboina; Arjun Pankajakshan', display:{Lore:['[{"text": "arXiv:2404.13008", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Generalization in Audio Deepfake Detection: A Neural Collapse based Sampling and Training Approach\\u00a7r\\n\\n\\u00a78\\u00a7oMohammed Yousif\\nJonat John Mathew\\nHuzaifa Pallan\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.13008\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Apr 2024 17:13:21 GMT)\\u00a7r"}']}
{title:'Han et al. (§72024§r)', author: 'Changheon Han; Suhyun Lee; Minsam Ko', display:{Lore:['[{"text": "arXiv:2404.13286", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTrack Role Prediction of Single-Instrumental Sequences\\u00a7r\\n\\n\\u00a78\\u00a7oChangheon Han\\nSuhyun Lee\\nMinsam Ko\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.13286\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 20 Apr 2024 06:22:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oISMIR LBD 2023\\u00a7r"}']}
{title:'Fei et al. (§72024§r)', author: 'Zhengcong Fei; Mingyuan Fan; Junshi Huang', display:{Lore:['[{"text": "arXiv:2404.13358", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Consistency Models\\u00a7r\\n\\n\\u00a78\\u00a7oZhengcong Fei\\nMingyuan Fan\\nJunshi Huang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.13358\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 20 Apr 2024 11:52:30 GMT)\\u00a7r"}']}
{title:'Hossein et al. (§72024§r)', author: 'Zeinali Hossein; Lee Kong Aik; Alam Jahangir; Burget Lukas', display:{Lore:['[{"text": "arXiv:2404.13428", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lText-dependent Speaker Verification (TdSV) Challenge 2024: Challenge Evaluation Plan\\u00a7r\\n\\n\\u00a78\\u00a7oZeinali Hossein\\nLee Kong Aik\\nAlam Jahangir\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.13428\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 20 Apr 2024 17:26:59 GMT)\\u00a7r"}']}
{title:'Jiao et al. (§72024§r)', author: 'Xinxin Jiao; Liejun Wang; Yinfeng Yu', display:{Lore:['[{"text": "arXiv:2404.13509", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMFHCA: Enhancing Speech Emotion Recognition Via Multi-Spatial Fusion and Hierarchical Cooperative Attention\\u00a7r\\n\\n\\u00a78\\u00a7oXinxin Jiao\\nLiejun Wang\\nYinfeng Yu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.13509\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 21 Apr 2024 02:44:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMain paper (5 pages). Accepted for publication by ICME 2024\\u00a7r"}']}
{title:'Lau et al. (§72024§r)', author: 'Kin Wai Lau; Yasar Abbas Ur Rehman; Lai-Man Po', display:{Lore:['[{"text": "arXiv:2404.13551", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudioRepInceptionNeXt: A lightweight single-stream architecture for efficient audio recognition\\u00a7r\\n\\n\\u00a78\\u00a7oKin Wai Lau\\nYasar Abbas Ur Rehman\\nLai-Man Po\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.13551\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 21 Apr 2024 06:33:04 GMT)\\u00a7r"}']}
{title:'Guo (§72024§r)', author: 'Jiabin Guo', display:{Lore:['[{"text": "arXiv:2404.13568", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSparse Direction of Arrival Estimation Method Based on Vector Signal Reconstruction with a Single Vector Sensor\\u00a7r\\n\\n\\u00a78\\u00a7oJiabin Guo\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.13568\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 21 Apr 2024 08:10:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o20 pages\\u00a7r"}']}
{title:'Doh et al. (§72024§r)', author: 'SeungHeon Doh; Jongpil Lee; Dasaem Jeong; Juhan Nam', display:{Lore:['[{"text": "arXiv:2404.13569", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusical Word Embedding for Music Tagging and Retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oSeungHeon Doh\\nJongpil Lee\\nDasaem Jeong\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.13569\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 23 Apr 2024 02:31:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEE/ACM Transactionson Audio, Speech, and Language Processing (TASLP)\\u00a7r"}']}
{title:'Zeng et al. (§72024§r)', author: 'Donghuo Zeng; Yanan Wang; Kazushi Ikeda; Yi Yu', display:{Lore:['[{"text": "arXiv:2404.13789", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnchor-aware Deep Metric Learning for Audio-visual Retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oDonghuo Zeng\\nYanan Wang\\nKazushi Ikeda\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.13789\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 21 Apr 2024 22:44:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 5 figures. Accepted by ACM ICMR 2024\\u00a7r"}']}
{title:'Kang et al. (§72024§r)', author: 'Zuheng Kang; Yayun He; Botao Zhao; Xiaoyang Qu; Junqing Peng; Jing Xiao; Jianzong Wang', display:{Lore:['[{"text": "arXiv:2404.13892", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRetrieval-Augmented Audio Deepfake Detection\\u00a7r\\n\\n\\u00a78\\u00a7oZuheng Kang\\nYayun He\\nBotao Zhao\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.13892\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3652583.3658086\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 23 Apr 2024 04:10:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the 2024 International Conference on Multimedia Retrieval (ICMR 2024)\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Menglu Li; Yasaman Ahmadiadli; Xiao-Ping Zhang', display:{Lore:['[{"text": "arXiv:2404.13914", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Anti-Spoofing Detection: A Survey\\u00a7r\\n\\n\\u00a78\\u00a7oMenglu Li\\nYasaman Ahmadiadli\\nXiao-Ping Zhang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.13914\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Apr 2024 06:52:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ACM Computing Surveys\\u00a7r"}']}
{title:'Guo et al. (§72024§r)', author: 'Jinyue Guo; Anna-Maria Christodoulou; Balint Laczko; Kyrre Glette', display:{Lore:['[{"text": "arXiv:2404.14063", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLVNS-RAVE: Diversified audio generation with RAVE and Latent Vector Novelty Search\\u00a7r\\n\\n\\u00a78\\u00a7oJinyue Guo\\nAnna-Maria Christodoulou\\nBalint Laczko\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.14063\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3638530.3654432\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Apr 2024 10:20:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to GECCO 24 Companion\\u00a7r"}']}
{title:'Huang et al. (§72024§r)', author: 'Hong Huang; Yuyi Wang; Luyao Li; Jun Lin', display:{Lore:['[{"text": "arXiv:2404.14771", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Style Transfer With Diffusion Model\\u00a7r\\n\\n\\u00a78\\u00a7oHong Huang\\nYuyi Wang\\nLuyao Li\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.14771\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInternational Computer Music Conference (ICMC 2023) pp. 40-47,\\n  October 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Apr 2024 06:22:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 6 figures, ICMC 2023\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Sen Liu; Yiwei Guo; Xie Chen; Kai Yu', display:{Lore:['[{"text": "arXiv:2404.14946", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStoryTTS: A Highly Expressive Text-to-Speech Dataset with Rich Textual Expressiveness Annotations\\u00a7r\\n\\n\\u00a78\\u00a7oSen Liu\\nYiwei Guo\\nXie Chen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.14946\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE International Conference on Acoustics, Speech and Signal\\n  Processing (ICASSP), 2024, pp. 11521-11525\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Apr 2024 11:41:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Layton et al. (§72024§r)', author: 'Seth Layton; Thiago De Andrade; Daniel Olszewski; Kevin Warren; Kevin Butler; Patrick Traynor', display:{Lore:['[{"text": "arXiv:2404.15143", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvery Breath You Don\'t Take: Deepfake Speech Detection Using Breath\\u00a7r\\n\\n\\u00a78\\u00a7oSeth Layton\\nThiago De Andrade\\nDaniel Olszewski\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.15143\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 26 Apr 2024 21:14:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ACM journal \\u2013 Digital Threats: Research and Practice\\u00a7r"}']}
{title:'Guo (§72024§r)', author: 'Jiabin Guo', display:{Lore:['[{"text": "arXiv:2404.15160", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVector Signal Reconstruction Sparse and Parametric Approach of direction of arrival Using Single Vector Hydrophone\\u00a7r\\n\\n\\u00a78\\u00a7oJiabin Guo\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.15160\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 21 Apr 2024 08:23:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o22 pages. arXiv admin note:substantial text overlap with arXiv:2404.13568\\u00a7r"}']}
{title:'Lee (§72024§r)', author: 'ChungHa Lee', display:{Lore:['[{"text": "arXiv:2404.15181", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTailors: New Music Timbre Visualizer to Entertain Music Through Imagery\\u00a7r\\n\\n\\u00a78\\u00a7oChungHa Lee\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.15181\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 13 Apr 2024 02:18:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o47 pages, 9 figures, 5 tables\\u00a7r"}']}
{title:'Niu et al. (§72024§r)', author: 'Xinlei Niu; Jing Zhang; Charles Patrick Martin', display:{Lore:['[{"text": "arXiv:2404.15637", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHybridVC: Efficient Voice Style Conversion with Text and Audio Prompts\\u00a7r\\n\\n\\u00a78\\u00a7oXinlei Niu\\nJing Zhang\\nCharles Patrick Martin\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.15637\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Apr 2024 04:18:31 GMT)\\u00a7r"}']}
{title:'Lugo et al. (§72024§r)', author: 'Igor Lugo; Martha G. Alatriste-Contreras', display:{Lore:['[{"text": "arXiv:2404.16259", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Experiment with Electric Guitar Signals for Exploring the Virtuosity based on the Entropy of Music\\u00a7r\\n\\n\\u00a78\\u00a7oIgor Lugo\\nMartha G. Alatriste-Contreras\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.16259\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 Apr 2024 00:06:28 GMT)\\u00a7r"}']}
{title:'Williams et al. (§72024§r)', author: 'Ben Williams; Bart van Merriënboer; Vincent Dumoulin; Jenny Hamer; Eleni Triantafillou; Abram B. Fleishman; Matthew McKown; Jill E. Munger; Aaron N. Rice; Ashlee Lillis; Clemency E. White; Catherine A. D. Hobbs; Tries B. Razak; Kate E. Jones; Tom Denton', display:{Lore:['[{"text": "arXiv:2404.16436", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging tropical reef, bird and unrelated sounds for superior transfer learning in marine bioacoustics\\u00a7r\\n\\n\\u00a78\\u00a7oBen Williams\\nBart van Merri\\u00ebnboer\\nVincent Dumoulin\\n+ 11 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.16436\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 7 May 2024 12:42:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages, 5 figures\\u00a7r"}']}
{title:'Zhou et al. (§72024§r)', author: 'Yixuan Zhou; Shuoyi Zhou; Shun Lei; Zhiyong Wu; Menglin Wu', display:{Lore:['[{"text": "arXiv:2404.16619", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe THU-HCSI Multi-Speaker Multi-Lingual Few-Shot Voice Cloning System for LIMMITS\'24 Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oYixuan Zhou\\nShuoyi Zhou\\nShun Lei\\nZhiyong Wu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.16619\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 Apr 2024 14:02:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in Grand Challenge of ICASSP 2024\\u00a7r"}']}
{title:'Ciranni et al. (§72024§r)', author: 'Ruben Ciranni; Emilian Postolache; Giorgio Mariani; Michele Mancusi; Luca Cosmo; Emanuele Rodolà', display:{Lore:['[{"text": "arXiv:2404.16969", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCOCOLA: Coherence-Oriented Contrastive Learning of Musical Audio Representations\\u00a7r\\n\\n\\u00a78\\u00a7oRuben Ciranni\\nEmilian Postolache\\nGiorgio Mariani\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.16969\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 29 Apr 2024 07:33:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDemo page: https://github.com/gladia-research-group/cocola\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Cong Zhang; Kathleen Jepson; Yu-Ying Chuang', display:{Lore:['[{"text": "arXiv:2404.17022", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating differences in lab-quality and remote recording methods with dynamic acoustic measures\\u00a7r\\n\\n\\u00a78\\u00a7oCong Zhang\\nKathleen Jepson\\nYu-Ying Chuang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.17022\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 Apr 2024 20:27:10 GMT)\\u00a7r"}']}
{title:'Gu et al. (§72024§r)', author: 'Yicheng Gu; Xueyao Zhang; Liumeng Xue; Haizhou Li; Zhizheng Wu', display:{Lore:['[{"text": "arXiv:2404.17161", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Investigation of Time-Frequency Representation Discriminators for High-Fidelity Vocoder\\u00a7r\\n\\n\\u00a78\\u00a7oYicheng Gu\\nXueyao Zhang\\nLiumeng Xue\\nHaizhou Li\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.17161\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Apr 2024 05:11:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2311.14957\\u00a7r"}']}
{title:'He et al. (§72024§r)', author: 'Mingrui He; Longting Xu; Han Wang; Mingjun Zhang; Rohan Kumar Das', display:{Lore:['[{"text": "arXiv:2404.17280", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDevice Feature based on Graph Fourier Transformation with Logarithmic Processing For Detection of Replay Speech Attacks\\u00a7r\\n\\n\\u00a78\\u00a7oMingrui He\\nLongting Xu\\nHan Wang\\nMingjun Zhang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.17280\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Apr 2024 09:36:49 GMT)\\u00a7r"}']}
{title:'Belinchon et al. (§72024§r)', author: 'Hugo Garrido-Lestache Belinchon; Helina Mulugeta; Adam Haile', display:{Lore:['[{"text": "arXiv:2404.17608", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynthesizing Audio from Silent Video using Sequence to Sequence Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oHugo Garrido-Lestache Belinchon\\nHelina Mulugeta\\nAdam Haile\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.17608\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 Apr 2024 22:19:42 GMT)\\u00a7r"}']}
{title:'AlAli et al. (§72024§r)', author: 'Abdulazeez AlAli; George Theodorakopoulos', display:{Lore:['[{"text": "arXiv:2404.17721", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn RFP dataset for Real, Fake, and Partially fake audio detection\\u00a7r\\n\\n\\u00a78\\u00a7oAbdulazeez AlAli\\nGeorge Theodorakopoulos\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.17721\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Apr 2024 23:00:56 GMT)\\u00a7r"}']}
{title:'Yuan et al. (§72024§r)', author: 'Yi Yuan; Zhuo Chen; Xubo Liu; Haohe Liu; Xuenan Xu; Dongya Jia; Yuanzhe Chen; Mark D. Plumbley; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2404.17806", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lT-CLAP: Temporal-Enhanced Contrastive Language-Audio Pretraining\\u00a7r\\n\\n\\u00a78\\u00a7oYi Yuan\\nZhuo Chen\\nXubo Liu\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.17806\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 27 Apr 2024 07:05:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint submitted to IEEE MLSP 2024\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Xiaojing Liu; Angeliki Mourgela; Hongwei Ai; Joshua D. Reiss', display:{Lore:['[{"text": "arXiv:2404.17821", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn automatic mixing speech enhancement system for multi-track audio\\u00a7r\\n\\n\\u00a78\\u00a7oXiaojing Liu\\nAngeliki Mourgela\\nHongwei Ai\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.17821\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 27 Apr 2024 08:00:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Feng et al. (§72024§r)', author: 'Tiantian Feng; Xuan Shi; Rahul Gupta; Shrikanth S. Narayanan', display:{Lore:['[{"text": "arXiv:2404.17983", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTI-ASU: Toward Robust Automatic Speech Understanding through Text-to-speech Imputation Against Missing Speech Modality\\u00a7r\\n\\n\\u00a78\\u00a7oTiantian Feng\\nXuan Shi\\nRahul Gupta\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.17983\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 27 Apr 2024 19:13:05 GMT)\\u00a7r"}']}
{title:'Chhaglani et al. (§72024§r)', author: 'Bhawana Chhaglani; Jeremy Gummeson; Prashant Shenoy', display:{Lore:['[{"text": "arXiv:2404.18002", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Privacy-Preserving Audio Classification Systems\\u00a7r\\n\\n\\u00a78\\u00a7oBhawana Chhaglani\\nJeremy Gummeson\\nPrashant Shenoy\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.18002\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 27 Apr 2024 20:36:52 GMT)\\u00a7r"}']}
{title:'Deng et al. (§72024§r)', author: 'Qixin Deng; Qikai Yang; Ruibin Yuan; Yipeng Huang; Yi Wang; Xubo Liu; Zeyue Tian; Jiahao Pan; Ge Zhang; Hanfeng Lin; Yizhi Li; Yinghao Ma; Jie Fu; Chenghua Lin; Emmanouil Benetos; Wenwu Wang; Guangyu Xia; Wei Xue; Yike Guo', display:{Lore:['[{"text": "arXiv:2404.18081", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComposerX: Multi-Agent Symbolic Music Composition with LLMs\\u00a7r\\n\\n\\u00a78\\u00a7oQixin Deng\\nQikai Yang\\nRuibin Yuan\\n+ 15 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.18081\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 Apr 2024 14:14:26 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Wenbin Wang; Yang Song; Sanjay Jha', display:{Lore:['[{"text": "arXiv:2404.18094", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUSAT: A Universal Speaker-Adaptive Text-to-Speech Approach\\u00a7r\\n\\n\\u00a78\\u00a7oWenbin Wang\\nYang Song\\nSanjay Jha\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.18094\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2024.3393714\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech and Language Processing,\\n  2024\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 28 Apr 2024 06:50:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 13 figures. Copyright has been transferred to IEEE\\u00a7r"}']}
{title:'Lugo et al. (§72024§r)', author: 'Igor Lugo; Martha G. Alatriste-Contreras', display:{Lore:['[{"text": "arXiv:2404.18355", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7cstat.AP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPi\\u00e8ces de viole des Cinq Livres and their statistical signatures: the musical work of Marin Marais and Jordi Savall\\u00a7r\\n\\n\\u00a78\\u00a7oIgor Lugo\\nMartha G. Alatriste-Contreras\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.18355\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 Apr 2024 01:35:58 GMT)\\u00a7r"}']}
{title:'Facchinetti et al. (§72024§r)', author: 'Nicolas Facchinetti; Federico Simonetta; Stavros Ntalampiras', display:{Lore:['[{"text": "arXiv:2404.18514", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Systematic Evaluation of Adversarial Attacks against Speech Emotion Recognition Models\\u00a7r\\n\\n\\u00a78\\u00a7oNicolas Facchinetti\\nFederico Simonetta\\nStavros Ntalampiras\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.18514\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.34133/icomputing.0088\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 Apr 2024 09:00:32 GMT)\\u00a7r"}']}
{title:'Korzh et al. (§72024§r)', author: 'Dmitrii Korzh; Elvir Karimov; Mikhail Pautov; Oleg Y. Rogov; Ivan Oseledets', display:{Lore:['[{"text": "arXiv:2404.18791", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCertification of Speaker Recognition Models to Additive Perturbations\\u00a7r\\n\\n\\u00a78\\u00a7oDmitrii Korzh\\nElvir Karimov\\nMikhail Pautov\\nOleg Y. Rogov\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.18791\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 Apr 2024 15:23:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 9 figures\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Jianzong Wang; Pengcheng Li; Xulong Zhang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2404.19187", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCONTUNER: Singing Voice Beautifying with Pitch and Expressiveness Condition\\u00a7r\\n\\n\\u00a78\\u00a7oJianzong Wang\\nPengcheng Li\\nXulong Zhang\\nNing Cheng\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.19187\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Apr 2024 01:27:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the 2024 International Joint Conference on Neural Networks (IJCNN2024)\\u00a7r"}']}
{title:'Liang et al. (§72024§r)', author: 'Ziqi Liang; Jianzong Wang; Xulong Zhang; Yong Zhang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2404.19212", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEAD-VC: Enhancing Speech Auto-Disentanglement for Voice Conversion with IFUB Estimator and Joint Text-Guided Consistent Learning\\u00a7r\\n\\n\\u00a78\\u00a7oZiqi Liang\\nJianzong Wang\\nXulong Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.19212\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Apr 2024 02:23:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the 2024 International Joint Conference on Neural Networks (IJCNN2024)\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Jianzong Wang; Ziqi Liang; Xulong Zhang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2404.19214", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficientASR: Speech Recognition Network Compression via Attention Redundancy and Chunk-Level FFN Optimization\\u00a7r\\n\\n\\u00a78\\u00a7oJianzong Wang\\nZiqi Liang\\nXulong Zhang\\nNing Cheng\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.19214\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Apr 2024 02:30:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the 2024 International Joint Conference on Neural Networks (IJCNN2024)\\u00a7r"}']}
{title:'Gu et al. (§72024§r)', author: 'Yuzhe Gu; Enmao Diao', display:{Lore:['[{"text": "arXiv:2404.19441", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oYuzhe Gu\\nEnmao Diao\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.19441\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Apr 2024 10:44:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Haohe Liu; Xuenan Xu; Yi Yuan; Mengyue Wu; Wenwu Wang; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:2405.00233", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for General Sound\\u00a7r\\n\\n\\u00a78\\u00a7oHaohe Liu\\nXuenan Xu\\nYi Yuan\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.00233\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Apr 2024 22:51:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDemo and code: https://haoheliu.github.io/SemantiCodec/\\u00a7r"}']}
{title:'Huang (§72024§r)', author: 'Qiang Huang', display:{Lore:['[{"text": "arXiv:2405.00248", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWho is Authentic Speaker\\u00a7r\\n\\n\\u00a78\\u00a7oQiang Huang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.00248\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Apr 2024 23:41:00 GMT)\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Dongyuan Li; Ying Zhang; Yusong Wang; Funakoshi Kataro; Manabu Okumura', display:{Lore:['[{"text": "arXiv:2405.00307", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lActive Learning with Task Adaptation Pre-training for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDongyuan Li\\nYing Zhang\\nYusong Wang\\nFunakoshi Kataro\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.00307\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 May 2024 04:05:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Journal of Natural Language Processing. arXiv admin note: text overlap with arXiv:2310.00283\\u00a7r"}']}
{title:'Deng et al. (§72024§r)', author: 'Yimin Deng; Jianzong Wang; Xulong Zhang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2405.00603", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Expressive Disentangled Speech Representations with Soft Speech Units and Adversarial Style Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oYimin Deng\\nJianzong Wang\\nXulong Zhang\\nNing Cheng\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.00603\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 May 2024 16:14:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the 2024 International Joint Conference on Neural Networks (IJCNN2024)\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Pengcheng Li; Jianzong Wang; Xulong Zhang; Yong Zhang; Jing Xiao; Ning Cheng', display:{Lore:['[{"text": "arXiv:2405.00930", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMAIN-VC: Lightweight Speech Representation Disentanglement for One-shot Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oPengcheng Li\\nJianzong Wang\\nXulong Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.00930\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 May 2024 01:11:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the 2024 International Joint Conference on Neural Networks (IJCNN2024)\\u00a7r"}']}
{title:'Chakravarty (§72024§r)', author: 'Aditya Chakravarty', display:{Lore:['[{"text": "arXiv:2405.01004", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning Models in Speech Recognition: Measuring GPU Energy Consumption, Impact of Noise and Model Quantization for Edge Deployment\\u00a7r\\n\\n\\u00a78\\u00a7oAditya Chakravarty\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.01004\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 May 2024 05:09:07 GMT)\\u00a7r"}']}
{title:'Sui et al. (§72024§r)', author: 'Yueyuan Sui; Minghui Zhao; Junxi Xia; Xiaofan Jiang; Stephen Xia', display:{Lore:['[{"text": "arXiv:2405.01242", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTRAMBA: A Hybrid Transformer and Mamba Architecture for Practical Audio and Bone Conduction Speech Super Resolution and Enhancement on Mobile and Wearable Platforms\\u00a7r\\n\\n\\u00a78\\u00a7oYueyuan Sui\\nMinghui Zhao\\nJunxi Xia\\nXiaofan Jiang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.01242\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 29 May 2024 15:46:57 GMT)\\u00a7r"}']}
{title:'Vu et al. (§72024§r)', author: 'Linh Vu; Thu Tran; Wern-Han Lim; Raphael Phan', display:{Lore:['[{"text": "arXiv:2405.01815", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lToward end-to-end interpretable convolutional neural networks for waveform signals\\u00a7r\\n\\n\\u00a78\\u00a7oLinh Vu\\nThu Tran\\nWern-Han Lim\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.01815\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 3 May 2024 02:24:27 GMT)\\u00a7r"}']}
{title:'Schaab et al. (§72024§r)', author: 'Lea Schaab; Anna Kruspe', display:{Lore:['[{"text": "arXiv:2405.01988", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint sentiment analysis of lyrics and audio in music\\u00a7r\\n\\n\\u00a78\\u00a7oLea Schaab\\nAnna Kruspe\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.01988\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 3 May 2024 10:42:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7opublished at DAGA 2024\\u00a7r"}']}
{title:'Moussa et al. (§72024§r)', author: 'Denise Moussa; Germans Hirsch; Christian Riess', display:{Lore:['[{"text": "arXiv:2405.02119", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCan We Identify Unknown Audio Recording Environments in Forensic Scenarios?\\u00a7r\\n\\n\\u00a78\\u00a7oDenise Moussa\\nGermans Hirsch\\nChristian Riess\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.02119\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 3 May 2024 14:19:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to the IEEE for possible publication\\u00a7r"}']}
{title:'Geng et al. (§72024§r)', author: 'Xuelong Geng; Tianyi Xu; Kun Wei; Bingshen Mu; Hongfei Xue; He Wang; Yangze Li; Pengcheng Guo; Yuhang Dai; Longhao Li; Mingchen Shao; Lei Xie', display:{Lore:['[{"text": "arXiv:2405.02132", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnveiling the Potential of LLM-Based ASR on Chinese Open-Source Datasets\\u00a7r\\n\\n\\u00a78\\u00a7oXuelong Geng\\nTianyi Xu\\nKun Wei\\n+ 8 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.02132\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 6 May 2024 08:56:50 GMT)\\u00a7r"}']}
{title:'Pan et al. (§72024§r)', author: 'Yu Pan; Yuguang Yang; Heng Lu; Lei Ma; Jianjun Zhao', display:{Lore:['[{"text": "arXiv:2405.02151", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGMP-ATL: Gender-augmented Multi-scale Pseudo-label Enhanced Adaptive Transfer Learning for Speech Emotion Recognition via HuBERT\\u00a7r\\n\\n\\u00a78\\u00a7oYu Pan\\nYuguang Yang\\nHeng Lu\\nLei Ma\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.02151\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 3 May 2024 14:58:46 GMT)\\u00a7r"}']}
{title:'Pianese et al. (§72024§r)', author: 'Alessandro Pianese; Davide Cozzolino; Giovanni Poggi; Luisa Verdoliva', display:{Lore:['[{"text": "arXiv:2405.02179", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTraining-Free Deepfake Voice Recognition by Leveraging Large-Scale Pre-Trained Models\\u00a7r\\n\\n\\u00a78\\u00a7oAlessandro Pianese\\nDavide Cozzolino\\nGiovanni Poggi\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.02179\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 6 May 2024 07:52:05 GMT)\\u00a7r"}']}
{title:'Draxler et al. (§72024§r)', author: 'Christoph Draxler; Henk van den Heuvel; Arjan van Hessen; Pavel Ircing; Jan Lehečka', display:{Lore:['[{"text": "arXiv:2405.02333", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Technology Services for Oral History Research\\u00a7r\\n\\n\\u00a78\\u00a7oChristoph Draxler\\nHenk van den Heuvel\\nArjan van Hessen\\nPavel Ircing\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.02333\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Apr 2024 09:44:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages plus references, 3 figures\\u00a7r"}']}
{title:'Salameh et al. (§72024§r)', author: 'Raghad Salameh; Mohamad Al Mdfaa; Nursultan Askarbekuly; Manuel Mazzara', display:{Lore:['[{"text": "arXiv:2405.02675", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQuranic Audio Dataset: Crowdsourced and Labeled Recitation from Non-Arabic Speakers\\u00a7r\\n\\n\\u00a78\\u00a7oRaghad Salameh\\nMohamad Al Mdfaa\\nNursultan Askarbekuly\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.02675\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 4 May 2024 14:29:05 GMT)\\u00a7r"}']}
{title:'Xu et al. (§72024§r)', author: 'Tianze Xu; Jiajun Li; Xuesong Chen; Xinrui Yao; Shuchang Liu', display:{Lore:['[{"text": "arXiv:2405.02801", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMozart\'s Touch: A Lightweight Multi-modal Music Generation Framework Based on Pre-Trained Large Models\\u00a7r\\n\\n\\u00a78\\u00a7oTianze Xu\\nJiajun Li\\nXuesong Chen\\nXinrui Yao\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.02801\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 7 May 2024 09:55:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 2 figures, submitted to ACM MM 2024\\u00a7r"}']}
{title:'Chen et al. (§72024§r)', author: 'Changan Chen; Jordi Ramos; Anshul Tomar; Kristen Grauman', display:{Lore:['[{"text": "arXiv:2405.02821", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSim2Real Transfer for Audio-Visual Navigation with Frequency-Adaptive Acoustic Field Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oChangan Chen\\nJordi Ramos\\nAnshul Tomar\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.02821\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 5 May 2024 06:01:31 GMT)\\u00a7r"}']}
{title:'Grinstein et al. (§72024§r)', author: 'Eric Grinstein; Elisa Tengan; Bilgesu Çakmak; Thomas Dietzen; Leonardo Nunes; Toon van Waterschoot; Mike Brookes; Patrick A. Naylor', display:{Lore:['[{"text": "arXiv:2405.02991", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSteered Response Power for Sound Source Localization: A Tutorial Review\\u00a7r\\n\\n\\u00a78\\u00a7oEric Grinstein\\nElisa Tengan\\nBilgesu \\u00c7akmak\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.02991\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 9 May 2024 12:50:32 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72024§r)', author: 'June-Woo Kim; Miika Toikkanen; Sangmin Bae; Minseok Kim; Ho-Young Jung', display:{Lore:['[{"text": "arXiv:2405.02996", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRepAugment: Input-Agnostic Representation-Level Augmentation for Respiratory Sound Classification\\u00a7r\\n\\n\\u00a78\\u00a7oJune-Woo Kim\\nMiika Toikkanen\\nSangmin Bae\\nMinseok Kim\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.02996\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 5 May 2024 16:45:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted EMBC 2024\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Jianyu Wang; Shanzheng Guan', display:{Lore:['[{"text": "arXiv:2405.03118", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetermined Multichannel Blind Source Separation with Clustered Source Model\\u00a7r\\n\\n\\u00a78\\u00a7oJianyu Wang\\nShanzheng Guan\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.03118\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 May 2024 02:23:34 GMT)\\u00a7r"}']}
{title:'Luo et al. (§72024§r)', author: 'Zhenye Luo; Min Ren; Xuecai Hu; Yongzhen Huang; Li Yao', display:{Lore:['[{"text": "arXiv:2405.03178", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPOPDG: Popular 3D Dance Generation with PopDanceSet\\u00a7r\\n\\n\\u00a78\\u00a7oZhenye Luo\\nMin Ren\\nXuecai Hu\\nYongzhen Huang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.03178\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 May 2024 05:59:30 GMT)\\u00a7r"}']}
{title:'Ernst et al. (§72024§r)', author: 'Daniel Ernst; Armin Goudarzi; Reinhard Geisler; Florian Philipp; Thomas Ahlefeldt; Carsten Spehr', display:{Lore:['[{"text": "arXiv:2405.03322", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a75physics.ins-det\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Aeroacoustic Wind Tunnel Studies through Massive Channel Upscaling with MEMS Microphones\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Ernst\\nArmin Goudarzi\\nReinhard Geisler\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.03322\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 May 2024 09:59:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o30th AIAA/CEAS Aeroacoustics Conference\\u00a7r"}']}
{title:'Sprunck et al. (§72024§r)', author: 'Tom Sprunck; Antoine Deleforge; Yannick Privat; Cédric Foy', display:{Lore:['[{"text": "arXiv:2405.03385", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a75physics.class-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFully Reversing the Shoebox Image Source Method: From Impulse Responses to Room Parameters\\u00a7r\\n\\n\\u00a78\\u00a7oTom Sprunck\\nAntoine Deleforge\\nYannick Privat\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.03385\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 May 2024 11:43:49 GMT)\\u00a7r"}']}
{title:'Bevilacqua et al. (§72024§r)', author: 'Antonio Bevilacqua; Paolo Saviano; Alessandro Amirante; Simon Pietro Romano', display:{Lore:['[{"text": "arXiv:2405.03484", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhispy: Adapting STT Whisper Models to Real-Time Environments\\u00a7r\\n\\n\\u00a78\\u00a7oAntonio Bevilacqua\\nPaolo Saviano\\nAlessandro Amirante\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.03484\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 May 2024 13:55:39 GMT)\\u00a7r"}']}
{title:'Ye et al. (§72024§r)', author: 'ShuQi Ye; Yuan Tian', display:{Lore:['[{"text": "arXiv:2405.03567", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Space Separable Distillation for Lightweight Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oShuQi Ye\\nYuan Tian\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.03567\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 May 2024 15:41:41 GMT)\\u00a7r"}']}
{title:'Dong et al. (§72024§r)', author: 'Zhongren Dong; Zixing Zhang; Weixiang Xu; Jing Han; Jianjun Ou; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2405.03952", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHAFFormer: A Hierarchical Attention-Free Framework for Alzheimer\'s Disease Detection From Spontaneous Speech\\u00a7r\\n\\n\\u00a78\\u00a7oZhongren Dong\\nZixing Zhang\\nWeixiang Xu\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.03952\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7npublised at ICASSP 2024\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 May 2024 02:19:16 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Zixing Zhang; Tao Pang; Jing Han; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2405.03953", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntelligent Cardiac Auscultation for Murmur Detection via Parallel-Attentive Models with Uncertainty Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oZixing Zhang\\nTao Pang\\nJing Han\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.03953\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7npublished at ICASSP 2024\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 May 2024 02:24:44 GMT)\\u00a7r"}']}
{title:'Gao et al. (§72024§r)', author: 'Yingxue Gao; Huan Zhao; Zixing Zhang', display:{Lore:['[{"text": "arXiv:2405.03956", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptive Speech Emotion Representation Learning Based On Dynamic Graph\\u00a7r\\n\\n\\u00a78\\u00a7oYingxue Gao\\nHuan Zhao\\nZixing Zhang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.03956\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7npublished at ICASSP 2024\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 May 2024 02:42:17 GMT)\\u00a7r"}']}
{title:'Simionato et al. (§72024§r)', author: 'Riccardo Simionato; Stefano Fasciani', display:{Lore:['[{"text": "arXiv:2405.04124", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparative Study of Recurrent Neural Networks for Virtual Analog Audio Effects Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oRiccardo Simionato\\nStefano Fasciani\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.04124\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 8 May 2024 10:35:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:1810.06603 by other authors\\u00a7r"}']}
{title:'Afchar et al. (§72024§r)', author: 'Darius Afchar; Gabriel Meseguer-Brocal; Romain Hennequin', display:{Lore:['[{"text": "arXiv:2405.04181", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetecting music deepfakes is easy but actually hard\\u00a7r\\n\\n\\u00a78\\u00a7oDarius Afchar\\nGabriel Meseguer-Brocal\\nRomain Hennequin\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.04181\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 22 May 2024 09:31:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review\\u00a7r"}']}
{title:'Sagasti et al. (§72024§r)', author: 'Amaia Sagasti; Davide Scaini; Daniel Arteaga', display:{Lore:['[{"text": "arXiv:2405.04471", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUniversal Spatial Audio Transcoder\\u00a7r\\n\\n\\u00a78\\u00a7oAmaia Sagasti\\nDavide Scaini\\nDaniel Arteaga\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.04471\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 15 May 2024 10:14:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 8 figures. Accepted for presentation at the AES156th Convention, Madrid, Spain (June 2024)\\u00a7r"}']}
{title:'Xie et al. (§72024§r)', author: 'Yuankun Xie; Yi Lu; Ruibo Fu; Zhengqi Wen; Zhiyong Wang; Jianhua Tao; Xin Qi; Xiaopeng Wang; Yukun Liu; Haonan Cheng; Long Ye; Yi Sun', display:{Lore:['[{"text": "arXiv:2405.04880", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Codecfake Dataset and Countermeasures for the Universally Detection of Deepfake Audio\\u00a7r\\n\\n\\u00a78\\u00a7oYuankun Xie\\nYi Lu\\nRuibo Fu\\n+ 8 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.04880\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 15 May 2024 12:24:52 GMT)\\u00a7r"}']}
{title:'Hu et al. (§72024§r)', author: 'Chuanbo Hu; Jacob Thrasher; Wenqi Li; Mindi Ruan; Xiangxu Yu; Lynn K Paul; Shuo Wang; Xin Li', display:{Lore:['[{"text": "arXiv:2405.05126", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Speech Pattern Disorders in Autism using Machine Learning\\u00a7r\\n\\n\\u00a78\\u00a7oChuanbo Hu\\nJacob Thrasher\\nWenqi Li\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.05126\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 3 May 2024 02:59:15 GMT)\\u00a7r"}']}
{title:'Hardwick (§72024§r)', author: 'Jack Hardwick', display:{Lore:['[{"text": "arXiv:2405.05240", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn LSTM-Based Chord Generation System Using Chroma Histogram Representations\\u00a7r\\n\\n\\u00a78\\u00a7oJack Hardwick\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.05240\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 8 May 2024 17:36:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 4 figures, 1 table\\u00a7r"}']}
{title:'Nadkarni et al. (§72024§r)', author: 'Rahul Nadkarni; Emmanouil Nikolakakis; Razvan Marinescu', display:{Lore:['[{"text": "arXiv:2405.05467", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAFEN: Respiratory Disease Classification using Ensemble Learning\\u00a7r\\n\\n\\u00a78\\u00a7oRahul Nadkarni\\nEmmanouil Nikolakakis\\nRazvan Marinescu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.05467\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 8 May 2024 23:50:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder Review Process for MLForHC 2024\\u00a7r"}']}
{title:'Tian et al. (§72024§r)', author: 'Jingguang Tian; Shuaishuai Ye; Shunfei Chen; Yang Xiang; Zhaohui Yin; Xinhui Hu; Xinkang Xu', display:{Lore:['[{"text": "arXiv:2405.05498", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe RoyalFlush Automatic Speech Diarization and Recognition System for In-Car Multi-Channel Automatic Speech Recognition Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oJingguang Tian\\nShuaishuai Ye\\nShunfei Chen\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.05498\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 May 2024 02:03:51 GMT)\\u00a7r"}']}
{title:'Veluri et al. (§72024§r)', author: 'Bandhav Veluri; Malek Itani; Tuochao Chen; Takuya Yoshioka; Shyamnath Gollakota', display:{Lore:['[{"text": "arXiv:2405.06289", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLook Once to Hear: Target Speech Hearing with Noisy Examples\\u00a7r\\n\\n\\u00a78\\u00a7oBandhav Veluri\\nMalek Itani\\nTuochao Chen\\nTakuya Yoshioka\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.06289\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3613904.3642057\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 29 May 2024 19:00:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oBest paper honorable mention at CHI 2024\\u00a7r"}']}
{title:'Chao et al. (§72024§r)', author: 'Rong Chao; Wen-Huang Cheng; Moreno La Quatra; Sabato Marco Siniscalchi; Chao-Han Huck Yang; Szu-Wei Fu; Yu Tsao', display:{Lore:['[{"text": "arXiv:2405.06573", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Investigation of Incorporating Mamba for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oRong Chao\\nWen-Huang Cheng\\nMoreno La Quatra\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.06573\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 10 May 2024 16:18:49 GMT)\\u00a7r"}']}
{title:'Chang et al. (§72024§r)', author: 'Xinyu Chang; Xiangyu Zhang; Haoruo Zhang; Yulu Ran', display:{Lore:['[{"text": "arXiv:2405.06747", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Emotion Prediction Using Recurrent Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oXinyu Chang\\nXiangyu Zhang\\nHaoruo Zhang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.06747\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 10 May 2024 18:03:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 13 figures\\u00a7r"}']}
{title:'Yu et al. (§72024§r)', author: 'Chin-Yun Yu; Johan Pauwels; György Fazekas', display:{Lore:['[{"text": "arXiv:2405.06804", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTime-of-arrival Estimation and Phase Unwrapping of Head-related Transfer Functions With Integer Linear Programming\\u00a7r\\n\\n\\u00a78\\u00a7oChin-Yun Yu\\nJohan Pauwels\\nGy\\u00f6rgy Fazekas\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.06804\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 10 May 2024 20:34:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to be presented at Audio Engineering Society 156th Convention, 2024 June, Madrid, Spain\\u00a7r"}']}
{title:'Guo et al. (§72024§r)', author: 'Xiaobao Guo; Zitong Yu; Nithish Muthuchamy Selvaraj; Bingquan Shen; Adams Wai-Kin Kong; Alex C. Kot', display:{Lore:['[{"text": "arXiv:2405.06995", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBenchmarking Cross-Domain Audio-Visual Deception Detection\\u00a7r\\n\\n\\u00a78\\u00a7oXiaobao Guo\\nZitong Yu\\nNithish Muthuchamy Selvaraj\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.06995\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 11 May 2024 12:06:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages\\u00a7r"}']}
{title:'Zheng et al. (§72024§r)', author: 'Litong Zheng; Feng Hong; Weijie Xu; Wan Zheng', display:{Lore:['[{"text": "arXiv:2405.07029", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA framework of text-dependent speaker verification for chinese numerical string corpus\\u00a7r\\n\\n\\u00a78\\u00a7oLitong Zheng\\nFeng Hong\\nWeijie Xu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.07029\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 21 May 2024 04:44:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2312.01645\\u00a7r"}']}
{title:'Wastnidge (§72024§r)', author: 'Alex Wastnidge', display:{Lore:['[{"text": "arXiv:2405.07034", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards an Accessible and Rapidly Trainable Rhythm Sequencer Using a Generative Stacked Autoencoder\\u00a7r\\n\\n\\u00a78\\u00a7oAlex Wastnidge\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.07034\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 11 May 2024 15:17:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 7 figures\\u00a7r"}']}
{title:'Gautam et al. (§72024§r)', author: 'Sushant Gautam; Mehdi Houshmand Sarkhoosh; Jan Held; Cise Midoglu; Anthony Cioppa; Silvio Giancola; Vajira Thambawita; Michael A. Riegler; Pål Halvorsen; Mubarak Shah', display:{Lore:['[{"text": "arXiv:2405.07354", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSoccerNet-Echoes: A Soccer Game Audio Commentary Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oSushant Gautam\\nMehdi Houshmand Sarkhoosh\\nJan Held\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.07354\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 12 May 2024 18:25:38 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Pengfei Zhang; Zhihang Zheng; Shichen Zhang; Minghao Yang; Shaojun Tang', display:{Lore:['[{"text": "arXiv:2405.07442", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRene: A Pre-trained Multi-modal Architecture for Auscultation of Respiratory Diseases\\u00a7r\\n\\n\\u00a78\\u00a7oPengfei Zhang\\nZhihang Zheng\\nShichen Zhang\\nMinghao Yang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.07442\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 May 2024 03:00:28 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72024§r)', author: 'Jianyi Chen; Wei Xue; Xu Tan; Zhen Ye; Qifeng Liu; Yike Guo', display:{Lore:['[{"text": "arXiv:2405.07682", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFastSAG: Towards Fast Non-Autoregressive Singing Accompaniment Generation\\u00a7r\\n\\n\\u00a78\\u00a7oJianyi Chen\\nWei Xue\\nXu Tan\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.07682\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 May 2024 12:14:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIJCAI 2024\\u00a7r"}']}
{title:'Ren et al. (§72024§r)', author: 'Zhao Ren; Kevin Scheck; Qinhan Hou; Stefano van Gogh; Michael Wand; Tanja Schultz', display:{Lore:['[{"text": "arXiv:2405.08021", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiff-ETS: Learning a Diffusion Probabilistic Model for Electromyography-to-Speech Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oZhao Ren\\nKevin Scheck\\nQinhan Hou\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.08021\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 11 May 2024 17:04:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by EMBC 2024\\u00a7r"}']}
{title:'Ariyanti et al. (§72024§r)', author: 'Whenty Ariyanti; Kai-Chun Liu; Kuan-Yu Chen; Yu Tsao', display:{Lore:['[{"text": "arXiv:2405.08342", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAbnormal Respiratory Sound Identification Using Audio-Spectrogram Vision Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oWhenty Ariyanti\\nKai-Chun Liu\\nKuan-Yu Chen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.08342\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/EMBC40787.2023.10341036\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n45th Annual International Conference of the IEEE Engineering in\\n  Medicine & Biology Society (2023) 1-4\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 May 2024 06:31:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in 202345th Annual International Conference of the IEEEEngineering in Medicine Biology Society (EMBC)\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Xiaohui Zhang; Jiangyan Yi; Jianhua Tao', display:{Lore:['[{"text": "arXiv:2405.08596", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEVDA: Evolving Deepfake Audio Detection Continual Learning Benchmark\\u00a7r\\n\\n\\u00a78\\u00a7oXiaohui Zhang\\nJiangyan Yi\\nJianhua Tao\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.08596\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 15 May 2024 06:52:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper need more modification\\u00a7r"}']}
{title:'Riou et al. (§72024§r)', author: 'Alain Riou; Stefan Lattner; Gaëtan Hadjeres; Geoffroy Peeters', display:{Lore:['[{"text": "arXiv:2405.08679", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating Design Choices in Joint-Embedding Predictive Architectures for General Audio Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oAlain Riou\\nStefan Lattner\\nGa\\u00ebtan Hadjeres\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.08679\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 May 2024 15:00:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSelf-supervision in Audio, Speech and Beyond workshop, IEEE International Conference on Acoustics, Speech, and Signal Processing, 2024\\u00a7r"}']}
{title:'Hou et al. (§72024§r)', author: 'Yang Hou; Haitao Fu; Chuankai Chen; Zida Li; Haoyu Zhang; Jianjun Zhao', display:{Lore:['[{"text": "arXiv:2405.08838", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPolyGlotFake: A Novel Multilingual and Multimodal DeepFake Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oYang Hou\\nHaitao Fu\\nChuankai Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.08838\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 May 2024 06:40:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 page, 4 figures\\u00a7r"}']}
{title:'Postolache et al. (§72024§r)', author: 'Emilian Postolache; Natalia Polouliakh; Hiroaki Kitano; Akima Connelly; Emanuele Rodolà; Taketo Akama', display:{Lore:['[{"text": "arXiv:2405.09062", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNaturalistic Music Decoding from EEG Data via Latent Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oEmilian Postolache\\nNatalia Polouliakh\\nHiroaki Kitano\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.09062\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 17 May 2024 13:43:22 GMT)\\u00a7r"}']}
{title:'Inoue et al. (§72024§r)', author: 'Sho Inoue; Kun Zhou; Shuai Wang; Haizhou Li', display:{Lore:['[{"text": "arXiv:2405.09171", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierarchical Emotion Prediction and Control in Text-to-Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oSho Inoue\\nKun Zhou\\nShuai Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.09171\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP48485.2024.10445996\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 May 2024 08:21:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis is accepted to IEEE ICASSP 2024\\u00a7r"}']}
{title:'Karystinaios et al. (§72024§r)', author: 'Emmanouil Karystinaios; Francesco Foscarin; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2405.09224", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerception-Inspired Graph Convolution for Music Understanding Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oEmmanouil Karystinaios\\nFrancesco Foscarin\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.09224\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 May 2024 10:04:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the 33rd International Joint Conference on Artificial Intelligence (IJCAI-24)\\u00a7r"}']}
{title:'Karystinaios et al. (§72024§r)', author: 'Emmanouil Karystinaios; Francesco Foscarin; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2405.09241", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSMUG-Explain: A Framework for Symbolic Music Graph Explanations\\u00a7r\\n\\n\\u00a78\\u00a7oEmmanouil Karystinaios\\nFrancesco Foscarin\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.09241\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 May 2024 10:41:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of the Sound and Music Computing Conference 2024 (SMC2024), Porto, Portugal\\u00a7r"}']}
{title:'Jin et al. (§72024§r)', author: 'Weifei Jin; Yuxin Cao; Junjie Su; Qi Shen; Kai Ye; Derui Wang; Jie Hao; Ziyao Liu', display:{Lore:['[{"text": "arXiv:2405.09470", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Evaluating the Robustness of Automatic Speech Recognition Systems via Audio Style Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oWeifei Jin\\nYuxin Cao\\nJunjie Su\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.09470\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 May 2024 16:05:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to SecTL (AsiaCCS Workshop) 2024\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Ziyu Wang; Lejun Min; Gus Xia', display:{Lore:['[{"text": "arXiv:2405.09901", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhole-Song Hierarchical Generation of Symbolic Music Using Cascaded Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oZiyu Wang\\nLejun Min\\nGus Xia\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.09901\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 May 2024 08:48:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of the International Conference on Learning Representations (ICLR 2024)\\u00a7r"}']}
{title:'Kagumire et al. (§72024§r)', author: 'Sulaiman Kagumire; Andrew Katumba; Joyce Nakatumba-Nabende; John Quinn', display:{Lore:['[{"text": "arXiv:2405.10211", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBuilding a Luganda Text-to-Speech Model From Crowdsourced Data\\u00a7r\\n\\n\\u00a78\\u00a7oSulaiman Kagumire\\nAndrew Katumba\\nJoyce Nakatumba-Nabende\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.10211\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 May 2024 16:00:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at theAfricaNLP workshop at ICLR 2024\\u00a7r"}']}
{title:'Braun (§72024§r)', author: 'David Braun', display:{Lore:['[{"text": "arXiv:2405.11554", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDAC-JAX: A JAX Implementation of the Descript Audio Codec\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Braun\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.11554\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 19 May 2024 14:07:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 2 tables\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Nian Li; Jianguo Wei', display:{Lore:['[{"text": "arXiv:2405.12031", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeighborhood Attention Transformer with Progressive Channel Fusion for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oNian Li\\nJianguo Wei\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.12031\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 30 May 2024 02:37:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 2 figures, 3 tables; added github link\\u00a7r"}']}
{title:'Jonason et al. (§72024§r)', author: 'Nicolas Jonason; Luca Casini; Bob L. T. Sturm', display:{Lore:['[{"text": "arXiv:2405.12666", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSYMPLEX: Controllable Symbolic Music Generation using Simplex Diffusion with Vocabulary Priors\\u00a7r\\n\\n\\u00a78\\u00a7oNicolas Jonason\\nLuca Casini\\nBob L. T. Sturm\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.12666\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 May 2024 10:27:34 GMT)\\u00a7r"}']}
{title:'Herdt et al. (§72024§r)', author: 'Rudolf Herdt; Louisa Kinzel; Johann Georg Maaß; Marvin Walther; Henning Fröhlich; Tim Schubert; Peter Maass; Christian Patrick Schaaf', display:{Lore:['[{"text": "arXiv:2405.12957", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing the analysis of murine neonatal ultrasonic vocalizations: Development, evaluation, and application of different mathematical models\\u00a7r\\n\\n\\u00a78\\u00a7oRudolf Herdt\\nLouisa Kinzel\\nJohann Georg Maa\\u00df\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.12957\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 May 2024 18:42:45 GMT)\\u00a7r"}']}
{title:'Nechaev et al. (§72024§r)', author: 'Vladimir Nechaev; Sergey Kosyakov', display:{Lore:['[{"text": "arXiv:2405.13162", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-autoregressive real-time Accent Conversion model with voice cloning\\u00a7r\\n\\n\\u00a78\\u00a7oVladimir Nechaev\\nSergey Kosyakov\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.13162\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 May 2024 19:07:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 6 figures, 3 tables\\u00a7r"}']}
{title:'Zang et al. (§72024§r)', author: 'Yongyi Zang; Yifan Wang; Minglun Lee', display:{Lore:['[{"text": "arXiv:2405.13428", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAmbisonizer: Neural Upmixing as Spherical Harmonics Generation\\u00a7r\\n\\n\\u00a78\\u00a7oYongyi Zang\\nYifan Wang\\nMinglun Lee\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.13428\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 May 2024 08:16:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review\\u00a7r"}']}
{title:'Zeng et al. (§72024§r)', author: 'Wei Zeng; Xian He; Ye Wang', display:{Lore:['[{"text": "arXiv:2405.13527", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Real-World Polyphonic Piano Audio-to-Score Transcription with Hierarchical Decoding\\u00a7r\\n\\n\\u00a78\\u00a7oWei Zeng\\nXian He\\nYe Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.13527\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 May 2024 10:52:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 5 figures, accepted by IJCAI 2024- AI, Arts Creativity Track\\u00a7r"}']}
{title:'Lin et al. (§72024§r)', author: 'Jiaju Lin; Haoxuan Hu', display:{Lore:['[{"text": "arXiv:2405.13636", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Mamba: Pretrained Audio State Space Model For Audio Tagging\\u00a7r\\n\\n\\u00a78\\u00a7oJiaju Lin\\nHaoxuan Hu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.13636\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 May 2024 13:35:56 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Hong Zhang; Jie Lin; Shengxuan Chen', display:{Lore:['[{"text": "arXiv:2405.13661", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTimbre Perception, Representation, and its Neuroscientific Exploration: A Comprehensive Review\\u00a7r\\n\\n\\u00a78\\u00a7oHong Zhang\\nJie Lin\\nShengxuan Chen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.13661\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 May 2024 14:06:20 GMT)\\u00a7r"}']}
{title:'Iwami et al. (§72024§r)', author: 'Takahiro Iwami; Akira Omoto', display:{Lore:['[{"text": "arXiv:2405.14290", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrequency-Domain Sound Field from the Perspective of Band-Limited Functions\\u00a7r\\n\\n\\u00a78\\u00a7oTakahiro Iwami\\nAkira Omoto\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.14290\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 May 2024 08:07:41 GMT)\\u00a7r"}']}
{title:'V et al. (§72024§r)', author: 'Kesavaraj V; Anuprabha M; Anil Kumar Vuppala', display:{Lore:['[{"text": "arXiv:2405.14489", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End User-Defined Keyword Spotting using Shifted Delta Coefficients\\u00a7r\\n\\n\\u00a78\\u00a7oKesavaraj V\\nAnuprabha M\\nAnil Kumar Vuppala\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.14489\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 May 2024 12:24:01 GMT)\\u00a7r"}']}
{title:'Pedroza et al. (§72024§r)', author: 'Hegel Pedroza; Wallace Abreu; Ryan Corey; Iran Roman', display:{Lore:['[{"text": "arXiv:2405.14679", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Electric Guitar Tones and Effects to Improve Robustness in Guitar Tablature Transcription Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oHegel Pedroza\\nWallace Abreu\\nRyan Corey\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.14679\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 May 2024 15:13:40 GMT)\\u00a7r"}']}
{title:'Mogonediwa (§72024§r)', author: 'Keoikantse Mogonediwa', display:{Lore:['[{"text": "arXiv:2405.15096", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Genre Classification: Training an AI model\\u00a7r\\n\\n\\u00a78\\u00a7oKeoikantse Mogonediwa\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.15096\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 May 2024 23:07:01 GMT)\\u00a7r"}']}
{title:'Collins (§72024§r)', author: 'Nick Collins', display:{Lore:['[{"text": "arXiv:2405.15103", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Rarity of Musical Audio Signals Within the Space of Possible Audio Generation\\u00a7r\\n\\n\\u00a78\\u00a7oNick Collins\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.15103\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 May 2024 23:25:46 GMT)\\u00a7r"}']}
{title:'Niu et al. (§72024§r)', author: 'Xinlei Niu; Jing Zhang; Christian Walder; Charles Patrick Martin', display:{Lore:['[{"text": "arXiv:2405.15338", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSoundLoCD: An Efficient Conditional Discrete Contrastive Latent Diffusion Model for Text-to-Sound Generation\\u00a7r\\n\\n\\u00a78\\u00a7oXinlei Niu\\nJing Zhang\\nChristian Walder\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.15338\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 May 2024 08:18:58 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Zhisheng Zhang; Pengyang Huang', display:{Lore:['[{"text": "arXiv:2405.15655", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHiddenSpeaker: Generate Imperceptible Unlearnable Audios for Speaker Verification System\\u00a7r\\n\\n\\u00a78\\u00a7oZhisheng Zhang\\nPengyang Huang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.15655\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 May 2024 02:33:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IJCNN 2024\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Chang Li; Ruoyu Wang; Lijuan Liu; Jun Du; Yixuan Sun; Zilu Guo; Zhenrong Zhang; Yuan Jiang', display:{Lore:['[{"text": "arXiv:2405.15863", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQuality-aware Masked Diffusion Transformer for Enhanced Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oChang Li\\nRuoyu Wang\\nLijuan Liu\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.15863\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 May 2024 18:09:27 GMT)\\u00a7r"}']}
{title:'Natesan et al. (§72024§r)', author: 'Sanjay Natesan; Homayoon Beigi', display:{Lore:['[{"text": "arXiv:2405.16000", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCarnatic Raga Identification System using Rigorous Time-Delay Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oSanjay Natesan\\nHomayoon Beigi\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.16000\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.13140/RG.2.2.17517.40164\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nRecognition Technologies, Inc. Technical Report (2024),\\n  RTI-20240524-01\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 25 May 2024 01:31:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 2 tables, 3 figures\\u00a7r"}']}
{title:'Riley et al. (§72024§r)', author: 'Xavier Riley; Simon Dixon', display:{Lore:['[{"text": "arXiv:2405.16687", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReconstructing the Charlie Parker Omnibook using an audio-to-score automatic transcription pipeline\\u00a7r\\n\\n\\u00a78\\u00a7oXavier Riley\\nSimon Dixon\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.16687\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 26 May 2024 20:41:36 GMT)\\u00a7r"}']}
{title:'Jia et al. (§72024§r)', author: 'Jidong Jia; Pei Zhao; Di Wang', display:{Lore:['[{"text": "arXiv:2405.16797", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Real-Time Voice Activity Detection Based On Lightweight Neural\\u00a7r\\n\\n\\u00a78\\u00a7oJidong Jia\\nPei Zhao\\nDi Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.16797\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 May 2024 03:31:16 GMT)\\u00a7r"}']}
{title:'Shi et al. (§72024§r)', author: 'Haoxiang Shi; Jianzong Wang; Xulong Zhang; Ning Cheng; Jun Yu; Jing Xiao', display:{Lore:['[{"text": "arXiv:2405.17028", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRSET: Remapping-based Sorting Method for Emotion Transfer Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oHaoxiang Shi\\nJianzong Wang\\nXulong Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.17028\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 May 2024 10:30:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the 8th APWeb-WAIM International Joint Conference on Web andBig Data\\u00a7r"}']}
{title:'Adnan et al. (§72024§r)', author: 'Tariq Adnan; Abdelrahman Abdelkader; Zipei Liu; Ekram Hossain; Sooyong Park; MD Saiful Islam; Ehsan Hoque', display:{Lore:['[{"text": "arXiv:2405.17206", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Novel Fusion Architecture for PD Detection Using Semi-Supervised Speech Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oTariq Adnan\\nAbdelrahman Abdelkader\\nZipei Liu\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.17206\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 May 2024 16:06:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o25 pages, 5 figures, and 4 tables\\u00a7r"}']}
{title:'Kamuni et al. (§72024§r)', author: 'Navin Kamuni; Dheerendra Panwar', display:{Lore:['[{"text": "arXiv:2405.17413", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Music Genre Classification through Multi-Algorithm Analysis and User-Friendly Visualization\\u00a7r\\n\\n\\u00a78\\u00a7oNavin Kamuni\\nDheerendra Panwar\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.17413\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.52783/jes.3178\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 May 2024 17:57:20 GMT)\\u00a7r"}']}
{title:'Paissan et al. (§72024§r)', author: 'Francesco Paissan; Luca Della Libera; Mirco Ravanelli; Cem Subakan', display:{Lore:['[{"text": "arXiv:2405.17615", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lListenable Maps for Zero-Shot Audio Classifiers\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesco Paissan\\nLuca Della Libera\\nMirco Ravanelli\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.17615\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 May 2024 19:25:42 GMT)\\u00a7r"}']}
{title:'Naranjo-Alcazar et al. (§72024§r)', author: 'Javier Naranjo-Alcazar; Jordi Grau-Haro; Ruben Ribes-Serrano; Pedro Zuccarello', display:{Lore:['[{"text": "arXiv:2405.18153", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPractical aspects for the creation of an audio dataset from field recordings with optimized labeling budget with AI-assisted strategy\\u00a7r\\n\\n\\u00a78\\u00a7oJavier Naranjo-Alcazar\\nJordi Grau-Haro\\nRuben Ribes-Serrano\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.18153\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 May 2024 13:14:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICML2024 Workshop on Data-Centric Machine Learning Research\\u00a7r"}']}
{title:'Brunetto et al. (§72024§r)', author: 'Amandine Brunetto; Sascha Hornauer; Fabien Moutarde', display:{Lore:['[{"text": "arXiv:2405.18213", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeRAF: 3D Scene Infused Neural Radiance and Acoustic Fields\\u00a7r\\n\\n\\u00a78\\u00a7oAmandine Brunetto\\nSascha Hornauer\\nFabien Moutarde\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.18213\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 May 2024 14:17:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProject Page: https://amandinebtto.github.io/NeRAF\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Yixiao Zhang; Yukara Ikemiya; Woosung Choi; Naoki Murata; Marco A. Martínez-Ramírez; Liwei Lin; Gus Xia; Wei-Hsiang Liao; Yuki Mitsufuji; Simon Dixon', display:{Lore:['[{"text": "arXiv:2405.18386", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInstruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning\\u00a7r\\n\\n\\u00a78\\u00a7oYixiao Zhang\\nYukara Ikemiya\\nWoosung Choi\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.18386\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 29 May 2024 17:05:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCode and demo are available at: https://github.com/ldzhangyx/instruct-musicgen\\u00a7r"}']}
{title:'Saito et al. (§72024§r)', author: 'Koichi Saito; Dongjun Kim; Takashi Shibuya; Chieh-Hsin Lai; Zhi Zhong; Yuhta Takida; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2405.18503", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSoundCTM: Uniting Score-based and Consistency Models for Text-to-Sound Generation\\u00a7r\\n\\n\\u00a78\\u00a7oKoichi Saito\\nDongjun Kim\\nTakashi Shibuya\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.18503\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 May 2024 18:14:52 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Che Liu; Changde Du; Xiaoyu Chen; Huiguang He', display:{Lore:['[{"text": "arXiv:2405.18726", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReverse the auditory processing pathway: Coarse-to-fine audio reconstruction from fMRI\\u00a7r\\n\\n\\u00a78\\u00a7oChe Liu\\nChangde Du\\nXiaoyu Chen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.18726\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 29 May 2024 03:16:14 GMT)\\u00a7r"}']}
{title:'Sekkat et al. (§72024§r)', author: 'Chloé Sekkat; Fanny Leroy; Salima Mdhaffar; Blake Perry Smith; Yannick Estève; Joseph Dureau; Alice Coucke', display:{Lore:['[{"text": "arXiv:2405.19342", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSonos Voice Control Bias Assessment Dataset: A Methodology for Demographic Bias Assessment in Voice Assistants\\u00a7r\\n\\n\\u00a78\\u00a7oChlo\\u00e9 Sekkat\\nFanny Leroy\\nSalima Mdhaffar\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.19342\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 May 2024 12:53:32 GMT)\\u00a7r"}']}
{title:'Katumba et al. (§72024§r)', author: 'Andrew Katumba; Sudi Murindanyi; John Trevor Kasule; Elvis Mugume', display:{Lore:['[{"text": "arXiv:2405.19343", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLuganda Speech Intent Recognition for IoT Applications\\u00a7r\\n\\n\\u00a78\\u00a7oAndrew Katumba\\nSudi Murindanyi\\nJohn Trevor Kasule\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.19343\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 May 2024 10:14:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented as a conference paper at ICLR 2024/AfricaNLP\\u00a7r"}']}
{title:'Wu et al. (§72024§r)', author: 'Xiaoliang Wu; Chau Luu; Peter Bell; Ajitha Rajan', display:{Lore:['[{"text": "arXiv:2405.19796", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExplainable Attribute-Based Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoliang Wu\\nChau Luu\\nPeter Bell\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.19796\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 May 2024 08:04:28 GMT)\\u00a7r"}']}
{title:'Sorrenti (§72024§r)', author: 'Adam Sorrenti', display:{Lore:['[{"text": "arXiv:2405.20059", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpectral Mapping of Singing Voices: U-Net-Assisted Vocal Segmentation\\u00a7r\\n\\n\\u00a78\\u00a7oAdam Sorrenti\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.20059\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 May 2024 13:47:53 GMT)\\u00a7r"}']}
{title:'Asaad et al. (§72024§r)', author: 'Ihab Asaad; Maxime Jacquelin; Olivier Perrotin; Laurent Girin; Thomas Hueber', display:{Lore:['[{"text": "arXiv:2405.20101", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFill in the Gap! Combining Self-supervised Representation Learning with Neural Audio Synthesis for Speech Inpainting\\u00a7r\\n\\n\\u00a78\\u00a7oIhab Asaad\\nMaxime Jacquelin\\nOlivier Perrotin\\nLaurent Girin\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.20101\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 May 2024 14:41:39 GMT)\\u00a7r"}']}
{title:'Nfissi et al. (§72024§r)', author: 'Alaa Nfissi; Wassim Bouachir; Nizar Bouguila; Brian Mishara', display:{Lore:['[{"text": "arXiv:2405.20172", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIterative Feature Boosting for Explainable Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAlaa Nfissi\\nWassim Bouachir\\nNizar Bouguila\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.20172\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICMLA58977.2023.00081\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2023 International Conference on Machine Learning and Applications\\n  (ICMLA), Jacksonville, FL, USA, 2023, pp. 543-549\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 May 2024 15:44:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in: 2023 International Conference on MachineLearning and Applications (ICMLA)\\u00a7r"}']}
{title:'Novack et al. (§72024§r)', author: 'Zachary Novack; Julian McAuley; Taylor Berg-Kirkpatrick; Nicholas Bryan', display:{Lore:['[{"text": "arXiv:2405.20289", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7acs.SD\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDITTO-2: Distilled Diffusion Inference-Time T-Optimization for Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oZachary Novack\\nJulian McAuley\\nTaylor Berg-Kirkpatrick\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.20289\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 May 2024 17:40:11 GMT)\\u00a7r"}']}
