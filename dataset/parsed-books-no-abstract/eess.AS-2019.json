{title:'Ernst et al. (§72019§r)', author: 'Ori Ernst; Shlomo E. Chazan; Sharon Gannot; Jacob Goldberger', display:{Lore:['[{"text": "arXiv:1803.08243", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Dereverberation Using Fully Convolutional Networks\\u00a7r\\n\\n\\u00a78\\u00a7oOri Ernst\\nShlomo E. Chazan\\nSharon Gannot\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1803.08243\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 3 Apr 2019 21:18:40 GMT)\\u00a7r"}']}
{title:'Okabe et al. (§72019§r)', author: 'Koji Okabe; Takafumi Koshinaka; Koichi Shinoda', display:{Lore:['[{"text": "arXiv:1803.10963", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttentive Statistics Pooling for Deep Speaker Embedding\\u00a7r\\n\\n\\u00a78\\u00a7oKoji Okabe\\nTakafumi Koshinaka\\nKoichi Shinoda\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1803.10963\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2018-993\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 25 Feb 2019 01:44:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProc. Interspeech 2018, pp2252\\u20132256. arXiv admin note: text overlapwith arXiv:1809.09311\\u00a7r"}']}
{title:'Meng et al. (§72019§r)', author: 'Zhong Meng; Jinyu Li; Yifan Gong; Biing-Hwang; Juang', display:{Lore:['[{"text": "arXiv:1804.00644", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Teacher-Student Learning for Unsupervised Domain Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oZhong Meng\\nJinyu Li\\nYifan Gong\\nBiing-Hwang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1804.00644\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2018.8461682\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2018 IEEE International Conference on Acoustics, Speech and Signal\\n  Processing (ICASSP), Calgary, Canada\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 Apr 2019 15:43:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, ICASSP 2018\\u00a7r"}']}
{title:'Meng et al. (§72019§r)', author: 'Zhong Meng; Jinyu Li; Zhuo Chen; Yong Zhao; Vadim Mazalov; Yifan Gong; Biing-Hwang; Juang', display:{Lore:['[{"text": "arXiv:1804.00732", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker-Invariant Training via Adversarial Learning\\u00a7r\\n\\n\\u00a78\\u00a7oZhong Meng\\nJinyu Li\\nZhuo Chen\\n+ 4 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1804.00732\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2018.8461932\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2018 IEEE International Conference on Acoustics, Speech and Signal\\n  Processing (ICASSP), Calgary, Canada\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 30 Apr 2019 15:35:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, ICASSP 2018\\u00a7r"}']}
{title:'Kinnunen et al. (§72019§r)', author: 'Tomi Kinnunen; Kong Aik Lee; Hector Delgado; Nicholas Evans; Massimiliano Todisco; Md Sahidullah; Junichi Yamagishi; Douglas A. Reynolds', display:{Lore:['[{"text": "arXiv:1804.09618", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lt-DCF: a Detection Cost Function for the Tandem Assessment of Spoofing Countermeasures and Automatic Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oTomi Kinnunen\\nKong Aik Lee\\nHector Delgado\\n+ 4 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1804.09618\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 11 Apr 2019 16:32:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in Odyssey 2018: the Speaker and Language Recognition Workshop [cleaned up source files]\\u00a7r"}']}
{title:'Roche et al. (§72019§r)', author: 'Fanny Roche; Thomas Hueber; Samuel Limier; Laurent Girin', display:{Lore:['[{"text": "arXiv:1806.04096", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutoencoders for music sound modeling: a comparison of linear, shallow, deep, recurrent and variational models\\u00a7r\\n\\n\\u00a78\\u00a7oFanny Roche\\nThomas Hueber\\nSamuel Limier\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1806.04096\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 24 May 2019 08:30:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSMC 2019\\u00a7r"}']}
{title:'Gerazov et al. (§72019§r)', author: 'Branislav Gerazov; Gérard Bailly; Omar Mohammed; Yi Xu; Philip N. Garner', display:{Lore:['[{"text": "arXiv:1806.08685", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Variational Prosody Model for Mapping the Context-Sensitive Variation of Functional Prosodic Prototypes\\u00a7r\\n\\n\\u00a78\\u00a7oBranislav Gerazov\\nG\\u00e9rard Bailly\\nOmar Mohammed\\nYi Xu\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1806.08685\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 18 Mar 2019 12:47:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUpdated with recurrent version of contour generators, unified prosodic latent space, and performance evaluation with baseline\\u00a7r"}']}
{title:'Zhao et al. (§72019§r)', author: 'Ziyue Zhao; Huijun Liu; Tim Fingscheidt', display:{Lore:['[{"text": "arXiv:1806.09411", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConvolutional Neural Networks to Enhance Coded Speech\\u00a7r\\n\\n\\u00a78\\u00a7oZiyue Zhao\\nHuijun Liu\\nTim Fingscheidt\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1806.09411\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 24 Jan 2019 09:53:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMore analysis are added for version 4\\u00a7r"}']}
{title:'Ravanelli et al. (§72019§r)', author: 'Mirco Ravanelli; Yoshua Bengio', display:{Lore:['[{"text": "arXiv:1808.00158", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Recognition from Raw Waveform with SincNet\\u00a7r\\n\\n\\u00a78\\u00a7oMirco Ravanelli\\nYoshua Bengio\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1808.00158\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 9 Aug 2019 15:52:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of SLT 2018\\u00a7r"}']}
{title:'Sterpu et al. (§72019§r)', author: 'George Sterpu; Christian Saam; Naomi Harte', display:{Lore:['[{"text": "arXiv:1809.01728", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention-based Audio-Visual Fusion for Robust Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGeorge Sterpu\\nChristian Saam\\nNaomi Harte\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1809.01728\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3242969.3243014\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 1 May 2019 11:21:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn ICMI\'18, October 16-20, 2018, Boulder, CO, USA. Equation (2) corrected on this version\\u00a7r"}']}
{title:'Meng et al. (§72019§r)', author: 'Zhong Meng; Jinyu Li; Yifan Gong; Biing-Hwang; Juang', display:{Lore:['[{"text": "arXiv:1809.02251", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Feature-Mapping for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oZhong Meng\\nJinyu Li\\nYifan Gong\\nBiing-Hwang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1809.02251\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2018-2461\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInterspeech 2018\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 Apr 2019 15:53:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, Interspeech 2018\\u00a7r"}']}
{title:'Meng et al. (§72019§r)', author: 'Zhong Meng; Jinyu Li; Yifan Gong; Biing-Hwang; Juang', display:{Lore:['[{"text": "arXiv:1809.02253", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCycle-Consistent Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oZhong Meng\\nJinyu Li\\nYifan Gong\\nBiing-Hwang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1809.02253\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2018-2409\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInterspeech 2018\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 Apr 2019 15:48:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures. Interspeech 2018. arXiv admin note: text overlap with arXiv:1809.02251\\u00a7r"}']}
{title:'Ghorbani et al. (§72019§r)', author: 'Shahram Ghorbani; Ahmet E. Bulut; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:1809.06833", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdvancing Multi-Accented LSTM-CTC Speech Recognition using a Domain Specific Student-Teacher Learning Paradigm\\u00a7r\\n\\n\\u00a78\\u00a7oShahram Ghorbani\\nAhmet E. Bulut\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1809.06833\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 1 Oct 2019 18:30:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at SLT 2018\\u00a7r"}']}
{title:'Zhang et al. (§72019§r)', author: 'Aonan Zhang; Quan Wang; Zhenyao Zhu; John Paisley; Chong Wang', display:{Lore:['[{"text": "arXiv:1810.04719", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFully Supervised Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oAonan Zhang\\nQuan Wang\\nZhenyao Zhu\\nJohn Paisley\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.04719\\u00a7r\\n\\nVersion:\\u00a77v7 (Tue, 19 Feb 2019 16:30:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2019\\u00a7r"}']}
{title:'Wang et al. (§72019§r)', author: 'Quan Wang; Hannah Muckenhirn; Kevin Wilson; Prashant Sridhar; Zelin Wu; John Hershey; Rif A. Saurous; Ron J. Weiss; Ye Jia; Ignacio Lopez Moreno', display:{Lore:['[{"text": "arXiv:1810.04826", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoiceFilter: Targeted Voice Separation by Speaker-Conditioned Spectrogram Masking\\u00a7r\\n\\n\\u00a78\\u00a7oQuan Wang\\nHannah Muckenhirn\\nKevin Wilson\\n+ 6 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.04826\\u00a7r\\n\\nVersion:\\u00a77v6 (Wed, 19 Jun 2019 17:10:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Interspeech 2019\\u00a7r"}']}
{title:'Rabiee et al. (§72019§r)', author: 'Azam Rabiee; Geonmin Kim; Tae-Ho Kim; Soo-Young Lee', display:{Lore:['[{"text": "arXiv:1810.05319", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Fully Time-domain Neural Model for Subband-based Speech Synthesizer\\u00a7r\\n\\n\\u00a78\\u00a7oAzam Rabiee\\nGeonmin Kim\\nTae-Ho Kim\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.05319\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 2 Jul 2019 00:19:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figure\\u00a7r"}']}
{title:'Leroy et al. (§72019§r)', author: 'David Leroy; Alice Coucke; Thibaut Lavril; Thibault Gisselbrecht; Joseph Dureau', display:{Lore:['[{"text": "arXiv:1810.05512", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFederated Learning for Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Leroy\\nAlice Coucke\\nThibaut Lavril\\nThibault Gisselbrecht\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.05512\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 18 Feb 2019 18:41:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication to ICASSP 2019\\u00a7r"}']}
{title:'Vesperini et al. (§72019§r)', author: 'Fabio Vesperini; Leonardo Gabrielli; Emanuele Principi; Stefano Squartini', display:{Lore:['[{"text": "arXiv:1810.06325", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPolyphonic Sound Event Detection by using Capsule Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oFabio Vesperini\\nLeonardo Gabrielli\\nEmanuele Principi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.06325\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JSTSP.2019.2902305\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 30 Jan 2019 19:47:31 GMT)\\u00a7r"}']}
{title:'Ramirez et al. (§72019§r)', author: 'Marco A. Martínez Ramirez; Joshua D. Reiss', display:{Lore:['[{"text": "arXiv:1810.06603", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModeling of nonlinear audio effects with end-to-end deep neural networks\\u00a7r\\n\\n\\u00a78\\u00a7oMarco A. Mart\\u00ednez Ramirez\\nJoshua D. Reiss\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.06603\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Mar 2019 12:46:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at the2019 IEEE InternationalConference on Acoustics, Speech, and Signal Processing (ICASSP), Brighton, UK, May 2019\\u00a7r"}']}
{title:'Jung et al. (§72019§r)', author: 'Jee-weon Jung; Hee-soo Heo; Hye-jin Shim; Ha-jin Yu', display:{Lore:['[{"text": "arXiv:1810.10884", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lShort utterance compensation in speaker verification via cosine-based teacher-student learning of speaker embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oJee-weon Jung\\nHee-soo Heo\\nHye-jin Shim\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.10884\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 10 Apr 2019 04:52:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, submitted to Interspeech 2019 as a conference paper\\u00a7r"}']}
{title:'Valin et al. (§72019§r)', author: 'Jean-Marc Valin; Jan Skoglund', display:{Lore:['[{"text": "arXiv:1810.11846", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLPCNet: Improving Neural Speech Synthesis Through Linear Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oJean-Marc Valin\\nJan Skoglund\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.11846\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 19 Feb 2019 05:08:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2019, 5 pages\\u00a7r"}']}
{title:'Wang et al. (§72019§r)', author: 'Xin Wang; Shinji Takaki; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:1810.11946", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural source-filter-based waveform model for statistical parametric speech synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oXin Wang\\nShinji Takaki\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.11946\\u00a7r\\n\\nVersion:\\u00a77v4 (Sat, 27 Apr 2019 02:00:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2019\\u00a7r"}']}
{title:'Yasuda et al. (§72019§r)', author: 'Yusuke Yasuda; Xin Wang; Shinji Takaki; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:1810.11960", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigation of enhanced Tacotron text-to-speech synthesis systems with self-attention for pitch accent language\\u00a7r\\n\\n\\u00a78\\u00a7oYusuke Yasuda\\nXin Wang\\nShinji Takaki\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.11960\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Feb 2019 09:27:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oto be appeared at ICASSP 2019\\u00a7r"}']}
{title:'Chen et al. (§72019§r)', author: 'Li-Wei Chen; Hung-Yi Lee; Yu Tsao', display:{Lore:['[{"text": "arXiv:1810.12656", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerative Adversarial Networks for Unpaired Voice Transformation on Impaired Speech\\u00a7r\\n\\n\\u00a78\\u00a7oLi-Wei Chen\\nHung-Yi Lee\\nYu Tsao\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.12656\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 22 Aug 2019 19:17:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished as a conference paper at INTERSPEECH 2019\\u00a7r"}']}
{title:'Hsieh et al. (§72019§r)', author: 'Tsung-Han Hsieh; Li Su; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:1810.12947", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Streamlined Encoder/Decoder Architecture for Melody Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oTsung-Han Hsieh\\nLi Su\\nYi-Hsuan Yang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.12947\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 18 Feb 2019 07:54:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis is a pre-print version of an ICASSP 2019 paper\\u00a7r"}']}
{title:'Li et al. (§72019§r)', author: 'Qiujia Li; Preben Ness; Anton Ragni; Mark Gales', display:{Lore:['[{"text": "arXiv:1810.13024", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBi-Directional Lattice Recurrent Neural Networks for Confidence Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oQiujia Li\\nPreben Ness\\nAnton Ragni\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1810.13024\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 18 Feb 2019 17:04:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2019\\u00a7r"}']}
{title:'Damskägg et al. (§72019§r)', author: 'Eero-Pekka Damskägg; Lauri Juvela; Etienne Thuillier; Vesa Välimäki', display:{Lore:['[{"text": "arXiv:1811.00334", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning for Tube Amplifier Emulation\\u00a7r\\n\\n\\u00a78\\u00a7oEero-Pekka Damsk\\u00e4gg\\nLauri Juvela\\nEtienne Thuillier\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.00334\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Feb 2019 08:34:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2019\\u00a7r"}']}
{title:"As'ad et al. (§72019§r)", author: "Hala As'ad; Martin Bouchard; Homayoun Kamkar-Parsi", display:{Lore:['[{"text": "arXiv:1811.01133", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Robust Target Linearly Constrained Minimum Variance Beamformer With Spatial Cues Preservation for Binaural Hearing Aids\\u00a7r\\n\\n\\u00a78\\u00a7oHala As\'ad\\nMartin Bouchard\\nHomayoun Kamkar-Parsi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.01133\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2019.2924321\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech and Language Processing\\n  (TASLP). 2019 Oct 1; 27(10):1549-63\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Nov 2019 14:38:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 16 figures\\u00a7r"}']}
{title:'He et al. (§72019§r)', author: 'Di He; Xuesong Yang; Boon Pang Lim; Yi Liang; Mark Hasegawa-Johnson; Deming Chen', display:{Lore:['[{"text": "arXiv:1811.02063", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhen CTC Training Meets Acoustic Landmarks\\u00a7r\\n\\n\\u00a78\\u00a7oDi He\\nXuesong Yang\\nBoon Pang Lim\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.02063\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 26 Feb 2019 21:37:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo Appear in ICASSP 2019; The first two authors contributed equally\\u00a7r"}']}
{title:'Koizumi et al. (§72019§r)', author: 'Yuma Koizumi; Noboru Harada; Yoichi Haneda', display:{Lore:['[{"text": "arXiv:1811.02438", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTrainable Adaptive Window Switching for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oYuma Koizumi\\nNoboru Harada\\nYoichi Haneda\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.02438\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 19 Feb 2019 23:56:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to the 44th International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2019)\\u00a7r"}']}
{title:'Yalta et al. (§72019§r)', author: 'Nelson Yalta; Shinji Watanabe; Takaaki Hori; Kazuhiro Nakadai; Tetsuya Ogata', display:{Lore:['[{"text": "arXiv:1811.02735", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCNN-based MultiChannel End-to-End Speech Recognition for everyday home environments\\u00a7r\\n\\n\\u00a78\\u00a7oNelson Yalta\\nShinji Watanabe\\nTakaaki Hori\\nKazuhiro Nakadai\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.02735\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 21 Jun 2019 00:52:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, EUSIPCO 2019\\u00a7r"}']}
{title:'Li et al. (§72019§r)', author: 'Lantian Li; Zhiyuan Tang; Ying Shi; Dong Wang', display:{Lore:['[{"text": "arXiv:1811.03258", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGaussian-Constrained training for speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oLantian Li\\nZhiyuan Tang\\nYing Shi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.03258\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 17 Feb 2019 12:58:11 GMT)\\u00a7r"}']}
{title:'Vestman et al. (§72019§r)', author: 'Ville Vestman; Bilal Soomro; Anssi Kanervisto; Ville Hautamäki; Tomi Kinnunen', display:{Lore:['[{"text": "arXiv:1811.03293", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWho Do I Sound Like? Showcasing Speaker Recognition Technology by YouTube Voice Search\\u00a7r\\n\\n\\u00a78\\u00a7oVille Vestman\\nBilal Soomro\\nAnssi Kanervisto\\nVille Hautam\\u00e4ki\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.03293\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 11 Feb 2019 03:17:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for presentation in ICASSP 2019\\u00a7r"}']}
{title:'Song et al. (§72019§r)', author: 'Eunwoo Song; Kyungguen Byun; Hong-Goo Kang', display:{Lore:['[{"text": "arXiv:1811.04769", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExcitNet vocoder: A neural excitation model for parametric speech synthesis systems\\u00a7r\\n\\n\\u00a78\\u00a7oEunwoo Song\\nKyungguen Byun\\nHong-Goo Kang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.04769\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 21 Aug 2019 09:56:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the conference of EUSIPCO2019. arXiv admin note: text overlap with arXiv:1811.03311\\u00a7r"}']}
{title:'Lorenzo-Trueba et al. (§72019§r)', author: 'Jaime Lorenzo-Trueba; Thomas Drugman; Javier Latorre; Thomas Merritt; Bartosz Putrycz; Roberto Barra-Chicote; Alexis Moinet; Vatsal Aggarwal', display:{Lore:['[{"text": "arXiv:1811.06292", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards achieving robust universal neural vocoding\\u00a7r\\n\\n\\u00a78\\u00a7oJaime Lorenzo-Trueba\\nThomas Drugman\\nJavier Latorre\\n+ 4 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.06292\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 4 Jul 2019 15:50:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 1 extra for references. Accepted on Interspeech 2019\\u00a7r"}']}
{title:'Ananthabhotla et al. (§72019§r)', author: 'Ishwarya Ananthabhotla; David B. Ramsay; Joseph A. Paradiso', display:{Lore:['[{"text": "arXiv:1811.06439", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHCU400: An Annotated Dataset for Exploring Aural Phenomenology Through Causal Uncertainty\\u00a7r\\n\\n\\u00a78\\u00a7oIshwarya Ananthabhotla\\nDavid B. Ramsay\\nJoseph A. Paradiso\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.06439\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8683147\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE International Conference on Acoustics, Speech and Signal\\n  Processing (ICASSP). IEEE, 2019\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 13 Nov 2019 03:35:50 GMT)\\u00a7r"}']}
{title:'Szoke et al. (§72019§r)', author: 'Igor Szoke; Miroslav Skacel; Ladislav Mosner; Jakub Paliesek; Jan "Honza" Cernocky', display:{Lore:['[{"text": "arXiv:1811.06795", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBuilding and Evaluation of a Real Room Impulse Response Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oIgor Szoke\\nMiroslav Skacel\\nLadislav Mosner\\nJakub Paliesek\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.06795\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JSTSP.2019.2917582\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 30 May 2019 11:01:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Journal of Selected Topics in Signal Processing, November 2018\\u00a7r"}']}
{title:'Chaman et al. (§72019§r)', author: 'Anadi Chaman; Yu-Jeh Liu; Jonah Casebeer; Ivan Dokmanić', display:{Lore:['[{"text": "arXiv:1811.07065", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultipath-enabled private audio with noise\\u00a7r\\n\\n\\u00a78\\u00a7oAnadi Chaman\\nYu-Jeh Liu\\nJonah Casebeer\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.07065\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 13 Mar 2019 08:21:34 GMT)\\u00a7r"}']}
{title:'Ravanelli et al. (§72019§r)', author: 'Mirco Ravanelli; Titouan Parcollet; Yoshua Bengio', display:{Lore:['[{"text": "arXiv:1811.07453", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe PyTorch-Kaldi Speech Recognition Toolkit\\u00a7r\\n\\n\\u00a78\\u00a7oMirco Ravanelli\\nTitouan Parcollet\\nYoshua Bengio\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.07453\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 15 Feb 2019 19:13:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2019\\u00a7r"}']}
{title:'Chen et al. (§72019§r)', author: 'Feiyang Chen; Ziqian Luo', display:{Lore:['[{"text": "arXiv:1811.08065", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Robust Heterogeneous Signal Features from Parallel Neural Network for Audio Sentiment Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oFeiyang Chen\\nZiqian Luo\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.08065\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 31 Jul 2019 06:00:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o21 pages, PR JOURNAL\\u00a7r"}']}
{title:'Menon et al. (§72019§r)', author: 'Raghav Menon; Herman Kamper; Ewald van der Westhuizen; John Quinn; Thomas Niesler', display:{Lore:['[{"text": "arXiv:1811.08284", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFeature exploration for almost zero-resource ASR-free keyword spotting using a multilingual bottleneck extractor and correspondence autoencoders\\u00a7r\\n\\n\\u00a78\\u00a7oRaghav Menon\\nHerman Kamper\\nEwald van der Westhuizen\\nJohn Quinn\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.08284\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 13 Jul 2019 01:58:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 2 tables, 38 references, Accepted at Interspeech 2019\\u00a7r"}']}
{title:'Loellmann et al. (§72019§r)', author: 'Heinrich W. Loellmann; Christine Evers; Alexander Schmidt; Hendrik Barfuss; Patrick A. Naylor; Walter Kellermann', display:{Lore:['[{"text": "arXiv:1811.08482", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProceedings of the LOCATA Challenge Workshop \\u2013 a satellite event of IWAENC 2018\\u00a7r\\n\\n\\u00a78\\u00a7oHeinrich W. Loellmann\\nChristine Evers\\nAlexander Schmidt\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.08482\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 20 Aug 2019 16:53:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWorkshop Proceedings\\u00a7r"}']}
{title:'Ravanelli et al. (§72019§r)', author: 'Mirco Ravanelli; Yoshua Bengio', display:{Lore:['[{"text": "arXiv:1811.09725", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInterpretable Convolutional Filters with SincNet\\u00a7r\\n\\n\\u00a78\\u00a7oMirco Ravanelli\\nYoshua Bengio\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.09725\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 9 Aug 2019 16:09:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of NIPS@IRASL 2018. arXivadmin note: substantial text overlap with arXiv:1808.00158\\u00a7r"}']}
{title:'Shon et al. (§72019§r)', author: 'Suwon Shon; Younggun Lee; Taesu Kim', display:{Lore:['[{"text": "arXiv:1811.10812", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.IR\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLarge-scale Speaker Retrieval on Random Speaker Variability Subspace\\u00a7r\\n\\n\\u00a78\\u00a7oSuwon Shon\\nYounggun Lee\\nTaesu Kim\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.10812\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 18 Jun 2019 01:57:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2019\\u00a7r"}']}
{title:'Huang et al. (§72019§r)', author: 'Wen-Chin Huang; Yi-Chiao Wu; Hsin-Te Hwang; Patrick Lumban Tobing; Tomoki Hayashi; Kazuhiro Kobayashi; Tomoki Toda; Yu Tsao; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:1811.11078", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRefined WaveNet Vocoder for Variational Autoencoder Based Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oWen-Chin Huang\\nYi-Chiao Wu\\nHsin-Te Hwang\\n+ 5 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.11078\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.23919/EUSIPCO.2019.8902651\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Jul 2019 08:30:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 7 figures, 1 table. Accepted to EUSIPCO 2019\\u00a7r"}']}
{title:'Grondin et al. (§72019§r)', author: 'Francois Grondin; James Glass', display:{Lore:['[{"text": "arXiv:1811.11785", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSVD-PHAT: A Fast Sound Source Localization Method\\u00a7r\\n\\n\\u00a78\\u00a7oFrancois Grondin\\nJames Glass\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.11785\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 2019 IEEE International Conference on\\n  Acoustics, Speech, and Signal Processing\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 11 Feb 2019 17:25:32 GMT)\\u00a7r"}']}
{title:'Wan et al. (§72019§r)', author: 'Li Wan; Prashant Sridhar; Yang Yu; Quan Wang; Ignacio Lopez Moreno', display:{Lore:['[{"text": "arXiv:1811.12290", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTuplemax Loss for Language Identification\\u00a7r\\n\\n\\u00a78\\u00a7oLi Wan\\nPrashant Sridhar\\nYang Yu\\nQuan Wang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1811.12290\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 17 Feb 2019 20:07:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2019\\u00a7r"}']}
{title:'Ravanelli et al. (§72019§r)', author: 'Mirco Ravanelli; Yoshua Bengio', display:{Lore:['[{"text": "arXiv:1812.00271", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Speaker Representations with Mutual Information\\u00a7r\\n\\n\\u00a78\\u00a7oMirco Ravanelli\\nYoshua Bengio\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1812.00271\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 5 Apr 2019 22:49:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2019\\u00a7r"}']}
{title:'Shon et al. (§72019§r)', author: 'Suwon Shon; Ahmed Ali; James Glass', display:{Lore:['[{"text": "arXiv:1812.01501", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDomain Attentive Fusion for End-to-end Dialect Identification with Unknown Target Domain\\u00a7r\\n\\n\\u00a78\\u00a7oSuwon Shon\\nAhmed Ali\\nJames Glass\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1812.01501\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 6 May 2019 15:36:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2019, revised typos\\u00a7r"}']}
{title:'Tian et al. (§72019§r)', author: 'Qiao Tian; Xucheng Wan; Shan Liu', display:{Lore:['[{"text": "arXiv:1812.02339", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerative Adversarial Network based Speaker Adaptation for High Fidelity WaveNet Vocoder\\u00a7r\\n\\n\\u00a78\\u00a7oQiao Tian\\nXucheng Wan\\nShan Liu\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1812.02339\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 19 Jul 2019 08:53:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figure, 1 table, 6 equations\\u00a7r"}']}
{title:'Wiesner et al. (§72019§r)', author: 'Matthew Wiesner; Adithya Renduchintala; Shinji Watanabe; Chunxi Liu; Najim Dehak; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:1812.03919", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPretraining by Backtranslation for End-to-end ASR in Low-Resource Settings\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew Wiesner\\nAdithya Renduchintala\\nShinji Watanabe\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1812.03919\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 2 Aug 2019 22:40:48 GMT)\\u00a7r"}']}
{title:'Deng et al. (§72019§r)', author: 'Yan Deng; Lei He; Frank Soong', display:{Lore:['[{"text": "arXiv:1812.05253", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModeling Multi-speaker Latent Space to Improve Neural TTS: Quick Enrolling New Speaker and Enhancing Premium Voice\\u00a7r\\n\\n\\u00a78\\u00a7oYan Deng\\nLei He\\nFrank Soong\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1812.05253\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 2 Sep 2019 02:30:19 GMT)\\u00a7r"}']}
{title:'Ravanelli et al. (§72019§r)', author: 'Mirco Ravanelli; Yoshua Bengio', display:{Lore:['[{"text": "arXiv:1812.05920", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech and Speaker Recognition from Raw Waveform with SincNet\\u00a7r\\n\\n\\u00a78\\u00a7oMirco Ravanelli\\nYoshua Bengio\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1812.05920\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 15 Feb 2019 19:48:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:1811.09725, arXiv:1808.00158\\u00a7r"}']}
{title:'Nakatani et al. (§72019§r)', author: 'Tomohiro Nakatani; Keisuke Kinoshita', display:{Lore:['[{"text": "arXiv:1812.08400", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA unified convolutional beamformer for simultaneous denoising and dereverberation\\u00a7r\\n\\n\\u00a78\\u00a7oTomohiro Nakatani\\nKeisuke Kinoshita\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1812.08400\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2019.2911179\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Signal Processing Letters, vol. 26, no. 6, pp. 903-907, June\\n  2019\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 5 May 2019 11:42:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in IEEESignal Processing Letters\\u00a7r"}']}
{title:'Kilgour et al. (§72019§r)', author: 'Kevin Kilgour; Mauricio Zuluaga; Dominik Roblek; Matthew Sharifi', display:{Lore:['[{"text": "arXiv:1812.08466", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFr\\u00e9chet Audio Distance: A Metric for Evaluating Music Enhancement Algorithms\\u00a7r\\n\\n\\u00a78\\u00a7oKevin Kilgour\\nMauricio Zuluaga\\nDominik Roblek\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1812.08466\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 17 Jan 2019 16:12:43 GMT)\\u00a7r"}']}
{title:'Mošner et al. (§72019§r)', author: 'Ladislav Mošner; Minhua Wu; Anirudh Raju; Sree Hari Krishnan Parthasarathi; Kenichi Kumatani; Shiva Sundaram; Roland Maas; Björn Hoffmeister', display:{Lore:['[{"text": "arXiv:1901.02348", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving noise robustness of automatic speech recognition via parallel data and teacher-student learning\\u00a7r\\n\\n\\u00a78\\u00a7oLadislav Mo\\u0161ner\\nMinhua Wu\\nAnirudh Raju\\n+ 4 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.02348\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 15 Mar 2019 20:16:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo Appear in ICASSP 2019\\u00a7r"}']}
{title:'Choe et al. (§72019§r)', author: 'Soyeon Choe; Soo-Whan Chung; Youna Ji; Hong-Goo Kang', display:{Lore:['[{"text": "arXiv:1901.04690", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOrthonormal Embedding-based Deep Clustering for Single-channel Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oSoyeon Choe\\nSoo-Whan Chung\\nYouna Ji\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.04690\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Jan 2019 07:31:54 GMT)\\u00a7r"}']}
{title:'Esterer et al. (§72019§r)', author: 'Nicholas Esterer; Philippe Depalle', display:{Lore:['[{"text": "arXiv:1901.05044", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA linear programming approach to the tracking of partials\\u00a7r\\n\\n\\u00a78\\u00a7oNicholas Esterer\\nPhilippe Depalle\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.05044\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Jan 2019 21:05:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 pdf figure\\u00a7r"}']}
{title:'Ma et al. (§72019§r)', author: 'Fei Ma; Wen Zhang; Thushara D. Abhayapala', display:{Lore:['[{"text": "arXiv:1901.05122", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-time separation of non-stationary sound fields on spheres\\u00a7r\\n\\n\\u00a78\\u00a7oFei Ma\\nWen Zhang\\nThushara D. Abhayapala\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.05122\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/1.5114819\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Jan 2019 03:13:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o34 pages, 15 figures\\u00a7r"}']}
{title:'Papayiannis et al. (§72019§r)', author: 'Constantinos Papayiannis; Christine Evers; Patrick A. Naylor', display:{Lore:['[{"text": "arXiv:1901.05852", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetecting Sound-Absorbing Materials in a Room from a Single Impulse Response using a CRNN\\u00a7r\\n\\n\\u00a78\\u00a7oConstantinos Papayiannis\\nChristine Evers\\nPatrick A. Naylor\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.05852\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 27 Oct 2019 22:19:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted for review for IEEE ICASSP 2020\\u00a7r"}']}
{title:'Strisciuglio et al. (§72019§r)', author: 'Nicola Strisciuglio; Mario Vento; Nicolai Petkov', display:{Lore:['[{"text": "arXiv:1901.06904", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning sound representations using trainable COPE feature extractors\\u00a7r\\n\\n\\u00a78\\u00a7oNicola Strisciuglio\\nMario Vento\\nNicolai Petkov\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.06904\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.patcog.2019.03.016\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPattern Recognition (2019)\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 22 Mar 2019 09:51:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in Pattern Recognition\\u00a7r"}']}
{title:'Valentini-Botinhao et al. (§72019§r)', author: 'Cassia Valentini-Botinhao; Mirjam Wester; Junichi Yamagishi; Markus Toman; Michael Pucher; Dietmar Schabus', display:{Lore:['[{"text": "arXiv:1901.07239", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon linear time compression of clear and normal speech at high rates\\u00a7r\\n\\n\\u00a78\\u00a7oCassia Valentini-Botinhao\\nMirjam Wester\\nJunichi Yamagishi\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.07239\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Jan 2019 10:16:58 GMT)\\u00a7r"}']}
{title:'Salazar et al. (§72019§r)', author: 'Julian Salazar; Katrin Kirchhoff; Zhiheng Huang', display:{Lore:['[{"text": "arXiv:1901.10055", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Attention Networks for Connectionist Temporal Classification in Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJulian Salazar\\nKatrin Kirchhoff\\nZhiheng Huang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.10055\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8682539\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 19 Feb 2019 10:12:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2019\\u00a7r"}']}
{title:'Rashno et al. (§72019§r)', author: 'Elyas Rashno; Ahmad Akbari; Babak Nasersharif', display:{Lore:['[{"text": "arXiv:1901.10629", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Convolutional Neural Network model based on Neutrosophy for Noisy Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oElyas Rashno\\nAhmad Akbari\\nBabak Nasersharif\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.10629\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/PRIA.2019.8786010\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 28 Feb 2019 10:41:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInternational conference on PatternRecognition and Image Analysis (IPRIA 2019)\\u00a7r"}']}
{title:'Nunes et al. (§72019§r)', author: 'João Antônio Chagas Nunes; David Macêdo; Cleber Zanchettin', display:{Lore:['[{"text": "arXiv:1901.10826", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdditive Margin SincNet for Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJo\\u00e3o Ant\\u00f4nio Chagas Nunes\\nDavid Mac\\u00eado\\nCleber Zanchettin\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1901.10826\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/IJCNN.2019.8852112\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2019 International Joint Conference on Neural Networks (IJCNN)\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Jan 2019 16:16:34 GMT)\\u00a7r"}']}
{title:'Yu et al. (§72019§r)', author: 'Chin-Yun Yu; Li Su', display:{Lore:['[{"text": "arXiv:1902.00539", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-layered Cepstrum for Instantaneous Frequency Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oChin-Yun Yu\\nLi Su\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.00539\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Feb 2019 20:03:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn 2018 6th IEEE Global Conference on Signal and Information Processing\\u00a7r"}']}
{title:'Bäckström (§72019§r)', author: 'Tom Bäckström', display:{Lore:['[{"text": "arXiv:1902.01053", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOverlap-Add Windows with Maximum Energy Concentration for Speech and Audio Processing\\u00a7r\\n\\n\\u00a78\\u00a7oTom B\\u00e4ckstr\\u00f6m\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.01053\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Feb 2019 06:58:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to Proc. ICASSP 2019\\u00a7r"}']}
{title:'Haubner et al. (§72019§r)', author: 'Thomas Haubner; Alexander Schmidt; Walter Kellermann', display:{Lore:['[{"text": "arXiv:1902.01299", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lActive Acoustic Source Tracking Exploiting Particle Filtering and Monte Carlo Tree Search\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Haubner\\nAlexander Schmidt\\nWalter Kellermann\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.01299\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 9 Sep 2019 09:37:02 GMT)\\u00a7r"}']}
{title:'Nguyen et al. (§72019§r)', author: 'Thai-Son Nguyen; Sebastian Stueker; Alex Waibel', display:{Lore:['[{"text": "arXiv:1902.01951", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing multi-task learning to improve the performance of acoustic-to-word and conventional hybrid models\\u00a7r\\n\\n\\u00a78\\u00a7oThai-Son Nguyen\\nSebastian Stueker\\nAlex Waibel\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.01951\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 15 May 2019 20:29:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted newer work which includes thispaper results\\u00a7r"}']}
{title:'Heo et al. (§72019§r)', author: 'Hee-Soo Heo; Jee-weon Jung; IL-Ho Yang; Sung-Hyun Yoon; Hye-jin Shim; Ha-Jin Yu', display:{Lore:['[{"text": "arXiv:1902.02455", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end losses based on speaker basis vectors and all-speaker hard negative mining for speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oHee-Soo Heo\\nJee-weon Jung\\nIL-Ho Yang\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.02455\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 17 Jul 2019 11:52:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages and 2 figures\\u00a7r"}']}
{title:'Thakur et al. (§72019§r)', author: 'Anshul Thakur; Pulkit Sharma; Vinayak Abrol; Padmanabhan Rajan', display:{Lore:['[{"text": "arXiv:1902.02498", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConv-codes: Audio Hashing For Bird Species Classification\\u00a7r\\n\\n\\u00a78\\u00a7oAnshul Thakur\\nPulkit Sharma\\nVinayak Abrol\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.02498\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Feb 2019 07:18:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for presentation at ICASSP 2019\\u00a7r"}']}
{title:'Rao et al. (§72019§r)', author: 'Wei Rao; Chenglin Xu; Eng Siong Chng; Haizhou Li', display:{Lore:['[{"text": "arXiv:1902.02546", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTarget Speaker Extraction for Overlapped Multi-Talker Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oWei Rao\\nChenglin Xu\\nEng Siong Chng\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.02546\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Feb 2019 09:55:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures. This paper is submitted to Interspeech 2019\\u00a7r"}']}
{title:'Tian et al. (§72019§r)', author: 'Xiaohai Tian; Eng Siong Chng; Haizhou Li', display:{Lore:['[{"text": "arXiv:1902.03705", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Vocoder-free WaveNet Voice Conversion with Non-Parallel Data\\u00a7r\\n\\n\\u00a78\\u00a7oXiaohai Tian\\nEng Siong Chng\\nHaizhou Li\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.03705\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 17 Sep 2019 08:32:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, This paper is submitted to INTERSPEECH 2019\\u00a7r"}']}
{title:'Nguyen et al. (§72019§r)', author: 'Duong Nguyen; Oliver S. Kirsebom; Fábio Frazão; Ronan Fablet; Stan Matwin', display:{Lore:['[{"text": "arXiv:1902.04980", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRecurrent Neural Networks with Stochastic Layers for Acoustic Novelty Detection\\u00a7r\\n\\n\\u00a78\\u00a7oDuong Nguyen\\nOliver S. Kirsebom\\nF\\u00e1bio Fraz\\u00e3o\\nRonan Fablet\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.04980\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8682901\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Feb 2019 16:13:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2019\\u00a7r"}']}
{title:'Cazau (§72019§r)', author: 'D. Cazau', display:{Lore:['[{"text": "arXiv:1902.06659", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a75astro-ph.IM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTheory-plus-code documentation of the DEPAM workflow for soundscape description\\u00a7r\\n\\n\\u00a78\\u00a7oD. Cazau\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.06659\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Feb 2019 15:45:36 GMT)\\u00a7r"}']}
{title:'Malik (§72019§r)', author: 'Hafiz Malik', display:{Lore:['[{"text": "arXiv:1902.06782", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSecuring Voice-driven Interfaces against Fake (Cloned) Audio Attacks\\u00a7r\\n\\n\\u00a78\\u00a7oHafiz Malik\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.06782\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Feb 2019 20:10:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, The 2nd IEEE International Workshopon \\"Fake MultiMedia\\" (FakeMM\'19) March 28-30, 2019, San Jose, CA, USA\\u00a7r"}']}
{title:'Guo et al. (§72019§r)', author: 'Jinxi Guo; Tara N. Sainath; Ron J. Weiss', display:{Lore:['[{"text": "arXiv:1902.07178", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA spelling correction model for end-to-end speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJinxi Guo\\nTara N. Sainath\\nRon J. Weiss\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.07178\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Feb 2019 18:18:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2019\\u00a7r"}']}
{title:'Cai et al. (§72019§r)', author: 'Weicheng Cai; Danwei Cai; Shen Huang; Ming Li', display:{Lore:['[{"text": "arXiv:1902.07374", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUtterance-level end-to-end language identification using attention-based CNN-BLSTM\\u00a7r\\n\\n\\u00a78\\u00a7oWeicheng Cai\\nDanwei Cai\\nShen Huang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.07374\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Feb 2019 02:14:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP 2019\\u00a7r"}']}
{title:'von Neumann et al. (§72019§r)', author: 'Thilo von Neumann; Keisuke Kinoshita; Marc Delcroix; Shoko Araki; Tomohiro Nakatani; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:1902.07881", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAll-neural online source separation, counting, and diarization for meeting analysis\\u00a7r\\n\\n\\u00a78\\u00a7oThilo von Neumann\\nKeisuke Kinoshita\\nMarc Delcroix\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.07881\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Feb 2019 06:32:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, to appear in ICASSP2019\\u00a7r"}']}
{title:'Dawalatabad et al. (§72019§r)', author: 'Nauman Dawalatabad; Srikanth Madikeri; C Chandra Sekhar; Hema A Murthy', display:{Lore:['[{"text": "arXiv:1902.08051", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIncremental Transfer Learning in Two-pass Information Bottleneck based Speaker Diarization System for Meetings\\u00a7r\\n\\n\\u00a78\\u00a7oNauman Dawalatabad\\nSrikanth Madikeri\\nC Chandra Sekhar\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.08051\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8683114\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Feb 2019 13:55:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, To appear in Proc. ICASSP 2019, May 12-17, 2019, Brighton, UK\\u00a7r"}']}
{title:'Fang et al. (§72019§r)', author: 'Xin Fang; Liang Zou; Jin Li; Lei Sun; Zhen-Hua Ling', display:{Lore:['[{"text": "arXiv:1902.09074", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChannel adversarial training for cross-channel text-independent speaker recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXin Fang\\nLiang Zou\\nJin Li\\nLei Sun\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.09074\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Feb 2019 03:32:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 2 tabels\\u00a7r"}']}
{title:'Thakur et al. (§72019§r)', author: 'Anshul Thakur; Padmanabhan Rajan', display:{Lore:['[{"text": "arXiv:1902.09765", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDirectional Embedding Based Semi-supervised Framework For Bird Vocalization Segmentation\\u00a7r\\n\\n\\u00a78\\u00a7oAnshul Thakur\\nPadmanabhan Rajan\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.09765\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Feb 2019 07:08:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in Applied Acoustics\\u00a7r"}']}
{title:'Xie et al. (§72019§r)', author: 'Weidi Xie; Arsha Nagrani; Joon Son Chung; Andrew Zisserman', display:{Lore:['[{"text": "arXiv:1902.10107", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUtterance-level Aggregation For Speaker Recognition In The Wild\\u00a7r\\n\\n\\u00a78\\u00a7oWeidi Xie\\nArsha Nagrani\\nJoon Son Chung\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.10107\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 17 May 2019 19:13:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in: InternationalConference on Acoustics, Speech, and Signal Processing (ICASSP), 2019. (Oral Presentation)\\u00a7r"}']}
{title:'Nandwana et al. (§72019§r)', author: 'Mahesh Kumar Nandwana; Julien van Hout; Mitchell McLaren; Colleen Richey; Aaron Lawson; Maria Alejandra Barrios', display:{Lore:['[{"text": "arXiv:1902.10828", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe VOiCES from a Distance Challenge 2019 Evaluation Plan\\u00a7r\\n\\n\\u00a78\\u00a7oMahesh Kumar Nandwana\\nJulien van Hout\\nMitchell McLaren\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.10828\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Feb 2019 23:22:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSpecial Session for Interspeech 2019\\u00a7r"}']}
{title:'Polyzos et al. (§72019§r)', author: 'Konstantinos Polyzos; Evangelos Dermatas', display:{Lore:['[{"text": "arXiv:1902.11130", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-Time detection, classification and DOA estimation of Unmanned Aerial Vehicle\\u00a7r\\n\\n\\u00a78\\u00a7oKonstantinos Polyzos\\nEvangelos Dermatas\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1902.11130\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Feb 2019 11:41:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oACOUSTICS 2018, Oral Presentation\\u00a7r"}']}
{title:'Bous et al. (§72019§r)', author: 'Frederik Bous; Axel Roebel', display:{Lore:['[{"text": "arXiv:1903.01161", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalysing Deep Learning-Spectral Envelope Prediction Methods for Singing Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oFrederik Bous\\nAxel Roebel\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.01161\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPublished in Proceedings of the 27th European Signal Processing\\n  Conference (EUSIPCO), 2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Mar 2019 10:23:55 GMT)\\u00a7r"}']}
{title:'Corey et al. (§72019§r)', author: 'Ryan M. Corey; Naoki Tsuda; Andrew C. Singer', display:{Lore:['[{"text": "arXiv:1903.02094", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic Impulse Responses for Wearable Audio Devices\\u00a7r\\n\\n\\u00a78\\u00a7oRyan M. Corey\\nNaoki Tsuda\\nAndrew C. Singer\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.02094\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8682733\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2019 IEEE International Conference on Acoustics, Speech and Signal\\n  Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Mar 2019 22:38:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at ICASSP 2019\\u00a7r"}']}
{title:'Chen et al. (§72019§r)', author: 'Xin Chen; Wei Chu; Jinxi Guo; Ning Xu', display:{Lore:['[{"text": "arXiv:1903.04124", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSinging voice conversion with non-parallel data\\u00a7r\\n\\n\\u00a78\\u00a7oXin Chen\\nWei Chu\\nJinxi Guo\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.04124\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Mar 2019 04:52:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to MIPR2019\\u00a7r"}']}
{title:'Wang et al. (§72019§r)', author: 'Peidong Wang; Ke Tan; DeLiang Wang', display:{Lore:['[{"text": "arXiv:1903.04567", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBridging the Gap Between Monaural Speech Enhancement and Recognition with Distortion-Independent Acoustic Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oPeidong Wang\\nKe Tan\\nDeLiang Wang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.04567\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 13 Mar 2019 00:34:13 GMT)\\u00a7r"}']}
{title:'Wu et al. (§72019§r)', author: 'Minhua Wu; Kenichi Kumatani; Shiva Sundaram; Nikko Strom; Bjorn Hoffmeister', display:{Lore:['[{"text": "arXiv:1903.05299", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrequency Domain Multi-channel Acoustic Modeling for Distant Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMinhua Wu\\nKenichi Kumatani\\nShiva Sundaram\\nNikko Strom\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.05299\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8682977\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. IEEE International Conference on Acoustics, Speech and\\n  Signal Processing (ICASSP) 2019, pages 6640-6644\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 28 Apr 2019 20:42:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2019, 5 pages\\u00a7r"}']}
{title:'Masuyama et al. (§72019§r)', author: 'Yoshiki Masuyama; Kohei Yatabe; Yasuhiro Oikawa', display:{Lore:['[{"text": "arXiv:1903.05600", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhase-aware Harmonic/Percussive Source Separation via Convex Optimization\\u00a7r\\n\\n\\u00a78\\u00a7oYoshiki Masuyama\\nKohei Yatabe\\nYasuhiro Oikawa\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.05600\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Mar 2019 17:01:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, to appear in IEEE ICASSP 2019 (Paper Code: AASP-P16.5, Session: Music Signal Analysis, Feedback and Echo Cancellation and Equalization)\\u00a7r"}']}
{title:'Masuyama et al. (§72019§r)', author: 'Yoshiki Masuyama; Kohei Yatabe; Yasuhiro Oikawa', display:{Lore:['[{"text": "arXiv:1903.05603", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-rankness of Complex-valued Spectrogram and Its Application to Phase-aware Audio Processing\\u00a7r\\n\\n\\u00a78\\u00a7oYoshiki Masuyama\\nKohei Yatabe\\nYasuhiro Oikawa\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.05603\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Mar 2019 17:06:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, to appear in IEEE ICASSP 2019 (Paper Code: AASP-P13.9, Session: Acoustic Scene Classification and MusicSignal Analysis)\\u00a7r"}']}
{title:'Bollepalli et al. (§72019§r)', author: 'Bajibabu Bollepalli; Lauri Juvela; Paavo Alku', display:{Lore:['[{"text": "arXiv:1903.05955", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerative adversarial network-based glottal waveform model for statistical parametric speech synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oBajibabu Bollepalli\\nLauri Juvela\\nPaavo Alku\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.05955\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2017-1288\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInterspeech-2017\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Mar 2019 12:53:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in Interspeech\\u00a7r"}']}
{title:'Kumatani et al. (§72019§r)', author: 'Kenichi Kumatani; Minhua Wu; Shiva Sundaram; Nikko Strom; Bjorn Hoffmeister', display:{Lore:['[{"text": "arXiv:1903.06539", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Geometry Spatial Acoustic Modeling for Distant Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oKenichi Kumatani\\nMinhua Wu\\nShiva Sundaram\\nNikko Strom\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.06539\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8682294\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. IEEE International Conference on Acoustics, Speech and\\n  Signal Processing (ICASSP) 2019, page 6635-6639\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 28 Apr 2019 19:40:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP2019, 5 pages. arXiv admin note:substantial text overlap with arXiv:1903.05299\\u00a7r"}']}
{title:'Avila et al. (§72019§r)', author: 'Anderson R. Avila; Hannes Gamper; Chandan Reddy; Ross Cutler; Ivan Tashev; Johannes Gehrke', display:{Lore:['[{"text": "arXiv:1903.06908", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-intrusive speech quality assessment using neural networks\\u00a7r\\n\\n\\u00a78\\u00a7oAnderson R. Avila\\nHannes Gamper\\nChandan Reddy\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.06908\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 16 Mar 2019 11:10:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2019\\u00a7r"}']}
{title:'Takeuchi et al. (§72019§r)', author: 'Daiki Takeuchi; Kohei Yatabe; Yuma Koizumi; Yasuhiro Oikawa; Noboru Harada', display:{Lore:['[{"text": "arXiv:1903.08876", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData-driven design of perfect reconstruction filterbank for DNN-based sound source enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oDaiki Takeuchi\\nKohei Yatabe\\nYuma Koizumi\\nYasuhiro Oikawa\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.08876\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Mar 2019 08:50:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, to appear in IEEE ICASSP 2019 (Paper Code: AASP-P8.8, Session: Spatial Audio, Audio Enhancement and Bandwidth Extension)\\u00a7r"}']}
{title:'Tu et al. (§72019§r)', author: 'Ming Tu; Yun Tang; Jing Huang; Xiaodong He; Bowen Zhou', display:{Lore:['[{"text": "arXiv:1903.09606", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards adversarial learning of speaker-invariant representation for speech emotion recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMing Tu\\nYun Tang\\nJing Huang\\nXiaodong He\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.09606\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Mar 2019 17:04:57 GMT)\\u00a7r"}']}
{title:'Xu et al. (§72019§r)', author: 'Chenglin Xu; Wei Rao; Eng Siong Chng; Haizhou Li', display:{Lore:['[{"text": "arXiv:1903.09952", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimization of Speaker Extraction Neural Network with Magnitude and Temporal Spectrum Approximation Loss\\u00a7r\\n\\n\\u00a78\\u00a7oChenglin Xu\\nWei Rao\\nEng Siong Chng\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.09952\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 24 Mar 2019 09:25:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ICASSP 2019\\u00a7r"}']}
{title:'Qin et al. (§72019§r)', author: 'Yao Qin; Nicholas Carlini; Ian Goodfellow; Garrison Cottrell; Colin Raffel', display:{Lore:['[{"text": "arXiv:1903.10346", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYao Qin\\nNicholas Carlini\\nIan Goodfellow\\nGarrison Cottrell\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.10346\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 7 Jun 2019 17:43:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInternational Conference on MachineLearning (ICML), 2019\\u00a7r"}']}
{title:'Thakur et al. (§72019§r)', author: 'Anshul Thakur; Daksh Thapar; Padmanabhan Rajan; Aditya Nigam', display:{Lore:['[{"text": "arXiv:1903.10713", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultiscale CNN based Deep Metric Learning for Bioacoustic Classification: Overcoming Training Data Scarcity Using Dynamic Triplet Loss\\u00a7r\\n\\n\\u00a78\\u00a7oAnshul Thakur\\nDaksh Thapar\\nPadmanabhan Rajan\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.10713\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/1.5118245\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 27 Mar 2019 05:48:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder Review at JASA. Primitiveversion of paper. We are still working on getting better performances out of the comparative methods\\u00a7r"}']}
{title:'Kostallari et al. (§72019§r)', author: 'Krist Kostallari; Etienne Parizet; Patrick Chevret; Jean-Noël Amato; Edith Galy', display:{Lore:['[{"text": "arXiv:1903.11386", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIrrelevant speech effect in open plan offices: A laboratory study\\u00a7r\\n\\n\\u00a78\\u00a7oKrist Kostallari\\nEtienne Parizet\\nPatrick Chevret\\nJean-No\\u00ebl Amato\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.11386\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n24th INTERNATIONAL CONGRESS ON SOUND AND VIBRATION, Jul 2017,\\n  London, United Kingdom\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Mar 2019 07:58:12 GMT)\\u00a7r"}']}
{title:'You et al. (§72019§r)', author: 'Lanhua You; Wu Guo; Lirong Dai; Jun Du', display:{Lore:['[{"text": "arXiv:1903.12058", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Task Learning with High-Order Statistics for X-vector based Text-Independent Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oLanhua You\\nWu Guo\\nLirong Dai\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.12058\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 4 Apr 2019 15:37:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages,2 figures, submitted to INTERSPEECH 2019\\u00a7r"}']}
{title:'Valin et al. (§72019§r)', author: 'Jean-Marc Valin; Jan Skoglund', display:{Lore:['[{"text": "arXiv:1903.12087", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Real-Time Wideband Neural Vocoder at 1.6 kb/s Using LPCNet\\u00a7r\\n\\n\\u00a78\\u00a7oJean-Marc Valin\\nJan Skoglund\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.12087\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 28 Jun 2019 02:14:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for Interspeech 2019, 5 pages\\u00a7r"}']}
{title:'You et al. (§72019§r)', author: 'Lanhua You; Wu Guo; Lirong Dai; Jun Du', display:{Lore:['[{"text": "arXiv:1903.12092", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Neural Network Embeddings with Gating Mechanisms for Text-Independent Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oLanhua You\\nWu Guo\\nLirong Dai\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.12092\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 4 Apr 2019 15:28:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, submitted to INTERSPEECH 2019\\u00a7r"}']}
{title:'P. et al. (§72019§r)', author: 'Prathosh A. P.; Varun Srivastava; Mayank Mishra', display:{Lore:['[{"text": "arXiv:1903.12248", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Approximate Inference for Speech to Electroglottograph Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oPrathosh A. P.\\nVarun Srivastava\\nMayank Mishra\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.12248\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2019.2942140\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 7 Sep 2019 20:04:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEE/ACM Transactionson Audio, Speech and Language Processing\\u00a7r"}']}
{title:'Zhao et al. (§72019§r)', author: 'Yi Zhao; Atsushi Ando; Shinji Takaki; Junichi Yamagishi; Satoshi Kobashikawa', display:{Lore:['[{"text": "arXiv:1903.12316", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDoes the Lombard Effect Improve Emotional Communication in Noise? - Analysis of Emotional Speech Acted in Noise -\\u00a7r\\n\\n\\u00a78\\u00a7oYi Zhao\\nAtsushi Ando\\nShinji Takaki\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.12316\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 Apr 2019 09:02:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2019, Graz, Austria\\u00a7r"}']}
{title:'Zhang et al. (§72019§r)', author: 'Mingyang Zhang; Xin Wang; Fuming Fang; Haizhou Li; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:1903.12389", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint training framework for text-to-speech and voice conversion using multi-source Tacotron and WaveNet\\u00a7r\\n\\n\\u00a78\\u00a7oMingyang Zhang\\nXin Wang\\nFuming Fang\\nHaizhou Li\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.12389\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 7 Apr 2019 23:40:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2019, Graz, Austria\\u00a7r"}']}
{title:'Takaki et al. (§72019§r)', author: 'Shinji Takaki; Hirokazu Kameoka; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:1903.12392", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTraining a Neural Speech Waveform Model using Spectral Losses of Short-Time Fourier Transform and Continuous Wavelet Transform\\u00a7r\\n\\n\\u00a78\\u00a7oShinji Takaki\\nHirokazu Kameoka\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.12392\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 7 Apr 2019 23:37:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2019, Graz, Austria\\u00a7r"}']}
{title:'You et al. (§72019§r)', author: 'Lanhua You; Bin Gu; Wu Guo', display:{Lore:['[{"text": "arXiv:1903.12428", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUSTCSpeech System for VOiCES from a Distance Challenge 2019\\u00a7r\\n\\n\\u00a78\\u00a7oLanhua You\\nBin Gu\\nWu Guo\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1903.12428\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Mar 2019 10:05:13 GMT)\\u00a7r"}']}
{title:'Qin et al. (§72019§r)', author: 'Ying Qin; Yuzhong Wu; Tan Lee; Anthony Pak Hin Kong', display:{Lore:['[{"text": "arXiv:1904.00361", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn End-to-End Approach to Automatic Speech Assessment for Cantonese-speaking People with Aphasia\\u00a7r\\n\\n\\u00a78\\u00a7oYing Qin\\nYuzhong Wu\\nTan Lee\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.00361\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 31 Mar 2019 09:04:17 GMT)\\u00a7r"}']}
{title:'Luong et al. (§72019§r)', author: 'Hieu-Thi Luong; Xin Wang; Junichi Yamagishi; Nobuyuki Nishizawa', display:{Lore:['[{"text": "arXiv:1904.00771", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTraining Multi-Speaker Neural Text-to-Speech Systems using Speaker-Imbalanced Speech Corpora\\u00a7r\\n\\n\\u00a78\\u00a7oHieu-Thi Luong\\nXin Wang\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.00771\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 7 Apr 2019 23:35:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2019, Graz, Austria\\u00a7r"}']}
{title:'Yu et al. (§72019§r)', author: 'Wangyang Yu; W. Bastiaan Kleijn', display:{Lore:['[{"text": "arXiv:1904.00869", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRoom Geometry Estimation from Room Impulse Responses using Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oWangyang Yu\\nW. Bastiaan Kleijn\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.00869\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 15 May 2019 07:59:20 GMT)\\u00a7r"}']}
{title:'Maiti et al. (§72019§r)', author: 'Soumi Maiti; Michael I Mandel', display:{Lore:['[{"text": "arXiv:1904.01537", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech denoising by parametric resynthesis\\u00a7r\\n\\n\\u00a78\\u00a7oSoumi Maiti\\nMichael I Mandel\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.01537\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Apr 2019 16:50:46 GMT)\\u00a7r"}']}
{title:'Nguyen et al. (§72019§r)', author: 'Thai-Son Nguyen; Sebastian Stueker; Alex Waibel', display:{Lore:['[{"text": "arXiv:1904.02147", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Shared Encoding Representation for End-to-End Speech Recognition Models\\u00a7r\\n\\n\\u00a78\\u00a7oThai-Son Nguyen\\nSebastian Stueker\\nAlex Waibel\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.02147\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 31 Mar 2019 20:46:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:1902.01951\\u00a7r"}']}
{title:'Narisetty et al. (§72019§r)', author: 'Chaitanya Narisetty; Tatsuya Komatsu; Reishi Kondo', display:{Lore:['[{"text": "arXiv:1904.02852", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModelling of Sound Events with Hidden Imbalances Based on Clustering and Separate Sub-Dictionary Learning\\u00a7r\\n\\n\\u00a78\\u00a7oChaitanya Narisetty\\nTatsuya Komatsu\\nReishi Kondo\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.02852\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Apr 2019 02:44:59 GMT)\\u00a7r"}']}
{title:'Romero et al. (§72019§r)', author: 'Hector E. Romero; Ning Ma; Guy J. Brown; Amy V. Beeston; Madina Hasan', display:{Lore:['[{"text": "arXiv:1904.02992", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning Features for Robust Detection of Acoustic Events in Sleep-Disordered Breathing\\u00a7r\\n\\n\\u00a78\\u00a7oHector E. Romero\\nNing Ma\\nGuy J. Brown\\nAmy V. Beeston\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.02992\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Apr 2019 11:07:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE ICASSP 2018\\u00a7r"}']}
{title:'Ma et al. (§72019§r)', author: 'Ning Ma; Tobias May; Guy J. Brown', display:{Lore:['[{"text": "arXiv:1904.03001", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting Deep Neural Networks and Head Movements for Robust Binaural Localisation of Multiple Sources in Reverberant Environments\\u00a7r\\n\\n\\u00a78\\u00a7oNing Ma\\nTobias May\\nGuy J. Brown\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.03001\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2017.2750760\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol. 25, no. 12, pp. 2444-2453, 2017\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Apr 2019 11:38:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages\\u00a7r"}']}
{title:'Ma et al. (§72019§r)', author: 'Ning Ma; Jose A. Gonzalez; Guy J. Brown', display:{Lore:['[{"text": "arXiv:1904.03006", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Binaural Localization of a Target Sound Source by Combining Spectral Source Models and Deep Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oNing Ma\\nJose A. Gonzalez\\nGuy J. Brown\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.03006\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2018.2855960\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio Speech and Language Processing,\\n  vol. 26, no. 11, pp. 2122-2131, 2018\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Apr 2019 11:50:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages\\u00a7r"}']}
{title:'Wood et al. (§72019§r)', author: 'Sean U. N. Wood; Jean Rouat', display:{Lore:['[{"text": "arXiv:1904.03130", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Low Latency Speech Enhancement with RT-GCC-NMF\\u00a7r\\n\\n\\u00a78\\u00a7oSean U. N. Wood\\nJean Rouat\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.03130\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JSTSP.2019.2909193\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Apr 2019 15:54:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in the IEEE JSTSP Special Issue on Data Science: Machine Learning for Audio Signal Processing\\u00a7r"}']}
{title:'Li et al. (§72019§r)', author: 'Jason Li; Vitaly Lavrukhin; Boris Ginsburg; Ryan Leary; Oleksii Kuchaiev; Jonathan M. Cohen; Huyen Nguyen; Ravi Teja Gadde', display:{Lore:['[{"text": "arXiv:1904.03288", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJasper: An End-to-End Convolutional Neural Acoustic Model\\u00a7r\\n\\n\\u00a78\\u00a7oJason Li\\nVitaly Lavrukhin\\nBoris Ginsburg\\n+ 4 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.03288\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 27 Aug 2019 00:02:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2019\\u00a7r"}']}
{title:'Shon et al. (§72019§r)', author: 'Suwon Shon; Hao Tang; James Glass', display:{Lore:['[{"text": "arXiv:1904.03601", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoiceID Loss: Speech Enhancement for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oSuwon Shon\\nHao Tang\\nJames Glass\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.03601\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 5 Jul 2019 17:23:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ointerspeech 2019; demo link : https://people.csail.mit.edu/swshon/supplement/voiceid-loss\\u00a7r"}']}
{title:'Lugosch et al. (§72019§r)', author: 'Loren Lugosch; Mirco Ravanelli; Patrick Ignoto; Vikrant Singh Tomar; Yoshua Bengio', display:{Lore:['[{"text": "arXiv:1904.03670", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Model Pre-training for End-to-End Spoken Language Understanding\\u00a7r\\n\\n\\u00a78\\u00a7oLoren Lugosch\\nMirco Ravanelli\\nPatrick Ignoto\\nVikrant Singh Tomar\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.03670\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 Jul 2019 17:56:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2019\\u00a7r"}']}
{title:'Wu et al. (§72019§r)', author: 'Jian Wu; Yong Xu; Shi-Xiong Zhang; Lian-Wu Chen; Meng Yu; Lei Xie; Dong Yu', display:{Lore:['[{"text": "arXiv:1904.03760", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTime Domain Audio Visual Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oJian Wu\\nYong Xu\\nShi-Xiong Zhang\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.03760\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 22 Sep 2019 14:32:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ASRU 2019\\u00a7r"}']}
{title:'Wu et al. (§72019§r)', author: 'Jian Wu; Yong Xu; Shi-Xiong Zhang; Lian-Wu Chen; Meng Yu; Lei Xie; Dong Yu', display:{Lore:['[{"text": "arXiv:1904.03792", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Speaker-Dependent Separation for CHiME-5 Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oJian Wu\\nYong Xu\\nShi-Xiong Zhang\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.03792\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Apr 2019 01:21:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2019, Graz, Austria\\u00a7r"}']}
{title:'Juvela et al. (§72019§r)', author: 'Lauri Juvela; Bajibabu Bollepalli; Junichi Yamagishi; Paavo Alku', display:{Lore:['[{"text": "arXiv:1904.03976", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGELP: GAN-Excited Linear Prediction for Speech Synthesis from Mel-spectrogram\\u00a7r\\n\\n\\u00a78\\u00a7oLauri Juvela\\nBajibabu Bollepalli\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.03976\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 26 Jun 2019 14:25:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2019 accepted version\\u00a7r"}']}
{title:'Biadsy et al. (§72019§r)', author: 'Fadi Biadsy; Ron J. Weiss; Pedro J. Moreno; Dimitri Kanevsky; Ye Jia', display:{Lore:['[{"text": "arXiv:1904.04169", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParrotron: An End-to-End Speech-to-Speech Conversion Model and its Applications to Hearing-Impaired Speech and Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oFadi Biadsy\\nRon J. Weiss\\nPedro J. Moreno\\nDimitri Kanevsky\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.04169\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 29 Oct 2019 13:37:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, submitted to Interspeech 2019\\u00a7r"}']}
{title:'Novotny et al. (§72019§r)', author: 'Ondrej Novotny; Oldrich Plchot; Ondrej Glembek; Lukas Burget', display:{Lore:['[{"text": "arXiv:1904.04235", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFactorization of Discriminatively Trained i-vector Extractor for Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oOndrej Novotny\\nOldrich Plchot\\nOndrej Glembek\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.04235\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Apr 2019 22:04:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2019, Graz, Austria. arXiv admin note: substantial text overlap with arXiv:1810.13183\\u00a7r"}']}
{title:'Shon et al. (§72019§r)', author: 'Suwon Shon; Najim Dehak; Douglas Reynolds; James Glass', display:{Lore:['[{"text": "arXiv:1904.04240", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMCE 2018: The 1st Multi-target Speaker Detection and Identification Challenge Evaluation\\u00a7r\\n\\n\\u00a78\\u00a7oSuwon Shon\\nNajim Dehak\\nDouglas Reynolds\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.04240\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 7 Apr 2019 08:09:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ohttp://mce.csail.mit.edu . arXivadmin note: text overlapwith arXiv:1807.06663\\u00a7r"}']}
{title:'Okawa et al. (§72019§r)', author: 'Masaki Okawa; Takuya Saito; Naoki Sawada; Hiromitsu Nishizaki', display:{Lore:['[{"text": "arXiv:1904.04364", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Classification of Bit-Representation Waveform\\u00a7r\\n\\n\\u00a78\\u00a7oMasaki Okawa\\nTakuya Saito\\nNaoki Sawada\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.04364\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 18 Sep 2019 14:22:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH2019\\u00a7r"}']}
{title:'Yamamoto et al. (§72019§r)', author: 'Ryuichi Yamamoto; Eunwoo Song; Jae-Min Kim', display:{Lore:['[{"text": "arXiv:1904.04472", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProbability density distillation with generative adversarial networks for high-quality parallel waveform generation\\u00a7r\\n\\n\\u00a78\\u00a7oRyuichi Yamamoto\\nEunwoo Song\\nJae-Min Kim\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.04472\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 28 Aug 2019 03:52:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the conference of INTERSPEECH 2019\\u00a7r"}']}
{title:'Llombart et al. (§72019§r)', author: 'Jorge Llombart; Dayana Ribas; Antonio Miguel; Luis Vicente; Alfonso Ortega; Eduardo Lleida', display:{Lore:['[{"text": "arXiv:1904.04511", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProgressive Speech Enhancement with Residual Connections\\u00a7r\\n\\n\\u00a78\\u00a7oJorge Llombart\\nDayana Ribas\\nAntonio Miguel\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.04511\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Apr 2019 08:04:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures\\u00a7r"}']}
{title:'Chettri et al. (§72019§r)', author: 'Bhusan Chettri; Daniel Stoller; Veronica Morfi; Marco A. Martínez Ramírez; Emmanouil Benetos; Bob L. Sturm', display:{Lore:['[{"text": "arXiv:1904.04589", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnsemble Models for Spoofing Detection in Automatic Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oBhusan Chettri\\nDaniel Stoller\\nVeronica Morfi\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.04589\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 4 Jul 2019 08:32:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2019, Graz, Austria\\u00a7r"}']}
{title:'Llombart et al. (§72019§r)', author: 'Jorge Llombart; Dayana Ribas; Antonio Miguel; Luis Vicente; Alfonso Ortega; Eduardo Lleida', display:{Lore:['[{"text": "arXiv:1904.05167", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Enhancement with Wide Residual Networks in Reverberant Environments\\u00a7r\\n\\n\\u00a78\\u00a7oJorge Llombart\\nDayana Ribas\\nAntonio Miguel\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.05167\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Apr 2019 07:46:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures. arXiv admin note: text overlapwith arXiv:1901.00660, arXiv:1904.04511\\u00a7r"}']}
{title:'Todisco et al. (§72019§r)', author: 'Massimiliano Todisco; Xin Wang; Ville Vestman; Md Sahidullah; Hector Delgado; Andreas Nautsch; Junichi Yamagishi; Nicholas Evans; Tomi Kinnunen; Kong Aik Lee', display:{Lore:['[{"text": "arXiv:1904.05441", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lASVspoof 2019: Future Horizons in Spoofed and Fake Audio Detection\\u00a7r\\n\\n\\u00a78\\u00a7oMassimiliano Todisco\\nXin Wang\\nVille Vestman\\n+ 6 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.05441\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2019\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 14 Apr 2019 11:38:32 GMT)\\u00a7r"}']}
{title:'Park et al. (§72019§r)', author: 'Jong-Hyeon Park; Myungwoo Oh; Hyung-Min Park', display:{Lore:['[{"text": "arXiv:1904.06086", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Speech Domain Adaptation Based on Disentangled Representation Learning for Robust Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJong-Hyeon Park\\nMyungwoo Oh\\nHyung-Min Park\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.06086\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Apr 2019 07:55:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2019\\u00a7r"}']}
{title:'Mimilakis et al. (§72019§r)', author: 'Stylianos Ioannis Mimilakis; Konstantinos Drossos; Estefanía Cano; Gerald Schuller', display:{Lore:['[{"text": "arXiv:1904.06157", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExamining the Mapping Functions of Denoising Autoencoders in Singing Voice Separation\\u00a7r\\n\\n\\u00a78\\u00a7oStylianos Ioannis Mimilakis\\nKonstantinos Drossos\\nEstefan\\u00eda Cano\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.06157\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 20 Oct 2019 14:41:05 GMT)\\u00a7r"}']}
{title:'Yoshioka et al. (§72019§r)', author: 'Takuya Yoshioka; Zhuo Chen; Changliang Liu; Xiong Xiao; Hakan Erdogan; Dimitrios Dimitriadis', display:{Lore:['[{"text": "arXiv:1904.06478", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Latency Speaker-Independent Continuous Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oTakuya Yoshioka\\nZhuo Chen\\nChangliang Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.06478\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 13 Apr 2019 04:24:12 GMT)\\u00a7r"}']}
{title:'Farzaneh et al. (§72019§r)', author: 'Majid Farzaneh; Rahil Mahdian; Mohammad Asgari', display:{Lore:['[{"text": "arXiv:1904.06588", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Compression Using Graph-based Transform\\u00a7r\\n\\n\\u00a78\\u00a7oMajid Farzaneh\\nRahil Mahdian\\nMohammad Asgari\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.06588\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ISTEL.2018.8661027\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 13 Apr 2019 19:33:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2018 9th International Symposium on Telecommunications (IST)\\u00a7r"}']}
{title:'Wang et al. (§72019§r)', author: 'Hao Wang; Jing Lu', display:{Lore:['[{"text": "arXiv:1904.06648", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA robust DOA estimation method for a linear microphone array under reverberant and noisy environments\\u00a7r\\n\\n\\u00a78\\u00a7oHao Wang\\nJing Lu\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.06648\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 14 Apr 2019 08:04:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 4 figures, 3 tables, 33 references\\u00a7r"}']}
{title:'Nakamura et al. (§72019§r)', author: 'Kazuhiro Nakamura; Kei Hashimoto; Keiichiro Oura; Yoshihiko Nankaku; Keiichi Tokuda', display:{Lore:['[{"text": "arXiv:1904.06868", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSinging voice synthesis based on convolutional neural networks\\u00a7r\\n\\n\\u00a78\\u00a7oKazuhiro Nakamura\\nKei Hashimoto\\nKeiichiro Oura\\nYoshihiko Nankaku\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.06868\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 25 Jun 2019 06:54:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSinging voice samples (Japanese, English, Chinese): https://www.techno-speech.com/news-20181214a-en\\u00a7r"}']}
{title:'Abdulbaqi et al. (§72019§r)', author: 'Jalal Abdulbaqi; Yue Gu; Ivan Marsic', display:{Lore:['[{"text": "arXiv:1904.07294", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRHR-Net: A Residual Hourglass Recurrent Neural Network for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oJalal Abdulbaqi\\nYue Gu\\nIvan Marsic\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.07294\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Apr 2019 19:17:28 GMT)\\u00a7r"}']}
{title:'Lee et al. (§72019§r)', author: 'Kong Aik Lee; Ville Hautamaki; Tomi Kinnunen; Hitoshi Yamamoto; Koji Okabe; Ville Vestman; Jing Huang; Guohong Ding; Hanwu Sun; Anthony Larcher; Rohan Kumar Das; Haizhou Li; Mickael Rouvier; Pierre-Michel Bousquet; Wei Rao; Qing Wang; Chunlei Zhang; Fahimeh Bahmaninezhad; Hector Delgado; Jose Patino; Qiongqiong Wang; Ling Guo; Takafumi Koshinaka; Jiacen Zhang; Koichi Shinoda; Trung Ngo Trong; Md Sahidullah; Fan Lu; Yun Tang; Ming Tu; Kah Kuan Teh; Huy Dat Tran; Kuruvachan K. George; Ivan Kukanov; Florent Desnous; Jichen Yang; Emre Yilmaz; Longting Xu; Jean-Francois Bonastre; Chenglin Xu; Zhi Hao Lim; Eng Siong Chng; Shivesh Ranjan; John H. L. Hansen; Massimiliano Todisco; Nicholas Evans', display:{Lore:['[{"text": "arXiv:1904.07386", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lI4U Submission to NIST SRE 2018: Leveraging from a Decade of Shared Experiences\\u00a7r\\n\\n\\u00a78\\u00a7oKong Aik Lee\\nVille Hautamaki\\nTomi Kinnunen\\n+ 42 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.07386\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Apr 2019 00:55:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Segal et al. (§72019§r)', author: 'Yael Segal; Tzeviya Sylvia Fuchs; Joseph Keshet', display:{Lore:['[{"text": "arXiv:1904.07704", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeechYOLO: Detection and Localization of Speech Objects\\u00a7r\\n\\n\\u00a78\\u00a7oYael Segal\\nTzeviya Sylvia Fuchs\\nJoseph Keshet\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.07704\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInterspeech 2019, pp. 4210-4214\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 30 Jun 2019 09:14:46 GMT)\\u00a7r"}']}
{title:'Jung et al. (§72019§r)', author: 'Jee-weon Jung; Hee-Soo Heo; Ju-ho Kim; Hye-jin Shim; Ha-Jin Yu', display:{Lore:['[{"text": "arXiv:1904.08104", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRawNet: Advanced end-to-end deep neural network using raw waveforms for text-independent speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oJee-weon Jung\\nHee-Soo Heo\\nJu-ho Kim\\nHye-jin Shim\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.08104\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 17 Jul 2019 03:52:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for oral presentation at Interspeech 2019, code available at http://github.com/Jungjee/RawNet\\u00a7r"}']}
{title:'Pasa et al. (§72019§r)', author: 'Luca Pasa; Giovanni Morrone; Leonardo Badino', display:{Lore:['[{"text": "arXiv:1904.08248", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Analysis of Speech Enhancement and Recognition Losses in Limited Resources Multi-talker Single Channel Audio-Visual ASR\\u00a7r\\n\\n\\u00a78\\u00a7oLuca Pasa\\nGiovanni Morrone\\nLeonardo Badino\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.08248\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 27 Nov 2019 16:37:24 GMT)\\u00a7r"}']}
{title:'Anand et al. (§72019§r)', author: 'Prashant Anand; Ajeet Kumar Singh; Siddharth Srivastava; Brejesh Lall', display:{Lore:['[{"text": "arXiv:1904.08775", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFew Shot Speaker Recognition using Deep Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oPrashant Anand\\nAjeet Kumar Singh\\nSiddharth Srivastava\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.08775\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Apr 2019 09:25:02 GMT)\\u00a7r"}']}
{title:'Park et al. (§72019§r)', author: 'Daniel S. Park; William Chan; Yu Zhang; Chung-Cheng Chiu; Barret Zoph; Ekin D. Cubuk; Quoc V. Le', display:{Lore:['[{"text": "arXiv:1904.08779", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel S. Park\\nWilliam Chan\\nYu Zhang\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.08779\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2019-2680\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2019, 2613-2617\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 3 Dec 2019 18:19:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 6 tables; v3: references added\\u00a7r"}']}
{title:'Ghorbani et al. (§72019§r)', author: 'Shahram Ghorbani; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:1904.09038", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging native language information for improved accented speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oShahram Ghorbani\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.09038\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2018-1378\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Apr 2019 23:35:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2018\\u00a7r"}']}
{title:'Subramanian et al. (§72019§r)', author: 'Aswin Shanmugam Subramanian; Xiaofei Wang; Shinji Watanabe; Toru Taniguchi; Dung Tran; Yuya Fujita', display:{Lore:['[{"text": "arXiv:1904.09049", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Investigation of End-to-End Multichannel Speech Recognition for Reverberant and Mismatch Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oAswin Shanmugam Subramanian\\nXiaofei Wang\\nShinji Watanabe\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.09049\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 28 Apr 2019 06:13:42 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72019§r)', author: 'Shiliang Zhang; Ming Lei; Zhijie Yan', display:{Lore:['[{"text": "arXiv:1904.10045", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Spelling Correction with Transformer for CTC-based End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oShiliang Zhang\\nMing Lei\\nZhijie Yan\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.10045\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Mar 2019 07:16:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6pages, 5 figures\\u00a7r"}']}
{title:'Jung et al. (§72019§r)', author: 'Jee-weon Jung; Hye-jin Shim; Hee-Soo Heo; Ha-Jin Yu', display:{Lore:['[{"text": "arXiv:1904.10134", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReplay attack detection with complementary high-resolution information using end-to-end DNN for the ASVspoof 2019 Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oJee-weon Jung\\nHye-jin Shim\\nHee-Soo Heo\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.10134\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 17 Jul 2019 04:02:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for oral presentation at Interspeech 2019, code available at https://github.com/Jungjee/ASVspoof2019_PA\\u00a7r"}']}
{title:'Heo et al. (§72019§r)', author: 'Hee-Soo Heo; Jee-weon Jung; Hye-jin Shim; Ha-Jin Yu', display:{Lore:['[{"text": "arXiv:1904.10135", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic scene classification using teacher-student learning with soft-labels\\u00a7r\\n\\n\\u00a78\\u00a7oHee-Soo Heo\\nJee-weon Jung\\nHye-jin Shim\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.10135\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 17 Jul 2019 04:10:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for presentation at Interspeech 2019\\u00a7r"}']}
{title:'Bear et al. (§72019§r)', author: 'Helen L. Bear; Ines Nolasco; Emmanouil Benetos', display:{Lore:['[{"text": "arXiv:1904.10408", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards joint sound scene and polyphonic sound event recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHelen L. Bear\\nInes Nolasco\\nEmmanouil Benetos\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.10408\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 1 Jul 2019 11:47:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2019\\u00a7r"}']}
{title:'Lazzarini et al. (§72019§r)', author: 'Victor Lazzarini; Joseph Timoney', display:{Lore:['[{"text": "arXiv:1904.10763", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Analogue Computer as a Voltage-Controlled Synthesiser\\u00a7r\\n\\n\\u00a78\\u00a7oVictor Lazzarini\\nJoseph Timoney\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.10763\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 Apr 2019 19:38:43 GMT)\\u00a7r"}']}
{title:'Yoon et al. (§72019§r)', author: 'Seunghyun Yoon; Seokhyun Byun; Subhadeep Dey; Kyomin Jung', display:{Lore:['[{"text": "arXiv:1904.10788", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Emotion Recognition Using Multi-hop Attention Mechanism\\u00a7r\\n\\n\\u00a78\\u00a7oSeunghyun Yoon\\nSeokhyun Byun\\nSubhadeep Dey\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.10788\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8683483\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 9 May 2019 13:34:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, Accepted as a conference paper at ICASSP 2019 (oral presentation)\\u00a7r"}']}
{title:'He et al. (§72019§r)', author: 'Liang He; Xianhong Chen; Can Xu; Yi Liu; Jia Liu; Michael T Johnson', display:{Lore:['[{"text": "arXiv:1904.11130", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLatent Class Model with Application to Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oLiang He\\nXianhong Chen\\nCan Xu\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.11130\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 Apr 2019 02:35:16 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72019§r)', author: 'Xin Wang; Shinji Takaki; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:1904.12088", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural source-filter waveform models for statistical parametric speech synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oXin Wang\\nShinji Takaki\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.12088\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 17 Nov 2019 13:02:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEE/ACM TASLP. Note: this paper is on a follow-up work of our ICASSP paper. Based onthe h-NSF introduced inthis work, we proposed a h-sinc-NSF model andpublished the third paper in SSW 10 (https://www.isca-speech.org/"}','{"text": "archive/SSW_2019/pdfs/SSW10_O_1-1.pdf)\\u00a7r"}']}
{title:'Shi et al. (§72019§r)', author: 'Bowen Shi; Ming Sun; Chieh-Chi Kao; Viktor Rozgic; Spyros Matsoukas; Chao Wang', display:{Lore:['[{"text": "arXiv:1904.12926", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-supervised Acoustic Event Detection based on tri-training\\u00a7r\\n\\n\\u00a78\\u00a7oBowen Shi\\nMing Sun\\nChieh-Chi Kao\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.12926\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 Apr 2019 19:51:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Kons et al. (§72019§r)', author: 'Zvi Kons; Slava Shechtman; Alex Sorin; Carmel Rabinovitz; Ron Hoory', display:{Lore:['[{"text": "arXiv:1905.00590", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHigh quality, lightweight and adaptable TTS using LPCNet\\u00a7r\\n\\n\\u00a78\\u00a7oZvi Kons\\nSlava Shechtman\\nAlex Sorin\\nCarmel Rabinovitz\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.00590\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 26 Jun 2019 12:19:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2019\\u00a7r"}']}
{title:'Huang et al. (§72019§r)', author: 'Wen-Chin Huang; Yi-Chiao Wu; Chen-Chou Lo; Patrick Lumban Tobing; Tomoki Hayashi; Kazuhiro Kobayashi; Tomoki Toda; Yu Tsao; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:1905.00615", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigation of F0 conditioning and Fully Convolutional Networks in Variational Autoencoder based Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oWen-Chin Huang\\nYi-Chiao Wu\\nChen-Chou Lo\\n+ 5 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.00615\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Jul 2019 08:42:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 6 figures, 3 tables; Accepted to Interspeech 2019\\u00a7r"}']}
{title:'Shi et al. (§72019§r)', author: 'Bowen Shi; Ming Sun; Chieh-Chi Kao; Viktor Rozgic; Spyros Matsoukas; Chao Wang', display:{Lore:['[{"text": "arXiv:1905.00855", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCompression of Acoustic Event Detection Models with Low-rank Matrix Factorization and Quantization Training\\u00a7r\\n\\n\\u00a78\\u00a7oBowen Shi\\nMing Sun\\nChieh-Chi Kao\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.00855\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 May 2019 17:07:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oNeuralPS 2018 CDNNRIA workshop\\u00a7r"}']}
{title:'Bear et al. (§72019§r)', author: 'Helen L. Bear; Toni Heittola; Annamaria Mesaros; Emmanouil Benetos; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:1905.00979", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCity classification from multiple real-world sound scenes\\u00a7r\\n\\n\\u00a78\\u00a7oHelen L. Bear\\nToni Heittola\\nAnnamaria Mesaros\\nEmmanouil Benetos\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.00979\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 29 Jul 2019 14:50:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to WASPAA 2019\\u00a7r"}']}
{title:'Sheng et al. (§72019§r)', author: 'Di Sheng; György Fazekas', display:{Lore:['[{"text": "arXiv:1905.01022", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Feature Learning Siamese Model for Intelligent Control of the Dynamic Range Compressor\\u00a7r\\n\\n\\u00a78\\u00a7oDi Sheng\\nGy\\u00f6rgy Fazekas\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.01022\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 May 2019 11:28:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, accepted in IJCNN 2019\\u00a7r"}']}
{title:'Baskar et al. (§72019§r)', author: 'Murali Karthick Baskar; Shinji Watanabe; Ramon Astudillo; Takaaki Hori; Lukáš Burget; Jan Černocký', display:{Lore:['[{"text": "arXiv:1905.01152", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-supervised Sequence-to-sequence ASR using Unpaired Speech and Text\\u00a7r\\n\\n\\u00a78\\u00a7oMurali Karthick Baskar\\nShinji Watanabe\\nRamon Astudillo\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.01152\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 20 Aug 2019 08:54:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2019\\u00a7r"}']}
{title:'Keskin et al. (§72019§r)', author: 'Gokce Keskin; Tyler Lee; Cory Stephenson; Oguz H. Elibol', display:{Lore:['[{"text": "arXiv:1905.02525", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMany-to-Many Voice Conversion with Out-of-Dataset Speaker Support\\u00a7r\\n\\n\\u00a78\\u00a7oGokce Keskin\\nTyler Lee\\nCory Stephenson\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.02525\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Apr 2019 15:35:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2019\\u00a7r"}']}
{title:'Yoshioka et al. (§72019§r)', author: 'Takuya Yoshioka; Zhuo Chen; Dimitrios Dimitriadis; William Hinthorn; Xuedong Huang; Andreas Stolcke; Michael Zeng', display:{Lore:['[{"text": "arXiv:1905.02545", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMeeting Transcription Using Virtual Microphone Arrays\\u00a7r\\n\\n\\u00a78\\u00a7oTakuya Yoshioka\\nZhuo Chen\\nDimitrios Dimitriadis\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.02545\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 7 Jul 2019 11:23:21 GMT)\\u00a7r"}']}
{title:'Karhila et al. (§72019§r)', author: 'Reima Karhila; Anna-Riikka Smolander; Sari Ylinen; Mikko Kurimo', display:{Lore:['[{"text": "arXiv:1905.02639", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransparent pronunciation scoring using articulatorily weighted phoneme edit distance\\u00a7r\\n\\n\\u00a78\\u00a7oReima Karhila\\nAnna-Riikka Smolander\\nSari Ylinen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.02639\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 May 2019 15:30:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2019\\u00a7r"}']}
{title:'Parthasarathy et al. (§72019§r)', author: 'Srinivas Parthasarathy; Carlos Busso', display:{Lore:['[{"text": "arXiv:1905.02921", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-Supervised Speech Emotion Recognition with Ladder Networks\\u00a7r\\n\\n\\u00a78\\u00a7oSrinivas Parthasarathy\\nCarlos Busso\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.02921\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2020.3023632\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol. 28, pp. 2697-2709, September 2020\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 8 May 2019 05:37:28 GMT)\\u00a7r"}']}
{title:'Ocal et al. (§72019§r)', author: 'Orhan Ocal; Oguz H. Elibol; Gokce Keskin; Cory Stephenson; Anil Thomas; Kannan Ramchandran', display:{Lore:['[{"text": "arXiv:1905.03864", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarially Trained Autoencoders for Parallel-Data-Free Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oOrhan Ocal\\nOguz H. Elibol\\nGokce Keskin\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.03864\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 May 2019 21:34:13 GMT)\\u00a7r"}']}
{title:'Elibol et al. (§72019§r)', author: 'Oguz H. Elibol; Gokce Keskin; Anil Thomas', display:{Lore:['[{"text": "arXiv:1905.04230", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-supervised and Population Based Training for Voice Commands Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oOguz H. Elibol\\nGokce Keskin\\nAnil Thomas\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.04230\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 10 May 2019 15:58:38 GMT)\\u00a7r"}']}
{title:'Qian et al. (§72019§r)', author: 'Kaizhi Qian; Yang Zhang; Shiyu Chang; Xuesong Yang; Mark Hasegawa-Johnson', display:{Lore:['[{"text": "arXiv:1905.05879", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAUTOVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss\\u00a7r\\n\\n\\u00a78\\u00a7oKaizhi Qian\\nYang Zhang\\nShiyu Chang\\nXuesong Yang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.05879\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 6 Jun 2019 05:44:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo Appear in Thirty-sixth International Conference on MachineLearning (ICML 2019)\\u00a7r"}']}
{title:'Ramírez et al. (§72019§r)', author: 'Marco A. Martínez Ramírez; Emmanouil Benetos; Joshua D. Reiss', display:{Lore:['[{"text": "arXiv:1905.06148", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA general-purpose deep learning approach to model time-varying audio effects\\u00a7r\\n\\n\\u00a78\\u00a7oMarco A. Mart\\u00ednez Ram\\u00edrez\\nEmmanouil Benetos\\nJoshua D. Reiss\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.06148\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 21 Jun 2019 11:07:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaudio files: https://mchijmma.github.io/modeling-time-varying/\\u00a7r"}']}
{title:'Abdelaziz et al. (§72019§r)', author: 'Ahmed Hussen Abdelaziz; Barry-John Theobald; Justin Binder; Gabriele Fanelli; Paul Dixon; Nicholas Apostoloff; Thibaut Weise; Sachin Kajareker', display:{Lore:['[{"text": "arXiv:1905.06860", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker-Independent Speech-Driven Visual Speech Synthesis using Domain-Adapted Acoustic Models\\u00a7r\\n\\n\\u00a78\\u00a7oAhmed Hussen Abdelaziz\\nBarry-John Theobald\\nJustin Binder\\n+ 4 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.06860\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 May 2019 00:23:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 2 figures, 2 tables\\u00a7r"}']}
{title:'Tsunoo et al. (§72019§r)', author: 'Emiru Tsunoo; Yosuke Kashiwagi; Satoshi Asakawa; Toshiyuki Kumakura', display:{Lore:['[{"text": "arXiv:1905.07149", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end Adaptation with Backpropagation through WFST for On-device Speech Recognition System\\u00a7r\\n\\n\\u00a78\\u00a7oEmiru Tsunoo\\nYosuke Kashiwagi\\nSatoshi Asakawa\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.07149\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 24 Jun 2019 09:50:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted for Interspeech 2019\\u00a7r"}']}
{title:'Kwon et al. (§72019§r)', author: 'Ohsung Kwon; Eunwoo Song; Jae-Min Kim; Hong-Goo Kang', display:{Lore:['[{"text": "arXiv:1905.08486", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEffective parameter estimation methods for an ExcitNet model in generative text-to-speech systems\\u00a7r\\n\\n\\u00a78\\u00a7oOhsung Kwon\\nEunwoo Song\\nJae-Min Kim\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.08486\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 May 2019 08:24:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 3 tables, submitted to Speech Synthesis Workshop 2019\\u00a7r"}']}
{title:'Huang et al. (§72019§r)', author: 'Andrew Huang; Puwei Bao', display:{Lore:['[{"text": "arXiv:1905.08632", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHuman Vocal Sentiment Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oAndrew Huang\\nPuwei Bao\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.08632\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 19 May 2019 06:27:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oNYU Shanghai CSCS 2019\\u00a7r"}']}
{title:'Zhao et al. (§72019§r)', author: 'Ziyue Zhao; Samy Elshamy; Tim Fingscheidt', display:{Lore:['[{"text": "arXiv:1905.09754", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Perceptual Weighting Filter Loss for DNN Training in Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oZiyue Zhao\\nSamy Elshamy\\nTim Fingscheidt\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.09754\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 18 Aug 2019 11:23:56 GMT)\\u00a7r"}']}
{title:'Schlittenlacher et al. (§72019§r)', author: 'Josef Schlittenlacher; Richard E. Turner; Brian C. J. Moore', display:{Lore:['[{"text": "arXiv:1905.10399", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast computation of loudness using a deep neural network\\u00a7r\\n\\n\\u00a78\\u00a7oJosef Schlittenlacher\\nRichard E. Turner\\nBrian C. J. Moore\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.10399\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 May 2019 18:28:39 GMT)\\u00a7r"}']}
{title:'Zajíc et al. (§72019§r)', author: 'Zbyněk Zajíc; Marie Kunešová; Marek Hrúz; Jan Vaněk', display:{Lore:['[{"text": "arXiv:1905.11276", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUWB-NTIS Speaker Diarization System for the DIHARD II 2019 Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oZbyn\\u011bk Zaj\\u00edc\\nMarie Kune\\u0161ov\\u00e1\\nMarek Hr\\u00faz\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.11276\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2019-1385\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nINTERSPEECH 2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 May 2019 14:52:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2019\\u00a7r"}']}
{title:'Poorjam et al. (§72019§r)', author: 'Amir Hossein Poorjam; Mathew Shaji Kavalekalam; Liming Shi; Yordan P. Raykov; Jesper Rindom Jensen; Max A. Little; Mads Græsbøll Christensen', display:{Lore:['[{"text": "arXiv:1905.11785", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Quality Control and Enhancement for Voice-Based Remote Parkinson\'s Disease Detection\\u00a7r\\n\\n\\u00a78\\u00a7oAmir Hossein Poorjam\\nMathew Shaji Kavalekalam\\nLiming Shi\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.11785\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 31 May 2019 08:41:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint, 12 pages, 6 figures\\u00a7r"}']}
{title:'Tagliasacchi et al. (§72019§r)', author: 'Marco Tagliasacchi; Beat Gfeller; Félix de Chaumont Quitry; Dominik Roblek', display:{Lore:['[{"text": "arXiv:1905.11796", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervised audio representation learning for mobile devices\\u00a7r\\n\\n\\u00a78\\u00a7oMarco Tagliasacchi\\nBeat Gfeller\\nF\\u00e9lix de Chaumont Quitry\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.11796\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 May 2019 13:57:40 GMT)\\u00a7r"}']}
{title:'Hawley et al. (§72019§r)', author: 'Scott H. Hawley; Benjamin Colburn; Stylianos I. Mimilakis', display:{Lore:['[{"text": "arXiv:1905.11928", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSignalTrain: Profiling Audio Compressors with Deep Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oScott H. Hawley\\nBenjamin Colburn\\nStylianos I. Mimilakis\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.11928\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 30 May 2019 01:59:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 10 figures. v2: typos references fixed\\u00a7r"}']}
{title:'Keskin et al. (§72019§r)', author: 'Gokce Keskin; Tyler Lee; Cory Stephenson; Oguz H. Elibol', display:{Lore:['[{"text": "arXiv:1905.12531", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMeasuring the Effectiveness of Voice Conversion on Speaker Identification and Automatic Speech Recognition Systems\\u00a7r\\n\\n\\u00a78\\u00a7oGokce Keskin\\nTyler Lee\\nCory Stephenson\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.12531\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 29 May 2019 15:28:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at ICML 2019Synthetic Realities: Workshop on Detecting Audio-Visual Fakes\\u00a7r"}']}
{title:'Michelsanti et al. (§72019§r)', author: 'Daniel Michelsanti; Zheng-Hua Tan; Sigurdur Sigurdsson; Jesper Jensen', display:{Lore:['[{"text": "arXiv:1905.12605", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep-Learning-Based Audio-Visual Speech Enhancement in Presence of Lombard Effect\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Michelsanti\\nZheng-Hua Tan\\nSigurdur Sigurdsson\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.12605\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.specom.2019.10.006\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 29 May 2019 17:37:41 GMT)\\u00a7r"}']}
{title:'Fang et al. (§72019§r)', author: 'Fuming Fang; Xin Wang; Junichi Yamagishi; Isao Echizen; Massimiliano Todisco; Nicholas Evans; Jean-Francois Bonastre', display:{Lore:['[{"text": "arXiv:1905.13561", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Anonymization Using X-vector and Neural Waveform Models\\u00a7r\\n\\n\\u00a78\\u00a7oFuming Fang\\nXin Wang\\nJunichi Yamagishi\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.13561\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 May 2019 01:33:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to the 10th ISCA Speech Synthesis Workshop (SSW10)\\u00a7r"}']}
{title:'Hung et al. (§72019§r)', author: 'Yun-Ning Hung; I-Tung Chiang; Yi-An Chen; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:1905.13567", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusical Composition Style Transfer via Disentangled Timbre Representations\\u00a7r\\n\\n\\u00a78\\u00a7oYun-Ning Hung\\nI-Tung Chiang\\nYi-An Chen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.13567\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 May 2019 06:30:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the 28th International JointConference on Artificial Intelligence. arXiv admin note: text overlap with arXiv:1811.03271\\u00a7r"}']}
{title:'McLeod (§72019§r)', author: 'Andrew McLeod', display:{Lore:['[{"text": "arXiv:1906.00566", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluating Non-aligned Musical Score Transcriptions with MV2H\\u00a7r\\n\\n\\u00a78\\u00a7oAndrew McLeod\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.00566\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Jul 2019 04:31:51 GMT)\\u00a7r"}']}
{title:'Wu et al. (§72019§r)', author: 'Jyun-Yi Wu; Cheng Yu; Szu-Wei Fu; Chih-Ting Liu; Shao-Yi Chien; Yu Tsao', display:{Lore:['[{"text": "arXiv:1906.01078", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIncreasing Compactness Of Deep Learning Based Speech Enhancement Models With Parameter Pruning And Quantization Techniques\\u00a7r\\n\\n\\u00a78\\u00a7oJyun-Yi Wu\\nCheng Yu\\nSzu-Wei Fu\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.01078\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2019.2951950\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 31 Jul 2019 18:22:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4pages, 6 figures\\u00a7r"}']}
{title:'Vasquez et al. (§72019§r)', author: 'Sean Vasquez; Mike Lewis', display:{Lore:['[{"text": "arXiv:1906.01083", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMelNet: A Generative Model for Audio in the Frequency Domain\\u00a7r\\n\\n\\u00a78\\u00a7oSean Vasquez\\nMike Lewis\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.01083\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 4 Jun 2019 04:58:19 GMT)\\u00a7r"}']}
{title:'Vestman et al. (§72019§r)', author: 'Ville Vestman; Tomi Kinnunen; Rosa González Hautamäki; Md Sahidullah', display:{Lore:['[{"text": "arXiv:1906.01454", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Mimicry Attacks Assisted by Automatic Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oVille Vestman\\nTomi Kinnunen\\nRosa Gonz\\u00e1lez Hautam\\u00e4ki\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.01454\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.csl.2019.05.005\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 4 Nov 2019 11:40:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in Computer Speech and Language. arXiv admin note: text overlap with arXiv:1811.03790\\u00a7r"}']}
{title:'Ma et al. (§72019§r)', author: 'Pingchuan Ma; Stavros Petridis; Maja Pantic', display:{Lore:['[{"text": "arXiv:1906.02112", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating the Lombard Effect Influence on End-to-End Audio-Visual Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oPingchuan Ma\\nStavros Petridis\\nMaja Pantic\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.02112\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 9 Jul 2019 17:51:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at Interspeech 2019\\u00a7r"}']}
{title:'Clink et al. (§72019§r)', author: 'Dena J. Clink; Holger Klinck', display:{Lore:['[{"text": "arXiv:1906.02572", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGIBBONFINDR: An R package for the detection and classification of acoustic signals\\u00a7r\\n\\n\\u00a78\\u00a7oDena J. Clink\\nHolger Klinck\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.02572\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 16 Nov 2019 02:12:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oR package\\u00a7r"}']}
{title:'Araujo et al. (§72019§r)', author: 'Flavio Abreu Araujo; Mathieu Riou; Jacob Torrejon; Sumito Tsunegi; Damien Querlioz; Kay Yakushiji; Akio Fukushima; Hitoshi Kubota; Shinji Yuasa; Mark D. Stiles; Julie Grollier', display:{Lore:['[{"text": "arXiv:1906.02812", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRole of non-linear data processing on speech recognition task in the framework of reservoir computing\\u00a7r\\n\\n\\u00a78\\u00a7oFlavio Abreu Araujo\\nMathieu Riou\\nJacob Torrejon\\n+ 7 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.02812\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1038/s41598-019-56991-x\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nScientific Reports 10, 328 (2020)\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 19 Dec 2019 19:11:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 5 figures\\u00a7r"}']}
{title:'Sijtsma et al. (§72019§r)', author: 'Pieter Sijtsma; Alice Dinsenmeyer; Jérôme Antoni; Quentin Leclere', display:{Lore:['[{"text": "arXiv:1906.02965", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBeamforming and other methods for denoising microphone array data\\u00a7r\\n\\n\\u00a78\\u00a7oPieter Sijtsma\\nAlice Dinsenmeyer\\nJ\\u00e9r\\u00f4me Antoni\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.02965\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 7 Jun 2019 08:50:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2019 AIAA/CEAS Aeroacoustics conference, May 2019, Delft, Netherlands\\u00a7r"}']}
{title:'Hodari et al. (§72019§r)', author: 'Zack Hodari; Oliver Watts; Simon King', display:{Lore:['[{"text": "arXiv:1906.04233", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing generative modelling to produce varied intonation for speech synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oZack Hodari\\nOliver Watts\\nSimon King\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.04233\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/SSW.2019-43\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 12 Sep 2019 10:14:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for the10th ISCA Speech Synthesis Workshop (SSW10)\\u00a7r"}']}
{title:'Rabiee et al. (§72019§r)', author: 'Azam Rabiee; Tae-Ho Kim; Soo-Young Lee', display:{Lore:['[{"text": "arXiv:1906.05507", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdjusting Pleasure-Arousal-Dominance for Continuous Emotional Text-to-speech Synthesizer\\u00a7r\\n\\n\\u00a78\\u00a7oAzam Rabiee\\nTae-Ho Kim\\nSoo-Young Lee\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.05507\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 13 Jun 2019 06:53:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech2019, Show and Tell demonstration https://www.youtube.com/watch?v=MAOk_ZxuA0I feature=youtu.be\\u00a7r"}']}
{title:'Larson et al. (§72019§r)', author: 'Chris Larson; Tarek Lahlou; Diana Mingels; Zachary Kulis; Erik Mueller', display:{Lore:['[{"text": "arXiv:1906.05678", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTelephonetic: Making Neural Language Models Robust to ASR and Semantic Noise\\u00a7r\\n\\n\\u00a78\\u00a7oChris Larson\\nTarek Lahlou\\nDiana Mingels\\nZachary Kulis\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.05678\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 13 Jun 2019 17:04:46 GMT)\\u00a7r"}']}
{title:'Tripathi et al. (§72019§r)', author: 'Suraj Tripathi; Abhay Kumar; Abhiram Ramesh; Chirag Singh; Promod Yenigalla', display:{Lore:['[{"text": "arXiv:1906.05681", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning based Emotion Recognition System Using Speech Features and Transcriptions\\u00a7r\\n\\n\\u00a78\\u00a7oSuraj Tripathi\\nAbhay Kumar\\nAbhiram Ramesh\\nChirag Singh\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.05681\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Jun 2019 17:35:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in CICLing 2019\\u00a7r"}']}
{title:'Tripathi et al. (§72019§r)', author: 'Suraj Tripathi; Abhay Kumar; Abhiram Ramesh; Chirag Singh; Promod Yenigalla', display:{Lore:['[{"text": "arXiv:1906.05682", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFocal Loss based Residual Convolutional Neural Network for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSuraj Tripathi\\nAbhay Kumar\\nAbhiram Ramesh\\nChirag Singh\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.05682\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Jun 2019 17:31:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in CICLing 2019\\u00a7r"}']}
{title:'Chao et al. (§72019§r)', author: 'Guan-Lin Chao; William Chan; Ian Lane', display:{Lore:['[{"text": "arXiv:1906.05962", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker-Targeted Audio-Visual Models for Speech Recognition in Cocktail-Party Environments\\u00a7r\\n\\n\\u00a78\\u00a7oGuan-Lin Chao\\nWilliam Chan\\nIan Lane\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.05962\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 13 Jun 2019 23:52:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in INTERSPEECH 2016\\u00a7r"}']}
{title:'Vougioukas et al. (§72019§r)', author: 'Konstantinos Vougioukas; Pingchuan Ma; Stavros Petridis; Maja Pantic', display:{Lore:['[{"text": "arXiv:1906.06301", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVideo-Driven Speech Reconstruction using Generative Adversarial Networks\\u00a7r\\n\\n\\u00a78\\u00a7oKonstantinos Vougioukas\\nPingchuan Ma\\nStavros Petridis\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.06301\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 Jun 2019 17:15:27 GMT)\\u00a7r"}']}
{title:'Szurley et al. (§72019§r)', author: 'Joseph Szurley; J. Zico Kolter', display:{Lore:['[{"text": "arXiv:1906.06355", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerceptual Based Adversarial Audio Attacks\\u00a7r\\n\\n\\u00a78\\u00a7oJoseph Szurley\\nJ. Zico Kolter\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.06355\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 Jun 2019 18:14:22 GMT)\\u00a7r"}']}
{title:'Henderson et al. (§72019§r)', author: 'Trevor Henderson; Justin Solomon', display:{Lore:['[{"text": "arXiv:1906.06763", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Transport: A Generalized Portamento via Optimal Transport\\u00a7r\\n\\n\\u00a78\\u00a7oTrevor Henderson\\nJustin Solomon\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.06763\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 16 Jun 2019 20:18:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to The 22nd International Conference on Digital Audio Effects (DAFx-19), Birmingham, UK, September 2-6, 2019\\u00a7r"}']}
{title:'Cances et al. (§72019§r)', author: 'Leo Cances; Patrice Guyot; Thomas Pellegrini', display:{Lore:['[{"text": "arXiv:1906.06909", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluation of post-processing algorithms for polyphonic sound event detection\\u00a7r\\n\\n\\u00a78\\u00a7oLeo Cances\\nPatrice Guyot\\nThomas Pellegrini\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.06909\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 24 Jun 2019 09:38:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 1 table 2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics\\u00a7r"}']}
{title:'Zhang et al. (§72019§r)', author: 'Larry Zhang; Xiaotong Chen; Abbad Vakil; Ali Byott; Reza Hosseini Ghomi', display:{Lore:['[{"text": "arXiv:1906.07222", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDigiVoice: Voice Biomarker Featurization and Analysis Pipeline\\u00a7r\\n\\n\\u00a78\\u00a7oLarry Zhang\\nXiaotong Chen\\nAbbad Vakil\\nAli Byott\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.07222\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 19 Jun 2019 03:45:30 GMT)\\u00a7r"}']}
{title:'Feng et al. (§72019§r)', author: 'Siyuan Feng; Tan Lee; Zhiyuan Peng', display:{Lore:['[{"text": "arXiv:1906.07234", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCombining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oSiyuan Feng\\nTan Lee\\nZhiyuan Peng\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.07234\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2019-1337\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 9 Aug 2019 15:55:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted for publication in INTERSPEECH 2019, Graz, Austria\\u00a7r"}']}
{title:'Feng et al. (§72019§r)', author: 'Siyuan Feng; Tan Lee', display:{Lore:['[{"text": "arXiv:1906.07245", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Unsupervised Subword Modeling via Disentangled Speech Representation Learning and Transformation\\u00a7r\\n\\n\\u00a78\\u00a7oSiyuan Feng\\nTan Lee\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.07245\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2019-1338\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 3 Jul 2019 12:24:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted for publication in INTERSPEECH 2019, Graz, Austria\\u00a7r"}']}
{title:'Novoa et al. (§72019§r)', author: 'José Novoa; Rodrigo Mahu; Alejandro Díaz; Jorge Wuth; Richard Stern; Nestor Becerra Yoma', display:{Lore:['[{"text": "arXiv:1906.07298", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeighted delay-and-sum beamforming guided by visual tracking for human-robot interaction\\u00a7r\\n\\n\\u00a78\\u00a7oJos\\u00e9 Novoa\\nRodrigo Mahu\\nAlejandro D\\u00edaz\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.07298\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Jun 2019 22:57:02 GMT)\\u00a7r"}']}
{title:'Novoa et al. (§72019§r)', author: 'José Novoa; Josué Fredes; Jorge Wuth; Fernando Huenupán; Richard M. Stern; Nestor Becerra Yoma', display:{Lore:['[{"text": "arXiv:1906.07299", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn combining features for single-channel robust speech recognition in reverberant environments\\u00a7r\\n\\n\\u00a78\\u00a7oJos\\u00e9 Novoa\\nJosu\\u00e9 Fredes\\nJorge Wuth\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.07299\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Jun 2019 23:04:37 GMT)\\u00a7r"}']}
{title:'Xiang et al. (§72019§r)', author: 'Xu Xiang; Shuai Wang; Houjun Huang; Yanmin Qian; Kai Yu', display:{Lore:['[{"text": "arXiv:1906.07317", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMargin Matters: Towards More Discriminative Deep Neural Network Embeddings for Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXu Xiang\\nShuai Wang\\nHoujun Huang\\nYanmin Qian\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.07317\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 Jun 2019 00:31:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7onot accepted by INTERSPEECH 2019\\u00a7r"}']}
{title:'Luong et al. (§72019§r)', author: 'Hieu-Thi Luong; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:1906.07414", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Unified Speaker Adaptation Method for Speech Synthesis using Transcribed and Untranscribed Speech with Backpropagation\\u00a7r\\n\\n\\u00a78\\u00a7oHieu-Thi Luong\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.07414\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 8 Oct 2019 02:36:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages, 10 figures\\u00a7r"}']}
{title:'Dietzen et al. (§72019§r)', author: 'T. Dietzen; S. Doclo; M. Moonen; T. van Waterschoot', display:{Lore:['[{"text": "arXiv:1906.07493", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSquare root-based multi-source early PSD estimation and recursive RETF update in reverberant environments by means of the orthogonal Procrustes problem\\u00a7r\\n\\n\\u00a78\\u00a7oT. Dietzen\\nS. Doclo\\nM. Moonen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.07493\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2020.2966891\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 Jun 2019 11:02:29 GMT)\\u00a7r"}']}
{title:'Dietzen et al. (§72019§r)', author: 'T. Dietzen; S. Doclo; M. Moonen; T. van Waterschoot', display:{Lore:['[{"text": "arXiv:1906.07512", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntegrated sidelobe cancellation and linear prediction Kalman filter for joint multi-microphone speech dereverberation, interfering speech cancellation, and noise reduction\\u00a7r\\n\\n\\u00a78\\u00a7oT. Dietzen\\nS. Doclo\\nM. Moonen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.07512\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2020.2966869\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 Jun 2019 11:58:30 GMT)\\u00a7r"}']}
{title:'Kong et al. (§72019§r)', author: 'Qiuqiang Kong; Yong Xu; Wenwu Wang; Philip J. B. Jackson; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:1906.07552", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSingle-Channel Signal Separation and Deconvolution with Generative Adversarial Networks\\u00a7r\\n\\n\\u00a78\\u00a7oQiuqiang Kong\\nYong Xu\\nWenwu Wang\\nPhilip J. B. Jackson\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.07552\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.24963/ijcai.2019/381\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInternational Joint Conference on Artificial Intelligence (IJCAI),\\n  2019, pp. 2747-2753\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 Jun 2019 22:00:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages. Accepted by IJCAI 2019\\u00a7r"}']}
{title:'Zhen et al. (§72019§r)', author: 'Kai Zhen; Jongmo Sung; Mi Suk Lee; Seungkwon Beack; Minje Kim', display:{Lore:['[{"text": "arXiv:1906.07769", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCascaded Cross-Module Residual Learning towards Lightweight End-to-End Speech Coding\\u00a7r\\n\\n\\u00a78\\u00a7oKai Zhen\\nJongmo Sung\\nMi Suk Lee\\nSeungkwon Beack\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.07769\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPublished in Interspeech 2019\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 13 Sep 2019 19:10:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in INTERSPEECH 2019\\u00a7r"}']}
{title:'Ryant et al. (§72019§r)', author: 'Neville Ryant; Kenneth Church; Christopher Cieri; Alejandrina Cristia; Jun Du; Sriram Ganapathy; Mark Liberman', display:{Lore:['[{"text": "arXiv:1906.07839", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Second DIHARD Diarization Challenge: Dataset, task, and baselines\\u00a7r\\n\\n\\u00a78\\u00a7oNeville Ryant\\nKenneth Church\\nChristopher Cieri\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.07839\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 Jun 2019 23:04:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2019\\u00a7r"}']}
{title:'Li et al. (§72019§r)', author: 'Ruizhi Li; Xiaofei Wang; Sri Harish Mallidi; Shinji Watanabe; Takaaki Hori; Hynek Hermansky', display:{Lore:['[{"text": "arXiv:1906.08041", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Stream End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oRuizhi Li\\nXiaofei Wang\\nSri Harish Mallidi\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.08041\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 18 Oct 2019 18:47:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to IEEETASLP (In review). arXivadmin note: substantial text overlap with arXiv:1811.04897, arXiv:1811.04903\\u00a7r"}']}
{title:'Parcollet et al. (§72019§r)', author: 'Titouan Parcollet; Mohamed Morchid; Georges Linarès; Renato De Mori', display:{Lore:['[{"text": "arXiv:1906.08043", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal to H-space Encoder for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oTitouan Parcollet\\nMohamed Morchid\\nGeorges Linar\\u00e8s\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.08043\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Jun 2019 20:07:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2019\\u00a7r"}']}
{title:'Jung et al. (§72019§r)', author: 'Youngmoon Jung; Younggwan Kim; Hyungjun Lim; Yeunju Choi; Hoirin Kim', display:{Lore:['[{"text": "arXiv:1906.08333", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatial Pyramid Encoding with Convex Length Normalization for Text-Independent Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oYoungmoon Jung\\nYounggwan Kim\\nHyungjun Lim\\nYeunju Choi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.08333\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2019-2177\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. of Interspeech 2019, 2019, pp. 4030-4034\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 19 Jun 2019 20:13:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, Interspeech 2019\\u00a7r"}']}
{title:'Hwang et al. (§72019§r)', author: 'Min-Jae Hwang; Hong-Goo Kang', display:{Lore:['[{"text": "arXiv:1906.08407", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParameter Enhancement for MELP Speech Codec in Noisy Communication Environment\\u00a7r\\n\\n\\u00a78\\u00a7oMin-Jae Hwang\\nHong-Goo Kang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.08407\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 20 Jun 2019 01:20:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the conference of INTERSPEECH 2019\\u00a7r"}']}
{title:'Chen et al. (§72019§r)', author: 'Kainan Chen; Wenyu Jin; Bharadwaj Desikan', display:{Lore:['[{"text": "arXiv:1906.08847", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Signal Subspace Rotation Method for Localization of Multiple Wideband Sound Sources\\u00a7r\\n\\n\\u00a78\\u00a7oKainan Chen\\nWenyu Jin\\nBharadwaj Desikan\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.08847\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 20 Jun 2019 20:49:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics\\u00a7r"}']}
{title:'Di Carlo et al. (§72019§r)', author: 'Diego Di Carlo; Antoine Deleforge; Nancy Bertin', display:{Lore:['[{"text": "arXiv:1906.08968", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a75physics.class-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMirage: 2D Source Localization Using Microphone Pair Augmentation with Echoes\\u00a7r\\n\\n\\u00a78\\u00a7oDiego Di Carlo\\nAntoine Deleforge\\nNancy Bertin\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.08968\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInternational Conferenze on Acoustic, Speech Signal Processing -\\n  ICASSP 2019, May 2019, Calgary, United Kingdom\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 Jun 2019 06:19:27 GMT)\\u00a7r"}']}
{title:'Srivastava et al. (§72019§r)', author: 'Brij Mohan Lal Srivastava; Basil Abraham; Sunayana Sitaram; Rupesh Mehta; Preethi Jyothi', display:{Lore:['[{"text": "arXiv:1906.09426", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End ASR for Code-switched Hindi-English Speech\\u00a7r\\n\\n\\u00a78\\u00a7oBrij Mohan Lal Srivastava\\nBasil Abraham\\nSunayana Sitaram\\nRupesh Mehta\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.09426\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 22 Jun 2019 10:23:42 GMT)\\u00a7r"}']}
{title:'Gupta et al. (§72019§r)', author: 'Chitralekha Gupta; Emre Yılmaz; Haizhou Li', display:{Lore:['[{"text": "arXiv:1906.10369", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic Modeling for Automatic Lyrics-to-Audio Alignment\\u00a7r\\n\\n\\u00a78\\u00a7oChitralekha Gupta\\nEmre Y\\u0131lmaz\\nHaizhou Li\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.10369\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Jun 2019 08:11:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at Interspeech 2019\\u00a7r"}']}
{title:'Zhang et al. (§72019§r)', author: 'Jing-Xuan Zhang; Zhen-Hua Ling; Li-Rong Dai', display:{Lore:['[{"text": "arXiv:1906.10508", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-Parallel Sequence-to-Sequence Voice Conversion with Disentangled Linguistic and Speaker Representations\\u00a7r\\n\\n\\u00a78\\u00a7oJing-Xuan Zhang\\nZhen-Hua Ling\\nLi-Rong Dai\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.10508\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2019.2960721\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech and Language Processing vol\\n  28 no 1 (2020) 540-552\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 13 Dec 2019 02:30:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE/ACM Transactionson Aduio, Speech and Language Processing\\u00a7r"}']}
{title:'Meseguer-Brocal et al. (§72019§r)', author: 'Gabriel Meseguer-Brocal; Alice Cohen-Hadria; Geoffroy Peeters', display:{Lore:['[{"text": "arXiv:1906.10606", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.DB\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDALI: a large Dataset of synchronized Audio, LyrIcs and notes, automatically created using teacher-student machine learning paradigm\\u00a7r\\n\\n\\u00a78\\u00a7oGabriel Meseguer-Brocal\\nAlice Cohen-Hadria\\nGeoffroy Peeters\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.10606\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5281/zenodo.1492443\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 19th International Society for Music\\n  Information Retrieval Conference, ISMIR, Paris, France, pp. 431-437, 2018\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Jun 2019 15:30:07 GMT)\\u00a7r"}']}
{title:'Wu et al. (§72019§r)', author: 'Peng-fei Wu; Zhen-hua Ling; Li-juan Liu; Yuan Jiang; Hong-chuan Wu; Li-rong Dai', display:{Lore:['[{"text": "arXiv:1906.10859", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Emotional Speech Synthesis Using Style Tokens and Semi-Supervised Training\\u00a7r\\n\\n\\u00a78\\u00a7oPeng-fei Wu\\nZhen-hua Ling\\nLi-juan Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.10859\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Jun 2019 06:12:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures submitted to APSIPA2019\\u00a7r"}']}
{title:'Lim et al. (§72019§r)', author: 'Minkyu Lim; Ji-Hwan Kim', display:{Lore:['[{"text": "arXiv:1906.11018", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntegration of TensorFlow based Acoustic Model with Kaldi WFST Decoder\\u00a7r\\n\\n\\u00a78\\u00a7oMinkyu Lim\\nJi-Hwan Kim\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.11018\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 Jun 2019 06:54:35 GMT)\\u00a7r"}']}
{title:'von Platen et al. (§72019§r)', author: 'Patrick von Platen; Chao Zhang; Philip Woodland', display:{Lore:['[{"text": "arXiv:1906.11047", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Span Acoustic Modelling using Raw Waveform Signals\\u00a7r\\n\\n\\u00a78\\u00a7oPatrick von Platen\\nChao Zhang\\nPhilip Woodland\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.11047\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2019-2454\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 3 Oct 2019 17:13:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in INTERSPEECH 2019\\u00a7r"}']}
{title:'Nakashima et al. (§72019§r)', author: 'Ryo Nakashima; Ryo Ozaki; Tadahiro Taniguchi', display:{Lore:['[{"text": "arXiv:1906.11049", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Phoneme and Word Discovery from Multiple Speakers using Double Articulation Analyzer and Neural Network with Parametric Bias\\u00a7r\\n\\n\\u00a78\\u00a7oRyo Nakashima\\nRyo Ozaki\\nTadahiro Taniguchi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.11049\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3389/frobt.2019.00092\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nFront. Robot. AI, 2019, 6:92\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 Jun 2019 02:24:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o21 pages. Submitted\\u00a7r"}']}
{title:'Leamy et al. (§72019§r)', author: 'Paul Leamy; Ted Burke; Damon Berry; David Dorran', display:{Lore:['[{"text": "arXiv:1906.11509", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRe-annotation of cough events in the AMI corpus\\u00a7r\\n\\n\\u00a78\\u00a7oPaul Leamy\\nTed Burke\\nDamon Berry\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.11509\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Jun 2019 09:05:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oData presented in this work is available to downloadusing the link included inthe references\\u00a7r"}']}
{title:'Bian et al. (§72019§r)', author: 'Wenhao Bian; Jie Wang; Bojin Zhuang; Jiankui Yang; Shaojun Wang; Jing Xiao', display:{Lore:['[{"text": "arXiv:1906.11620", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-Based Music Classification with DenseNet And Data Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oWenhao Bian\\nJie Wang\\nBojin Zhuang\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.11620\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 15 Jun 2019 09:16:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by The 16th Pacific Rim International Conference on AI\\u00a7r"}']}
{title:'Gabdrakhmanov et al. (§72019§r)', author: 'Lenar Gabdrakhmanov; Rustem Garaev; Evgenii Razinkov', display:{Lore:['[{"text": "arXiv:1906.11645", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRUSLAN: Russian Spoken Language Corpus for Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oLenar Gabdrakhmanov\\nRustem Garaev\\nEvgenii Razinkov\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.11645\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Jun 2019 11:06:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to SPECOM\'2019\\u00a7r"}']}
{title:'Grondin et al. (§72019§r)', author: 'Francois Grondin; James Glass', display:{Lore:['[{"text": "arXiv:1906.11913", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultiple Sound Source Localization with SVD-PHAT\\u00a7r\\n\\n\\u00a78\\u00a7oFrancois Grondin\\nJames Glass\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1906.11913\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Jun 2019 19:06:55 GMT)\\u00a7r"}']}
{title:'Mustafa et al. (§72019§r)', author: 'Ahmed Mustafa; Arijit Biswas; Christian Bergler; Julia Schottenhamml; Andreas Maier', display:{Lore:['[{"text": "arXiv:1907.00772", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalysis by Adversarial Synthesis \\u2013 A Novel Approach for Speech Vocoding\\u00a7r\\n\\n\\u00a78\\u00a7oAhmed Mustafa\\nArijit Biswas\\nChristian Bergler\\nJulia Schottenhamml\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.00772\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Jul 2019 13:46:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2019\\u00a7r"}']}
{title:'Ribeiro et al. (§72019§r)', author: 'Manuel Sam Ribeiro; Aciel Eshky; Korin Richmond; Steve Renals', display:{Lore:['[{"text": "arXiv:1907.00818", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUltrasound tongue imaging for diarization and alignment of child speech therapy sessions\\u00a7r\\n\\n\\u00a78\\u00a7oManuel Sam Ribeiro\\nAciel Eshky\\nKorin Richmond\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.00818\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 15 Aug 2019 09:40:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, Accepted for publication at Interspeech 2019\\u00a7r"}']}
{title:'Shi et al. (§72019§r)', author: 'Bowen Shi; Ming Sun; Chieh-Chi Kao; Viktor Rozgic; Spyros Matsoukas; Chao Wang', display:{Lore:['[{"text": "arXiv:1907.00873", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCompression of Acoustic Event Detection Models With Quantized Distillation\\u00a7r\\n\\n\\u00a78\\u00a7oBowen Shi\\nMing Sun\\nChieh-Chi Kao\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.00873\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Jul 2019 15:37:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2019\\u00a7r"}']}
{title:'Beck et al. (§72019§r)', author: 'Eugen Beck; Wei Zhou; Ralf Schlüter; Hermann Ney', display:{Lore:['[{"text": "arXiv:1907.01030", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLSTM Language Models for LVCSR in First-Pass Decoding and Lattice-Rescoring\\u00a7r\\n\\n\\u00a78\\u00a7oEugen Beck\\nWei Zhou\\nRalf Schl\\u00fcter\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.01030\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Jul 2019 19:29:04 GMT)\\u00a7r"}']}
{title:'Meseguer-Brocal et al. (§72019§r)', author: 'Gabriel Meseguer-Brocal; Geoffroy Peeters', display:{Lore:['[{"text": "arXiv:1907.01277", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConditioned-U-Net: Introducing a Control Mechanism in the U-Net for Multiple Source Separations\\u00a7r\\n\\n\\u00a78\\u00a7oGabriel Meseguer-Brocal\\nGeoffroy Peeters\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.01277\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5281/zenodo.3527766\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 20th International Society for Music\\n  Information Retrieval Conference, ISMIR, Delft, Netherlands, 2019\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 21 Nov 2019 18:12:29 GMT)\\u00a7r"}']}
{title:'Kumar et al. (§72019§r)', author: 'Yaman Kumar; Rohit Jain; Khwaja Mohd. Salik; Rajiv Ratn Shah; Yifang yin; Roger Zimmermann', display:{Lore:['[{"text": "arXiv:1907.01367", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLipper: Synthesizing Thy Speech using Multi-View Lipreading\\u00a7r\\n\\n\\u00a78\\u00a7oYaman Kumar\\nRohit Jain\\nKhwaja Mohd. Salik\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.01367\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Jun 2019 10:26:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at AAAI2019\\u00a7r"}']}
{title:'Kubasova et al. (§72019§r)', author: 'Uliyana Kubasova; Gabriel Murray; McKenzie Braley', display:{Lore:['[{"text": "arXiv:1907.01369", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalyzing Verbal and Nonverbal Features for Predicting Group Performance\\u00a7r\\n\\n\\u00a78\\u00a7oUliyana Kubasova\\nGabriel Murray\\nMcKenzie Braley\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.01369\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 3 Jul 2019 20:53:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2019 (Graz, Austria)\\u00a7r"}']}
{title:'Peyser et al. (§72019§r)', author: 'Cal Peyser; Hao Zhang; Tara N. Sainath; Zelin Wu', display:{Lore:['[{"text": "arXiv:1907.01372", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Performance of End-to-End ASR on Numeric Sequences\\u00a7r\\n\\n\\u00a78\\u00a7oCal Peyser\\nHao Zhang\\nTara N. Sainath\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.01372\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Jul 2019 14:21:09 GMT)\\u00a7r"}']}
{title:'Michel et al. (§72019§r)', author: 'Wilfried Michel; Ralf Schlüter; Hermann Ney', display:{Lore:['[{"text": "arXiv:1907.01409", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparison of Lattice-Free and Lattice-Based Sequence Discriminative Training Criteria for LVCSR\\u00a7r\\n\\n\\u00a78\\u00a7oWilfried Michel\\nRalf Schl\\u00fcter\\nHermann Ney\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.01409\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2019-2254\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInterspeech 2019, 20th Annual Conference of the International\\n  Speech Communication Association, Graz, Austria, 15-19 September 2019, pp.\\n  1601--1605\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Jul 2019 15:16:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2019\\u00a7r"}']}
{title:'Ribeiro et al. (§72019§r)', author: 'Manuel Sam Ribeiro; Aciel Eshky; Korin Richmond; Steve Renals', display:{Lore:['[{"text": "arXiv:1907.01413", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker-independent classification of phonetic segments from raw ultrasound in child speech\\u00a7r\\n\\n\\u00a78\\u00a7oManuel Sam Ribeiro\\nAciel Eshky\\nKorin Richmond\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.01413\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8683564\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Jul 2019 12:04:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, published in ICASSP2019 (IEEE International Conference on Acoustics, Speech and Signal Processing, 2019)\\u00a7r"}']}
{title:'Kao et al. (§72019§r)', author: 'Chieh-Chi Kao; Ming Sun; Yixin Gao; Shiv Vitaladevuni; Chao Wang', display:{Lore:['[{"text": "arXiv:1907.01448", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSub-band Convolutional Neural Networks for Small-footprint Spoken Term Classification\\u00a7r\\n\\n\\u00a78\\u00a7oChieh-Chi Kao\\nMing Sun\\nYixin Gao\\nShiv Vitaladevuni\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.01448\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Jul 2019 15:29:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2019\\u00a7r"}']}
{title:'Liang et al. (§72019§r)', author: 'Xia Liang; Junmin Wu; Yan Yin', display:{Lore:['[{"text": "arXiv:1907.01607", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMIDI-Sandwich: Multi-model Multi-task Hierarchical Conditional VAE-GAN networks for Symbolic Single-track Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oXia Liang\\nJunmin Wu\\nYan Yin\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.01607\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 4 Jul 2019 06:59:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ocast KSEM2019 on May 3, 2019 (weak rejected)\\u00a7r"}']}
{title:'Karaulov et al. (§72019§r)', author: 'Ievgen Karaulov; Dmytro Tkanov', display:{Lore:['[{"text": "arXiv:1907.01914", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention model for articulatory features detection\\u00a7r\\n\\n\\u00a78\\u00a7oIevgen Karaulov\\nDmytro Tkanov\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.01914\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Jul 2019 10:30:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2019, 5 pages, 2 figures\\u00a7r"}']}
{title:'Do (§72019§r)', author: 'Cong-Thanh Do', display:{Lore:['[{"text": "arXiv:1907.01957", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Speech Recognition with High-Frame-Rate Features Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oCong-Thanh Do\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.01957\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 12 Jul 2019 15:48:37 GMT)\\u00a7r"}']}
{title:'Cai et al. (§72019§r)', author: 'Danwei Cai; Weicheng Cai; Ming Li', display:{Lore:['[{"text": "arXiv:1907.02191", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe DKU-SMIIP System for NIST 2018 Speaker Recognition Evaluation\\u00a7r\\n\\n\\u00a78\\u00a7oDanwei Cai\\nWeicheng Cai\\nMing Li\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.02191\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Jul 2019 02:31:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2019\\u00a7r"}']}
{title:'Cai et al. (§72019§r)', author: 'Danwei Cai; Xiaoyi Qin; Weicheng Cai; Ming Li', display:{Lore:['[{"text": "arXiv:1907.02194", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe DKU System for the Speaker Recognition Task of the 2019 VOiCES from a Distance Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oDanwei Cai\\nXiaoyi Qin\\nWeicheng Cai\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.02194\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Jul 2019 02:42:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2019\\u00a7r"}']}
{title:'Klimkov et al. (§72019§r)', author: 'Viacheslav Klimkov; Srikanth Ronanki; Jonas Rohnke; Thomas Drugman', display:{Lore:['[{"text": "arXiv:1907.02479", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFine-grained robust prosody transfer for single-speaker neural text-to-speech\\u00a7r\\n\\n\\u00a78\\u00a7oViacheslav Klimkov\\nSrikanth Ronanki\\nJonas Rohnke\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.02479\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Jul 2019 16:20:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 7 figures, Accepted for Interspeech 2019\\u00a7r"}']}
{title:'Cai et al. (§72019§r)', author: 'Weicheng Cai; Haiwei Wu; Danwei Cai; Ming Li', display:{Lore:['[{"text": "arXiv:1907.02663", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe DKU Replay Detection System for the ASVspoof 2019 Challenge: On Data Augmentation, Feature Representation, Classification, and Fusion\\u00a7r\\n\\n\\u00a78\\u00a7oWeicheng Cai\\nHaiwei Wu\\nDanwei Cai\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.02663\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Jul 2019 03:00:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for INTERSPEECH 2019\\u00a7r"}']}
{title:'Tits (§72019§r)', author: 'Noé Tits', display:{Lore:['[{"text": "arXiv:1907.02784", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Methodology for Controlling the Emotional Expressiveness in Synthetic Speech \\u2013 a Deep Learning approach\\u00a7r\\n\\n\\u00a78\\u00a7oNo\\u00e9 Tits\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.02784\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Jul 2019 12:00:53 GMT)\\u00a7r"}']}
{title:'Nautsch et al. (§72019§r)', author: 'Andreas Nautsch; Jose Patino; Amos Treiber; Themos Stafylakis; Petr Mizera; Massimiliano Todisco; Thomas Schneider; Nicholas Evans', display:{Lore:['[{"text": "arXiv:1907.03454", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrivacy-Preserving Speaker Recognition with Cohort Score Normalisation\\u00a7r\\n\\n\\u00a78\\u00a7oAndreas Nautsch\\nJose Patino\\nAmos Treiber\\n+ 4 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.03454\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Jul 2019 08:33:44 GMT)\\u00a7r"}']}
{title:'Nautsch et al. (§72019§r)', author: 'Andreas Nautsch; Catherine Jasserand; Els Kindt; Massimiliano Todisco; Isabel Trancoso; Nicholas Evans', display:{Lore:['[{"text": "arXiv:1907.03458", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CY\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe GDPR     Speech Data: Reflections of Legal and Technology Communities, First Steps towards a Common Understanding\\u00a7r\\n\\n\\u00a78\\u00a7oAndreas Nautsch\\nCatherine Jasserand\\nEls Kindt\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.03458\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2019-2647\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Jul 2019 08:42:42 GMT)\\u00a7r"}']}
{title:'Korzekwa et al. (§72019§r)', author: 'Daniel Korzekwa; Roberto Barra-Chicote; Bozena Kostek; Thomas Drugman; Mateusz Lajszczak', display:{Lore:['[{"text": "arXiv:1907.04743", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInterpretable Deep Learning Model for the Detection and Reconstruction of Dysarthric Speech\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Korzekwa\\nRoberto Barra-Chicote\\nBozena Kostek\\nThomas Drugman\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.04743\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Jul 2019 14:20:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, Accepted for Interspeech 2019\\u00a7r"}']}
{title:'Mac et al. (§72019§r)', author: 'Khoi-Nguyen C. Mac; Xiaodong Cui; Wei Zhang; Michael Picheny', display:{Lore:['[{"text": "arXiv:1907.04887", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLarge-Scale Mixed-Bandwidth Deep Neural Network Acoustic Modeling for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oKhoi-Nguyen C. Mac\\nXiaodong Cui\\nWei Zhang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.04887\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Jul 2019 18:52:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2019\\u00a7r"}']}
{title:'Weninger et al. (§72019§r)', author: 'Felix Weninger; Jesús Andrés-Ferrer; Xinwei Li; Puming Zhan', display:{Lore:['[{"text": "arXiv:1907.04916", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lListen, Attend, Spell and Adapt: Speaker Adapted Sequence-to-Sequence ASR\\u00a7r\\n\\n\\u00a78\\u00a7oFelix Weninger\\nJes\\u00fas Andr\\u00e9s-Ferrer\\nXinwei Li\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.04916\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Jul 2019 15:09:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in INTERSPEECH 2019\\u00a7r"}']}
{title:'Sanz et al. (§72019§r)', author: 'Javier Sanz; Andreas Wulff-Abramsson; Carlos Aguilar-Paredes; Luis Emilio Bruni; Lydia Sanchez', display:{Lore:['[{"text": "arXiv:1907.04926", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynchronizing Audio-Visual Film Stimuli in Unity (version 5.5.1f1): Game Engines as a Tool for Research\\u00a7r\\n\\n\\u00a78\\u00a7oJavier Sanz\\nAndreas Wulff-Abramsson\\nCarlos Aguilar-Paredes\\nLuis Emilio Bruni\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.04926\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Jul 2019 09:05:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 Pages\\u00a7r"}']}
{title:'Gupta et al. (§72019§r)', author: 'Archit Gupta; Brendan Shillingford; Yannis Assael; Thomas C. Walters', display:{Lore:['[{"text": "arXiv:1907.04927", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech bandwidth extension with WaveNet\\u00a7r\\n\\n\\u00a78\\u00a7oArchit Gupta\\nBrendan Shillingford\\nYannis Assael\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.04927\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Jul 2019 20:17:31 GMT)\\u00a7r"}']}
{title:'Senoussaoui et al. (§72019§r)', author: 'Mohammed Senoussaoui; Patrick Cardinal; Alessandro Lameiras Koerich', display:{Lore:['[{"text": "arXiv:1907.04928", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBag-of-Audio-Words based on Autoencoder Codebook for Continuous Emotion Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oMohammed Senoussaoui\\nPatrick Cardinal\\nAlessandro Lameiras Koerich\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.04928\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 6 Jul 2019 21:16:53 GMT)\\u00a7r"}']}
{title:'Pankajakshan et al. (§72019§r)', author: 'Arjun Pankajakshan; Helen L. Bear; Emmanouil Benetos', display:{Lore:['[{"text": "arXiv:1907.05122", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPolyphonic Sound Event and Sound Activity Detection: A Multi-task approach\\u00a7r\\n\\n\\u00a78\\u00a7oArjun Pankajakshan\\nHelen L. Bear\\nEmmanouil Benetos\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.05122\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 1 Aug 2019 12:53:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to WASPAA 2019\\u00a7r"}']}
{title:'Zhao et al. (§72019§r)', author: 'Tianyu Zhao; Tatsuya Kawahara', display:{Lore:['[{"text": "arXiv:1907.05599", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEffective Incorporation of Speaker Information in Utterance Encoding in Dialog\\u00a7r\\n\\n\\u00a78\\u00a7oTianyu Zhao\\nTatsuya Kawahara\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.05599\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Jul 2019 07:37:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8+1 pages, 3 figures, and 5 tables. Rejected by SIGDIAL 2019\\u00a7r"}']}
{title:'You et al. (§72019§r)', author: 'Zhao You; Dan Su; Dong Yu', display:{Lore:['[{"text": "arXiv:1907.05698", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTeach an all-rounder with experts in different domains\\u00a7r\\n\\n\\u00a78\\u00a7oZhao You\\nDan Su\\nDong Yu\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.05698\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Jul 2019 08:26:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages and 2 figures; accepted by 2019 IEEE International Conference on Acoustics, Speech and Signal Processing\\u00a7r"}']}
{title:'Zhang et al. (§72019§r)', author: 'Wei Zhang; Xiaodong Cui; Ulrich Finkler; George Saon; Abdullah Kayi; Alper Buyuktosunoglu; Brian Kingsbury; David Kung; Michael Picheny', display:{Lore:['[{"text": "arXiv:1907.05701", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.DC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Highly Efficient Distributed Deep Learning System For Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oWei Zhang\\nXiaodong Cui\\nUlrich Finkler\\n+ 5 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.05701\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nINTERSPEECH 2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Jul 2019 14:32:59 GMT)\\u00a7r"}']}
{title:'Perna et al. (§72019§r)', author: 'Diego Perna; Andrea Tagarelli', display:{Lore:['[{"text": "arXiv:1907.05708", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep auscultation: Predicting respiratory anomalies and diseases via recurrent neural networks\\u00a7r\\n\\n\\u00a78\\u00a7oDiego Perna\\nAndrea Tagarelli\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.05708\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Jul 2019 17:16:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper accepted for publication with Procs. of the 32th IEEE CBMS International Symposium on Computer-Based Medical Systems (CBMS 2019)\\u00a7r"}']}
{title:'Harar et al. (§72019§r)', author: 'Pavol Harar; Jesus B. Alonso-Hernandez; Jiri Mekyska; Zoltan Galaz; Radim Burget; Zdenek Smekal', display:{Lore:['[{"text": "arXiv:1907.05905", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Pathology Detection Using Deep Learning: a Preliminary Study\\u00a7r\\n\\n\\u00a78\\u00a7oPavol Harar\\nJesus B. Alonso-Hernandez\\nJiri Mekyska\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.05905\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/IWOBI.2017.7985525\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIn 2017 international conference and workshop on bioinspired\\n  intelligence (IWOBI), pp. 1-4. IEEE, 2017\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Jul 2019 18:06:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 1 figure, 5 tables\\u00a7r"}']}
{title:'Bai et al. (§72019§r)', author: 'Ye Bai; Jiangyan Yi; Jianhua Tao; Zhengkun Tian; Zhengqi Wen', display:{Lore:['[{"text": "arXiv:1907.06017", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearn Spelling from Teachers: Transferring Knowledge from Language Models to Sequence-to-Sequence Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYe Bai\\nJiangyan Yi\\nJianhua Tao\\nZhengkun Tian\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.06017\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 13 Jul 2019 06:27:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted by INTERSPEECH 2019\\u00a7r"}']}
{title:'Maghsoodi et al. (§72019§r)', author: 'Nooshin Maghsoodi; Hossein Sameti; Hossein Zeinali; Themos~Stafylakis', display:{Lore:['[{"text": "arXiv:1907.06111", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Recognition with Random Digit Strings Using Uncertainty Normalized HMM-based i-vectors\\u00a7r\\n\\n\\u00a78\\u00a7oNooshin Maghsoodi\\nHossein Sameti\\nHossein Zeinali\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.06111\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 13 Jul 2019 17:52:17 GMT)\\u00a7r"}']}
{title:'Zeinali et al. (§72019§r)', author: 'Hossein Zeinali; Pavel Matějka; Ladislav Mošner; Oldřich Plchot; Anna Silnova; Ondřej Novotný; Ján Profant; Ondřej Glembek; Lukáš Burget', display:{Lore:['[{"text": "arXiv:1907.06112", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBUT VOiCES 2019 System Description\\u00a7r\\n\\n\\u00a78\\u00a7oHossein Zeinali\\nPavel Mat\\u011bjka\\nLadislav Mo\\u0161ner\\n+ 5 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.06112\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 13 Jul 2019 17:57:55 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72019§r)', author: 'Hangting Chen; Zuozhen Liu; Zongming Liu; Pengyuan Zhang; Yonghong Yan', display:{Lore:['[{"text": "arXiv:1907.06639", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntegrating the Data Augmentation Scheme with Various Classifiers for Acoustic Scene Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oHangting Chen\\nZuozhen Liu\\nZongming Liu\\nPengyuan Zhang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.06639\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Jul 2019 08:17:34 GMT)\\u00a7r"}']}
{title:'Dhawan et al. (§72019§r)', author: 'Kunal Dhawan; Colin Vaz; Ruchir Travadi; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:1907.06859", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Adapting NMF Dictionaries Using Total Variability Modeling for Noise-Robust Acoustic Features\\u00a7r\\n\\n\\u00a78\\u00a7oKunal Dhawan\\nColin Vaz\\nRuchir Travadi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.06859\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Jul 2019 06:30:29 GMT)\\u00a7r"}']}
{title:'Zeinali et al. (§72019§r)', author: 'Hossein Zeinali; Lukáš Burget; Jan "Honza\'\' Černocký', display:{Lore:['[{"text": "arXiv:1907.07127", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic Scene Classification Using Fusion of Attentive Convolutional Neural Networks for DCASE2019 Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oHossein Zeinali\\nLuk\\u00e1\\u0161 Burget\\nJan \\"Honza\'\' \\u010cernock\\u00fd\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.07127\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 13 Jul 2019 18:10:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:1810.04273\\u00a7r"}']}
{title:'Tang et al. (§72019§r)', author: 'Zhiyuan Tang; Dong Wang; Liming Song', display:{Lore:['[{"text": "arXiv:1907.07626", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAP19-OLR Challenge: Three Tasks and Their Baselines\\u00a7r\\n\\n\\u00a78\\u00a7oZhiyuan Tang\\nDong Wang\\nLiming Song\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.07626\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 1 Sep 2019 10:25:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:1806.00616, arXiv:1706.09742, arXiv:1609.08445\\u00a7r"}']}
{title:'Narayanan et al. (§72019§r)', author: 'Praveen Narayanan; Punarjay Chakravarty; Francois Charette; Gint Puskorius', display:{Lore:['[{"text": "arXiv:1907.07769", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierarchical Sequence to Sequence Voice Conversion with Limited Data\\u00a7r\\n\\n\\u00a78\\u00a7oPraveen Narayanan\\nPunarjay Chakravarty\\nFrancois Charette\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.07769\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Jul 2019 07:54:46 GMT)\\u00a7r"}']}
{title:'Dhawan et al. (§72019§r)', author: 'Kunal Dhawan; Ganji Sreeram; Kumar Priyadarshi; Rohit Sinha', display:{Lore:['[{"text": "arXiv:1907.08293", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating Target Set Reduction for End-to-End Speech Recognition of Hindi-English Code-Switching Data\\u00a7r\\n\\n\\u00a78\\u00a7oKunal Dhawan\\nGanji Sreeram\\nKumar Priyadarshi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.08293\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Jul 2019 06:34:28 GMT)\\u00a7r"}']}
{title:'Saito et al. (§72019§r)', author: 'Yuki Saito; Shinnosuke Takamichi; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:1907.08294", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDNN-based Speaker Embedding Using Subjective Inter-speaker Similarity for Multi-speaker Modeling in Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYuki Saito\\nShinnosuke Takamichi\\nHiroshi Saruwatari\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.08294\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Jul 2019 09:11:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 7 figures, accepted for The 10th ISCA Speech Synthesis Workshop (SSW10)\\u00a7r"}']}
{title:'Koizumi et al. (§72019§r)', author: 'Yuma Koizumi; Shoichiro Saito; Masataka Yamaguchi; Shin Murata; Noboru Harada', display:{Lore:['[{"text": "arXiv:1907.08338", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBatch Uniformization for Minimizing Maximum Anomaly Score of DNN-based Anomaly Detection in Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oYuma Koizumi\\nShoichiro Saito\\nMasataka Yamaguchi\\nShin Murata\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.08338\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Jul 2019 01:58:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, to appear in IEEE WASPAA 2019\\u00a7r"}']}
{title:'Zheng et al. (§72019§r)', author: 'Yibin Zheng; Xi Wang; Lei He; Shifeng Pan; Frank K. Soong; Zhengqi Wen; Jianhua Tao', display:{Lore:['[{"text": "arXiv:1907.09006", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lForward-Backward Decoding for Regularizing End-to-End TTS\\u00a7r\\n\\n\\u00a78\\u00a7oYibin Zheng\\nXi Wang\\nLei He\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.09006\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Jul 2019 12:24:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH2019. arXiv admin note: text overlap with arXiv:1808.04064, arXiv:1804.05374 by other authors\\u00a7r"}']}
{title:'Tobing et al. (§72019§r)', author: 'Patrick Lumban Tobing; Yi-Chiao Wu; Tomoki Hayashi; Kazuhiro Kobayashi; Tomoki Toda', display:{Lore:['[{"text": "arXiv:1907.10185", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-Parallel Voice Conversion with Cyclic Variational Autoencoder\\u00a7r\\n\\n\\u00a78\\u00a7oPatrick Lumban Tobing\\nYi-Chiao Wu\\nTomoki Hayashi\\nKazuhiro Kobayashi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.10185\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Jul 2019 00:37:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2019\\u00a7r"}']}
{title:'Lin et al. (§72019§r)', author: 'Qingjian Lin; Ruiqing Yin; Ming Li; Hervé Bredin; Claude Barras', display:{Lore:['[{"text": "arXiv:1907.10393", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLSTM based Similarity Measurement with Spectral Clustering for Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oQingjian Lin\\nRuiqing Yin\\nMing Li\\nHerv\\u00e9 Bredin\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.10393\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2019-1388\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Jul 2019 04:17:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for INTERSPEECH 2019\\u00a7r"}']}
{title:'Hajavi et al. (§72019§r)', author: 'Amirhossein Hajavi; Ali Etemad', display:{Lore:['[{"text": "arXiv:1907.10420", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Deep Neural Network for Short-Segment Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAmirhossein Hajavi\\nAli Etemad\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.10420\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Jul 2019 23:43:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in Interspeech 2019\\u00a7r"}']}
{title:'Kim et al. (§72019§r)', author: 'Suyoun Kim; Siddharth Dalmia; Florian Metze', display:{Lore:['[{"text": "arXiv:1907.10726", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-Attention End-to-End ASR for Two-Party Conversations\\u00a7r\\n\\n\\u00a78\\u00a7oSuyoun Kim\\nSiddharth Dalmia\\nFlorian Metze\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.10726\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Jul 2019 21:18:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2019\\u00a7r"}']}
{title:'Badi et al. (§72019§r)', author: 'Alzahra Badi; Sangwook Park; David K. Han; Hanseok Ko', display:{Lore:['[{"text": "arXiv:1907.11361", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCorrelation Distance Skip Connection Denoising Autoencoder (CDSK-DAE) for Speech Feature Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oAlzahra Badi\\nSangwook Park\\nDavid K. Han\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.11361\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Jul 2019 02:25:44 GMT)\\u00a7r"}']}
{title:'Huang et al. (§72019§r)', author: 'Wen-Chin Huang; Yi-Chiao Wu; Kazuhiro Kobayashi; Yu-Huai Peng; Hsin-Te Hwang; Patrick Lumban Tobing; Yu Tsao; Hsin-Min Wang; Tomoki Toda', display:{Lore:['[{"text": "arXiv:1907.11898", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeneralization of Spectrum Differential based Direct Waveform Modification for Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oWen-Chin Huang\\nYi-Chiao Wu\\nKazuhiro Kobayashi\\n+ 5 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.11898\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 27 Jul 2019 11:57:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 4 figures, 1 table; accepted to the 10th ISCA speech synthesis workshop (SSW10)\\u00a7r"}']}
{title:'Čmejla et al. (§72019§r)', author: 'Jaroslav Čmejla; Tomáš Kounovský; Sharon Gannot; Zbyněk Koldovský; Pinchas Tandeitnik', display:{Lore:['[{"text": "arXiv:1907.12421", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMIRaGe: Multichannel Database Of Room Impulse Responses Measured On High-Resolution Cube-Shaped Grid In Multiple Acoustic Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oJaroslav \\u010cmejla\\nTom\\u00e1\\u0161 Kounovsk\\u00fd\\nSharon Gannot\\nZbyn\\u011bk Koldovsk\\u00fd\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.12421\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 Jul 2019 13:35:49 GMT)\\u00a7r"}']}
{title:'Grondin et al. (§72019§r)', author: 'Francois Grondin; James Glass', display:{Lore:['[{"text": "arXiv:1907.12621", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast and Robust 3-D Sound Source Localization with DSVD-PHAT\\u00a7r\\n\\n\\u00a78\\u00a7oFrancois Grondin\\nJames Glass\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.12621\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 Jul 2019 20:04:14 GMT)\\u00a7r"}']}
{title:'Sercu et al. (§72019§r)', author: 'Tom Sercu; Neil Mallinar', display:{Lore:['[{"text": "arXiv:1907.13121", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Frame Cross-Entropy Training for Convolutional Neural Networks in Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oTom Sercu\\nNeil Mallinar\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1907.13121\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 Jul 2019 19:56:46 GMT)\\u00a7r"}']}
{title:'Kapka et al. (§72019§r)', author: 'Sławomir Kapka; Mateusz Lewandowski', display:{Lore:['[{"text": "arXiv:1908.00766", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound source detection, localization and classification using consecutive ensemble of CRNN models\\u00a7r\\n\\n\\u00a78\\u00a7oS\\u0142awomir Kapka\\nMateusz Lewandowski\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.00766\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.33682/1syg-dy60\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the Detection and Classification of Acoustic Scenes\\n  and Events 2019 Workshop (DCASE2019), New York University, NY, USA, October\\n  2019\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 5 Sep 2019 09:55:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, conference\\u00a7r"}']}
{title:'Xia et al. (§72019§r)', author: 'Wei Xia; Kazuhito Koishida', display:{Lore:['[{"text": "arXiv:1908.01399", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Event Detection in Multichannel Audio using Convolutional Time-Frequency-Channel Squeeze and Excitation\\u00a7r\\n\\n\\u00a78\\u00a7oWei Xia\\nKazuhito Koishida\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.01399\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 4 Aug 2019 20:58:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2019\\u00a7r"}']}
{title:'Xia et al. (§72019§r)', author: 'Wei Xia; Jing Huang; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:1908.01447", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-lingual Text-independent Speaker Verification using Unsupervised Adversarial Discriminative Domain Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oWei Xia\\nJing Huang\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.01447\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8682259\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Aug 2019 02:35:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in ICASSP2019\\u00a7r"}']}
{title:'Yousefi et al. (§72019§r)', author: 'Midia Yousefi; Soheil Khorram; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:1908.01768", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProbabilistic Permutation Invariant Training for Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oMidia Yousefi\\nSoheil Khorram\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.01768\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 4 Aug 2019 17:42:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2019\\u00a7r"}']}
{title:'Hatala (§72019§r)', author: 'Zulkarnaen Hatala', display:{Lore:['[{"text": "arXiv:1908.02119", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPractical Speech Recognition with HTK\\u00a7r\\n\\n\\u00a78\\u00a7oZulkarnaen Hatala\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.02119\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Aug 2019 13:12:57 GMT)\\u00a7r"}']}
{title:'Ren et al. (§72019§r)', author: 'Zongze Ren; Zhiyong Chen; Shugong Xu', display:{Lore:['[{"text": "arXiv:1908.02283", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTriplet Based Embedding Distance and Similarity Learning for Text-independent Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oZongze Ren\\nZhiyong Chen\\nShugong Xu\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.02283\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Aug 2019 04:23:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, Accepted to The Asia-Pacific Signal and Information ProcessingAssociation Annual Summit and Conference 2019 (APSIPA ASC 2019)\\u00a7r"}']}
{title:'Yun et al. (§72019§r)', author: 'Sungrack Yun; Janghoon Cho; Jungyun Eum; Wonil Chang; Kyuwoong Hwang', display:{Lore:['[{"text": "arXiv:1908.02612", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn End-to-End Text-independent Speaker Verification Framework with a Keyword Adversarial Network\\u00a7r\\n\\n\\u00a78\\u00a7oSungrack Yun\\nJanghoon Cho\\nJungyun Eum\\nWonil Chang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.02612\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Aug 2019 11:05:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWill be appeared in INTERSPEECH 2019\\u00a7r"}']}
{title:'Nakatani et al. (§72019§r)', author: 'Tomohiro Nakatani; Keisuke Kinoshita', display:{Lore:['[{"text": "arXiv:1908.02710", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMaximum likelihood convolutional beamformer for simultaneous denoising and dereverberation\\u00a7r\\n\\n\\u00a78\\u00a7oTomohiro Nakatani\\nKeisuke Kinoshita\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.02710\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Aug 2019 10:19:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for EUSIPCO 2019. arXiv admin note: text overlapwith arXiv:1812.08400\\u00a7r"}']}
{title:'Kim et al. (§72019§r)', author: 'Bongjun Kim; Shabnam Ghaffarzadegan', display:{Lore:['[{"text": "arXiv:1908.02876", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervised Attention Model for Weakly Labeled Audio Event Classification\\u00a7r\\n\\n\\u00a78\\u00a7oBongjun Kim\\nShabnam Ghaffarzadegan\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.02876\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nEuropean Signal Processing Conference, EUSIPCO 2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Aug 2019 23:48:34 GMT)\\u00a7r"}']}
{title:'Gupta et al. (§72019§r)', author: 'Shruti Gupta; Md. Shah Fahad; Akshay Deepak', display:{Lore:['[{"text": "arXiv:1908.03054", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPitch-Synchronous Single Frequency Filtering Spectrogram for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oShruti Gupta\\nMd. Shah Fahad\\nAkshay Deepak\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.03054\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Aug 2019 11:49:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages and less than 20 figures\\u00a7r"}']}
{title:'Koizumi et al. (§72019§r)', author: 'Yuma Koizumi; Shoichiro Saito; Hisashi Uematsu; Noboru Harada; Keisuke Imoto', display:{Lore:['[{"text": "arXiv:1908.03299", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lToyADMOS: A Dataset of Miniature-Machine Operating Sounds for Anomalous Sound Detection\\u00a7r\\n\\n\\u00a78\\u00a7oYuma Koizumi\\nShoichiro Saito\\nHisashi Uematsu\\nNoboru Harada\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.03299\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Aug 2019 03:52:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, to appear in IEEE WASPAA 2019\\u00a7r"}']}
{title:'Feng et al. (§72019§r)', author: 'Siyuan Feng; Tan Lee', display:{Lore:['[{"text": "arXiv:1908.03538", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting Cross-Lingual Speaker and Phonetic Diversity for Unsupervised Subword Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oSiyuan Feng\\nTan Lee\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.03538\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2019.2937953\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 29 Sep 2019 09:36:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 6 figures. Manuscript published inthe IEEE/ACM Transactions on Audio, Speech and Language Processing (Volume: 27 ,Issue: 12 , Dec. 2019)\\u00a7r"}']}
{title:'Sadiq et al. (§72019§r)', author: 'Rizwan Sadiq; Sasan AsadiAbadi; Engin Erzin', display:{Lore:['[{"text": "arXiv:1908.03904", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotion Dependent Facial Animation from Affective Speech\\u00a7r\\n\\n\\u00a78\\u00a7oRizwan Sadiq\\nSasan AsadiAbadi\\nEngin Erzin\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.03904\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 11 Aug 2019 13:15:18 GMT)\\u00a7r"}']}
{title:'Matt et al. (§72019§r)', author: 'Amogh Matt; Dan Stowell', display:{Lore:['[{"text": "arXiv:1908.04672", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEstimating     Mitigating the Impact of Acoustic Environments on Machine-to-Machine Signalling\\u00a7r\\n\\n\\u00a78\\u00a7oAmogh Matt\\nDan Stowell\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.04672\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Aug 2019 14:45:27 GMT)\\u00a7r"}']}
{title:'Denisov et al. (§72019§r)', author: 'Pavel Denisov; Ngoc Thang Vu', display:{Lore:['[{"text": "arXiv:1908.04737", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Multi-Speaker Speech Recognition using Speaker Embeddings and Transfer Learning\\u00a7r\\n\\n\\u00a78\\u00a7oPavel Denisov\\nNgoc Thang Vu\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.04737\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Aug 2019 16:56:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2019\\u00a7r"}']}
{title:'Gößling et al. (§72019§r)', author: 'Nico Gößling; Wiebke Middelberg; Simon Doclo', display:{Lore:['[{"text": "arXiv:1908.04848", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRTF-steered binaural MVDR beamforming incorporating multiple external microphones\\u00a7r\\n\\n\\u00a78\\u00a7oNico G\\u00f6\\u00dfling\\nWiebke Middelberg\\nSimon Doclo\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.04848\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/WASPAA.2019.8937151\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Aug 2019 20:37:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at WASPAA 2019\\u00a7r"}']}
{title:'Xu et al. (§72019§r)', author: 'Ziyi Xu; Samy Elshamy; Ziyue Zhao; Tim Fingscheidt', display:{Lore:['[{"text": "arXiv:1908.05087", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComponents Loss for Neural Networks in Mask-Based Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oZiyi Xu\\nSamy Elshamy\\nZiyue Zhao\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.05087\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Aug 2019 12:03:27 GMT)\\u00a7r"}']}
{title:'Dey et al. (§72019§r)', author: 'Subhadeep Dey; Petr Motlicek; Trung Bui; Franck Dernoncourt', display:{Lore:['[{"text": "arXiv:1908.05227", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting semi-supervised training through a dropout regularization in end-to-end speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSubhadeep Dey\\nPetr Motlicek\\nTrung Bui\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.05227\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Aug 2019 19:21:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2019\\u00a7r"}']}
{title:'Gref et al. (§72019§r)', author: 'Michael Gref; Christoph Schmidt; Sven Behnke; Joachim Köhler', display:{Lore:['[{"text": "arXiv:1908.06709", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTwo-Staged Acoustic Modeling Adaption for Robust Speech Recognition by the Example of German Oral History Interviews\\u00a7r\\n\\n\\u00a78\\u00a7oMichael Gref\\nChristoph Schmidt\\nSven Behnke\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.06709\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICME.2019.00142\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE International Conference on Multimedia and Expo (ICME),\\n  Shanghai, China, July 2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Aug 2019 11:45:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for IEEE International Conference on Multimedia and Expo (ICME), Shanghai, China, July 2019\\u00a7r"}']}
{title:'Kleijn et al. (§72019§r)', author: 'W. Bastiaan Kleijn; Felicia S. C. Lim; Michael Chinen; Jan Skoglund', display:{Lore:['[{"text": "arXiv:1908.07045", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSalient Speech Representations Based on Cloned Networks\\u00a7r\\n\\n\\u00a78\\u00a7oW. Bastiaan Kleijn\\nFelicia S. C. Lim\\nMichael Chinen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.07045\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Aug 2019 19:38:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2019\\u00a7r"}']}
{title:'Tripathi et al. (§72019§r)', author: 'Kumud Tripathi; K. Sreenivasa Rao', display:{Lore:['[{"text": "arXiv:1908.08668", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVOP Detection for Read and Conversation Speech using CWT Coefficients and Phone Boundaries\\u00a7r\\n\\n\\u00a78\\u00a7oKumud Tripathi\\nK. Sreenivasa Rao\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.08668\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Aug 2019 05:14:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o21 pages, 8 figures, 4 tables, article\\u00a7r"}']}
{title:'Kim et al. (§72019§r)', author: 'Sunwoo Kim; Mrinmoy Maity; Minje Kim', display:{Lore:['[{"text": "arXiv:1908.08898", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIncremental Binarization On Recurrent Neural Networks For Single-Channel Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oSunwoo Kim\\nMrinmoy Maity\\nMinje Kim\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.08898\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8682595\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Aug 2019 16:38:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2019)\\u00a7r"}']}
{title:'Tripathi et al. (§72019§r)', author: 'Kumud Tripathi; M. Kiran Reddy; K. Sreenivasa Rao', display:{Lore:['[{"text": "arXiv:1908.09634", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultilingual and Multimode Phone Recognition System for Indian Languages\\u00a7r\\n\\n\\u00a78\\u00a7oKumud Tripathi\\nM. Kiran Reddy\\nK. Sreenivasa Rao\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.09634\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Aug 2019 06:16:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o33 pages, 5 figures, 6 tables, article\\u00a7r"}']}
{title:'Pusateri et al. (§72019§r)', author: 'Ernest Pusateri; Christophe Van Gysel; Rami Botros; Sameer Badaskar; Mirko Hannemann; Youssef Oualil; Ilya Oparin', display:{Lore:['[{"text": "arXiv:1908.09738", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConnecting and Comparing Language Model Interpolation Techniques\\u00a7r\\n\\n\\u00a78\\u00a7oErnest Pusateri\\nChristophe Van Gysel\\nRami Botros\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.09738\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Aug 2019 15:32:44 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72019§r)', author: 'Sunwoo Kim; Minje Kim', display:{Lore:['[{"text": "arXiv:1908.09799", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNearest Neighbor Search-Based Bitwise Source Separation Using Discriminant Winner-Take-All Hashing\\u00a7r\\n\\n\\u00a78\\u00a7oSunwoo Kim\\nMinje Kim\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.09799\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Aug 2019 16:58:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures\\u00a7r"}']}
{title:'Wang et al. (§72019§r)', author: 'Xueyi Wang; Lantian Li; Dong Wang', display:{Lore:['[{"text": "arXiv:1908.10092", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVAE-based Domain Adaptation for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oXueyi Wang\\nLantian Li\\nDong Wang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.10092\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Aug 2019 09:09:48 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72019§r)', author: 'Xin Wang; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:1908.10256", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Harmonic-plus-Noise Waveform Model with Trainable Maximum Voice Frequency for Text-to-Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oXin Wang\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.10256\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Aug 2019 15:01:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Speech Synthesis Workshop 2019\\u00a7r"}']}
{title:'Yasuda et al. (§72019§r)', author: 'Yusuke Yasuda; Xin Wang; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:1908.11535", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInitial investigation of an encoder-decoder end-to-end TTS framework using marginalization of monotonic hard latent alignments\\u00a7r\\n\\n\\u00a78\\u00a7oYusuke Yasuda\\nXin Wang\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1908.11535\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Aug 2019 05:00:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be appeared at SSW10\\u00a7r"}']}
{title:'Dimitriadis (§72019§r)', author: 'Dimitrios Dimitriadis', display:{Lore:['[{"text": "arXiv:1909.00082", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancements for Audio-only Diarization Systems\\u00a7r\\n\\n\\u00a78\\u00a7oDimitrios Dimitriadis\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.00082\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Aug 2019 22:33:06 GMT)\\u00a7r"}']}
{title:'Yu et al. (§72019§r)', author: 'Yue Yu; Siyao Peng; Grace Hui Yang', display:{Lore:['[{"text": "arXiv:1909.00521", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModeling Long-Range Context for Concurrent Dialogue Acts Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYue Yu\\nSiyao Peng\\nGrace Hui Yang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.00521\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3357384.3358145\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 22 Oct 2019 13:28:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to CIKM\'19\\u00a7r"}']}
{title:'Liu et al. (§72019§r)', author: 'Peng Liu; Xixin Wu; Shiyin Kang; Guangzhi Li; Dan Su; Dong Yu', display:{Lore:['[{"text": "arXiv:1909.01145", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMaximizing Mutual Information for Tacotron\\u00a7r\\n\\n\\u00a78\\u00a7oPeng Liu\\nXixin Wu\\nShiyin Kang\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.01145\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 18 Nov 2019 07:24:35 GMT)\\u00a7r"}']}
{title:'Mantena et al. (§72019§r)', author: 'Gautam Mantena; Ozlem Kalinli; Ossama Abdel-Hamid; Don McAllaster', display:{Lore:['[{"text": "arXiv:1909.02667", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBandwidth Embeddings for Mixed-bandwidth Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGautam Mantena\\nOzlem Kalinli\\nOssama Abdel-Hamid\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.02667\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 5 Sep 2019 23:07:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oA part of this work is acceptedin Interspeech 2019 https://interspeech2019.org\\u00a7r"}']}
{title:'Mizgajski et al. (§72019§r)', author: 'Jan Mizgajski; Adrian Szymczak; Robert Głowski; Piotr Szymański; Piotr Żelasko; Łukasz Augustyniak; Mikołaj Morzy; Yishay Carmiel; Jeff Hodson; Łukasz Wójciak; Daniel Smoczyk; Adam Wróbel; Bartosz Borowik; Adam Artajew; Marcin Baran; Cezary Kwiatkowski; Marzena Żyła-Hoppe', display:{Lore:['[{"text": "arXiv:1909.02851", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAvaya Conversational Intelligence: A Real-Time System for Spoken Language Understanding in Human-Human Call Center Conversations\\u00a7r\\n\\n\\u00a78\\u00a7oJan Mizgajski\\nAdrian Szymczak\\nRobert G\\u0142owski\\n+ 13 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.02851\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 2 Sep 2019 22:57:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for Interspeech 2019\\u00a7r"}']}
{title:'Koutini et al. (§72019§r)', author: 'Khaled Koutini; Hamid Eghbal-zadeh; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:1909.02859", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReceptive-field-regularized CNN variants for acoustic scene classification\\u00a7r\\n\\n\\u00a78\\u00a7oKhaled Koutini\\nHamid Eghbal-zadeh\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.02859\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 5 Sep 2019 12:40:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Detection and Classification of Acoustic Scenes and Events 2019 (DCASE Workshop 2019)\\u00a7r"}']}
{title:'Primus et al. (§72019§r)', author: 'Paul Primus; Hamid Eghbal-zadeh; David Eitelsebner; Khaled Koutini; Andreas Arzt; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:1909.02869", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting Parallel Audio Recordings to Enforce Device Invariance in CNN-based Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oPaul Primus\\nHamid Eghbal-zadeh\\nDavid Eitelsebner\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.02869\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 4 Sep 2019 16:19:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at the Workshop on Detection and Classification of Acoustic Scenes and Events, 25-26 October 2019, New York, USA\\u00a7r"}']}
{title:'Clark et al. (§72019§r)', author: 'Rob Clark; Hanna Silen; Tom Kenter; Ralph Leith', display:{Lore:['[{"text": "arXiv:1909.03965", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluating Long-form Text-to-Speech: Comparing the Ratings of Sentences and Paragraphs\\u00a7r\\n\\n\\u00a78\\u00a7oRob Clark\\nHanna Silen\\nTom Kenter\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.03965\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 9 Sep 2019 16:13:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for The 10th ISCA Speech Synthesis Workshop (SSW10), 6 pages\\u00a7r"}']}
{title:'Reddy et al. (§72019§r)', author: 'M Kiran Reddy; K Sreenivasa Rao', display:{Lore:['[{"text": "arXiv:1909.03974", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDNN-based cross-lingual voice conversion using Bottleneck Features\\u00a7r\\n\\n\\u00a78\\u00a7oM Kiran Reddy\\nK Sreenivasa Rao\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.03974\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s11063-019-10149-y\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 10 Sep 2019 04:46:07 GMT)\\u00a7r"}']}
{title:'Lu et al. (§72019§r)', author: 'Liang Lu; Eric Sun; Yifan Gong', display:{Lore:['[{"text": "arXiv:1909.04157", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Teaching Networks\\u00a7r\\n\\n\\u00a78\\u00a7oLiang Lu\\nEric Sun\\nYifan Gong\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.04157\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 9 Sep 2019 21:11:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, Interspeech 2019\\u00a7r"}']}
{title:'Kawahara et al. (§72019§r)', author: 'Hideki Kawahara; Ken-Ichi Sakakibara; Mitsunori Mizumachi; Hideki Banno; Masanori Morise; Toshio Irino', display:{Lore:['[{"text": "arXiv:1909.04301", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrequency domain variant of Velvet noise and its application to acoustic measurements\\u00a7r\\n\\n\\u00a78\\u00a7oHideki Kawahara\\nKen-Ichi Sakakibara\\nMitsunori Mizumachi\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.04301\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/APSIPAASC47483.2019.9023247\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2019 Asia-Pacific Signal and Information Processing Association\\n  Annual Summit and Conference (APSIPA ASC), Lanzhou, China, 2019, pp.\\n  1523-1532\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Sep 2019 05:36:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 14 figures, APSIPA ASC 2019. arXiv admin note: text overlapwith arXiv:1806.06812\\u00a7r"}']}
{title:'Chinen et al. (§72019§r)', author: 'Michael Chinen; W. Bastiaan Kleijn; Felicia S. C. Lim; Jan Skoglund', display:{Lore:['[{"text": "arXiv:1909.04776", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerative Speech Enhancement Based on Cloned Networks\\u00a7r\\n\\n\\u00a78\\u00a7oMichael Chinen\\nW. Bastiaan Kleijn\\nFelicia S. C. Lim\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.04776\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Sep 2019 22:06:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted WASPAA 2019\\u00a7r"}']}
{title:'Kannan et al. (§72019§r)', author: 'Anjuli Kannan; Arindrima Datta; Tara N. Sainath; Eugene Weinstein; Bhuvana Ramabhadran; Yonghui Wu; Ankur Bapna; Zhifeng Chen; Seungji Lee', display:{Lore:['[{"text": "arXiv:1909.05330", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLarge-Scale Multilingual Speech Recognition with a Streaming End-to-End Model\\u00a7r\\n\\n\\u00a78\\u00a7oAnjuli Kannan\\nArindrima Datta\\nTara N. Sainath\\n+ 5 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.05330\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 11 Sep 2019 19:46:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in Interspeech 2019\\u00a7r"}']}
{title:'Gibbon et al. (§72019§r)', author: 'Dafydd Gibbon; Peng Li', display:{Lore:['[{"text": "arXiv:1909.05639", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQuantifying and Correlating Rhythm Formants in Speech\\u00a7r\\n\\n\\u00a78\\u00a7oDafydd Gibbon\\nPeng Li\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.05639\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 Sep 2019 16:18:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pagers, 7 figures, 2 tables, accepted: LPSS (Linguistic Properties of Spontaneous Speech, Taipei 2019)\\u00a7r"}']}
{title:'Fujita et al. (§72019§r)', author: 'Yusuke Fujita; Naoyuki Kanda; Shota Horiguchi; Kenji Nagamatsu; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:1909.05952", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Neural Speaker Diarization with Permutation-Free Objectives\\u00a7r\\n\\n\\u00a78\\u00a7oYusuke Fujita\\nNaoyuki Kanda\\nShota Horiguchi\\nKenji Nagamatsu\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.05952\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 12 Sep 2019 21:12:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2019\\u00a7r"}']}
{title:'Lin et al. (§72019§r)', author: 'Liwei Lin; Xiangdong Wang; Hong Liu; Yueliang Qian', display:{Lore:['[{"text": "arXiv:1909.06178", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGuided Learning Convolution System for DCASE 2019 Task 4\\u00a7r\\n\\n\\u00a78\\u00a7oLiwei Lin\\nXiangdong Wang\\nHong Liu\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.06178\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 11 Sep 2019 03:42:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccept by DCASE2019 Workshop\\u00a7r"}']}
{title:'Fujita et al. (§72019§r)', author: 'Yusuke Fujita; Naoyuki Kanda; Shota Horiguchi; Yawen Xue; Kenji Nagamatsu; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:1909.06247", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Neural Speaker Diarization with Self-attention\\u00a7r\\n\\n\\u00a78\\u00a7oYusuke Fujita\\nNaoyuki Kanda\\nShota Horiguchi\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.06247\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 13 Sep 2019 14:18:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ASRU 2019\\u00a7r"}']}
{title:'Raj et al. (§72019§r)', author: 'Desh Raj; David Snyder; Daniel Povey; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:1909.06351", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProbing the Information Encoded in X-vectors\\u00a7r\\n\\n\\u00a78\\u00a7oDesh Raj\\nDavid Snyder\\nDaniel Povey\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.06351\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ASRU46091.2019.9003979\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Automatic Speech Recognition and Understanding Workshop\\n  (ASRU) (2019): 726-733\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 30 Sep 2019 22:55:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at IEEEWorkshop on Automatic Speech Recognition andUnderstanding (ASRU) 2019\\u00a7r"}']}
{title:'Luong et al. (§72019§r)', author: 'Hieu-Thi Luong; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:1909.06532", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBootstrapping non-parallel voice conversion from speaker-adaptive text-to-speech\\u00a7r\\n\\n\\u00a78\\u00a7oHieu-Thi Luong\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.06532\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 14 Sep 2019 04:43:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for IEEE ASRU 2019\\u00a7r"}']}
{title:'Li et al. (§72019§r)', author: 'Qiujia Li; Chao Zhang; Philip C. Woodland', display:{Lore:['[{"text": "arXiv:1909.06614", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntegrating Source-channel and Attention-based Sequence-to-sequence Models for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oQiujia Li\\nChao Zhang\\nPhilip C. Woodland\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.06614\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 1 Oct 2019 11:09:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proc. ASRU2019, December 14-18, 2019, Sentosa, Singapore\\u00a7r"}']}
{title:'Sim et al. (§72019§r)', author: 'Khe Chai Sim; Petr Zadrazil; Françoise Beaufays', display:{Lore:['[{"text": "arXiv:1909.06678", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Investigation Into On-device Personalization of End-to-end Automatic Speech Recognition Models\\u00a7r\\n\\n\\u00a78\\u00a7oKhe Chai Sim\\nPetr Zadrazil\\nFran\\u00e7oise Beaufays\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.06678\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 14 Sep 2019 21:12:38 GMT)\\u00a7r"}']}
{title:'Tian et al. (§72019§r)', author: 'Xiaohai Tian; Rohan Kumar Das; Haizhou Li', display:{Lore:['[{"text": "arXiv:1909.07655", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBlack-box Attacks on Automatic Speaker Verification using Feedback-controlled Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oXiaohai Tian\\nRohan Kumar Das\\nHaizhou Li\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.07655\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 29 Oct 2019 09:35:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 3 figures, This paper is submitted to ICASSP 2020\\u00a7r"}']}
{title:'Ramos et al. (§72019§r)', author: 'Daniel Ramos; Juan Maroñas; Alicia Lozano-Diez', display:{Lore:['[{"text": "arXiv:1909.08315", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBayesian Strategies for Likelihood Ratio Computation in Forensic Voice Comparison with Automatic Systems\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Ramos\\nJuan Maro\\u00f1as\\nAlicia Lozano-Diez\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.08315\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 18 Sep 2019 09:39:41 GMT)\\u00a7r"}']}
{title:'Aloufi et al. (§72019§r)', author: 'Ranya Aloufi; Hamed Haddadi; David Boyle', display:{Lore:['[{"text": "arXiv:1909.08500", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotion Filtering at the Edge\\u00a7r\\n\\n\\u00a78\\u00a7oRanya Aloufi\\nHamed Haddadi\\nDavid Boyle\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.08500\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 18 Sep 2019 15:28:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 6 figures, Sensys-ML19 workshopin conjunction with the 17th ACM Conference on Embedded NetworkedSensor Systems (SenSys 2019)\\u00a7r"}']}
{title:'Wang et al. (§72019§r)', author: 'Weimin Wang; Weiran Wang; Ming Sun; Chao Wang', display:{Lore:['[{"text": "arXiv:1909.08961", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic scene analysis with multi-head attention networks\\u00a7r\\n\\n\\u00a78\\u00a7oWeimin Wang\\nWeiran Wang\\nMing Sun\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.08961\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 16 Sep 2019 14:53:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 6 figures\\u00a7r"}']}
{title:'Catellier et al. (§72019§r)', author: 'Andrew A. Catellier; Stephen D. Voran', display:{Lore:['[{"text": "arXiv:1909.09024", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWEnets: A Convolutional Framework for Evaluating Audio Waveforms\\u00a7r\\n\\n\\u00a78\\u00a7oAndrew A. Catellier\\nStephen D. Voran\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.09024\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 19 Sep 2019 14:45:59 GMT)\\u00a7r"}']}
{title:'Gupta et al. (§72019§r)', author: 'Chitralekha Gupta; Emre Yılmaz; Haizhou Li', display:{Lore:['[{"text": "arXiv:1909.10200", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Lyrics Alignment and Transcription in Polyphonic Music: Does Background Music Help?\\u00a7r\\n\\n\\u00a78\\u00a7oChitralekha Gupta\\nEmre Y\\u0131lmaz\\nHaizhou Li\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.10200\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 22 Oct 2019 07:21:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to 45thInternational Conference on Acoustics, Speech, and Signal Processing (ICASSP 2020)\\u00a7r"}']}
{title:'Shechtman et al. (§72019§r)', author: 'Slava Shechtman; Alex Sorin', display:{Lore:['[{"text": "arXiv:1909.10302", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSequence to Sequence Neural Speech Synthesis with Prosody Modification Capabilities\\u00a7r\\n\\n\\u00a78\\u00a7oSlava Shechtman\\nAlex Sorin\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.10302\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/SSW.2019-49\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. 10th ISCA Speech Synthesis Workshop, 275-280 (2019)\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 23 Sep 2019 11:43:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7opublished at 10thISCA Speech Synthesis Workshop (SSW-10, 2019)\\u00a7r"}']}
{title:'Wang et al. (§72019§r)', author: 'Pengwei Wang; Liangchen Wei; Yong Cao; Jinghui Xie; Yuji Cao; Zaiqing Nie', display:{Lore:['[{"text": "arXiv:1909.10924", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnderstanding Semantics from Speech Through Pre-training\\u00a7r\\n\\n\\u00a78\\u00a7oPengwei Wang\\nLiangchen Wei\\nYong Cao\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.10924\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 24 Sep 2019 13:49:14 GMT)\\u00a7r"}']}
{title:'Simon et al. (§72019§r)', author: 'Christian Simon; Matteo Torcoli; Jouni Paulus', display:{Lore:['[{"text": "arXiv:1909.11549", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMPEG-H Audio for Improving Accessibility in Broadcasting and Streaming\\u00a7r\\n\\n\\u00a78\\u00a7oChristian Simon\\nMatteo Torcoli\\nJouni Paulus\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.11549\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 25 Sep 2019 15:22:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWhite Paper\\u00a7r"}']}
{title:'Gurunath et al. (§72019§r)', author: 'Nishant Gurunath; Sai Krishna Rallabandi; Alan Black', display:{Lore:['[{"text": "arXiv:1909.11727", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDisentangling Speech and Non-Speech Components for Building Robust Acoustic Models from Found Data\\u00a7r\\n\\n\\u00a78\\u00a7oNishant Gurunath\\nSai Krishna Rallabandi\\nAlan Black\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.11727\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 25 Sep 2019 19:37:47 GMT)\\u00a7r"}']}
{title:'Jung et al. (§72019§r)', author: 'Youngmoon Jung; Yeunju Choi; Hoirin Kim', display:{Lore:['[{"text": "arXiv:1909.11886", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Adaptive Soft Voice Activity Detection using Deep Neural Networks for Robust Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oYoungmoon Jung\\nYeunju Choi\\nHoirin Kim\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.11886\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ASRU46091.2019.9003935\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. of ASRU 2019, pp. 365-372\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 26 Sep 2019 04:38:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at 2019 IEEE Automatic Speech Recognition andUnderstanding Workshop (ASRU 2019)\\u00a7r"}']}
{title:'Tian et al. (§72019§r)', author: 'Zhengkun Tian; Jiangyan Yi; Jianhua Tao; Ye Bai; Zhengqi Wen', display:{Lore:['[{"text": "arXiv:1909.13037", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Attention Transducers for End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhengkun Tian\\nJiangyan Yi\\nJianhua Tao\\nYe Bai\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.13037\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2019-2203\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2019, 4395-4399\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 28 Sep 2019 06:48:47 GMT)\\u00a7r"}']}
{title:'Luo et al. (§72019§r)', author: 'Yi Luo; Enea Ceolini; Cong Han; Shih-Chii Liu; Nima Mesgarani', display:{Lore:['[{"text": "arXiv:1909.13387", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFaSNet: Low-latency Adaptive Beamforming for Multi-microphone Audio Processing\\u00a7r\\n\\n\\u00a78\\u00a7oYi Luo\\nEnea Ceolini\\nCong Han\\nShih-Chii Liu\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.13387\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 1 Oct 2019 02:07:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ASRU 2019\\u00a7r"}']}
{title:'Van Segbroeck et al. (§72019§r)', author: 'Maarten Van Segbroeck; Ahmed Zaid; Ksenia Kutsenko; Cirenia Huerta; Tinh Nguyen; Xuewen Luo; Björn Hoffmeister; Jan Trmal; Maurizio Omologo; Roland Maas', display:{Lore:['[{"text": "arXiv:1909.13447", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiPCo \\u2013 Dinner Party Corpus\\u00a7r\\n\\n\\u00a78\\u00a7oMaarten Van Segbroeck\\nAhmed Zaid\\nKsenia Kutsenko\\n+ 6 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.13447\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 Sep 2019 04:15:59 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72019§r)', author: 'Linlin Wang; Yu Wang; Mark J. F. Gales', display:{Lore:['[{"text": "arXiv:1909.13695", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-native Speaker Verification for Spoken Language Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oLinlin Wang\\nYu Wang\\nMark J. F. Gales\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.13695\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 Sep 2019 13:42:06 GMT)\\u00a7r"}']}
{title:'Fainberg et al. (§72019§r)', author: 'Joachim Fainberg; Ondřej Klejch; Erfan Loweimi; Peter Bell; Steve Renals', display:{Lore:['[{"text": "arXiv:1909.13759", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic Model Adaptation from Raw Waveforms with SincNet\\u00a7r\\n\\n\\u00a78\\u00a7oJoachim Fainberg\\nOnd\\u0159ej Klejch\\nErfan Loweimi\\nPeter Bell\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1909.13759\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 Sep 2019 14:49:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEASRU 2019\\u00a7r"}']}
{title:'Jung et al. (§72019§r)', author: 'Myunghun Jung; Hyungjun Lim; Jahyun Goo; Youngmoon Jung; Hoirin Kim', display:{Lore:['[{"text": "arXiv:1910.00341", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdditional Shared Decoder on Siamese Multi-view Encoders for Learning Acoustic Word Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oMyunghun Jung\\nHyungjun Lim\\nJahyun Goo\\nYoungmoon Jung\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.00341\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Oct 2019 12:36:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at 2019 IEEE Automatic Speech Recognition andUnderstanding Workshop (ASRU 2019)\\u00a7r"}']}
{title:'Zhou et al. (§72019§r)', author: 'Yi Zhou; Xiaohai Tian; Emre Yılmaz; Rohan Kumar Das; Haizhou Li', display:{Lore:['[{"text": "arXiv:1910.00496", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Modularized Neural Network with Language-Specific Output Layers for Cross-lingual Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oYi Zhou\\nXiaohai Tian\\nEmre Y\\u0131lmaz\\nRohan Kumar Das\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.00496\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Oct 2019 15:52:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at IEEE ASRUWorkshop 2019\\u00a7r"}']}
{title:'Ghorbani et al. (§72019§r)', author: 'Shahram Ghorbani; Soheil Khorram; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:1910.00565", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDomain Expansion in DNN-based Acoustic Models for Robust Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oShahram Ghorbani\\nSoheil Khorram\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.00565\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Oct 2019 17:40:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ASRU, 2019\\u00a7r"}']}
{title:'Le et al. (§72019§r)', author: 'Duc Le; Xiaohui Zhang; Weiyi Zheng; Christian Fügen; Geoffrey Zweig; Michael L. Seltzer', display:{Lore:['[{"text": "arXiv:1910.01493", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrom Senones to Chenones: Tied Context-Dependent Graphemes for Hybrid Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDuc Le\\nXiaohui Zhang\\nWeiyi Zheng\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.01493\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 11 Oct 2019 21:45:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at ASRU 2019\\u00a7r"}']}
{title:'Vieira et al. (§72019§r)', author: 'V. Vieira; R. Coelho; F. Assis', display:{Lore:['[{"text": "arXiv:1910.01967", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lObjective Human Affective Vocal Expression Detection and Automatic Classification with Stochastic Models and Learning Systems\\u00a7r\\n\\n\\u00a78\\u00a7oV. Vieira\\nR. Coelho\\nF. Assis\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.01967\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Oct 2019 14:29:34 GMT)\\u00a7r"}']}
{title:'Abad et al. (§72019§r)', author: 'Alberto Abad; Peter Bell; Andrea Carmantini; Steve Renals', display:{Lore:['[{"text": "arXiv:1910.02168", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross lingual transfer learning for zero-resource domain adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oAlberto Abad\\nPeter Bell\\nAndrea Carmantini\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.02168\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 29 Oct 2019 18:03:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020. Main updates wrt previous versions: same networkconfig in all experiments, added Babel/Material LR target language experiments, added comparison with alternative/similar methods of "}','{"text": "cross-lingual adaptation\\u00a7r"}']}
{title:'Alves et al. (§72019§r)', author: 'M. Alves; R. Coelho; E. Dranka', display:{Lore:['[{"text": "arXiv:1910.02709", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEffective Acoustic Energy Sensing Exploitation for Target Sources Localization in Urban Acoustic Scenes\\u00a7r\\n\\n\\u00a78\\u00a7oM. Alves\\nR. Coelho\\nE. Dranka\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.02709\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Oct 2019 10:32:24 GMT)\\u00a7r"}']}
{title:'Medina et al. (§72019§r)', author: 'C. Medina; R. Coelho', display:{Lore:['[{"text": "arXiv:1910.02710", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImpulsive Noise Detection for Intelligibility and Quality Improvement of Speech Enhancement Methods Applied in Time-Domain\\u00a7r\\n\\n\\u00a78\\u00a7oC. Medina\\nR. Coelho\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.02710\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Oct 2019 10:32:50 GMT)\\u00a7r"}']}
{title:'Zucatelli et al. (§72019§r)', author: 'G. Zucatelli; R. Coelho', display:{Lore:['[{"text": "arXiv:1910.02712", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptive Reverberation Absorption using Non-stationary Masking Components Detection for Intelligibility Improvement\\u00a7r\\n\\n\\u00a78\\u00a7oG. Zucatelli\\nR. Coelho\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.02712\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2019.2950618\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Oct 2019 10:33:08 GMT)\\u00a7r"}']}
{title:'Pasini (§72019§r)', author: 'Marco Pasini', display:{Lore:['[{"text": "arXiv:1910.03713", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMelGAN-VC: Voice Conversion and Audio Style Transfer on arbitrarily long samples using Spectrograms\\u00a7r\\n\\n\\u00a78\\u00a7oMarco Pasini\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.03713\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 5 Dec 2019 16:28:50 GMT)\\u00a7r"}']}
{title:'Mazzon et al. (§72019§r)', author: 'Luca Mazzon; Yuma Koizumi; Masahiro Yasuda; Noboru Harada', display:{Lore:['[{"text": "arXiv:1910.04388", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFirst Order Ambisonics Domain Spatial Augmentation for DNN-based Direction of Arrival Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oLuca Mazzon\\nYuma Koizumi\\nMasahiro Yasuda\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.04388\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Oct 2019 06:38:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, to appear in DCASE 2019\\u00a7r"}']}
{title:'Yasuda et al. (§72019§r)', author: 'Masahiro Yasuda; Yuma Koizumi; Luca Mazzon; Shoichiro Saito; Hisashi Uematsu', display:{Lore:['[{"text": "arXiv:1910.04415", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDOA Estimation by DNN-based Denoising and Dereverberation from Sound Intensity Vector\\u00a7r\\n\\n\\u00a78\\u00a7oMasahiro Yasuda\\nYuma Koizumi\\nLuca Mazzon\\nShoichiro Saito\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.04415\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Oct 2019 07:57:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages\\u00a7r"}']}
{title:'Tits et al. (§72019§r)', author: 'Noé Tits; Kevin El Haddad; Thierry Dutoit', display:{Lore:['[{"text": "arXiv:1910.06234", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Theory behind Controllable Expressive Speech Synthesis: a Cross-disciplinary Approach\\u00a7r\\n\\n\\u00a78\\u00a7oNo\\u00e9 Tits\\nKevin El Haddad\\nThierry Dutoit\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.06234\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Oct 2019 16:08:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o19 pages, 6 figures. To be published in the book \\"Human Computer Interaction\\" edited by Prof. Yves Rybarczyk, published by IntechOpen\\u00a7r"}']}
{title:'Chang et al. (§72019§r)', author: 'Xuankai Chang; Wangyou Zhang; Yanmin Qian; Jonathan Le Roux; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:1910.06522", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMIMO-SPEECH: End-to-End Multi-Channel Multi-Speaker Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXuankai Chang\\nWangyou Zhang\\nYanmin Qian\\nJonathan Le Roux\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.06522\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 16 Oct 2019 02:18:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ASRU 2019\\u00a7r"}']}
{title:'Kumar et al. (§72019§r)', author: 'Kundan Kumar; Rithesh Kumar; Thibault de Boissiere; Lucas Gestin; Wei Zhen Teoh; Jose Sotelo; Alexandre de Brebisson; Yoshua Bengio; Aaron Courville', display:{Lore:['[{"text": "arXiv:1910.06711", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oKundan Kumar\\nRithesh Kumar\\nThibault de Boissiere\\n+ 5 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.06711\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 9 Dec 2019 01:17:32 GMT)\\u00a7r"}']}
{title:'Jafarlou et al. (§72019§r)', author: 'Salar Jafarlou; Soheil Khorram; Vinay Kothapally; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:1910.07047", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalyzing Large Receptive Field Convolutional Networks for Distant Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSalar Jafarlou\\nSoheil Khorram\\nVinay Kothapally\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.07047\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Oct 2019 20:47:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oASRU 2019\\u00a7r"}']}
{title:'Tsunoo et al. (§72019§r)', author: 'Emiru Tsunoo; Yosuke Kashiwagi; Toshiyuki Kumakura; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:1910.07204", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransformer ASR with Contextual Block Processing\\u00a7r\\n\\n\\u00a78\\u00a7oEmiru Tsunoo\\nYosuke Kashiwagi\\nToshiyuki Kumakura\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.07204\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Oct 2019 08:04:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ASRU 2019\\u00a7r"}']}
{title:'Chen et al. (§72019§r)', author: 'Hangting Chen; Pengyuan Zhang; Yonghong Yan', display:{Lore:['[{"text": "arXiv:1910.07753", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Talker MVDR Beamforming Based on Extended Complex Gaussian Mixture Model\\u00a7r\\n\\n\\u00a78\\u00a7oHangting Chen\\nPengyuan Zhang\\nYonghong Yan\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.07753\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Oct 2019 07:51:36 GMT)\\u00a7r"}']}
{title:'Togami (§72019§r)', author: 'Masahito Togami', display:{Lore:['[{"text": "arXiv:1910.08710", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-channel Time-Varying Covariance Matrix Model for Late Reverberation Reduction\\u00a7r\\n\\n\\u00a78\\u00a7oMasahito Togami\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.08710\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 19 Oct 2019 06:15:29 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72019§r)', author: 'Songxiang Liu; Haibin Wu; Hung-yi Lee; Helen Meng', display:{Lore:['[{"text": "arXiv:1910.08716", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Attacks on Spoofing Countermeasures of automatic speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oSongxiang Liu\\nHaibin Wu\\nHung-yi Lee\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.08716\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 19 Oct 2019 07:28:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ASRU 2019\\u00a7r"}']}
{title:'Landini et al. (§72019§r)', author: 'Federico Landini; Shuai Wang; Mireia Diez; Lukáš Burget; Pavel Matějka; Kateřina Žmolíková; Ladislav Mošner; Oldřich Plchot; Ondřej Novotný; Hossein Zeinali; Johan Rohdin', display:{Lore:['[{"text": "arXiv:1910.08847", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBUT System Description for DIHARD Speech Diarization Challenge 2019\\u00a7r\\n\\n\\u00a78\\u00a7oFederico Landini\\nShuai Wang\\nMireia Diez\\n+ 7 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.08847\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 19 Oct 2019 21:37:20 GMT)\\u00a7r"}']}
{title:'Lugosch et al. (§72019§r)', author: 'Loren Lugosch; Brett Meyer; Derek Nowrouzezahrai; Mirco Ravanelli', display:{Lore:['[{"text": "arXiv:1910.09463", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing Speech Synthesis to Train End-to-End Spoken Language Understanding Models\\u00a7r\\n\\n\\u00a78\\u00a7oLoren Lugosch\\nBrett Meyer\\nDerek Nowrouzezahrai\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.09463\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Oct 2019 15:47:52 GMT)\\u00a7r"}']}
{title:'Spadini et al. (§72019§r)', author: 'Tito Spadini; Ricardo Suyama', display:{Lore:['[{"text": "arXiv:1910.09522", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparative Study between Adversarial Networks and Classical Techniques for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oTito Spadini\\nRicardo Suyama\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.09522\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Oct 2019 17:28:12 GMT)\\u00a7r"}']}
{title:'Chetupalli et al. (§72019§r)', author: 'Srikanth Raj Chetupalli; Thippur V. Sreenivas', display:{Lore:['[{"text": "arXiv:1910.09782", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint spatial filter and time-varying MCLP for dereverberation and interference suppression of a dynamic/static speech source\\u00a7r\\n\\n\\u00a78\\u00a7oSrikanth Raj Chetupalli\\nThippur V. Sreenivas\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.09782\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Oct 2019 06:16:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oManuscript submitted for review to IEEE/ACM Transactionson Audio, Speech, and Language Processing on 18 Jul 2019\\u00a7r"}']}
{title:'Grondin et al. (§72019§r)', author: 'Francois Grondin; James Glass; Iwona Sobieraj; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:1910.10049", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Event Localization and Detection Using CRNN on Pairs of Microphones\\u00a7r\\n\\n\\u00a78\\u00a7oFrancois Grondin\\nJames Glass\\nIwona Sobieraj\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.10049\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Oct 2019 15:41:52 GMT)\\u00a7r"}']}
{title:'Kriman et al. (§72019§r)', author: 'Samuel Kriman; Stanislav Beliaev; Boris Ginsburg; Jocelyn Huang; Oleksii Kuchaiev; Vitaly Lavrukhin; Ryan Leary; Jason Li; Yang Zhang', display:{Lore:['[{"text": "arXiv:1910.10261", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQuartzNet: Deep Automatic Speech Recognition with 1D Time-Channel Separable Convolutions\\u00a7r\\n\\n\\u00a78\\u00a7oSamuel Kriman\\nStanislav Beliaev\\nBoris Ginsburg\\n+ 5 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.10261\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Oct 2019 22:34:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Lu (§72019§r)', author: 'Liang Lu', display:{Lore:['[{"text": "arXiv:1910.10352", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Transformer with Interleaved Self-attention and Convolution for Hybrid Acoustic Models\\u00a7r\\n\\n\\u00a78\\u00a7oLiang Lu\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.10352\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 23 Oct 2019 04:57:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, submitted to ICASSP 2019\\u00a7r"}']}
{title:'Li et al. (§72019§r)', author: 'Meng-Zhen Li; Xiao-Lei Zhang', display:{Lore:['[{"text": "arXiv:1910.10969", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning deep representations by multilayer bootstrap networks for speaker diarization\\u00a7r\\n\\n\\u00a78\\u00a7oMeng-Zhen Li\\nXiao-Lei Zhang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.10969\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Oct 2019 08:11:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4figures,coference\\u00a7r"}']}
{title:'Sivasankaran et al. (§72019§r)', author: 'Sunit Sivasankaran; Emmaneul Vincent; Dominique Fohr', display:{Lore:['[{"text": "arXiv:1910.11114", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalyzing the impact of speaker localization errors on speech separation for automatic speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSunit Sivasankaran\\nEmmaneul Vincent\\nDominique Fohr\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.11114\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Oct 2019 13:53:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Sivasankaran et al. (§72019§r)', author: 'Sunit Sivasankaran; Emmanuel Vincent; Dominique Fohr', display:{Lore:['[{"text": "arXiv:1910.11131", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSLOGD: Speaker LOcation Guided Deflation approach to speech separation\\u00a7r\\n\\n\\u00a78\\u00a7oSunit Sivasankaran\\nEmmanuel Vincent\\nDominique Fohr\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.11131\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Oct 2019 14:00:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Pal et al. (§72019§r)', author: 'Monisankha Pal; Manoj Kumar; Raghuveer Peri; Tae Jin Park; So Hyun Kim; Catherine Lord; Somer Bishop; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:1910.11398", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker diarization using latent space clustering in generative adversarial network\\u00a7r\\n\\n\\u00a78\\u00a7oMonisankha Pal\\nManoj Kumar\\nRaghuveer Peri\\n+ 4 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.11398\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Oct 2019 19:52:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Koluguri et al. (§72019§r)', author: 'Nithin Rao Koluguri; Manoj Kumar; So Hyun Kim; Catherine Lord; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:1910.11400", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMeta-learning for robust child-adult classification from speech\\u00a7r\\n\\n\\u00a78\\u00a7oNithin Rao Koluguri\\nManoj Kumar\\nSo Hyun Kim\\nCatherine Lord\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.11400\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 28 Oct 2019 20:43:53 GMT)\\u00a7r"}']}
{title:'Pal et al. (§72019§r)', author: 'Monisankha Pal; Manoj Kumar; Raghuveer Peri; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:1910.11416", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA study of semi-supervised speaker diarization system using gan mixture model\\u00a7r\\n\\n\\u00a78\\u00a7oMonisankha Pal\\nManoj Kumar\\nRaghuveer Peri\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.11416\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Oct 2019 20:34:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Narayanan et al. (§72019§r)', author: 'Arun Narayanan; Rohit Prabhavalkar; Chung-Cheng Chiu; David Rybach; Tara N. Sainath; Trevor Strohman', display:{Lore:['[{"text": "arXiv:1910.11455", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRecognizing long-form speech using streaming end-to-end models\\u00a7r\\n\\n\\u00a78\\u00a7oArun Narayanan\\nRohit Prabhavalkar\\nChung-Cheng Chiu\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.11455\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Oct 2019 23:18:07 GMT)\\u00a7r"}']}
{title:'Lahiri et al. (§72019§r)', author: 'Rimita Lahiri; Manoj Kumar; Somer Bishop; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:1910.11472", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Domain Invariant Representations for Child-Adult Classification from Speech\\u00a7r\\n\\n\\u00a78\\u00a7oRimita Lahiri\\nManoj Kumar\\nSomer Bishop\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.11472\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Oct 2019 00:53:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Bullock et al. (§72019§r)', author: 'Latané Bullock; Hervé Bredin; Leibny Paola Garcia-Perera', display:{Lore:['[{"text": "arXiv:1910.11646", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOverlap-aware diarization: resegmentation using neural end-to-end overlapped speech detection\\u00a7r\\n\\n\\u00a78\\u00a7oLatan\\u00e9 Bullock\\nHerv\\u00e9 Bredin\\nLeibny Paola Garcia-Perera\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.11646\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Oct 2019 12:22:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Janský et al. (§72019§r)', author: 'Jakub Janský; Jiří Málek; Jaroslav Čmejla; Tomáš Kounovský; Zbyněk Koldovský; Jindřich Žďánský', display:{Lore:['[{"text": "arXiv:1910.11824", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptive blind audio source extraction supervised by dominant speaker identification using x-vectors\\u00a7r\\n\\n\\u00a78\\u00a7oJakub Jansk\\u00fd\\nJi\\u0159\\u00ed M\\u00e1lek\\nJaroslav \\u010cmejla\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.11824\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Oct 2019 16:08:09 GMT)\\u00a7r"}']}
{title:'Tsunoo et al. (§72019§r)', author: 'Emiru Tsunoo; Yosuke Kashiwagi; Toshiyuki Kumakura; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:1910.11871", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Online End-to-end Transformer Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oEmiru Tsunoo\\nYosuke Kashiwagi\\nToshiyuki Kumakura\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.11871\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Oct 2019 05:28:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:1910.07204\\u00a7r"}']}
{title:'Nidadavolu et al. (§72019§r)', author: 'Phani Sankar Nidadavolu; Saurabh Kataria; Jesús Villalba; Najim Dehak', display:{Lore:['[{"text": "arXiv:1910.11909", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Resource Domain Adaptation for Speaker Recognition Using Cycle-GANs\\u00a7r\\n\\n\\u00a78\\u00a7oPhani Sankar Nidadavolu\\nSaurabh Kataria\\nJes\\u00fas Villalba\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.11909\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Oct 2019 19:36:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, accepted to ASRU 2019\\u00a7r"}']}
{title:'Quitry et al. (§72019§r)', author: 'Félix de Chaumont Quitry; Marco Tagliasacchi; Dominik Roblek', display:{Lore:['[{"text": "arXiv:1910.11910", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning audio representations via phase prediction\\u00a7r\\n\\n\\u00a78\\u00a7oF\\u00e9lix de Chaumont Quitry\\nMarco Tagliasacchi\\nDominik Roblek\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.11910\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Oct 2019 19:36:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Kashani et al. (§72019§r)', author: 'Hamidreza Baradaran Kashani; Ata Jodeiri; Mohammad Mohsen Goodarzi; Shabnam Gholamdokht Firooz', display:{Lore:['[{"text": "arXiv:1910.12116", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImage to Image Translation based on Convolutional Neural Network Approach for Speech Declipping\\u00a7r\\n\\n\\u00a78\\u00a7oHamidreza Baradaran Kashani\\nAta Jodeiri\\nMohammad Mohsen Goodarzi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.12116\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 26 Oct 2019 18:57:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at 4th Conference on Technology In Electrical and Computer Engineering (ETECH 2019)\\u00a7r"}']}
{title:'Zhao et al. (§72019§r)', author: 'Yi Zhao; Xin Wang; Lauri Juvela; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:1910.12381", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransferring neural speech waveform synthesizers to musical instrument sounds generation\\u00a7r\\n\\n\\u00a78\\u00a7oYi Zhao\\nXin Wang\\nLauri Juvela\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.12381\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 19 Nov 2019 02:31:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Yasuda et al. (§72019§r)', author: 'Yusuke Yasuda; Xin Wang; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:1910.12383", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEffect of choice of probability distribution, randomness, and search methods for alignment modeling in sequence-to-sequence text-to-speech synthesis using hard alignment\\u00a7r\\n\\n\\u00a78\\u00a7oYusuke Yasuda\\nXin Wang\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.12383\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Oct 2019 00:01:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Dellaferrera et al. (§72019§r)', author: 'Giorgia Dellaferrera; Flavio Martinelli; Milos Cernak', display:{Lore:['[{"text": "arXiv:1910.12459", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Bin Encoding Training of a Spiking Neural Network-based Voice Activity Detection\\u00a7r\\n\\n\\u00a78\\u00a7oGiorgia Dellaferrera\\nFlavio Martinelli\\nMilos Cernak\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.12459\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP40776.2020.9054761\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2020 - 2020 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Oct 2019 06:25:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 1 table\\u00a7r"}']}
{title:'Lee et al. (§72019§r)', author: 'Tyler Lee; Ting Gong; Suchismita Padhy; Andrew Rouditchenko; Anthony Ndirango', display:{Lore:['[{"text": "arXiv:1910.12587", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLabel-efficient audio classification through multitask learning and self-supervision\\u00a7r\\n\\n\\u00a78\\u00a7oTyler Lee\\nTing Gong\\nSuchismita Padhy\\nAndrew Rouditchenko\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.12587\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 19 Oct 2019 00:58:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at ICLR 2019 Limited Labeled Data (LLD) Workshop\\u00a7r"}']}
{title:'Kourkounakis et al. (§72019§r)', author: 'Tedd Kourkounakis; Amirhossein Hajavi; Ali Etemad', display:{Lore:['[{"text": "arXiv:1910.12590", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetecting Multiple Speech Disfluencies using a Deep Residual Network with Bidirectional Long Short-Term Memory\\u00a7r\\n\\n\\u00a78\\u00a7oTedd Kourkounakis\\nAmirhossein Hajavi\\nAli Etemad\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.12590\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Oct 2019 21:32:47 GMT)\\u00a7r"}']}
{title:'Zeinali et al. (§72019§r)', author: 'Hossein Zeinali; Shuai Wang; Anna Silnova; Pavel Matějka; Oldřich Plchot', display:{Lore:['[{"text": "arXiv:1910.12592", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBUT System Description to VoxCeleb Speaker Recognition Challenge 2019\\u00a7r\\n\\n\\u00a78\\u00a7oHossein Zeinali\\nShuai Wang\\nAnna Silnova\\nPavel Mat\\u011bjka\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.12592\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Oct 2019 11:27:27 GMT)\\u00a7r"}']}
{title:'Yeh et al. (§72019§r)', author: 'Ching-Feng Yeh; Jay Mahadeokar; Kaustubh Kalgaonkar; Yongqiang Wang; Duc Le; Mahaveer Jain; Kjell Schubert; Christian Fuegen; Michael L. Seltzer', display:{Lore:['[{"text": "arXiv:1910.12977", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransformer-Transducer: End-to-End Speech Recognition with Self-Attention\\u00a7r\\n\\n\\u00a78\\u00a7oChing-Feng Yeh\\nJay Mahadeokar\\nKaustubh Kalgaonkar\\n+ 5 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.12977\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Oct 2019 21:29:21 GMT)\\u00a7r"}']}
{title:'Yuan et al. (§72019§r)', author: 'Mingrui Yuan; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:1910.13054", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpoofing Speaker Verification Systems with Deep Multi-speaker Text-to-speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oMingrui Yuan\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.13054\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Oct 2019 02:50:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Shrem et al. (§72019§r)', author: 'Yosi Shrem; Matthew Goldrick; Joseph Keshet', display:{Lore:['[{"text": "arXiv:1910.13255", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDr.VOT : Measuring Positive and Negative Voice Onset Time in the Wild\\u00a7r\\n\\n\\u00a78\\u00a7oYosi Shrem\\nMatthew Goldrick\\nJoseph Keshet\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.13255\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7ninterspeech 2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 27 Oct 2019 12:42:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ointerspeech 2019\\u00a7r"}']}
{title:'Zhou et al. (§72019§r)', author: 'Xinyong Zhou; Hao Che; Xiaorui Wang; Lei Xie', display:{Lore:['[{"text": "arXiv:1910.13276", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7la novel cross-lingual voice cloning approach with a few text-free samples\\u00a7r\\n\\n\\u00a78\\u00a7oXinyong Zhou\\nHao Che\\nXiaorui Wang\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.13276\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 30 Oct 2019 06:13:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'You et al. (§72019§r)', author: 'Zhao You; Dan Su; Jie Chen; Chao Weng; Dong Yu', display:{Lore:['[{"text": "arXiv:1910.13282", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDFSMN-SAN with Persistent Memory Model for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhao You\\nDan Su\\nJie Chen\\nChao Weng\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.13282\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Oct 2019 04:58:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, subbmitted to ICASSP 2020\\u00a7r"}']}
{title:'Adiban et al. (§72019§r)', author: 'Mohammad Adiban; Hossein Sameti; Saeedreza Shehnepoor', display:{Lore:['[{"text": "arXiv:1910.13345", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReplay Spoofing Countermeasure Using Autoencoder and Siamese Network on ASVspoof 2019 Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Adiban\\nHossein Sameti\\nSaeedreza Shehnepoor\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.13345\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Oct 2019 16:03:04 GMT)\\u00a7r"}']}
{title:'Ghai et al. (§72019§r)', author: 'Bhavya Ghai; Buvana Ramanan; Klaus Mueller', display:{Lore:['[{"text": "arXiv:1910.13488", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDoes Speech enhancement of publicly available data help build robust Speech Recognition Systems?\\u00a7r\\n\\n\\u00a78\\u00a7oBhavya Ghai\\nBuvana Ramanan\\nKlaus Mueller\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.13488\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Nov 2019 05:53:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to AAAIconference of Artificial Intelligence 2020 (abstract)\\u00a7r"}']}
{title:'Abanto-Leon et al. (§72019§r)', author: 'Luis F. Abanto-Leon; Guillermo Kemper Vasquez; Joel Telles', display:{Lore:['[{"text": "arXiv:1910.13571", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA novel fuzzy logic-based metric for audio quality assessment: Objective audio quality assessment\\u00a7r\\n\\n\\u00a78\\u00a7oLuis F. Abanto-Leon\\nGuillermo Kemper Vasquez\\nJoel Telles\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.13571\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Oct 2019 23:10:01 GMT)\\u00a7r"}']}
{title:'Banga et al. (§72019§r)', author: 'Subham Banga; Ujjwal Upadhyay; Piyush Agarwal; Aniket Sharma; Prerana Mukherjee', display:{Lore:['[{"text": "arXiv:1910.13801", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIndian EmoSpeech Command Dataset: A dataset for emotion based speech recognition in the wild\\u00a7r\\n\\n\\u00a78\\u00a7oSubham Banga\\nUjjwal Upadhyay\\nPiyush Agarwal\\nAniket Sharma\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.13801\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Oct 2019 06:55:18 GMT)\\u00a7r"}']}
{title:'Lian et al. (§72019§r)', author: 'Zheng Lian; Jianhua Tao; Bin Liu; Jian Huang', display:{Lore:['[{"text": "arXiv:1910.13806", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Representation Learning with Future Observation Prediction for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZheng Lian\\nJianhua Tao\\nBin Liu\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.13806\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2019-1582\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2019, 3840-3844\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Oct 2019 16:29:16 GMT)\\u00a7r"}']}
{title:'Lian et al. (§72019§r)', author: 'Zheng Lian; Jianhua Tao; Bin Liu; Jian Huang', display:{Lore:['[{"text": "arXiv:1910.13807", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDomain adversarial learning for emotion recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZheng Lian\\nJianhua Tao\\nBin Liu\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.13807\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Oct 2019 16:33:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP2020\\u00a7r"}']}
{title:'Wu et al. (§72019§r)', author: 'Bo Wu; Meng Yu; Lianwu Chen; Chao Weng; Dan Su; Dong Yu', display:{Lore:['[{"text": "arXiv:1910.13825", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOverlapped speech recognition from a jointly learned multi-channel neural speech extraction and representation\\u00a7r\\n\\n\\u00a78\\u00a7oBo Wu\\nMeng Yu\\nLianwu Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.13825\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Oct 2019 13:05:50 GMT)\\u00a7r"}']}
{title:'Cho et al. (§72019§r)', author: 'Jaejin Cho; Raghavendra Pappagari; Purva Kulkarni; Jesus Villalba; Yishay Carmiel; Najim Dehak', display:{Lore:['[{"text": "arXiv:1911.00432", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep neural networks for emotion recognition combining audio and transcripts\\u00a7r\\n\\n\\u00a78\\u00a7oJaejin Cho\\nRaghavendra Pappagari\\nPurva Kulkarni\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.00432\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Nov 2019 15:44:52 GMT)\\u00a7r"}']}
{title:'Nicodemo et al. (§72019§r)', author: 'Niccoló Nicodemo; Gaurav Naithani; Konstantinos Drossos; Tuomas Virtanen; Roberto Saletti', display:{Lore:['[{"text": "arXiv:1911.00527", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.PF\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMemory Requirement Reduction of Deep Neural Networks Using Low-bit Quantization of Parameters\\u00a7r\\n\\n\\u00a78\\u00a7oNiccol\\u00f3 Nicodemo\\nGaurav Naithani\\nKonstantinos Drossos\\nTuomas Virtanen\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.00527\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Nov 2019 18:03:12 GMT)\\u00a7r"}']}
{title:'Peri et al. (§72019§r)', author: 'Raghuveer Peri; Monisankha Pal; Arindam Jati; Krishna Somandepalli; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:1911.00940", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust speaker recognition using unsupervised adversarial invariance\\u00a7r\\n\\n\\u00a78\\u00a7oRaghuveer Peri\\nMonisankha Pal\\nArindam Jati\\nKrishna Somandepalli\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.00940\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 3 Nov 2019 18:14:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Ni et al. (§72019§r)', author: 'Zhaoheng Ni; Michael I Mandel', display:{Lore:['[{"text": "arXiv:1911.00982", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOnssen: an open-source speech separation and enhancement library\\u00a7r\\n\\n\\u00a78\\u00a7oZhaoheng Ni\\nMichael I Mandel\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.00982\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 3 Nov 2019 22:00:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Sholokhov et al. (§72019§r)', author: 'Alexey Sholokhov; Tomi Kinnunen; Ville Vestman; Kong Aik Lee', display:{Lore:['[{"text": "arXiv:1911.01182", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Biometrics Security: Extrapolating False Alarm Rate via Hierarchical Bayesian Modeling of Speaker Verification Scores\\u00a7r\\n\\n\\u00a78\\u00a7oAlexey Sholokhov\\nTomi Kinnunen\\nVille Vestman\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.01182\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Nov 2019 13:13:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to be published in Computer Speech and Language\\u00a7r"}']}
{title:'Bredin et al. (§72019§r)', author: 'Hervé Bredin; Ruiqing Yin; Juan Manuel Coria; Gregory Gelly; Pavel Korshunov; Marvin Lavechin; Diego Fustes; Hadrien Titeux; Wassim Bouaziz; Marie-Philippe Gill', display:{Lore:['[{"text": "arXiv:1911.01255", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lpyannote.audio: neural building blocks for speaker diarization\\u00a7r\\n\\n\\u00a78\\u00a7oHerv\\u00e9 Bredin\\nRuiqing Yin\\nJuan Manuel Coria\\n+ 6 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.01255\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Nov 2019 14:46:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Fini et al. (§72019§r)', author: 'Enrico Fini; Alessio Brutti', display:{Lore:['[{"text": "arXiv:1911.01266", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSupervised online diarization with sample mean loss for multi-domain data\\u00a7r\\n\\n\\u00a78\\u00a7oEnrico Fini\\nAlessio Brutti\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.01266\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 13 Nov 2019 11:20:25 GMT)\\u00a7r"}']}
{title:'Um et al. (§72019§r)', author: 'Se-Yun Um; Sangshin Oh; Kyungguen Byun; Inseon Jang; Chunghyun Ahn; Hong-Goo Kang', display:{Lore:['[{"text": "arXiv:1911.01635", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotional speech synthesis with rich and granularized control\\u00a7r\\n\\n\\u00a78\\u00a7oSe-Yun Um\\nSangshin Oh\\nKyungguen Byun\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.01635\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Nov 2019 03:01:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Fan et al. (§72019§r)', author: 'Yue Fan; Jiawen Kang; Lantian Li; Kaicheng Li; Haolin Chen; Sitong Cheng; Pengyuan Zhang; Ziya Zhou; Yunqi Cai; Dong Wang', display:{Lore:['[{"text": "arXiv:1911.01799", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCN-CELEB: a challenging Chinese speaker recognition dataset\\u00a7r\\n\\n\\u00a78\\u00a7oYue Fan\\nJiawen Kang\\nLantian Li\\n+ 6 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.01799\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Oct 2019 08:25:45 GMT)\\u00a7r"}']}
{title:'Peng et al. (§72019§r)', author: 'Zhiyuan Peng; Siyuan Feng; Tan Lee', display:{Lore:['[{"text": "arXiv:1911.01806", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMixture factorized auto-encoder for unsupervised hierarchical deep factorization of speech signal\\u00a7r\\n\\n\\u00a78\\u00a7oZhiyuan Peng\\nSiyuan Feng\\nTan Lee\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.01806\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Oct 2019 08:54:34 GMT)\\u00a7r"}']}
{title:'Kashani et al. (§72019§r)', author: 'Hamidreza Baradaran Kashani; Ata Jodeiri; Mohammad Mohsen Goodarzi; Iman Sarraf Rezaei', display:{Lore:['[{"text": "arXiv:1911.01902", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Enhancement via Deep Spectrum Image Translation Network\\u00a7r\\n\\n\\u00a78\\u00a7oHamidreza Baradaran Kashani\\nAta Jodeiri\\nMohammad Mohsen Goodarzi\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.01902\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Nov 2019 16:05:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICBME 2019\\u00a7r"}']}
{title:'Cadoux et al. (§72019§r)', author: 'Cyril Cadoux; Stefan Uhlich; Marc Ferras; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:1911.02091", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lClosing the Training/Inference Gap for Deep Attractor Networks\\u00a7r\\n\\n\\u00a78\\u00a7oCyril Cadoux\\nStefan Uhlich\\nMarc Ferras\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.02091\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Nov 2019 21:18:47 GMT)\\u00a7r"}']}
{title:'Fujioka et al. (§72019§r)', author: 'Takuya Fujioka; Dario Bertero; Takeshi Homma; Kenji Nagamatsu', display:{Lore:['[{"text": "arXiv:1911.02216", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAddressing Ambiguity of Emotion Labels Through Meta-Learning\\u00a7r\\n\\n\\u00a78\\u00a7oTakuya Fujioka\\nDario Bertero\\nTakeshi Homma\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.02216\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 7 Nov 2019 06:24:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Chiu et al. (§72019§r)', author: 'Chung-Cheng Chiu; Wei Han; Yu Zhang; Ruoming Pang; Sergey Kishchenko; Patrick Nguyen; Arun Narayanan; Hank Liao; Shuyuan Zhang; Anjuli Kannan; Rohit Prabhavalkar; Zhifeng Chen; Tara Sainath; Yonghui Wu', display:{Lore:['[{"text": "arXiv:1911.02242", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA comparison of end-to-end models for long-form speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oChung-Cheng Chiu\\nWei Han\\nYu Zhang\\n+ 10 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.02242\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Nov 2019 08:01:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oASRU camera-ready version\\u00a7r"}']}
{title:'Sahidullah et al. (§72019§r)', author: 'Md Sahidullah; Jose Patino; Samuele Cornell; Ruiqing Yin; Sunit Sivasankaran; Hervé Bredin; Pavel Korshunov; Alessio Brutti; Romain Serizel; Emmanuel Vincent; Nicholas Evans; Sébastien Marcel; Stefano Squartini; Claude Barras', display:{Lore:['[{"text": "arXiv:1911.02388", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Speed Submission to DIHARD II: Contributions     Lessons Learned\\u00a7r\\n\\n\\u00a78\\u00a7oMd Sahidullah\\nJose Patino\\nSamuele Cornell\\n+ 10 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.02388\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Nov 2019 13:53:18 GMT)\\u00a7r"}']}
{title:'Werner et al. (§72019§r)', author: 'Johnny Werner; Marcio H. Costa', display:{Lore:['[{"text": "arXiv:1911.03750", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Dereverberation and Noise Reduction for both diffusive noise field and point noise source in Binaural Hearing Aids: Preliminary Version\\u00a7r\\n\\n\\u00a78\\u00a7oJohnny Werner\\nMarcio H. Costa\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.03750\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 9 Nov 2019 18:28:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis is a preliminary version. The final work will be available in its full version soon\\u00a7r"}']}
{title:'Jati et al. (§72019§r)', author: 'Arindam Jati; Amrutha Nadarajan; Karel Mundnich; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:1911.03843", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCharacterizing dynamically varying acoustic scenes from egocentric audio recordings in workplace setting\\u00a7r\\n\\n\\u00a78\\u00a7oArindam Jati\\nAmrutha Nadarajan\\nKarel Mundnich\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.03843\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 10 Nov 2019 04:11:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe paper is submitted to IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020\\u00a7r"}']}
{title:'Sadeghi et al. (§72019§r)', author: 'Mostafa Sadeghi; Xavier Alameda-Pineda', display:{Lore:['[{"text": "arXiv:1911.03930", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Unsupervised Audio-visual Speech Enhancement Using a Mixture of Variational Autoencoders\\u00a7r\\n\\n\\u00a78\\u00a7oMostafa Sadeghi\\nXavier Alameda-Pineda\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.03930\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 10 Nov 2019 13:36:47 GMT)\\u00a7r"}']}
{title:'Togami et al. (§72019§r)', author: 'Masahito Togami; Yoshiki Masuyama; Tatsuya Komatsu; Yu Nakagome', display:{Lore:['[{"text": "arXiv:1911.04228", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Training for Deep Speech Source Separation with Kullback-Leibler Divergence Based Probabilistic Loss Function\\u00a7r\\n\\n\\u00a78\\u00a7oMasahito Togami\\nYoshiki Masuyama\\nTatsuya Komatsu\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.04228\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Nov 2019 13:12:18 GMT)\\u00a7r"}']}
{title:'Foleiss et al. (§72019§r)', author: 'Juliano Henrique Foleiss; Tiago Fernandes Tavares', display:{Lore:['[{"text": "arXiv:1911.04666", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSegment Relevance Estimation for Audio Analysis and Weakly-Labelled Classification\\u00a7r\\n\\n\\u00a78\\u00a7oJuliano Henrique Foleiss\\nTiago Fernandes Tavares\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.04666\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Nov 2019 04:19:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEESignal Processing Letters\\u00a7r"}']}
{title:'Cámbara et al. (§72019§r)', author: 'Guillermo Cámbara; Jordi Luque; Mireia Farrús', display:{Lore:['[{"text": "arXiv:1911.04808", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetection of speech events and speaker characteristics through photo-plethysmographic signal neural processing\\u00a7r\\n\\n\\u00a78\\u00a7oGuillermo C\\u00e1mbara\\nJordi Luque\\nMireia Farr\\u00fas\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.04808\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Nov 2019 11:58:42 GMT)\\u00a7r"}']}
{title:'Ruan et al. (§72019§r)', author: 'Yong Ruan; Xiangdong Wang; Hong Liu; Zhigang Ou; Yun Gao; Jianfeng Cheng; Yueliang Qian', display:{Lore:['[{"text": "arXiv:1911.04862", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn End-to-end Approach for Lexical Stress Detection based on Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oYong Ruan\\nXiangdong Wang\\nHong Liu\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.04862\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Nov 2019 11:29:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmission to ICASSP 2020\\u00a7r"}']}
{title:'Makino et al. (§72019§r)', author: 'Takaki Makino; Hank Liao; Yannis Assael; Brendan Shillingford; Basilio Garcia; Otavio Braga; Olivier Siohan', display:{Lore:['[{"text": "arXiv:1911.04890", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRecurrent Neural Network Transducer for Audio-Visual Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oTakaki Makino\\nHank Liao\\nYannis Assael\\n+ 3 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.04890\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Nov 2019 22:01:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWill be presented in 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2019)\\u00a7r"}']}
{title:'Czedik-Eysenberg et al. (§72019§r)', author: 'Isabella Czedik-Eysenberg; Oliver Wieczorek; Christoph Reuter', display:{Lore:['[{"text": "arXiv:1911.04952", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l\'Warriors of the Word\' \\u2013 Deciphering Lyrical Topics in Music and Their Connection to Audio Feature Dimensions Based on a Corpus of Over 100,000 Metal Songs\\u00a7r\\n\\n\\u00a78\\u00a7oIsabella Czedik-Eysenberg\\nOliver Wieczorek\\nChristoph Reuter\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.04952\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 13 Nov 2019 11:06:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCorrected typo in abstract (subsample)\\u00a7r"}']}
{title:'Nyshadham et al. (§72019§r)', author: 'Phani Kumar Nyshadham; D R Shivakumar; Peter Kroon; Shmulik Markovich-Golan', display:{Lore:['[{"text": "arXiv:1911.05560", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhanced Voice Post Processing Using Voice Decoder Guidance Indicators\\u00a7r\\n\\n\\u00a78\\u00a7oPhani Kumar Nyshadham\\nD R Shivakumar\\nPeter Kroon\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.05560\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Nov 2019 15:48:27 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72019§r)', author: 'Tae-Ho Kim; Sungjae Cho; Shinkook Choi; Sejik Park; Soo-Young Lee', display:{Lore:['[{"text": "arXiv:1911.06149", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotional Voice Conversion using Multitask Learning with Text-to-speech\\u00a7r\\n\\n\\u00a78\\u00a7oTae-Ho Kim\\nSungjae Cho\\nShinkook Choi\\nSejik Park\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.06149\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 27 Nov 2019 08:17:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 3 figures, submitted to ICASSP2020\\u00a7r"}']}
{title:'Parrot et al. (§72019§r)', author: 'Maud Parrot; Juliette Millet; Ewan Dunbar', display:{Lore:['[{"text": "arXiv:1911.06573", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIndependent and automatic evaluation of acoustic-to-articulatory inversion models\\u00a7r\\n\\n\\u00a78\\u00a7oMaud Parrot\\nJuliette Millet\\nEwan Dunbar\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.06573\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Nov 2019 11:33:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure\\u00a7r"}']}
{title:'Sztahó et al. (§72019§r)', author: 'Dávid Sztahó; György Szaszák; András Beke', display:{Lore:['[{"text": "arXiv:1911.06615", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep learning methods in speaker recognition: a review\\u00a7r\\n\\n\\u00a78\\u00a7oD\\u00e1vid Sztah\\u00f3\\nGy\\u00f6rgy Szasz\\u00e1k\\nAndr\\u00e1s Beke\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.06615\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3311/PPee.17024\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Nov 2019 12:32:07 GMT)\\u00a7r"}']}
{title:'Ding et al. (§72019§r)', author: 'Wenhao Ding; Liang He', display:{Lore:['[{"text": "arXiv:1911.06878", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptive Multi-scale Detection of Acoustic Events\\u00a7r\\n\\n\\u00a78\\u00a7oWenhao Ding\\nLiang He\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.06878\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 24 Nov 2019 17:47:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING (TASLP)\\u00a7r"}']}
{title:'Ram et al. (§72019§r)', author: 'Dhananjay Ram; Lesly Miculicich; Hervé Bourlard', display:{Lore:['[{"text": "arXiv:1911.08332", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Network based End-to-End Query by Example Spoken Term Detection\\u00a7r\\n\\n\\u00a78\\u00a7oDhananjay Ram\\nLesly Miculicich\\nHerv\\u00e9 Bourlard\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.08332\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Nov 2019 15:07:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING\\u00a7r"}']}
{title:'Subramani et al. (§72019§r)', author: "Krishna Subramani; Alexandre D'Hooge; Preeti Rao", display:{Lore:['[{"text": "arXiv:1911.08335", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerative Audio Synthesis with a Parametric Model\\u00a7r\\n\\n\\u00a78\\u00a7oKrishna Subramani\\nAlexandre D\'Hooge\\nPreeti Rao\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.08335\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Nov 2019 20:59:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oISMIR 2019 Late Breaking/Demo\\u00a7r"}']}
{title:'Wright et al. (§72019§r)', author: 'Alec Wright; Vesa Välimäki', display:{Lore:['[{"text": "arXiv:1911.08922", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerceptual Loss Function for Neural Modelling of Audio Systems\\u00a7r\\n\\n\\u00a78\\u00a7oAlec Wright\\nVesa V\\u00e4lim\\u00e4ki\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.08922\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Nov 2019 14:08:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Chen et al. (§72019§r)', author: 'Jiaxu Chen; Jing Hao; Kai Chen; Di Xie; Shicai Yang; Shiliang Pu', display:{Lore:['[{"text": "arXiv:1911.09349", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn End-to-End Audio Classification System based on Raw Waveforms and Mix-Training Strategy\\u00a7r\\n\\n\\u00a78\\u00a7oJiaxu Chen\\nJing Hao\\nKai Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.09349\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Nov 2019 08:54:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterSpeech 2019\\u00a7r"}']}
{title:'Nguyen et al. (§72019§r)', author: 'T. N. T. Nguyen; D. L. Jones; R. Ranjan; S. Jayabalan; W. S. Gan', display:{Lore:['[{"text": "arXiv:1911.11373", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA two-step system for sound event localization and detection\\u00a7r\\n\\n\\u00a78\\u00a7oT. N. T. Nguyen\\nD. L. Jones\\nR. Ranjan\\nS. Jayabalan\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.11373\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Nov 2019 07:09:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Liu et al. (§72019§r)', author: 'Zhaoyu Liu; Brian Mak', display:{Lore:['[{"text": "arXiv:1911.11601", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-lingual Multi-speaker Text-to-speech Synthesis for Voice Cloning without Using Parallel Corpus for Unseen Speakers\\u00a7r\\n\\n\\u00a78\\u00a7oZhaoyu Liu\\nBrian Mak\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.11601\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Nov 2019 15:01:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Krishna et al. (§72019§r)', author: 'Gautam Krishna; Co Tran; Mason Carnahan; Yan Han; Ahmed H Tewfik', display:{Lore:['[{"text": "arXiv:1911.11610", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving EEG based Continuous Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGautam Krishna\\nCo Tran\\nMason Carnahan\\nYan Han\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.11610\\u00a7r\\n\\nVersion:\\u00a77v6 (Tue, 24 Dec 2019 04:52:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oOn preparation for submission to EUSIPCO 2020. arXiv admin note: text overlap with arXiv:1911.04261, arXiv:1906.08871\\u00a7r"}']}
{title:'Chakravarthula et al. (§72019§r)', author: 'Sandeep Nallan Chakravarthula; Md Nasir; Shao-Yen Tseng; Haoqi Li; Tae Jin Park; Brian Baucom; Craig J. Bryan; Shrikanth Narayanan; Panayiotis Georgiou', display:{Lore:['[{"text": "arXiv:1911.11927", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic prediction of suicidal risk in military couples using multimodal interaction cues from couples conversations\\u00a7r\\n\\n\\u00a78\\u00a7oSandeep Nallan Chakravarthula\\nMd Nasir\\nShao-Yen Tseng\\n+ 5 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.11927\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Nov 2019 02:54:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Zhang et al. (§72019§r)', author: 'Zhe Zhang; Ming Wu; Xinyu Han; Jun Yang', display:{Lore:['[{"text": "arXiv:1911.12616", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerformance Comparison of UCA and UCCA based Real-time Sound Source Localization Systems using Circular Harmonics SRP Method\\u00a7r\\n\\n\\u00a78\\u00a7oZhe Zhang\\nMing Wu\\nXinyu Han\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.12616\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Nov 2019 09:49:36 GMT)\\u00a7r"}']}
{title:'Kumar et al. (§72019§r)', author: 'Rohit Kumar; Anirudh Sreeram; Anurenjan Purushothaman; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:1911.12617", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Neural Mask Estimator For Generalized Eigen-Value Beamforming Based ASR\\u00a7r\\n\\n\\u00a78\\u00a7oRohit Kumar\\nAnirudh Sreeram\\nAnurenjan Purushothaman\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.12617\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Nov 2019 09:53:45 GMT)\\u00a7r"}']}
{title:'Garcia et al. (§72019§r)', author: 'Paola Garcia; Jesus Villalba; Herve Bredin; Jun Du; Diego Castan; Alejandrina Cristia; Latane Bullock; Ling Guo; Koji Okabe; Phani Sankar Nidadavolu; Saurabh Kataria; Sizhu Chen; Leo Galmant; Marvin Lavechin; Lei Sun; Marie-Philippe Gill; Bar Ben-Yair; Sajjad Abdoli; Xin Wang; Wassim Bouaziz; Hadrien Titeux; Emmanuel Dupoux; Kong Aik Lee; Najim Dehak', display:{Lore:['[{"text": "arXiv:1912.00938", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker detection in the wild: Lessons learned from JSALT 2019\\u00a7r\\n\\n\\u00a78\\u00a7oPaola Garcia\\nJesus Villalba\\nHerve Bredin\\n+ 20 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.00938\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 2 Dec 2019 17:07:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2020\\u00a7r"}']}
{title:'Sheng et al. (§72019§r)', author: 'Leyuan Sheng; Dong-Yan Huang; Evgeniy N. Pavlovskiy', display:{Lore:['[{"text": "arXiv:1912.01167", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHigh-quality Speech Synthesis Using Super-resolution Mel-Spectrogram\\u00a7r\\n\\n\\u00a78\\u00a7oLeyuan Sheng\\nDong-Yan Huang\\nEvgeniy N. Pavlovskiy\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.01167\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 Dec 2019 02:53:54 GMT)\\u00a7r"}']}
{title:'Heusser et al. (§72019§r)', author: 'Verena Heusser; Niklas Freymuth; Stefan Constantin; Alex Waibel', display:{Lore:['[{"text": "arXiv:1912.02610", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBimodal Speech Emotion Recognition Using Pre-Trained Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oVerena Heusser\\nNiklas Freymuth\\nStefan Constantin\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.02610\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Nov 2019 23:25:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oLife-Long Learning for Spoken Language Systems ASRU 2019\\u00a7r"}']}
{title:'Boes et al. (§72019§r)', author: 'Wim Boes; Hugo Van hamme', display:{Lore:['[{"text": "arXiv:1912.02615", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudiovisual Transformer Architectures for Large-Scale Classification and Synchronization of Weakly Labeled Audio Events\\u00a7r\\n\\n\\u00a78\\u00a7oWim Boes\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.02615\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3343031.3350873\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 27th ACM International Conference on Multimedia\\n  (MM \'19). ACM, New York, NY, USA, 1961-1969\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 2 Dec 2019 15:26:37 GMT)\\u00a7r"}']}
{title:'Zeinali et al. (§72019§r)', author: 'Hossein Zeinali; Lukáš Burget; Jan "Honza\'\' Černocký', display:{Lore:['[{"text": "arXiv:1912.03627", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Multi Purpose and Large Scale Speech Corpus in Persian and English for Speaker and Speech Recognition: the DeepMine Database\\u00a7r\\n\\n\\u00a78\\u00a7oHossein Zeinali\\nLuk\\u00e1\\u0161 Burget\\nJan \\"Honza\'\' \\u010cernock\\u00fd\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.03627\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 8 Dec 2019 07:05:36 GMT)\\u00a7r"}']}
{title:'Krug et al. (§72019§r)', author: 'Andreas Krug; Sebastian Stober', display:{Lore:['[{"text": "arXiv:1912.04067", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisualizing Deep Neural Networks for Speech Recognition with Learned Topographic Filter Maps\\u00a7r\\n\\n\\u00a78\\u00a7oAndreas Krug\\nSebastian Stober\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.04067\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 Dec 2019 10:31:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for 2019 ACL Workshop BlackboxNLP: Analyzingand Interpreting Neural Networks for NLP\\u00a7r"}']}
{title:'Balagopalan et al. (§72019§r)', author: 'Aparna Balagopalan; Jekaterina Novikova; Matthew B. A. McDermott; Bret Nestor; Tristan Naumann; Marzyeh Ghassemi', display:{Lore:['[{"text": "arXiv:1912.04370", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-Language Aphasia Detection using Optimal Transport Domain Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oAparna Balagopalan\\nJekaterina Novikova\\nMatthew B. A. McDermott\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.04370\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 4 Dec 2019 19:48:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ML4H at NeurIPS 2019\\u00a7r"}']}
{title:'Khante et al. (§72019§r)', author: 'Priyanka Khante; Mai Lee Chang; Domingo Martinez; Kaya de Barbaro; Edison Thomaz', display:{Lore:['[{"text": "arXiv:1912.04844", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQuantifying the Chaos Level of Infants\' Environment via Unsupervised Learning\\u00a7r\\n\\n\\u00a78\\u00a7oPriyanka Khante\\nMai Lee Chang\\nDomingo Martinez\\nKaya de Barbaro\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.04844\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Dec 2019 17:34:31 GMT)\\u00a7r"}']}
{title:'Yoshioka et al. (§72019§r)', author: 'Takuya Yoshioka; Igor Abramovski; Cem Aksoylar; Zhuo Chen; Moshe David; Dimitrios Dimitriadis; Yifan Gong; Ilya Gurvich; Xuedong Huang; Yan Huang; Aviv Hurvitz; Li Jiang; Sharon Koubi; Eyal Krupka; Ido Leichter; Changliang Liu; Partha Parthasarathy; Alon Vinnikov; Lingfeng Wu; Xiong Xiao; Wayne Xiong; Huaming Wang; Zhenghao Wang; Jun Zhang; Yong Zhao; Tianyan Zhou', display:{Lore:['[{"text": "arXiv:1912.04979", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdvances in Online Audio-Visual Meeting Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oTakuya Yoshioka\\nIgor Abramovski\\nCem Aksoylar\\n+ 22 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.04979\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Dec 2019 20:59:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proc. IEEE ASRU Workshop 2019\\u00a7r"}']}
{title:'Corey et al. (§72019§r)', author: 'Ryan M. Corey; Matthew D. Skarha; Andrew C. Singer', display:{Lore:['[{"text": "arXiv:1912.05038", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCooperative Audio Source Separation and Enhancement Using Distributed Microphone Arrays and Wearable Devices\\u00a7r\\n\\n\\u00a78\\u00a7oRyan M. Corey\\nMatthew D. Skarha\\nAndrew C. Singer\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.05038\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Dec 2019 22:55:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at CAMSAP 2019\\u00a7r"}']}
{title:'Corey et al. (§72019§r)', author: 'Ryan M. Corey; Andrew C. Singer', display:{Lore:['[{"text": "arXiv:1912.05043", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMotion-Tolerant Beamforming with Deformable Microphone Arrays\\u00a7r\\n\\n\\u00a78\\u00a7oRyan M. Corey\\nAndrew C. Singer\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.05043\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Dec 2019 23:17:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at WASPAA 2019\\u00a7r"}']}
{title:'Park et al. (§72019§r)', author: 'Daniel S. Park; Yu Zhang; Chung-Cheng Chiu; Youzheng Chen; Bo Li; William Chan; Quoc V. Le; Yonghui Wu', display:{Lore:['[{"text": "arXiv:1912.05533", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpecAugment on Large Scale Datasets\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel S. Park\\nYu Zhang\\nChung-Cheng Chiu\\n+ 4 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.05533\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 11 Dec 2019 18:58:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 tables; submitted to ICASSP 2020\\u00a7r"}']}
{title:'Abdelaziz et al. (§72019§r)', author: 'Ahmed Hussen Abdelaziz; Shuo-Yiin Chang; Nelson Morgan; Erik Edwards; Dorothea Kolossa; Dan Ellis; David A. Moses; Edward F. Chang', display:{Lore:['[{"text": "arXiv:1912.05869", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Neural Phone Recognition of Mixed-Source ECoG Signals\\u00a7r\\n\\n\\u00a78\\u00a7oAhmed Hussen Abdelaziz\\nShuo-Yiin Chang\\nNelson Morgan\\n+ 4 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.05869\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 12 Dec 2019 10:37:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, showing algorithms, results and references from our collaboration during a 2017 postdoc stay of the first author\\u00a7r"}']}
{title:'Yao et al. (§72019§r)', author: 'Xuewen Yao; Dong He; Tiancheng Jing; Kaya de Barbaro', display:{Lore:['[{"text": "arXiv:1912.05920", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMeasuring Mother-Infant Emotions By Audio Sensing\\u00a7r\\n\\n\\u00a78\\u00a7oXuewen Yao\\nDong He\\nTiancheng Jing\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.05920\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Dec 2019 19:49:35 GMT)\\u00a7r"}']}
{title:'Huang et al. (§72019§r)', author: 'Wen-Chin Huang; Tomoki Hayashi; Yi-Chiao Wu; Hirokazu Kameoka; Tomoki Toda', display:{Lore:['[{"text": "arXiv:1912.06813", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Transformer Network: Sequence-to-Sequence Voice Conversion Using Transformer with Text-to-Speech Pretraining\\u00a7r\\n\\n\\u00a78\\u00a7oWen-Chin Huang\\nTomoki Hayashi\\nYi-Chiao Wu\\nHirokazu Kameoka\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.06813\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 14 Dec 2019 09:30:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint. Work in progress\\u00a7r"}']}
{title:'Miao et al. (§72019§r)', author: 'Xiaoxiao Miao; Ian McLoughlin', display:{Lore:['[{"text": "arXiv:1912.09003", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLSTM-TDNN with convolutional front-end for Dialect Identification in the 2019 Multi-Genre Broadcast Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoxiao Miao\\nIan McLoughlin\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.09003\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 19 Dec 2019 03:20:33 GMT)\\u00a7r"}']}
{title:'Sim et al. (§72019§r)', author: 'Khe Chai Sim; Françoise Beaufays; Arnaud Benard; Dhruv Guliani; Andreas Kabel; Nikhil Khare; Tamar Lucassen; Petr Zadrazil; Harry Zhang; Leif Johnson; Giovanni Motta; Lillian Zhou', display:{Lore:['[{"text": "arXiv:1912.09251", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPersonalization of End-to-end Speech Recognition On Mobile Devices For Named Entities\\u00a7r\\n\\n\\u00a78\\u00a7oKhe Chai Sim\\nFran\\u00e7oise Beaufays\\nArnaud Benard\\n+ 8 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.09251\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 14 Dec 2019 21:18:53 GMT)\\u00a7r"}']}
{title:'Vecchi et al. (§72019§r)', author: 'Alejandro Osses Vecchi; Sarah Verhulst', display:{Lore:['[{"text": "arXiv:1912.10026", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCalibration and reference simulations for the auditory periphery model of Verhulst et al. 2018 version 1.2\\u00a7r\\n\\n\\u00a78\\u00a7oAlejandro Osses Vecchi\\nSarah Verhulst\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.10026\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 21 Dec 2019 22:44:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn response to the email from ArXiv about the previous submission of this document (on 11 Dec 2019, to alejandro.osses@ugent.be, arXiv #295093), the new document is now self-contained and adds new information, as "}','{"text": "highlighted in the abstract of this document. Please do nothesitate in contacting me back for any further detail. Alejandro\\u00a7r"}']}
{title:'Kim et al. (§72019§r)', author: 'Juntae Kim; Jaesung Bae; Minsoo Hahn', display:{Lore:['[{"text": "arXiv:1912.10442", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-Point Detection with State Transition Model based on Chunk-Wise Classification\\u00a7r\\n\\n\\u00a78\\u00a7oJuntae Kim\\nJaesung Bae\\nMinsoo Hahn\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.10442\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 22 Dec 2019 13:09:36 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72019§r)', author: 'Chanwoo Kim; Sungsoo Kim; Kwangyoun Kim; Mehul Kumar; Jiyeon Kim; Kyungmin Lee; Changwoo Han; Abhinav Garg; Eunhyang Kim; Minkyoo Shin; Shatrughan Singh; Larry Heck; Dhananjaya Gowda', display:{Lore:['[{"text": "arXiv:1912.11040", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lend-to-end training of a large vocabulary end-to-end speech recognition system\\u00a7r\\n\\n\\u00a78\\u00a7oChanwoo Kim\\nSungsoo Kim\\nKwangyoun Kim\\n+ 9 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.11040\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 22 Dec 2019 02:59:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted and presented at the ASRU 2019 conference\\u00a7r"}']}
{title:'Kim et al. (§72019§r)', author: 'Chanwoo Kim; Mehul Kumar; Kwangyoun Kim; Dhananjaya Gowda', display:{Lore:['[{"text": "arXiv:1912.11041", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lpower-law nonlinearity with maximally uniform distribution criterion for improved neural network training in automatic speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oChanwoo Kim\\nMehul Kumar\\nKwangyoun Kim\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.11041\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 22 Dec 2019 04:40:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted and presented at the ASRU 2019 conference\\u00a7r"}']}
{title:'Dumpala et al. (§72019§r)', author: 'Sri Harsha Dumpala; Imran Sheikh; Rupayan Chakraborty; Sunil Kumar Kopparapu', display:{Lore:['[{"text": "arXiv:1912.11151", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Cycle-GAN Approach to Model Natural Perturbations in Speech for ASR Applications\\u00a7r\\n\\n\\u00a78\\u00a7oSri Harsha Dumpala\\nImran Sheikh\\nRupayan Chakraborty\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.11151\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 18 Dec 2019 12:26:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 3 figures, ICASSP-2019\\u00a7r"}']}
{title:'Marczewski et al. (§72019§r)', author: 'Alison Marczewski; Adriano Veloso; Nívio Ziviani', display:{Lore:['[{"text": "arXiv:1912.11547", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Transferable Features for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAlison Marczewski\\nAdriano Veloso\\nN\\u00edvio Ziviani\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.11547\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3126686.3126735\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the on Thematic Workshops of ACM Multimedia 2017.\\n  ACM, 2017. Pages 529-536\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 23 Dec 2019 18:06:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oACM-MM\'17, October 23-27, 2017\\u00a7r"}']}
{title:'Mars et al. (§72019§r)', author: 'Rohith Mars; Hiroyuki Ehara; Srikanth Nagisetty; Chong Soon Lim', display:{Lore:['[{"text": "arXiv:1912.11781", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Source Direction-of-Arrival Estimation Using Improved Estimation Consistency Method\\u00a7r\\n\\n\\u00a78\\u00a7oRohith Mars\\nHiroyuki Ehara\\nSrikanth Nagisetty\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.11781\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 26 Dec 2019 05:53:47 GMT)\\u00a7r"}']}
{title:'Garg et al. (§72019§r)', author: 'Abhinav Garg; Dhananjaya Gowda; Ankur Kumar; Kwangyoun Kim; Mehul Kumar; Chanwoo Kim', display:{Lore:['[{"text": "arXiv:1912.12384", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Multi-Stage Training of Online Attention-based Encoder-Decoder Models\\u00a7r\\n\\n\\u00a78\\u00a7oAbhinav Garg\\nDhananjaya Gowda\\nAnkur Kumar\\n+ 2 others\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.12384\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 28 Dec 2019 02:29:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted and presented at the ASRU 2019 conference\\u00a7r"}']}
{title:'Ding et al. (§72019§r)', author: 'Fenglin Ding; Wu Guo; Lirong Dai; Jun Du', display:{Lore:['[{"text": "arXiv:1912.13307", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention-based gated scaling adaptative acoustic model for ctc-based speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oFenglin Ding\\nWu Guo\\nLirong Dai\\u00a7r\\n\\n\\u00a772019\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.13307\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 31 Dec 2019 13:58:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages,2 figures, submitted to ICASSP 2020\\u00a7r"}']}
