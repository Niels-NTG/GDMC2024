{title:'Ronan et al. (§72021§r)', author: 'David Ronan; Zheng Ma; Paul Mc Namara; Hatice Gunes; Joshua D. Reiss', display:{Lore:['[{"text": "arXiv:1803.09960", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Minimisation of Masking in Multitrack Audio using Subgroups\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Ronan\\nZheng Ma\\nPaul Mc Namara\\nHatice Gunes\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1803.09960\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 5 Jan 2021 21:03:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oNeed to resolve ownership of intellectual property\\u00a7r"}']}
{title:'Schulze et al. (§72021§r)', author: 'Sören Schulze; Emily J. King', display:{Lore:['[{"text": "arXiv:1806.00273", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSparse Pursuit and Dictionary Learning for Blind Source Separation in Polyphonic Music Recordings\\u00a7r\\n\\n\\u00a78\\u00a7oS\\u00f6ren Schulze\\nEmily J. King\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1806.00273\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1186/s13636-020-00190-4\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJ. Audio Speech Music Proc. (2021) 2021:6\\u00a7r\\n\\nVersion:\\u00a77v5 (Mon, 1 Feb 2021 19:14:18 GMT)\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Haoqi Li; Ming Tu; Jing Huang; Shrikanth Narayanan; Panayiotis Georgiou', display:{Lore:['[{"text": "arXiv:1911.01533", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker-invariant Affective Representation Learning via Adversarial Training\\u00a7r\\n\\n\\u00a78\\u00a7oHaoqi Li\\nMing Tu\\nJing Huang\\nShrikanth Narayanan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.01533\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 12 Aug 2021 04:43:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2020; 5 pages\\u00a7r"}']}
{title:'Bai et al. (§72021§r)', author: 'Ye Bai; Jiangyan Yi; Jianhua Tao; Zhengqi Wen; Zhengkun Tian; Shuai Zhang', display:{Lore:['[{"text": "arXiv:1912.01777", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntegrating Knowledge into End-to-End Speech Recognition from External Text-Only Data\\u00a7r\\n\\n\\u00a78\\u00a7oYe Bai\\nJiangyan Yi\\nJianhua Tao\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.01777\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 Mar 2021 02:44:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted TASLP\\u00a7r"}']}
{title:'Arriandiaga et al. (§72021§r)', author: 'Ander Arriandiaga; Giovanni Morrone; Luca Pasa; Leonardo Badino; Chiara Bartolozzi', display:{Lore:['[{"text": "arXiv:1912.02671", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-Visual Target Speaker Enhancement on Multi-Talker Environment using Event-Driven Cameras\\u00a7r\\n\\n\\u00a78\\u00a7oAnder Arriandiaga\\nGiovanni Morrone\\nLuca Pasa\\nLeonardo Badino\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.02671\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 22 Feb 2021 11:31:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ISCAS 2021\\u00a7r"}']}
{title:'Llorach et al. (§72021§r)', author: 'Gerard Llorach; Frederike Kirschner; Giso Grimm; Melanie A. Zokoll; Kirsten C. Wagener; Volker Hohmann', display:{Lore:['[{"text": "arXiv:1912.04700", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDevelopment and Evaluation of Video Recordings for the OLSA Matrix Sentence Test\\u00a7r\\n\\n\\u00a78\\u00a7oGerard Llorach\\nFrederike Kirschner\\nGiso Grimm\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.04700\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 31 Mar 2021 08:27:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 9 figures\\u00a7r"}']}
{title:'Zeinali et al. (§72021§r)', author: 'Hossein Zeinali; Kong Aik Lee; Jahangir Alam; Lukas Burget', display:{Lore:['[{"text": "arXiv:1912.06311", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lShort-duration Speaker Verification (SdSV) Challenge 2021: the Challenge Evaluation Plan\\u00a7r\\n\\n\\u00a78\\u00a7oHossein Zeinali\\nKong Aik Lee\\nJahangir Alam\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.06311\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 24 Mar 2021 21:07:51 GMT)\\u00a7r"}']}
{title:'Sadeghi et al. (§72021§r)', author: 'Mostafa Sadeghi; Xavier Alameda-Pineda', display:{Lore:['[{"text": "arXiv:1912.10647", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMixture of Inference Networks for VAE-based Audio-visual Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oMostafa Sadeghi\\nXavier Alameda-Pineda\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.10647\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 8 Mar 2021 20:22:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE Transactions on Signal Processing\\u00a7r"}']}
{title:'Vadillo et al. (§72021§r)', author: 'Jon Vadillo; Roberto Santana', display:{Lore:['[{"text": "arXiv:2001.08444", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the human evaluation of audio adversarial examples\\u00a7r\\n\\n\\u00a78\\u00a7oJon Vadillo\\nRoberto Santana\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2001.08444\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 12 Feb 2021 14:27:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint. 17 pages, 7 figures, 4 tables\\u00a7r"}']}
{title:'Janský et al. (§72021§r)', author: 'Jakub Janský; Zbyněk Koldovský; Jiří Málek; Tomáš Kounovský; Jaroslav Čmejla', display:{Lore:['[{"text": "arXiv:2002.12619", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAuxiliary Function-Based Algorithm for Blind Extraction of a Moving Speaker\\u00a7r\\n\\n\\u00a78\\u00a7oJakub Jansk\\u00fd\\nZbyn\\u011bk Koldovsk\\u00fd\\nJi\\u0159\\u00ed M\\u00e1lek\\nTom\\u00e1\\u0161 Kounovsk\\u00fd\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2002.12619\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 5 Feb 2021 13:36:13 GMT)\\u00a7r"}']}
{title:'Rehr et al. (§72021§r)', author: 'Robert Rehr; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2004.03512", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSNR-Based Features and Diverse Training Data for Robust DNN-Based Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oRobert Rehr\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2004.03512\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3082702\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  Vol. 29, 2021. (c) 2021 IEEE\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 15 May 2021 14:04:40 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Weiran Wang; Guangsen Wang; Aadyot Bhatnagar; Yingbo Zhou; Caiming Xiong; Richard Socher', display:{Lore:['[{"text": "arXiv:2004.04290", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn investigation of phone-based subword units for end-to-end speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oWeiran Wang\\nGuangsen Wang\\nAadyot Bhatnagar\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2004.04290\\u00a7r\\n\\nVersion:\\u00a77v6 (Mon, 21 Jun 2021 19:02:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2020 final version. Implementation for reproducing the results can be found at: https://github.com/salesforce/transformerasr\\u00a7r"}']}
{title:'Raponi et al. (§72021§r)', author: 'Simone Raponi; Isra Ali; Gabriele Oligeri', display:{Lore:['[{"text": "arXiv:2004.07948", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound of Guns: Digital Forensics of Gun Audio Samples meets Artificial Intelligence\\u00a7r\\n\\n\\u00a78\\u00a7oSimone Raponi\\nIsra Ali\\nGabriele Oligeri\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2004.07948\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 1 Mar 2021 14:01:00 GMT)\\u00a7r"}']}
{title:'Niranjan et al. (§72021§r)', author: 'Abhishek Niranjan; Mukesh Sharma; Sai Bharath Chandra Gutha; M Ali Basha Shaik', display:{Lore:['[{"text": "arXiv:2004.09347", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Whisper to Natural Speech Conversion using Modified Transformer Network\\u00a7r\\n\\n\\u00a78\\u00a7oAbhishek Niranjan\\nMukesh Sharma\\nSai Bharath Chandra Gutha\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2004.09347\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 5 Apr 2021 09:27:12 GMT)\\u00a7r"}']}
{title:'Gu et al. (§72021§r)', author: 'Yu Gu; Xiang Yin; Yonghui Rao; Yuan Wan; Benlai Tang; Yang Zhang; Jitong Chen; Yuxuan Wang; Zejun Ma', display:{Lore:['[{"text": "arXiv:2004.11012", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lByteSing: A Chinese Singing Voice Synthesis System Using Duration Allocated Encoder-Decoder Acoustic Models and WaveRNN Vocoders\\u00a7r\\n\\n\\u00a78\\u00a7oYu Gu\\nXiang Yin\\nYonghui Rao\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2004.11012\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 25 Jan 2021 04:00:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ISCSLP2021\\u00a7r"}']}
{title:'Qian et al. (§72021§r)', author: 'Kaizhi Qian; Yang Zhang; Shiyu Chang; David Cox; Mark Hasegawa-Johnson', display:{Lore:['[{"text": "arXiv:2004.11284", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Speech Decomposition via Triple Information Bottleneck\\u00a7r\\n\\n\\u00a78\\u00a7oKaizhi Qian\\nYang Zhang\\nShiyu Chang\\nDavid Cox\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2004.11284\\u00a7r\\n\\nVersion:\\u00a77v6 (Sat, 13 Mar 2021 15:31:35 GMT)\\u00a7r"}']}
{title:'Xie et al. (§72021§r)', author: 'Yi Xie; Zhuohang Li; Cong Shi; Jian Liu; Yingying Chen; Bo Yuan', display:{Lore:['[{"text": "arXiv:2004.12261", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnabling Fast and Universal Audio Adversarial Attack Using Generative Model\\u00a7r\\n\\n\\u00a78\\u00a7oYi Xie\\nZhuohang Li\\nCong Shi\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2004.12261\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 7 Feb 2021 17:59:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublish on AAAI21\\u00a7r"}']}
{title:'Yatabe et al. (§72021§r)', author: 'Kohei Yatabe; Daichi Kitamura', display:{Lore:['[{"text": "arXiv:2004.14091", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetermined BSS based on time-frequency masking and its application to harmonic vector analysis\\u00a7r\\n\\n\\u00a78\\u00a7oKohei Yatabe\\nDaichi Kitamura\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2004.14091\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 14 Apr 2021 14:18:48 GMT)\\u00a7r"}']}
{title:'Shukla et al. (§72021§r)', author: 'Abhinav Shukla; Stavros Petridis; Maja Pantic', display:{Lore:['[{"text": "arXiv:2005.01400", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDoes Visual Self-Supervision Improve Learning of Speech Representations for Emotion Recognition?\\u00a7r\\n\\n\\u00a78\\u00a7oAbhinav Shukla\\nStavros Petridis\\nMaja Pantic\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2005.01400\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TAFFC.2021.3062406\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 18 Mar 2021 11:35:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in IEEE Transactions on Affective Computing; v3:Publication-ready version including additional experiments and discussion\\u00a7r"}']}
{title:'Fu et al. (§72021§r)', author: 'Li Fu; Xiaoxiao Li; Libo Zi; Zhengchen Zhang; Youzheng Wu; Xiaodong He; Bowen Zhou', display:{Lore:['[{"text": "arXiv:2005.04288", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIncremental Learning for End-to-End Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oLi Fu\\nXiaoxiao Li\\nLibo Zi\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2005.04288\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 16 Sep 2021 02:47:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oASRU 2021\\u00a7r"}']}
{title:'Raissi et al. (§72021§r)', author: 'Tina Raissi; Eugen Beck; Ralf Schlüter; Hermann Ney', display:{Lore:['[{"text": "arXiv:2005.07578", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContext-Dependent Acoustic Modeling without Explicit Phone Clustering\\u00a7r\\n\\n\\u00a78\\u00a7oTina Raissi\\nEugen Beck\\nRalf Schl\\u00fcter\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2005.07578\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2020-1244\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 7 Apr 2021 12:32:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of Interspeech 2020\\u00a7r"}']}
{title:'Naowarat et al. (§72021§r)', author: 'Burin Naowarat; Thananchai Kongthaworn; Korrawe Karunratanakul; Sheng Hui Wu; Ekapol Chuangsuwanich', display:{Lore:['[{"text": "arXiv:2005.07920", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReducing Spelling Inconsistencies in Code-Switching ASR using Contextualized CTC Loss\\u00a7r\\n\\n\\u00a78\\u00a7oBurin Naowarat\\nThananchai Kongthaworn\\nKorrawe Karunratanakul\\nSheng Hui Wu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2005.07920\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 22 Jun 2021 18:21:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2021\\u00a7r"}']}
{title:'Chi et al. (§72021§r)', author: 'Po-Han Chi; Pei-Hung Chung; Tsung-Han Wu; Chun-Cheng Hsieh; Yen-Hao Chen; Shang-Wen Li; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2005.08575", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio ALBERT: A Lite BERT for Self-supervised Learning of Audio Representation\\u00a7r\\n\\n\\u00a78\\u00a7oPo-Han Chi\\nPei-Hung Chung\\nTsung-Han Wu\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2005.08575\\u00a7r\\n\\nVersion:\\u00a77v5 (Mon, 3 May 2021 09:33:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE Spoken Language Technology Workshop 2021\\u00a7r"}']}
{title:'Huang et al. (§72021§r)', author: 'Chien-yu Huang; Yist Y. Lin; Hung-yi Lee; Lin-shan Lee', display:{Lore:['[{"text": "arXiv:2005.08781", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDefending Your Voice: Adversarial Attack on Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oChien-yu Huang\\nYist Y. Lin\\nHung-yi Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2005.08781\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 4 May 2021 15:02:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by SLT 2021\\u00a7r"}']}
{title:'Zeineldeen et al. (§72021§r)', author: 'Mohammad Zeineldeen; Albert Zeyer; Wei Zhou; Thomas Ng; Ralf Schlüter; Hermann Ney', display:{Lore:['[{"text": "arXiv:2005.09336", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA systematic comparison of grapheme-based vs. phoneme-based label units for encoder-decoder-attention models\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Zeineldeen\\nAlbert Zeyer\\nWei Zhou\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2005.09336\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 15 Apr 2021 16:59:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 6 tables\\u00a7r"}']}
{title:'Gerazov et al. (§72021§r)', author: 'Branislav Gerazov; Daniel van Niekerk; Anqi Xu; Paul Konstantin Krug; Peter Birkholz; Yi Xu', display:{Lore:['[{"text": "arXiv:2005.09986", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluating Features and Metrics for High-Quality Simulation of Early Vocal Learning of Vowels\\u00a7r\\n\\n\\u00a78\\u00a7oBranislav Gerazov\\nDaniel van Niekerk\\nAnqi Xu\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2005.09986\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 2 Apr 2021 15:31:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Huo et al. (§72021§r)', author: 'Jingjing Huo; Yingbo Gao; Weiyue Wang; Ralf Schlüter; Hermann Ney', display:{Lore:['[{"text": "arXiv:2005.10089", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigation of Large-Margin Softmax in Neural Language Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oJingjing Huo\\nYingbo Gao\\nWeiyue Wang\\nRalf Schl\\u00fcter\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2005.10089\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 21 Apr 2021 12:45:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH 2020\\u00a7r"}']}
{title:'Han et al. (§72021§r)', author: 'Kyu J. Han; Jing Pan; Venkata Krishna Naveen Tadala; Tao Ma; Dan Povey', display:{Lore:['[{"text": "arXiv:2005.10470", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultistream CNN for Robust Acoustic Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oKyu J. Han\\nJing Pan\\nVenkata Krishna Naveen Tadala\\nTao Ma\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2005.10470\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 25 Apr 2021 05:47:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Wu et al. (§72021§r)', author: 'Zhaofeng Wu; Ding Zhao; Qiao Liang; Jiahui Yu; Anmol Gulati; Ruoming Pang', display:{Lore:['[{"text": "arXiv:2005.10627", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDynamic Sparsity Neural Networks for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhaofeng Wu\\nDing Zhao\\nQiao Liang\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2005.10627\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 8 Feb 2021 08:01:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2021. (c) 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for "}','{"text": "advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'You-Jin Li; Syu-Siang Wang; Yu Tsao; Borching Su', display:{Lore:['[{"text": "arXiv:2005.11704", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMIMO Speech Compression and Enhancement Based on Convolutional Denoising Autoencoder\\u00a7r\\n\\n\\u00a78\\u00a7oYou-Jin Li\\nSyu-Siang Wang\\nYu Tsao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2005.11704\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 7 Jun 2021 13:43:58 GMT)\\u00a7r"}']}
{title:'Lavechin et al. (§72021§r)', author: 'Marvin Lavechin; Ruben Bousbib; Hervé Bredin; Emmanuel Dupoux; Alejandrina Cristia', display:{Lore:['[{"text": "arXiv:2005.12656", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn open-source voice type classifier for child-centered daylong recordings\\u00a7r\\n\\n\\u00a78\\u00a7oMarvin Lavechin\\nRuben Bousbib\\nHerv\\u00e9 Bredin\\nEmmanuel Dupoux\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2005.12656\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 22 Jan 2021 17:14:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to Interspeech 2020\\u00a7r"}']}
{title:'Ebbers et al. (§72021§r)', author: 'Janek Ebbers; Michael Kuhlmann; Tobias Cord-Landwehr; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2005.12963", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContrastive Predictive Coding Supported Factorized Variational Autoencoder for Unsupervised Learning of Disentangled Speech Representations\\u00a7r\\n\\n\\u00a78\\u00a7oJanek Ebbers\\nMichael Kuhlmann\\nTobias Cord-Landwehr\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2005.12963\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 11 Mar 2021 09:52:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by icassp 2021\\u00a7r"}']}
{title:'Bianco et al. (§72021§r)', author: 'Michael J. Bianco; Sharon Gannot; Peter Gerstoft', display:{Lore:['[{"text": "arXiv:2005.13163", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-supervised source localization with deep generative modeling\\u00a7r\\n\\n\\u00a78\\u00a7oMichael J. Bianco\\nSharon Gannot\\nPeter Gerstoft\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2005.13163\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/MLSP49062.2020.9231825\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 12 Feb 2021 01:46:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in proceedings of IEEE International Workshopon Machine Learning for Signal Processing. arXiv admin note: substantial text overlap with arXiv:2101.10636\\u00a7r"}']}
{title:'Port et al. (§72021§r)', author: 'Andrew Port; Chelhwon Kim; Mitesh Patel', display:{Lore:['[{"text": "arXiv:2005.13291", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Sensory Substitution: Noninvasively Enabling Biological Neural Networks to Receive Input from Artificial Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oAndrew Port\\nChelhwon Kim\\nMitesh Patel\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2005.13291\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 25 Aug 2021 23:20:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 3 figures\\u00a7r"}']}
{title:'Chen et al. (§72021§r)', author: 'Liang Chen; Yanchun Liang; Xiaohu Shi; You Zhou; Chunguo Wu', display:{Lore:['[{"text": "arXiv:2006.00452", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCrossed-Time Delay Neural Network for Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oLiang Chen\\nYanchun Liang\\nXiaohu Shi\\nYou Zhou\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2006.00452\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nMMM 2021\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 7 Dec 2021 06:22:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMMM 2021 Paper, add GitHub address\\u00a7r"}']}
{title:'Xue et al. (§72021§r)', author: 'Yawen Xue; Shota Horiguchi; Yusuke Fujita; Shinji Watanabe; Kenji Nagamatsu', display:{Lore:['[{"text": "arXiv:2006.02616", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOnline End-to-End Neural Diarization with Speaker-Tracing Buffer\\u00a7r\\n\\n\\u00a78\\u00a7oYawen Xue\\nShota Horiguchi\\nYusuke Fujita\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2006.02616\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 7 Mar 2021 04:40:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to SLT 2021\\u00a7r"}']}
{title:'Shifas et al. (§72021§r)', author: 'Muhammed PV Shifas; Santelli Claudio; Vassilis Tsiaras; Yannis Stylianou', display:{Lore:['[{"text": "arXiv:2006.05233", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA fully recurrent feature extraction for single channel speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oMuhammed PV Shifas\\nSantelli Claudio\\nVassilis Tsiaras\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2006.05233\\u00a7r\\n\\nVersion:\\u00a77v7 (Thu, 3 Jun 2021 15:16:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Łańcucki (§72021§r)', author: 'Adrian Łańcucki', display:{Lore:['[{"text": "arXiv:2006.06873", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFastPitch: Parallel Text-to-speech with Pitch Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oAdrian \\u0141a\\u0144cucki\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2006.06873\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 Feb 2021 14:23:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Fu et al. (§72021§r)', author: 'Szu-Wei Fu; Chien-Feng Liao; Tsun-An Hsieh; Kuo-Hsuan Hung; Syu-Siang Wang; Cheng Yu; Heng-Cheng Kuo; Ryandhimas E. Zezario; You-Jin Li; Shang-Yi Chuang; Yen-Ju Lu; Yu Tsao', display:{Lore:['[{"text": "arXiv:2006.10296", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBoosting Objective Scores of a Speech Enhancement Model by MetricGAN Post-processing\\u00a7r\\n\\n\\u00a78\\u00a7oSzu-Wei Fu\\nChien-Feng Liao\\nTsun-An Hsieh\\n+ 8 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2006.10296\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 3 Mar 2021 06:36:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by APSIPA 2020\\u00a7r"}']}
{title:'Chen et al. (§72021§r)', author: 'Hangting Chen; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2007.00272", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring the time-domain deep attractor network with two-stream architectures in a reverberant environment\\u00a7r\\n\\n\\u00a78\\u00a7oHangting Chen\\nPengyuan Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.00272\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.neunet.2021.04.023\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 21 Apr 2021 05:26:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oNeuralNetworks (2021)\\u00a7r"}']}
{title:'Haubner et al. (§72021§r)', author: 'Thomas Haubner; Andreas Brendel; Mohamed Elminshawi; Walter Kellermann', display:{Lore:['[{"text": "arXiv:2007.01579", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoise-Robust Adaptation Control for Supervised Acoustic System Identification Exploiting A Noise Dictionary\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Haubner\\nAndreas Brendel\\nMohamed Elminshawi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.01579\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 3 Feb 2021 09:56:58 GMT)\\u00a7r"}']}
{title:'Siegel et al. (§72021§r)', author: 'Joshua E. Siegel; Umberto Coda', display:{Lore:['[{"text": "arXiv:2007.03759", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSurveying Off-Board and Extra-Vehicular Monitoring and Progress Towards Pervasive Diagnostics\\u00a7r\\n\\n\\u00a78\\u00a7oJoshua E. Siegel\\nUmberto Coda\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.03759\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 1 Mar 2021 21:14:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o25 pages; 4 figures\\u00a7r"}']}
{title:'Lee et al. (§72021§r)', author: 'Hyeonseung Lee; Woo Hyun Kang; Sung Jun Cheon; Hyeongju Kim; Nam Soo Kim', display:{Lore:['[{"text": "arXiv:2007.05214", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGated Recurrent Context: Softmax-free Attention for Online Encoder-Decoder Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHyeonseung Lee\\nWoo Hyun Kang\\nSung Jun Cheon\\nHyeongju Kim\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.05214\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3049344\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 14 Jan 2021 09:10:27 GMT)\\u00a7r"}']}
{title:'Hülsmeier et al. (§72021§r)', author: 'David Hülsmeier; Marc René Schädler; Birger Kollmeier', display:{Lore:['[{"text": "arXiv:2007.05378", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDARF: A data-reduced FADE version for simulations of speech recognition thresholds with real hearing aids\\u00a7r\\n\\n\\u00a78\\u00a7oDavid H\\u00fclsmeier\\nMarc Ren\\u00e9 Sch\\u00e4dler\\nBirger Kollmeier\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.05378\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 11 Feb 2021 09:24:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o19 pages, 14 figures, submitted to Hearing Research\\u00a7r"}']}
{title:'Wu et al. (§72021§r)', author: 'Yi-Chiao Wu; Tomoki Hayashi; Patrick Lumban Tobing; Kazuhiro Kobayashi; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2007.05663", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQuasi-Periodic WaveNet: An Autoregressive Raw Waveform Generative Model with Pitch-dependent Dilated Convolution Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oYi-Chiao Wu\\nTomoki Hayashi\\nPatrick Lumban Tobing\\nKazuhiro Kobayashi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.05663\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3061245\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol. 29, pp. 1134-1148, 2021\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 27 Mar 2021 06:41:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 12 figures, 11 tables\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Andy T. Liu; Shang-Wen Li; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2007.06028", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTERA: Self-Supervised Learning of Transformer Encoder Representation for Speech\\u00a7r\\n\\n\\u00a78\\u00a7oAndy T. Liu\\nShang-Wen Li\\nHung-yi Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.06028\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3095662\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING,\\n  Vol. 29, 2021\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 4 Aug 2021 05:38:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in IEEE/ACM TASLP, final published article available at https://ieeexplore.ieee.org/document/9478264\\u00a7r"}']}
{title:'Tikhonov et al. (§72021§r)', author: 'Alexey Tikhonov; Ivan P. Yamshchikov', display:{Lore:['[{"text": "arXiv:2007.06284", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lArtificial Neural Networks Jamming on the Beat\\u00a7r\\n\\n\\u00a78\\u00a7oAlexey Tikhonov\\nIvan P. Yamshchikov\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.06284\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5220/0010461200370044\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 20 May 2021 07:00:23 GMT)\\u00a7r"}']}
{title:'Záviška et al. (§72021§r)', author: 'Pavel Záviška; Pavel Rajmic; Alexey Ozerov; Lucas Rencker', display:{Lore:['[{"text": "arXiv:2007.07663", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA survey and an extensive evaluation of popular audio declipping methods\\u00a7r\\n\\n\\u00a78\\u00a7oPavel Z\\u00e1vi\\u0161ka\\nPavel Rajmic\\nAlexey Ozerov\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.07663\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JSTSP.2020.3042071\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Journal of Selected Topics in Signal Processing, vol. 15, no.\\n  1, pp. 5-24, Jan. 2021\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 4 Jan 2021 13:43:42 GMT)\\u00a7r"}']}
{title:'Nanni et al. (§72021§r)', author: 'Loris Nanni; Gianluca Maguolo; Sheryl Brahnam; Michelangelo Paci', display:{Lore:['[{"text": "arXiv:2007.07966", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Ensemble of Convolutional Neural Networks for Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oLoris Nanni\\nGianluca Maguolo\\nSheryl Brahnam\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.07966\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3390/app11135796\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nAppl. Sci. 2021, 11(13), 5796\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 27 Apr 2021 22:34:00 GMT)\\u00a7r"}']}
{title:'Hu et al. (§72021§r)', author: 'Shoukang Hu; Xurong Xie; Shansong Liu; Mingyu Cui; Mengzhe Geng; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2007.08818", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Architecture Search For LF-MMI Trained Time Delay Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oShoukang Hu\\nXurong Xie\\nShansong Liu\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.08818\\u00a7r\\n\\nVersion:\\u00a77v4 (Sun, 7 Feb 2021 14:54:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2021\\u00a7r"}']}
{title:'Lee et al. (§72021§r)', author: 'Kyungmin Lee; Hyunwhan Joe; Hyeontaek Lim; Kwangyoun Kim; Sungsoo Kim; Chang Woo Han; Hong-Gee Kim', display:{Lore:['[{"text": "arXiv:2007.11747", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSequential Routing Framework: Fully Capsule Network-based Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oKyungmin Lee\\nHyunwhan Joe\\nHyeontaek Lim\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.11747\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 1 Apr 2021 09:09:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o42 pages, 8 figures (totally 11 figures), submitted to Computer Speech and Language (Only line numbers wereremoved from the submitted version)\\u00a7r"}']}
{title:'Wu et al. (§72021§r)', author: 'Yi-Chiao Wu; Tomoki Hayashi; Takuma Okamoto; Hisashi Kawai; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2007.12955", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQuasi-Periodic Parallel WaveGAN: A Non-autoregressive Raw Waveform Generative Model with Pitch-dependent Dilated Convolution Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oYi-Chiao Wu\\nTomoki Hayashi\\nTakuma Okamoto\\nHisashi Kawai\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.12955\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3051765\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol. 29, pp. 792-806, 2021\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 19 Feb 2021 07:44:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 10 figures, 8 tables\\u00a7r"}']}
{title:'India et al. (§72021§r)', author: 'Miquel India; Pooyan Safari; Javier Hernando', display:{Lore:['[{"text": "arXiv:2007.13199", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDouble Multi-Head Attention for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oMiquel India\\nPooyan Safari\\nJavier Hernando\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.13199\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 9 Jan 2021 19:28:59 GMT)\\u00a7r"}']}
{title:'Esmaeilpour et al. (§72021§r)', author: 'Mohammad Esmaeilpour; Patrick Cardinal; Alessandro Lameiras Koerich', display:{Lore:['[{"text": "arXiv:2007.13703", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrom Sound Representation to Model Robustness\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Esmaeilpour\\nPatrick Cardinal\\nAlessandro Lameiras Koerich\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.13703\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 18 Jan 2021 03:24:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages\\u00a7r"}']}
{title:'Xue et al. (§72021§r)', author: 'Wei Xue; Gang Quan; Chao Zhang; Guohong Ding; Xiaodong He; Bowen Zhou', display:{Lore:['[{"text": "arXiv:2007.13962", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Kalman Filtering for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oWei Xue\\nGang Quan\\nChao Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.13962\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 16 Apr 2021 06:30:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP2021\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Ruixiong Zhang; Haiwei Wu; Wubo Li; Dongwei Jiang; Wei Zou; Xiangang Li', display:{Lore:['[{"text": "arXiv:2007.14602", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransformer based unsupervised pre-training for acoustic representation learning\\u00a7r\\n\\n\\u00a78\\u00a7oRuixiong Zhang\\nHaiwei Wu\\nWubo Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.14602\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 8 Feb 2021 08:11:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2021\\u00a7r"}']}
{title:'Abdelaziz et al. (§72021§r)', author: 'Ahmed Hussen Abdelaziz; Anushree Prasanna Kumar; Chloe Seivwright; Gabriele Fanelli; Justin Binder; Yannis Stylianou; Sachin Kajarekar', display:{Lore:['[{"text": "arXiv:2008.00620", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudiovisual Speech Synthesis using Tacotron2\\u00a7r\\n\\n\\u00a78\\u00a7oAhmed Hussen Abdelaziz\\nAnushree Prasanna Kumar\\nChloe Seivwright\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.00620\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 30 Aug 2021 02:54:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to the 23rd ACM International Conference on Multimodal Interaction for possible publication\\u00a7r"}']}
{title:'Yoon et al. (§72021§r)', author: 'Ji Won Yoon; Hyeonseung Lee; Hyung Yong Kim; Won Ik Cho; Nam Soo Kim', display:{Lore:['[{"text": "arXiv:2008.00671", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTutorNet: Towards Flexible Knowledge Distillation for End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJi Won Yoon\\nHyeonseung Lee\\nHyung Yong Kim\\nWon Ik Cho\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.00671\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3071662\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 17 Sep 2021 01:30:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE/ACM Transactionson Audio, Speech and Language Processing\\u00a7r"}']}
{title:'Zhao et al. (§72021§r)', author: 'Yilun Zhao; Jia Guo', display:{Lore:['[{"text": "arXiv:2008.00781", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusiCoder: A Universal Music-Acoustic Encoder Based on Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oYilun Zhao\\nJia Guo\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.00781\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-030-67832-6_34\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 31 Jan 2021 09:52:26 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Jinglin Liu; Yi Ren; Zhou Zhao; Chen Zhang; Baoxing Huai; Nicholas Jing Yuan', display:{Lore:['[{"text": "arXiv:2008.02516", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFastLR: Non-Autoregressive Lipreading Model with Integrate-and-Fire\\u00a7r\\n\\n\\u00a78\\u00a7oJinglin Liu\\nYi Ren\\nZhou Zhao\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.02516\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 15 Mar 2021 07:23:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ACM MM 2020\\u00a7r"}']}
{title:'Shetty et al. (§72021§r)', author: 'Vishwas M. Shetty; Metilda Sagaya Mary N J; S. Umesh', display:{Lore:['[{"text": "arXiv:2008.03247", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigation of Speaker-adaptation methods in Transformer based ASR\\u00a7r\\n\\n\\u00a78\\u00a7oVishwas M. Shetty\\nMetilda Sagaya Mary N J\\nS. Umesh\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.03247\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 17 Nov 2021 21:11:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 6 figures\\u00a7r"}']}
{title:'Eskimez et al. (§72021§r)', author: 'Sefik Emre Eskimez; You Zhang; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2008.03592", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Driven Talking Face Generation from a Single Image and an Emotion Condition\\u00a7r\\n\\n\\u00a78\\u00a7oSefik Emre Eskimez\\nYou Zhang\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.03592\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 21 Jul 2021 22:45:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEETransactions on Multimedia\\u00a7r"}']}
{title:'Mary et al. (§72021§r)', author: 'N J Metilda Sagaya Mary; S Umesh; Sandesh V Katta', display:{Lore:['[{"text": "arXiv:2008.04659", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lS-vectors and TESA: Speaker Embeddings and a Speaker Authenticator Based on Transformer Encoder\\u00a7r\\n\\n\\u00a78\\u00a7oN J Metilda Sagaya Mary\\nS Umesh\\nSandesh V Katta\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.04659\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 12 Dec 2021 09:08:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oVersion 2, Accepted for publication in IEEE TASLP\\u00a7r"}']}
{title:'Ding et al. (§72021§r)', author: 'Shaojin Ding; Ye Jia; Ke Hu; Quan Wang', display:{Lore:['[{"text": "arXiv:2008.06006", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTextual Echo Cancellation\\u00a7r\\n\\n\\u00a78\\u00a7oShaojin Ding\\nYe Jia\\nKe Hu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.06006\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 17 Sep 2021 01:58:28 GMT)\\u00a7r"}']}
{title:'Lee et al. (§72021§r)', author: 'Taewoo Lee; Min-Joong Lee; Tae Gyoon Kang; Seokyeoung Jung; Minseok Kwon; Yeona Hong; Jungin Lee; Kyoung-Gu Woo; Ho-Gyeong Kim; Jiseung Jeong; Jihyun Lee; Hosik Lee; Young Sang Choi', display:{Lore:['[{"text": "arXiv:2008.06208", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptable Multi-Domain Language Model for Transformer ASR\\u00a7r\\n\\n\\u00a78\\u00a7oTaewoo Lee\\nMin-Joong Lee\\nTae Gyoon Kang\\n+ 9 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.06208\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 11 Feb 2021 03:17:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper is accepted for presentation at IEEE International Conference on Acoustics, Speech and Signal Processing (IEEEICASSP), 2021\\u00a7r"}']}
{title:'Bell et al. (§72021§r)', author: 'Peter Bell; Joachim Fainberg; Ondrej Klejch; Jinyu Li; Steve Renals; Pawel Swietojanski', display:{Lore:['[{"text": "arXiv:2008.06580", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptation Algorithms for Neural Network-Based Speech Recognition: An Overview\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Bell\\nJoachim Fainberg\\nOndrej Klejch\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.06580\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/OJSP.2020.3045349\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Open Journal of Signal Processing, vol. 2, pp. 33-66, 2021\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 28 Feb 2021 19:41:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTotal of 31 pages, 27 figures.Associated repository: https://github.com/pswietojanski/ojsp_adaptation_review_2020\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Zhuohuang Zhang; Yong Xu; Meng Yu; Shi-Xiong Zhang; Lianwu Chen; Dong Yu', display:{Lore:['[{"text": "arXiv:2008.06994", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lADL-MVDR: All deep learning MVDR beamformer for target speech separation\\u00a7r\\n\\n\\u00a78\\u00a7oZhuohuang Zhang\\nYong Xu\\nMeng Yu\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.06994\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 9 Feb 2021 00:52:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021, 5 pages, 2 figures; Demos are available at https://zzhang68.github.io/adlmvdr/\\u00a7r"}']}
{title:'Nguyen et al. (§72021§r)', author: 'Viet-Nhat Nguyen; Mostafa Sadeghi; Elisa Ricci; Xavier Alameda-Pineda', display:{Lore:['[{"text": "arXiv:2008.07191", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Variational Generative Models for Audio-visual Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oViet-Nhat Nguyen\\nMostafa Sadeghi\\nElisa Ricci\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.07191\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 31 Aug 2021 10:01:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the 31st IEEE InternationalWorkshop on Machine Learning for Signal Processing (MLSP), Oct. 25-28, 2021, Gold Coast,Queensland, Australia\\u00a7r"}']}
{title:'Hernandez-Olivan et al. (§72021§r)', author: 'Carlos Hernandez-Olivan; Jose R. Beltran; David Diaz-Guerra', display:{Lore:['[{"text": "arXiv:2008.07527", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Boundary Detection using Convolutional Neural Networks: A comparative analysis of combined input features\\u00a7r\\n\\n\\u00a78\\u00a7oCarlos Hernandez-Olivan\\nJose R. Beltran\\nDavid Diaz-Guerra\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.07527\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.9781/ijimai.2021.10.005\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInternational Journal of Interactive Multimedia & Artificial\\n  Intelligence (2021), vol. 7, no 2, p. 78-88\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 1 Dec 2021 15:01:19 GMT)\\u00a7r"}']}
{title:'Michelsanti et al. (§72021§r)', author: 'Daniel Michelsanti; Zheng-Hua Tan; Shi-Xiong Zhang; Yong Xu; Meng Yu; Dong Yu; Jesper Jensen', display:{Lore:['[{"text": "arXiv:2008.09586", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Overview of Deep-Learning-Based Audio-Visual Speech Enhancement and Separation\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Michelsanti\\nZheng-Hua Tan\\nShi-Xiong Zhang\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.09586\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 12 Mar 2021 22:27:01 GMT)\\u00a7r"}']}
{title:'He et al. (§72021§r)', author: 'Liqiang He; Dan Su; Dong Yu', display:{Lore:['[{"text": "arXiv:2008.11589", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearned Transferable Architectures Can Surpass Hand-Designed Architectures for Large Scale Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oLiqiang He\\nDan Su\\nDong Yu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.11589\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 8 May 2021 09:24:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Tandogan et al. (§72021§r)', author: 'Erkam Sinan Tandogan; Husrev Taha Sencar', display:{Lore:['[{"text": "arXiv:2008.11985", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEstimating Uniqueness of I-Vector Representation of Human Voice\\u00a7r\\n\\n\\u00a78\\u00a7oErkam Sinan Tandogan\\nHusrev Taha Sencar\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.11985\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 3 Mar 2021 11:52:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Zhan Zhang; Yuehai Wang; Jianyi Yang', display:{Lore:['[{"text": "arXiv:2008.12424", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lText-Conditioned Transformer for Automatic Pronunciation Error Detection\\u00a7r\\n\\n\\u00a78\\u00a7oZhan Zhang\\nYuehai Wang\\nJianyi Yang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.12424\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.specom.2021.04.004\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nSpeech Communication Volume 130, June 2021, Pages 55-63\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 5 May 2021 07:04:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7opublished for Speech Communication journal\\u00a7r"}']}
{title:'Pandey et al. (§72021§r)', author: 'Ashutosh Pandey; DeLiang Wang', display:{Lore:['[{"text": "arXiv:2009.01941", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDense CNN with Self-Attention for Time-Domain Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oAshutosh Pandey\\nDeLiang Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2009.01941\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Mar 2021 08:55:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to IEEE/ACM Transactionson Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Songxiang Liu; Yuewen Cao; Disong Wang; Xixin Wu; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2009.02725", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAny-to-Many Voice Conversion with Location-Relative Sequence-to-Sequence Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oSongxiang Liu\\nYuewen Cao\\nDisong Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2009.02725\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3076867\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 23 May 2021 09:14:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE/ACM Transactionson Audio, Speech and Language Processing\\u00a7r"}']}
{title:'Politis et al. (§72021§r)', author: 'Archontis Politis; Annamaria Mesaros; Sharath Adavanne; Toni Heittola; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2009.02792", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOverview and Evaluation of Sound Event Localization and Detection in DCASE 2019\\u00a7r\\n\\n\\u00a78\\u00a7oArchontis Politis\\nAnnamaria Mesaros\\nSharath Adavanne\\nToni Heittola\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2009.02792\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2020.3047233\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 11 Jan 2021 16:35:22 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Bo Zhang; Wenfeng Li; Qingyuan Li; Weiji Zhuang; Xiangxiang Chu; Yujun Wang', display:{Lore:['[{"text": "arXiv:2009.03658", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutoKWS: Keyword Spotting with Differentiable Architecture Search\\u00a7r\\n\\n\\u00a78\\u00a7oBo Zhang\\nWenfeng Li\\nQingyuan Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2009.03658\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 22 Feb 2021 14:31:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2021\\u00a7r"}']}
{title:'Blouw et al. (§72021§r)', author: 'Peter Blouw; Gurshaant Malik; Benjamin Morcos; Aaron R. Voelker; Chris Eliasmith', display:{Lore:['[{"text": "arXiv:2009.04465", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHardware Aware Training for Efficient Keyword Spotting on General Purpose and Specialized Hardware\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Blouw\\nGurshaant Malik\\nBenjamin Morcos\\nAaron R. Voelker\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2009.04465\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 10 Mar 2021 03:10:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, TinyML Research Symposium \'21\\u00a7r"}']}
{title:'Dong et al. (§72021§r)', author: 'Yingjun Dong; Neil G. MacLaren; Yiding Cao; Francis J. Yammarino; Shelley D. Dionne; Michael D. Mumford; Shane Connelly; Hiroki Sayama; Gregory A. Ruark', display:{Lore:['[{"text": "arXiv:2009.05076", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUtterance Clustering Using Stereo Audio Channels\\u00a7r\\n\\n\\u00a78\\u00a7oYingjun Dong\\nNeil G. MacLaren\\nYiding Cao\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2009.05076\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 21 Sep 2021 00:27:18 GMT)\\u00a7r"}']}
{title:'Hono et al. (§72021§r)', author: 'Yukiya Hono; Kazuna Tsuboi; Kei Sawada; Kei Hashimoto; Keiichiro Oura; Yoshihiko Nankaku; Keiichi Tokuda', display:{Lore:['[{"text": "arXiv:2009.08474", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierarchical Multi-Grained Generative Model for Expressive Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYukiya Hono\\nKazuna Tsuboi\\nKei Sawada\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2009.08474\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 26 Dec 2021 08:42:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted to INTERSPEECH 2020, demo page: https://www.rinna.jp/research/interspeech2020/\\u00a7r"}']}
{title:'Watanabe et al. (§72021§r)', author: 'Chihiro Watanabe; Hirokazu Kameoka', display:{Lore:['[{"text": "arXiv:2009.08661", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lX-DC: Explainable Deep Clustering based on Learnable Spectrogram Templates\\u00a7r\\n\\n\\u00a78\\u00a7oChihiro Watanabe\\nHirokazu Kameoka\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2009.08661\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 19 Apr 2021 06:53:09 GMT)\\u00a7r"}']}
{title:'Chan et al. (§72021§r)', author: 'Teck Kai Chan; Cheng Siong Chin', display:{Lore:['[{"text": "arXiv:2009.09632", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetecting Sound Events Using Convolutional Macaron Net With Pseudo Strong Labels\\u00a7r\\n\\n\\u00a78\\u00a7oTeck Kai Chan\\nCheng Siong Chin\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2009.09632\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 2 Aug 2021 10:40:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUpdated\\u00a7r"}']}
{title:'Jung et al. (§72021§r)', author: 'Jee-weon Jung; Hye-jin Shim; Ju-ho Kim; Ha-Jin Yu', display:{Lore:['[{"text": "arXiv:2009.09642", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDcaseNet: An integrated pretrained deep neural network for detecting and classifying acoustic scenes and events\\u00a7r\\n\\n\\u00a78\\u00a7oJee-weon Jung\\nHye-jin Shim\\nJu-ho Kim\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2009.09642\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 8 Feb 2021 09:29:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, 3 tables. accepted for presentation at ICASSP 2021 as a conference paper\\u00a7r"}']}
{title:'Kong et al. (§72021§r)', author: 'Zhifeng Kong; Wei Ping; Jiaji Huang; Kexin Zhao; Bryan Catanzaro', display:{Lore:['[{"text": "arXiv:2009.09761", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffWave: A Versatile Diffusion Model for Audio Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oZhifeng Kong\\nWei Ping\\nJiaji Huang\\nKexin Zhao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2009.09761\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 30 Mar 2021 19:48:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICLR 2021 (oral)\\u00a7r"}']}
{title:'Khassanov et al. (§72021§r)', author: 'Yerbolat Khassanov; Saida Mussakhojayeva; Almas Mirzakhmetov; Alen Adiyev; Mukhamet Nurpeiissov; Huseyin Atakan Varol', display:{Lore:['[{"text": "arXiv:2009.10334", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Crowdsourced Open-Source Kazakh Speech Corpus and Initial Speech Recognition Baseline\\u00a7r\\n\\n\\u00a78\\u00a7oYerbolat Khassanov\\nSaida Mussakhojayeva\\nAlmas Mirzakhmetov\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2009.10334\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nhttps://aclanthology.org/2021.eacl-main.58\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 13 Jan 2021 09:08:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 5 figures, 4 tables, accepted by EACL2021\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Mingyang Zhang; Yi Zhou; Li Zhao; Haizhou Li', display:{Lore:['[{"text": "arXiv:2009.14399", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransfer Learning from Speech Synthesis to Voice Conversion with Non-Parallel Training Data\\u00a7r\\n\\n\\u00a78\\u00a7oMingyang Zhang\\nYi Zhou\\nLi Zhao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2009.14399\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Jan 2021 06:34:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEE/ACM Transactionson Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Serrà et al. (§72021§r)', author: 'Joan Serrà; Jordi Pons; Santiago Pascual', display:{Lore:['[{"text": "arXiv:2010.00368", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSESQA: semi-supervised learning for speech quality assessment\\u00a7r\\n\\n\\u00a78\\u00a7oJoan Serr\\u00e0\\nJordi Pons\\nSantiago Pascual\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.00368\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Feb 2021 09:01:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oLong version (with appendix) of the paper with the same title accepted for ICASSP2021\\u00a7r"}']}
{title:'Shuvo et al. (§72021§r)', author: 'Samiul Based Shuvo; Shams Nafisa Ali; Soham Irtiza Swapnil; Mabrook S. Al-Rakhami; Abdu Gumaei', display:{Lore:['[{"text": "arXiv:2010.01392", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCardioXNet: A Novel Lightweight Deep Learning Framework for Cardiovascular Disease Classification Using Heart Sound Recordings\\u00a7r\\n\\n\\u00a78\\u00a7oSamiul Based Shuvo\\nShams Nafisa Ali\\nSoham Irtiza Swapnil\\nMabrook S. Al-Rakhami\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.01392\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ACCESS.2021.3063129\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Mar 2021 14:56:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages\\u00a7r"}']}
{title:'Takahashi et al. (§72021§r)', author: 'Naoya Takahashi; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2010.01733", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lD3Net: Densely connected multidilated DenseNet for music source separation\\u00a7r\\n\\n\\u00a78\\u00a7oNaoya Takahashi\\nYuki Mitsufuji\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.01733\\u00a7r\\n\\nVersion:\\u00a77v4 (Sat, 27 Mar 2021 04:55:38 GMT)\\u00a7r"}']}
{title:'Sawata et al. (§72021§r)', author: 'Ryosuke Sawata; Stefan Uhlich; Shusuke Takahashi; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2010.04228", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAll for One and One for All: Improving Music Separation by Bridging Networks\\u00a7r\\n\\n\\u00a78\\u00a7oRyosuke Sawata\\nStefan Uhlich\\nShusuke Takahashi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.04228\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 11 May 2021 12:17:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe both implementations of our code, i.e., NNabla and PyTorch, are available on this latest paper\\u00a7r"}']}
{title:'Steinmetz et al. (§72021§r)', author: 'Christian J. Steinmetz; Joshua D. Reiss', display:{Lore:['[{"text": "arXiv:2010.04237", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRandomized Overdrive Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oChristian J. Steinmetz\\nJoshua D. Reiss\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.04237\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 4 Aug 2021 13:51:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUpdating project URL. Now https://csteinmetz1.github.io/ronn\\u00a7r"}']}
{title:'Morrone et al. (§72021§r)', author: 'Giovanni Morrone; Daniel Michelsanti; Zheng-Hua Tan; Jesper Jensen', display:{Lore:['[{"text": "arXiv:2010.04556", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-Visual Speech Inpainting with Deep Learning\\u00a7r\\n\\n\\u00a78\\u00a7oGiovanni Morrone\\nDaniel Michelsanti\\nZheng-Hua Tan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.04556\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 3 Feb 2021 11:36:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2021\\u00a7r"}']}
{title:'Pan et al. (§72021§r)', author: 'Zexu Pan; Ruijie Tao; Chenglin Xu; Haizhou Li', display:{Lore:['[{"text": "arXiv:2010.07775", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMuse: Multi-modal target speaker extraction with visual cues\\u00a7r\\n\\n\\u00a78\\u00a7oZexu Pan\\nRuijie Tao\\nChenglin Xu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.07775\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 10 Feb 2021 04:40:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2021\\u00a7r"}']}
{title:'Karbasi et al. (§72021§r)', author: 'Mahdie Karbasi; Stefan Bleeck; Dorothea Kolossa', display:{Lore:['[{"text": "arXiv:2010.08574", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-intrusive speech intelligibility prediction using automatic speech recognition derived measures\\u00a7r\\n\\n\\u00a78\\u00a7oMahdie Karbasi\\nStefan Bleeck\\nDorothea Kolossa\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.08574\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 28 Oct 2021 17:25:53 GMT)\\u00a7r"}']}
{title:'Guo et al. (§72021§r)', author: 'Tingwei Guo; Cheng Wen; Dongwei Jiang; Ne Luo; Ruixiong Zhang; Shuaijiang Zhao; Wubo Li; Cheng Gong; Wei Zou; Kun Han; Xiangang Li', display:{Lore:['[{"text": "arXiv:2010.09275", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiDiSpeech: A Large Scale Mandarin Speech Corpus\\u00a7r\\n\\n\\u00a78\\u00a7oTingwei Guo\\nCheng Wen\\nDongwei Jiang\\n+ 7 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.09275\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 8 Feb 2021 09:36:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 11 tables\\u00a7r"}']}
{title:'Diwan et al. (§72021§r)', author: 'Anuj Diwan; Preethi Jyothi', display:{Lore:['[{"text": "arXiv:2010.09322", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReduce and Reconstruct: ASR for Low-Resource Phonetic Languages\\u00a7r\\n\\n\\u00a78\\u00a7oAnuj Diwan\\nPreethi Jyothi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.09322\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-644\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 3 Jun 2021 10:11:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure. Accepted at INTERSPEECH 2021\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Yunpeng Li; Marco Tagliasacchi; Oleg Rybakov; Victor Ungureanu; Dominik Roblek', display:{Lore:['[{"text": "arXiv:2010.10677", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-time Speech Frequency Bandwidth Extension\\u00a7r\\n\\n\\u00a78\\u00a7oYunpeng Li\\nMarco Tagliasacchi\\nOleg Rybakov\\nVictor Ungureanu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.10677\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 Feb 2021 12:52:17 GMT)\\u00a7r"}']}
{title:'Williams et al. (§72021§r)', author: 'Jennifer Williams; Yi Zhao; Erica Cooper; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2010.10727", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Disentangled Phone and Speaker Representations in a Semi-Supervised VQ-VAE Paradigm\\u00a7r\\n\\n\\u00a78\\u00a7oJennifer Williams\\nYi Zhao\\nErica Cooper\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.10727\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 10 Feb 2021 07:48:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Han et al. (§72021§r)', author: 'Jiangyu Han; Wei Rao; Yanhua Long; Jiaen Liang', display:{Lore:['[{"text": "arXiv:2010.10923", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention-based scaling adaptation for target speech extraction\\u00a7r\\n\\n\\u00a78\\u00a7oJiangyu Han\\nWei Rao\\nYanhua Long\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.10923\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 19 Oct 2021 01:08:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures. Accepted by ASRU 2021\\u00a7r"}']}
{title:'Yu et al. (§72021§r)', author: 'Jiahui Yu; Chung-Cheng Chiu; Bo Li; Shuo-yiin Chang; Tara N. Sainath; Yanzhang He; Arun Narayanan; Wei Han; Anmol Gulati; Yonghui Wu; Ruoming Pang', display:{Lore:['[{"text": "arXiv:2010.11148", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization\\u00a7r\\n\\n\\u00a78\\u00a7oJiahui Yu\\nChung-Cheng Chiu\\nBo Li\\n+ 7 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.11148\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 3 Feb 2021 20:59:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ICASSP 2021\\u00a7r"}']}
{title:'Huang et al. (§72021§r)', author: 'Qiang Huang; Thomas Hain', display:{Lore:['[{"text": "arXiv:2010.11286", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Audio Anomalies Recognition Using Temporal Convolutional Attention Network\\u00a7r\\n\\n\\u00a78\\u00a7oQiang Huang\\nThomas Hain\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.11286\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 Feb 2021 23:51:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted by ICASSP\'2021\\u00a7r"}']}
{title:'Chung et al. (§72021§r)', author: 'Yu-An Chung; Yonatan Belinkov; James Glass', display:{Lore:['[{"text": "arXiv:2010.11481", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSimilarity Analysis of Self-Supervised Speech Representations\\u00a7r\\n\\n\\u00a78\\u00a7oYu-An Chung\\nYonatan Belinkov\\nJames Glass\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.11481\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 2 Feb 2021 14:42:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021. Supplementary materials available at https://github.com/iamyuanchung/ICASSP21-Similarity-Supplementary\\u00a7r"}']}
{title:'Peng et al. (§72021§r)', author: 'Yizhou Peng; Jicheng Zhang; Haobo Zhang; Haihua Xu; Hao Huang; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2010.11483", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultilingual Approach to Joint Speech and Accent Recognition with DNN-HMM Framework\\u00a7r\\n\\n\\u00a78\\u00a7oYizhou Peng\\nJicheng Zhang\\nHaobo Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.11483\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 8 May 2021 10:12:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, Conference\\u00a7r"}']}
{title:'Jung et al. (§72021§r)', author: 'Jee-weon Jung; Hee-Soo Heo; Ha-Jin Yu; Joon Son Chung', display:{Lore:['[{"text": "arXiv:2010.11543", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGraph Attention Networks for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oJee-weon Jung\\nHee-Soo Heo\\nHa-Jin Yu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.11543\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Feb 2021 08:12:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, 2 tables, accepted for presentation at ICASSP 2021 as a conference paper\\u00a7r"}']}
{title:'Landini et al. (§72021§r)', author: 'Federico Landini; Ondřej Glembek; Pavel Matějka; Johan Rohdin; Lukáš Burget; Mireia Diez; Anna Silnova', display:{Lore:['[{"text": "arXiv:2010.11718", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalysis of the BUT Diarization System for VoxConverse Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oFederico Landini\\nOnd\\u0159ej Glembek\\nPavel Mat\\u011bjka\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.11718\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 Feb 2021 16:00:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Yemini et al. (§72021§r)', author: 'Yochai Yemini; Ethan Fetaya; Haggai Maron; Sharon Gannot', display:{Lore:['[{"text": "arXiv:2010.11875", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScene-Agnostic Multi-Microphone Speech Dereverberation\\u00a7r\\n\\n\\u00a78\\u00a7oYochai Yemini\\nEthan Fetaya\\nHaggai Maron\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.11875\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 10 Jun 2021 18:17:26 GMT)\\u00a7r"}']}
{title:'Shi et al. (§72021§r)', author: 'Jiatong Shi; Shuai Guo; Nan Huo; Yuekai Zhang; Qin Jin', display:{Lore:['[{"text": "arXiv:2010.12024", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSequence-to-sequence Singing Voice Synthesis with Perceptual Entropy Loss\\u00a7r\\n\\n\\u00a78\\u00a7oJiatong Shi\\nShuai Guo\\nNan Huo\\nYuekai Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.12024\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 26 Feb 2021 16:33:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2021\\u00a7r"}']}
{title:'Feng et al. (§72021§r)', author: 'Siyuan Feng; Piotr Żelasko; Laureano Moro-Velázquez; Ali Abavisani; Mark Hasegawa-Johnson; Odette Scharenborg; Najim Dehak', display:{Lore:['[{"text": "arXiv:2010.12104", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHow Phonotactics Affect Multilingual and Zero-shot ASR Performance\\u00a7r\\n\\n\\u00a78\\u00a7oSiyuan Feng\\nPiotr \\u017belasko\\nLaureano Moro-Vel\\u00e1zquez\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.12104\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9414478\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 10 Feb 2021 18:53:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in IEEE ICASSP 2021. The first 2authors contributed equally to this work\\u00a7r"}']}
{title:'Maciejewski et al. (§72021§r)', author: 'Matthew Maciejewski; Jing Shi; Shinji Watanabe; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:2010.12430", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTraining Noisy Single-Channel Speech Separation With Noisy Oracle Sources: A Large Gap and A Small Step\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew Maciejewski\\nJing Shi\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.12430\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 22 Feb 2021 16:50:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Tan et al. (§72021§r)', author: 'Xu Tan; Xiao-Lei Zhang', display:{Lore:['[{"text": "arXiv:2010.12484", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech enhancement aided end-to-end multi-task learning for voice activity detection\\u00a7r\\n\\n\\u00a78\\u00a7oXu Tan\\nXiao-Lei Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.12484\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 13 Apr 2021 08:03:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2021\\u00a7r"}']}
{title:'Kataria et al. (§72021§r)', author: 'Saurabh Kataria; Shi-Xiong Zhang; Dong Yu', display:{Lore:['[{"text": "arXiv:2010.12692", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Channel Speaker Verification for Single and Multi-talker Speech\\u00a7r\\n\\n\\u00a78\\u00a7oSaurabh Kataria\\nShi-Xiong Zhang\\nDong Yu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.12692\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 9 Apr 2021 15:37:58 GMT)\\u00a7r"}']}
{title:'Zhu et al. (§72021§r)', author: 'Ge Zhu; Fei Jiang; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2010.12951", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lY-Vector: Multiscale Waveform Encoder for Speaker Embedding\\u00a7r\\n\\n\\u00a78\\u00a7oGe Zhu\\nFei Jiang\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.12951\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 9 Jun 2021 02:17:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2021\\u00a7r"}']}
{title:'Ma et al. (§72021§r)', author: 'Danni Ma; Neville Ryant; Mark Liberman', display:{Lore:['[{"text": "arXiv:2010.13007", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProbing Acoustic Representations for Phonetic Properties\\u00a7r\\n\\n\\u00a78\\u00a7oDanni Ma\\nNeville Ryant\\nMark Liberman\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.13007\\u00a7r\\n\\nVersion:\\u00a77v4 (Sun, 14 Feb 2021 21:22:48 GMT)\\u00a7r"}']}
{title:'Subakan et al. (§72021§r)', author: 'Cem Subakan; Mirco Ravanelli; Samuele Cornell; Mirko Bronzi; Jianyuan Zhong', display:{Lore:['[{"text": "arXiv:2010.13154", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention is All You Need in Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oCem Subakan\\nMirco Ravanelli\\nSamuele Cornell\\nMirko Bronzi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.13154\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Mar 2021 21:24:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Naderi et al. (§72021§r)', author: 'Babak Naderi; Ross Cutler', display:{Lore:['[{"text": "arXiv:2010.13200", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSubjective Evaluation of Noise Suppression Algorithms in Crowdsourcing\\u00a7r\\n\\n\\u00a78\\u00a7oBabak Naderi\\nRoss Cutler\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.13200\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 16 Apr 2021 04:51:58 GMT)\\u00a7r"}']}
{title:'Higuchi et al. (§72021§r)', author: 'Yosuke Higuchi; Hirofumi Inaguma; Shinji Watanabe; Tetsuji Ogawa; Tetsunori Kobayashi', display:{Lore:['[{"text": "arXiv:2010.13270", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Mask-CTC for Non-Autoregressive End-to-End ASR\\u00a7r\\n\\n\\u00a78\\u00a7oYosuke Higuchi\\nHirofumi Inaguma\\nShinji Watanabe\\nTetsuji Ogawa\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.13270\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 Feb 2021 05:46:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP2021\\u00a7r"}']}
{title:'Cai et al. (§72021§r)', author: 'Xiong Cai; Dongyang Dai; Zhiyong Wu; Xiang Li; Jingbei Li; Helen Meng', display:{Lore:['[{"text": "arXiv:2010.13350", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotion controllable speech synthesis using emotion-unlabeled dataset with the assistance of cross-domain speech emotion recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXiong Cai\\nDongyang Dai\\nZhiyong Wu\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.13350\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 17 Jan 2021 13:45:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oicassp2021 final version\\u00a7r"}']}
{title:'Kinoshita et al. (§72021§r)', author: 'Keisuke Kinoshita; Marc Delcroix; Naohiro Tawara', display:{Lore:['[{"text": "arXiv:2010.13366", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntegrating end-to-end neural and clustering-based diarization: Getting the best of both worlds\\u00a7r\\n\\n\\u00a78\\u00a7oKeisuke Kinoshita\\nMarc Delcroix\\nNaohiro Tawara\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.13366\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 5 Feb 2021 02:34:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP 2021\\u00a7r"}']}
{title:'Jia et al. (§72021§r)', author: 'Fei Jia; Somshubra Majumdar; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2010.13886", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMarbleNet: Deep 1D Time-Channel Separable Convolutional Neural Network for Voice Activity Detection\\u00a7r\\n\\n\\u00a78\\u00a7oFei Jia\\nSomshubra Majumdar\\nBoris Ginsburg\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.13886\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 11 Feb 2021 19:53:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'You Zhang; Fei Jiang; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2010.13995", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOne-class Learning Towards Synthetic Voice Spoofing Detection\\u00a7r\\n\\n\\u00a78\\u00a7oYou Zhang\\nFei Jiang\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.13995\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2021.3076358\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 Feb 2021 06:12:10 GMT)\\u00a7r"}']}
{title:'Lin et al. (§72021§r)', author: 'Yist Y. Lin; Chung-Ming Chien; Jheng-Hao Lin; Hung-yi Lee; Lin-shan Lee', display:{Lore:['[{"text": "arXiv:2010.14150", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFragmentVC: Any-to-Any Voice Conversion by End-to-End Extracting and Fusing Fine-Grained Voice Fragments With Attention\\u00a7r\\n\\n\\u00a78\\u00a7oYist Y. Lin\\nChung-Ming Chien\\nJheng-Hao Lin\\nHung-yi Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.14150\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 3 May 2021 16:05:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in the proceedings of ICASSP 2021, equal contribution from first two authors\\u00a7r"}']}
{title:'Yamamoto et al. (§72021§r)', author: 'Ryuichi Yamamoto; Eunwoo Song; Min-Jae Hwang; Jae-Min Kim', display:{Lore:['[{"text": "arXiv:2010.14151", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParallel waveform synthesis based on generative adversarial networks with voicing-aware conditional discriminators\\u00a7r\\n\\n\\u00a78\\u00a7oRyuichi Yamamoto\\nEunwoo Song\\nMin-Jae Hwang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.14151\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 26 Apr 2021 08:37:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the conference of ICASSP 2021\\u00a7r"}']}
{title:'Wu et al. (§72021§r)', author: 'Yifan Wu; Roshan Ayyalasomayajula; Michael J. Bianco; Dinesh Bharadia; Peter Gerstoft', display:{Lore:['[{"text": "arXiv:2010.14420", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSSLIDE: Sound Source Localization for Indoors based on Deep Learning\\u00a7r\\n\\n\\u00a78\\u00a7oYifan Wu\\nRoshan Ayyalasomayajula\\nMichael J. Bianco\\nDinesh Bharadia\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.14420\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 15 Feb 2021 21:20:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper has been accepted by ICASSP 2021\\u00a7r"}']}
{title:'Fan et al. (§72021§r)', author: 'Ruchao Fan; Wei Chu; Peng Chang; Jing Xiao', display:{Lore:['[{"text": "arXiv:2010.14725", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCASS-NAT: CTC Alignment-based Single Step Non-autoregressive Transformer for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oRuchao Fan\\nWei Chu\\nPeng Chang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.14725\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 11 Feb 2021 22:40:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP2021, camera ready version\\u00a7r"}']}
{title:'Weng et al. (§72021§r)', author: 'Shi-Yan Weng; Berlin Chen', display:{Lore:['[{"text": "arXiv:2010.14764", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEffective Decoder Masking for Transformer Based End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oShi-Yan Weng\\nBerlin Chen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.14764\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 22 Jul 2021 03:02:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMore extensions and experiments are under exploration\\u00a7r"}']}
{title:'Tian et al. (§72021§r)', author: 'Zhengkun Tian; Jiangyan Yi; Ye Bai; Jianhua Tao; Shuai Zhang; Zhengqi Wen', display:{Lore:['[{"text": "arXiv:2010.14791", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOne In A Hundred: Select The Best Predicted Sequence from Numerous Candidates for Streaming Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhengkun Tian\\nJiangyan Yi\\nYe Bai\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.14791\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 4 Apr 2021 02:45:45 GMT)\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Xu Li; Na Li; Chao Weng; Xunying Liu; Dan Su; Dong Yu; Helen Meng', display:{Lore:['[{"text": "arXiv:2010.15006", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReplay and Synthetic Speech Detection with Res2net Architecture\\u00a7r\\n\\n\\u00a78\\u00a7oXu Li\\nNa Li\\nChao Weng\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.15006\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 13 Feb 2021 16:01:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP2021\\u00a7r"}']}
{title:'Zhao et al. (§72021§r)', author: 'An Zhao; Krishna Subramani; Paris Smaragdis', display:{Lore:['[{"text": "arXiv:2010.15049", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimizing Short-Time Fourier Transform Parameters via Gradient Descent\\u00a7r\\n\\n\\u00a78\\u00a7oAn Zhao\\nKrishna Subramani\\nParis Smaragdis\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.15049\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9413704\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 18 Feb 2021 22:42:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP 2021\\u00a7r"}']}
{title:'Shimada et al. (§72021§r)', author: 'Kazuki Shimada; Yuichiro Koyama; Naoya Takahashi; Shusuke Takahashi; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2010.15306", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lACCDOA: Activity-Coupled Cartesian Direction of Arrival Representation for Sound Event Localization and Detection\\u00a7r\\n\\n\\u00a78\\u00a7oKazuki Shimada\\nYuichiro Koyama\\nNaoya Takahashi\\nShusuke Takahashi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.15306\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 15 Feb 2021 03:48:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, accepted for publication in IEEE ICASSP 2021\\u00a7r"}']}
{title:'Huang et al. (§72021§r)', author: 'Zhiying Huang; Hao Li; Ming Lei', display:{Lore:['[{"text": "arXiv:2010.15311", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeviceTTS: A Small-Footprint, Fast, Stable Network for On-Device Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oZhiying Huang\\nHao Li\\nMing Lei\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.15311\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 15 Jan 2021 04:40:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, Submitted to ICASSP2021\\u00a7r"}']}
{title:'Sigtia et al. (§72021§r)', author: 'Siddharth Sigtia; John Bridle; Hywel Richards; Pascal Clark; Erik Marchi; Vineet Garg', display:{Lore:['[{"text": "arXiv:2010.15446", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProgressive Voice Trigger Detection: Accuracy vs Latency\\u00a7r\\n\\n\\u00a78\\u00a7oSiddharth Sigtia\\nJohn Bridle\\nHywel Richards\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.15446\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 2 Mar 2021 15:16:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCamera Ready Version: ICASSP 2021\\u00a7r"}']}
{title:'Hao et al. (§72021§r)', author: 'Xiang Hao; Xiangdong Su; Radu Horaud; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2010.15508", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFullSubNet: A Full-Band and Sub-Band Fusion Model for Real-Time Single-Channel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oXiang Hao\\nXiangdong Su\\nRadu Horaud\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.15508\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 24 Jan 2021 01:57:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, submitted to 2021 IEEE InternationalConference on Acoustics, Speech, and Signal Processing (ICASSP 2021)\\u00a7r"}']}
{title:'Záviška et al. (§72021§r)', author: 'Pavel Záviška; Pavel Rajmic; Ondřej Mokrý', display:{Lore:['[{"text": "arXiv:2010.16386", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Dequantization Using (Co)Sparse (Non)Convex Methods\\u00a7r\\n\\n\\u00a78\\u00a7oPavel Z\\u00e1vi\\u0161ka\\nPavel Rajmic\\nOnd\\u0159ej Mokr\\u00fd\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.16386\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9414637\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2021 - 2021 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 10 Feb 2021 08:00:32 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Shanshan Wang; Annamaria Mesaros; Toni Heittola; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2011.00030", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Curated Dataset of Urban Scenes for Audio-Visual Scene Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oShanshan Wang\\nAnnamaria Mesaros\\nToni Heittola\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.00030\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 11 Feb 2021 09:27:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by ICASSP 2021\\u00a7r"}']}
{title:'Tak et al. (§72021§r)', author: 'Hemlata Tak; Jose Patino; Massimiliano Todisco; Andreas Nautsch; Nicholas Evans; Anthony Larcher', display:{Lore:['[{"text": "arXiv:2011.01108", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end anti-spoofing with RawNet2\\u00a7r\\n\\n\\u00a78\\u00a7oHemlata Tak\\nJose Patino\\nMassimiliano Todisco\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.01108\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 16 Dec 2021 16:04:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Patino et al. (§72021§r)', author: 'Jose Patino; Natalia Tomashenko; Massimiliano Todisco; Andreas Nautsch; Nicholas Evans', display:{Lore:['[{"text": "arXiv:2011.01130", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker anonymisation using the McAdams coefficient\\u00a7r\\n\\n\\u00a78\\u00a7oJose Patino\\nNatalia Tomashenko\\nMassimiliano Todisco\\nAndreas Nautsch\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.01130\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 1 Sep 2021 14:28:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2021\\u00a7r"}']}
{title:'Hodari et al. (§72021§r)', author: 'Zack Hodari; Alexis Moinet; Sri Karlapati; Jaime Lorenzo-Trueba; Thomas Merritt; Arnaud Joly; Ammar Abbas; Penny Karanasou; Thomas Drugman', display:{Lore:['[{"text": "arXiv:2011.01175", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCAMP: a Two-Stage Approach to Modelling Prosody in Context\\u00a7r\\n\\n\\u00a78\\u00a7oZack Hodari\\nAlexis Moinet\\nSri Karlapati\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.01175\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 12 Feb 2021 11:27:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages. Published in the 2021 IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP 2021)\\u00a7r"}']}
{title:'Mustafa et al. (§72021§r)', author: 'Ahmed Mustafa; Nicola Pia; Guillaume Fuchs', display:{Lore:['[{"text": "arXiv:2011.01557", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStyleMelGAN: An Efficient High-Fidelity Adversarial Vocoder with Temporal Adaptive Normalization\\u00a7r\\n\\n\\u00a78\\u00a7oAhmed Mustafa\\nNicola Pia\\nGuillaume Fuchs\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.01557\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 12 Feb 2021 18:21:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP2021\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Disong Wang; Songxiang Liu; Lifa Sun; Xixin Wu; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2011.01678", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Explicit Prosody Models and Deep Speaker Embeddings for Atypical Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oDisong Wang\\nSongxiang Liu\\nLifa Sun\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.01678\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 17 Jun 2021 12:50:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021\\u00a7r"}']}
{title:'Chen et al. (§72021§r)', author: 'Yu-Wen Chen; Kuo-Hsuan Hung; Shang-Yi Chuang; Jonathan Sherman; Xugang Lu; Yu Tsao', display:{Lore:['[{"text": "arXiv:2011.01691", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Study of Incorporating Articulatory Movement Information in Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oYu-Wen Chen\\nKuo-Hsuan Hung\\nShang-Yi Chuang\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.01691\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 9 Jun 2021 05:45:20 GMT)\\u00a7r"}']}
{title:'Raj et al. (§72021§r)', author: 'Desh Raj; Jesus Villalba; Daniel Povey; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:2011.02090", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrustratingly Easy Noise-aware Training of Acoustic Models\\u00a7r\\n\\n\\u00a78\\u00a7oDesh Raj\\nJesus Villalba\\nDaniel Povey\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.02090\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 2 Feb 2021 17:31:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 + 3 (Appendix) pages\\u00a7r"}']}
{title:'Heydari et al. (§72021§r)', author: 'Mojtaba Heydari; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2011.02619", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDon\'t look back: an online beat tracking method using RNN and enhanced particle filtering\\u00a7r\\n\\n\\u00a78\\u00a7oMojtaba Heydari\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.02619\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 1 Mar 2021 06:30:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE International Conference on Acoustics, Speech and Signal Processing, (ICASSP 2021). (ACCEPTED)\\u00a7r"}']}
{title:'Tan et al. (§72021§r)', author: 'Daxin Tan; Tan Lee', display:{Lore:['[{"text": "arXiv:2011.03943", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFine-grained Style Modeling, Transfer and Prediction in Text-to-Speech Synthesis via Phone-Level Content-Style Disentanglement\\u00a7r\\n\\n\\u00a78\\u00a7oDaxin Tan\\nTan Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.03943\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-1129\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 8 Oct 2021 03:22:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2021\\u00a7r"}']}
{title:'Elminshawi et al. (§72021§r)', author: 'Mohamed Elminshawi; Wolfgang Mack; Emanuël A. P. Habets', display:{Lore:['[{"text": "arXiv:2011.04569", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInformed Source Extraction With Application to Acoustic Echo Reduction\\u00a7r\\n\\n\\u00a78\\u00a7oMohamed Elminshawi\\nWolfgang Mack\\nEmanu\\u00ebl A. P. Habets\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.04569\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 26 Oct 2021 14:51:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at ITG 2021\\u00a7r"}']}
{title:'Huybrechts et al. (§72021§r)', author: 'Goeric Huybrechts; Thomas Merritt; Giulia Comini; Bartek Perz; Raahil Shah; Jaime Lorenzo-Trueba', display:{Lore:['[{"text": "arXiv:2011.05707", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-resource expressive text-to-speech using data augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oGoeric Huybrechts\\nThomas Merritt\\nGiulia Comini\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.05707\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 1 Jun 2021 20:18:08 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Songxiang Liu; Yuewen Cao; Na Hu; Dan Su; Helen Meng', display:{Lore:['[{"text": "arXiv:2011.05731", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFastSVC: Fast Cross-Domain Singing Voice Conversion with Feature-wise Linear Modulation\\u00a7r\\n\\n\\u00a78\\u00a7oSongxiang Liu\\nYuewen Cao\\nNa Hu\\nDan Su\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.05731\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 23 May 2021 09:12:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE International Conference on Multimedia and Expo (ICME) 2021\\u00a7r"}']}
{title:'Chien et al. (§72021§r)', author: 'Chung-Ming Chien; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2011.06465", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierarchical Prosody Modeling for Non-Autoregressive Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oChung-Ming Chien\\nHung-yi Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.06465\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 1 May 2021 07:59:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by SLT 2021\\u00a7r"}']}
{title:'Seneviratne et al. (§72021§r)', author: 'Nadee Seneviratne; Carol Espy-Wilson', display:{Lore:['[{"text": "arXiv:2011.06739", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeneralized Dilated CNN Models for Depression Detection Using Inverted Vocal Tract Variables\\u00a7r\\n\\n\\u00a78\\u00a7oNadee Seneviratne\\nCarol Espy-Wilson\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.06739\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 9 Apr 2021 04:29:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, Submitted to Interspeech 2021\\u00a7r"}']}
{title:'Sulun et al. (§72021§r)', author: 'Serkan Sulun; Matthew E. P. Davies', display:{Lore:['[{"text": "arXiv:2011.07274", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Filter Generalization for Music Bandwidth Extension Using Deep Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oSerkan Sulun\\nMatthew E. P. Davies\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.07274\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JSTSP.2020.3037485\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Jan 2021 08:45:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oQualitative examples on https://serkansulun.com/bwe. Source code on https://github.com/serkansulun/deep-music-enhancer\\u00a7r"}']}
{title:'Janbakhshi et al. (§72021§r)', author: 'P. Janbakhshi; I. Kodrasi; H. Bourlard', display:{Lore:['[{"text": "arXiv:2011.07545", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic dysarthric speech detection exploiting pairwise distance-based convolutional neural networks\\u00a7r\\n\\n\\u00a78\\u00a7oP. Janbakhshi\\nI. Kodrasi\\nH. Bourlard\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.07545\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 28 May 2021 18:56:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at ICASSP 2021\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'L. Wang; J. Zhu; I. Kodrasi', display:{Lore:['[{"text": "arXiv:2011.07547", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-task single channel speech enhancement using speech presence probability as a secondary task training target\\u00a7r\\n\\n\\u00a78\\u00a7oL. Wang\\nJ. Zhu\\nI. Kodrasi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.07547\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 2 Jun 2021 08:41:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oEUSIPCO 2021\\u00a7r"}']}
{title:'Yu et al. (§72021§r)', author: 'Jianwei Yu; Shi-Xiong Zhang; Bo Wu; Shansong Liu; Shoukang Hu; Mengzhe Geng; Xunying Liu; Helen Meng; Dong Yu', display:{Lore:['[{"text": "arXiv:2011.07755", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-visual Multi-channel Integration and Recognition of Overlapped Speech\\u00a7r\\n\\n\\u00a78\\u00a7oJianwei Yu\\nShi-Xiong Zhang\\nBo Wu\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.07755\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 30 Aug 2021 07:26:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTASLP 2021\\u00a7r"}']}
{title:'Agrawal et al. (§72021§r)', author: 'Bhuvan Agrawal; Markus Müller; Martin Radfar; Samridhi Choudhary; Athanasios Mouchtaris; Siegfried Kunzmann', display:{Lore:['[{"text": "arXiv:2011.09044", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTie Your Embeddings Down: Cross-Modal Latent Spaces for End-to-end Spoken Language Understanding\\u00a7r\\n\\n\\u00a78\\u00a7oBhuvan Agrawal\\nMarkus M\\u00fcller\\nMartin Radfar\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.09044\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 15 Apr 2021 16:38:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 6 figures\\u00a7r"}']}
{title:'Ge et al. (§72021§r)', author: 'Meng Ge; Chenglin Xu; Longbiao Wang; Eng Siong Chng; Jianwu Dang; Haizhou Li', display:{Lore:['[{"text": "arXiv:2011.09624", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-stage Speaker Extraction with Utterance and Frame-Level Reference Signals\\u00a7r\\n\\n\\u00a78\\u00a7oMeng Ge\\nChenglin Xu\\nLongbiao Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.09624\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 2 Apr 2021 08:38:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ICASSP 2021\\u00a7r"}']}
{title:'Jang et al. (§72021§r)', author: 'Won Jang; Dan Lim; Jaesam Yoon', display:{Lore:['[{"text": "arXiv:2011.09631", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUniversal MelGAN: A Robust Neural Vocoder for High-Fidelity Waveform Generation in Multiple Domains\\u00a7r\\n\\n\\u00a78\\u00a7oWon Jang\\nDan Lim\\nJaesam Yoon\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.09631\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 4 Mar 2021 02:00:12 GMT)\\u00a7r"}']}
{title:'Schwarz et al. (§72021§r)', author: 'Andreas Schwarz; Ilya Sklyar; Simon Wiesler', display:{Lore:['[{"text": "arXiv:2011.10538", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving RNN-T ASR Accuracy Using Context Audio\\u00a7r\\n\\n\\u00a78\\u00a7oAndreas Schwarz\\nIlya Sklyar\\nSimon Wiesler\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.10538\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 15 Jun 2021 15:49:49 GMT)\\u00a7r"}']}
{title:'Saddler et al. (§72021§r)', author: 'Mark R. Saddler; Andrew Francl; Jenelle Feather; Kaizhi Qian; Yang Zhang; Josh H. McDermott', display:{Lore:['[{"text": "arXiv:2011.10706", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Denoising with Auditory Models\\u00a7r\\n\\n\\u00a78\\u00a7oMark R. Saddler\\nAndrew Francl\\nJenelle Feather\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.10706\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 13 Aug 2021 01:20:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oFirst two authors contributed equally, 5 pages, 3 PDF figures\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Bo Li; Anmol Gulati; Jiahui Yu; Tara N. Sainath; Chung-Cheng Chiu; Arun Narayanan; Shuo-Yiin Chang; Ruoming Pang; Yanzhang He; James Qin; Wei Han; Qiao Liang; Yu Zhang; Trevor Strohman; Yonghui Wu', display:{Lore:['[{"text": "arXiv:2011.10798", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Better and Faster End-to-End Model for Streaming ASR\\u00a7r\\n\\n\\u00a78\\u00a7oBo Li\\nAnmol Gulati\\nJiahui Yu\\n+ 11 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.10798\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 11 Feb 2021 14:07:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ICASSP 2021\\u00a7r"}']}
{title:'Zheng et al. (§72021§r)', author: 'Xianrui Zheng; Yulan Liu; Deniz Gunceler; Daniel Willett', display:{Lore:['[{"text": "arXiv:2011.11564", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing Synthetic Audio to Improve The Recognition of Out-Of-Vocabulary Words in End-To-End ASR Systems\\u00a7r\\n\\n\\u00a78\\u00a7oXianrui Zheng\\nYulan Liu\\nDeniz Gunceler\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.11564\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 10 Feb 2021 18:24:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proc. ICASSP2021, June 06-11, 2021, Toronto, Ontario, Canada\\u00a7r"}']}
{title:'Sklyar et al. (§72021§r)', author: 'Ilya Sklyar; Anna Piunova; Yulan Liu', display:{Lore:['[{"text": "arXiv:2011.11671", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreaming Multi-speaker ASR with RNN-T\\u00a7r\\n\\n\\u00a78\\u00a7oIlya Sklyar\\nAnna Piunova\\nYulan Liu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.11671\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 19 Feb 2021 16:42:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP2021\\u00a7r"}']}
{title:'Huang et al. (§72021§r)', author: 'Tzu-hsien Huang; Jheng-hao Lin; Chien-yu Huang; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2011.12063", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHow Far Are We from Robust Voice Conversion: A Survey\\u00a7r\\n\\n\\u00a78\\u00a7oTzu-hsien Huang\\nJheng-hao Lin\\nChien-yu Huang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.12063\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 3 May 2021 08:32:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by SLT 2021\\u00a7r"}']}
{title:'Xie et al. (§72021§r)', author: 'Huang Xie; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2011.12133", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero-Shot Audio Classification via Semantic Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oHuang Xie\\nTuomas Virtanen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.12133\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 11 Feb 2021 12:19:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Transactions on Audio, Speech and Language Processing\\u00a7r"}']}
{title:'Zhao et al. (§72021§r)', author: 'Fengnian Zhao; Ruwei Li; Xin Liu; Liwen Xu', display:{Lore:['[{"text": "arXiv:2011.12564", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSoft-Median Choice: An Automatic Feature Smoothing Method for Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oFengnian Zhao\\nRuwei Li\\nXin Liu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.12564\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 22 Mar 2021 14:15:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 6 tables\\u00a7r"}']}
{title:'Xie et al. (§72021§r)', author: 'Huang Xie; Okko Räsänen; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2011.12657", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero-Shot Audio Classification with Factored Linear and Nonlinear Acoustic-Semantic Projections\\u00a7r\\n\\n\\u00a78\\u00a7oHuang Xie\\nOkko R\\u00e4s\\u00e4nen\\nTuomas Virtanen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.12657\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 2 Feb 2021 18:14:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2021\\u00a7r"}']}
{title:'Giollo et al. (§72021§r)', author: 'Manuel Giollo; Deniz Gunceler; Yulan Liu; Daniel Willett', display:{Lore:['[{"text": "arXiv:2011.12696", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBootstrap an end-to-end ASR system by multilingual training, transfer learning, text-to-text mapping and synthetic audio\\u00a7r\\n\\n\\u00a78\\u00a7oManuel Giollo\\nDeniz Gunceler\\nYulan Liu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.12696\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 18 Jun 2021 12:12:44 GMT)\\u00a7r"}']}
{title:'Sung et al. (§72021§r)', author: 'Man-Ling Sung; Tan Lee', display:{Lore:['[{"text": "arXiv:2011.14062", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Spoken Term Discovery Based on Re-clustering of Hypothesized Speech Segments with Siamese and Triplet Networks\\u00a7r\\n\\n\\u00a78\\u00a7oMan-Ling Sung\\nTan Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.14062\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 2 Jun 2021 21:06:44 GMT)\\u00a7r"}']}
{title:'Bai et al. (§72021§r)', author: 'Zhongxin Bai; Xiao-Lei Zhang', display:{Lore:['[{"text": "arXiv:2012.00931", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Recognition Based on Deep Learning: An Overview\\u00a7r\\n\\n\\u00a78\\u00a7oZhongxin Bai\\nXiao-Lei Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.00931\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 4 Apr 2021 01:59:30 GMT)\\u00a7r"}']}
{title:'Ryant et al. (§72021§r)', author: 'Neville Ryant; Prachi Singh; Venkat Krishnamohan; Rajat Varma; Kenneth Church; Christopher Cieri; Jun Du; Sriram Ganapathy; Mark Liberman', display:{Lore:['[{"text": "arXiv:2012.01477", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Third DIHARD Diarization Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oNeville Ryant\\nPrachi Singh\\nVenkat Krishnamohan\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.01477\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 5 Apr 2021 17:23:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:1906.07839\\u00a7r"}']}
{title:'Schlittenlacher et al. (§72021§r)', author: 'Josef Schlittenlacher; Thomas Baer', display:{Lore:['[{"text": "arXiv:2012.02174", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lText-to-speech for the hearing impaired\\u00a7r\\n\\n\\u00a78\\u00a7oJosef Schlittenlacher\\nThomas Baer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.02174\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 22 Mar 2021 12:33:59 GMT)\\u00a7r"}']}
{title:'Nustede et al. (§72021§r)', author: 'Eike J. Nustede; Jörn Anemüller', display:{Lore:['[{"text": "arXiv:2012.03594", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards speech enhancement using a variational U-Net architecture\\u00a7r\\n\\n\\u00a78\\u00a7oEike J. Nustede\\nJ\\u00f6rn Anem\\u00fcller\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.03594\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 3 Mar 2021 09:56:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to EUSIPCO 2021\\u00a7r"}']}
{title:'Noé et al. (§72021§r)', author: 'Paul-Gauthier Noé; Mohammad Mohammadamini; Driss Matrouf; Titouan Parcollet; Andreas Nautsch; Jean-François Bonastre', display:{Lore:['[{"text": "arXiv:2012.04454", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CR\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Disentanglement of Speaker Representation for Attribute-Driven Privacy Preservation\\u00a7r\\n\\n\\u00a78\\u00a7oPaul-Gauthier No\\u00e9\\nMohammad Mohammadamini\\nDriss Matrouf\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.04454\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 16 Jun 2021 08:40:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021\\u00a7r"}']}
{title:'Hu et al. (§72021§r)', author: 'Shoukang Hu; Xurong Xie; Shansong Liu; Jianwei Yu; Zi Ye; Mengzhe Geng; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2012.04494", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBayesian Learning of LF-MMI Trained Time Delay Neural Networks for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oShoukang Hu\\nXurong Xie\\nShansong Liu\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.04494\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 10 May 2021 06:47:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in TASLP\\u00a7r"}']}
{title:'Weng et al. (§72021§r)', author: 'Zhenzi Weng; Zhijin Qin; Geoffrey Ye Li', display:{Lore:['[{"text": "arXiv:2012.05369", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemantic Communications for Speech Signals\\u00a7r\\n\\n\\u00a78\\u00a7oZhenzi Weng\\nZhijin Qin\\nGeoffrey Ye Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.05369\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 7 Sep 2021 20:18:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages. arXiv admin note:text overlap with arXiv:2107.11190\\u00a7r"}']}
{title:'Moing et al. (§72021§r)', author: 'Guillaume Le Moing; Phongtharin Vinayavekhin; Don Joven Agravante; Tadanobu Inoue; Jayakorn Vongkulbhisal; Asim Munawar; Ryuki Tachibana', display:{Lore:['[{"text": "arXiv:2012.05533", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData-Efficient Framework for Real-world Multiple Sound Source 2D Localization\\u00a7r\\n\\n\\u00a78\\u00a7oGuillaume Le Moing\\nPhongtharin Vinayavekhin\\nDon Joven Agravante\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.05533\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 17 Mar 2021 08:50:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEICASSP 2021. This article supersedes arXiv:2012.05908\\u00a7r"}']}
{title:'Moing et al. (§72021§r)', author: 'Guillaume Le Moing; Don Joven Agravante; Tadanobu Inoue; Jayakorn Vongkulbhisal; Asim Munawar; Ryuki Tachibana; Phongtharin Vinayavekhin', display:{Lore:['[{"text": "arXiv:2012.05908", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnsemble of Discriminators for Domain Adaptation in Multiple Sound Source 2D Localization\\u00a7r\\n\\n\\u00a78\\u00a7oGuillaume Le Moing\\nDon Joven Agravante\\nTadanobu Inoue\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.05908\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 Mar 2021 08:42:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPleaserefer to final version at arXiv:2012.05533\\u00a7r"}']}
{title:'Gburrek et al. (§72021§r)', author: 'Tobias Gburrek; Joerg Schmalenstroeer; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2012.06142", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIterative Geometry Calibration from Distance Estimates for Wireless Acoustic Sensor Networks\\u00a7r\\n\\n\\u00a78\\u00a7oTobias Gburrek\\nJoerg Schmalenstroeer\\nReinhold Haeb-Umbach\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.06142\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 14 Apr 2021 11:10:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP 2021\\u00a7r"}']}
{title:'Sang et al. (§72021§r)', author: 'Mufan Sang; Wei Xia; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:2012.06896", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDEAAN: Disentangled Embedding and Adversarial Adaptation Network for Robust Speaker Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oMufan Sang\\nWei Xia\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.06896\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 22 Feb 2021 22:25:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Xia et al. (§72021§r)', author: 'Wei Xia; Chunlei Zhang; Chao Weng; Meng Yu; Dong Yu', display:{Lore:['[{"text": "arXiv:2012.07178", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervised Text-independent Speaker Verification using Prototypical Momentum Contrastive Learning\\u00a7r\\n\\n\\u00a78\\u00a7oWei Xia\\nChunlei Zhang\\nChao Weng\\nMeng Yu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.07178\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 14 Feb 2021 05:46:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP2021\\u00a7r"}']}
{title:'Luo et al. (§72021§r)', author: 'Yi Luo; Cong Han; Nima Mesgarani', display:{Lore:['[{"text": "arXiv:2012.07291", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGroup Communication with Context Codec for Lightweight Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oYi Luo\\nCong Han\\nNima Mesgarani\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.07291\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3078640\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 16 May 2021 05:11:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)\\u00a7r"}']}
{title:'Hu et al. (§72021§r)', author: 'Hu Hu; Xuesong Yang; Zeynab Raeesy; Jinxi Guo; Gokce Keskin; Harish Arsikere; Ariya Rastrow; Andreas Stolcke; Roland Maas', display:{Lore:['[{"text": "arXiv:2012.07353", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lREDAT: Accent-Invariant Representation for End-to-End ASR by Domain Adversarial Training with Relabeling\\u00a7r\\n\\n\\u00a78\\u00a7oHu Hu\\nXuesong Yang\\nZeynab Raeesy\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.07353\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 12 Feb 2021 06:44:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted in ICASSP 2021; final camera-ready version\\u00a7r"}']}
{title:'Zezario et al. (§72021§r)', author: 'Ryandhimas E. Zezario; Chiou-Shann Fuh; Hsin-Min Wang; Yu Tsao', display:{Lore:['[{"text": "arXiv:2012.09359", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Enhancement with Zero-Shot Model Selection\\u00a7r\\n\\n\\u00a78\\u00a7oRyandhimas E. Zezario\\nChiou-Shann Fuh\\nHsin-Min Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.09359\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 31 Aug 2021 08:24:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in EUSIPCO 2021\\u00a7r"}']}
{title:'Zheng et al. (§72021§r)', author: 'Chengyu Zheng; Xiulian Peng; Yuan Zhang; Sriram Srinivasan; Yan Lu', display:{Lore:['[{"text": "arXiv:2012.09408", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInteractive Speech and Noise Modeling for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oChengyu Zheng\\nXiulian Peng\\nYuan Zhang\\nSriram Srinivasan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.09408\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 14 Apr 2021 04:20:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAAAI 2021 (Accepted)\\u00a7r"}']}
{title:'Dietzen et al. (§72021§r)', author: 'Thomas Dietzen; Enzo De Sena; Toon van Waterschoot', display:{Lore:['[{"text": "arXiv:2012.09499", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Complexity Steered Response Power Mapping based on Nyquist-Shannon Sampling\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Dietzen\\nEnzo De Sena\\nToon van Waterschoot\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.09499\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/WASPAA52581.2021.9632774\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 22 Jul 2021 16:11:53 GMT)\\u00a7r"}']}
{title:'Feng et al. (§72021§r)', author: 'Siyuan Feng; Odette Scharenborg', display:{Lore:['[{"text": "arXiv:2012.09544", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe effectiveness of unsupervised subword modeling with autoregressive and cross-lingual phone-aware networks\\u00a7r\\n\\n\\u00a78\\u00a7oSiyuan Feng\\nOdette Scharenborg\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.09544\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/OJSP.2021.3076914\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 28 Apr 2021 09:50:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages (including 1 pageas supplementary material), 13 figures. Accepted for publication in IEEE OpenJournal of Signal Processing (OJ-SP)\\u00a7r"}']}
{title:'Fujita et al. (§72021§r)', author: 'Yuya Fujita; Tianzi Wang; Shinji Watanabe; Motoi Omachi', display:{Lore:['[{"text": "arXiv:2012.10128", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lToward Streaming ASR with Non-Autoregressive Insertion-based Model\\u00a7r\\n\\n\\u00a78\\u00a7oYuya Fujita\\nTianzi Wang\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.10128\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 16 Jul 2021 06:08:24 GMT)\\u00a7r"}']}
{title:'Huang et al. (§72021§r)', author: 'Huixiang Huang; Renjie Wu; Jingbiao Huang; Jucai Lin; Jun Yin', display:{Lore:['[{"text": "arXiv:2012.10732", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDCCRGAN: Deep Complex Convolution Recurrent Generator Adversarial Network for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oHuixiang Huang\\nRenjie Wu\\nJingbiao Huang\\nJucai Lin\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.10732\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 7 Mar 2021 11:00:31 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Zhuohuang Zhang; Yong Xu; Meng Yu; Shi-Xiong Zhang; Lianwu Chen; Donald S. Williamson; Dong Yu', display:{Lore:['[{"text": "arXiv:2012.13442", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-channel Multi-frame ADL-MVDR for Target Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oZhuohuang Zhang\\nYong Xu\\nMeng Yu\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.13442\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 15 Nov 2021 20:54:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE/ACM Transactionson Audio, Speech, and Language Processing (TASLP); Demos available at https://zzhang68.github.io/mcmf-adl-mvdr/\\u00a7r"}']}
{title:'Korzekwa et al. (§72021§r)', author: 'Daniel Korzekwa; Roberto Barra-Chicote; Szymon Zaporowski; Grzegorz Beringer; Jaime Lorenzo-Trueba; Alicja Serafinowicz; Jasha Droppo; Thomas Drugman; Bozena Kostek', display:{Lore:['[{"text": "arXiv:2012.14788", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetection of Lexical Stress Errors in Non-Native (L2) English with Data Augmentation and Attention\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Korzekwa\\nRoberto Barra-Chicote\\nSzymon Zaporowski\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.14788\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 7 Jun 2021 15:39:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021\\u00a7r"}']}
{title:'Lerch (§72021§r)', author: 'Alexander Lerch', display:{Lore:['[{"text": "arXiv:2101.00132", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Content Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oAlexander Lerch\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.00132\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Jan 2021 01:22:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint for a book chapter introducing Audio Content Analysis\\u00a7r"}']}
{title:'Jia et al. (§72021§r)', author: 'Yan Jia; Xingming Wang; Xiaoyi Qin; Yinping Zhang; Xuyang Wang; Junjie Wang; Ming Li', display:{Lore:['[{"text": "arXiv:2101.01935", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe 2020 Personalized Voice Trigger Challenge: Open Database, Evaluation Metrics and the Baseline Systems\\u00a7r\\n\\n\\u00a78\\u00a7oYan Jia\\nXingming Wang\\nXiaoyi Qin\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.01935\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Jan 2021 09:26:21 GMT)\\u00a7r"}']}
{title:'Peng et al. (§72021§r)', author: 'Chiang-Jen Peng; Yun-Ju Chan; Cheng Yu; Syu-Siang Wang; Yu Tsao; Tai-Shih Chi', display:{Lore:['[{"text": "arXiv:2101.02550", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention-based multi-task learning for speech-enhancement and speaker-identification in multi-speaker dialogue scenario\\u00a7r\\n\\n\\u00a78\\u00a7oChiang-Jen Peng\\nYun-Ju Chan\\nCheng Yu\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.02550\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE International Symposium on Circuits and Systems 2021\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 21 Feb 2021 22:47:59 GMT)\\u00a7r"}']}
{title:'Lu et al. (§72021§r)', author: 'Xugang Lu; Peng Shen; Yu Tsao; Hisashi Kawai', display:{Lore:['[{"text": "arXiv:2101.03329", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCoupling a generative model with a discriminative learning framework for speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oXugang Lu\\nPeng Shen\\nYu Tsao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.03329\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 24 Nov 2021 13:02:10 GMT)\\u00a7r"}']}
{title:'Ochiai et al. (§72021§r)', author: 'Tsubasa Ochiai; Marc Delcroix; Tomohiro Nakatani; Rintaro Ikeshita; Keisuke Kinoshita; Shoko Araki', display:{Lore:['[{"text": "arXiv:2101.04315", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Network-based Virtual Microphone Estimator\\u00a7r\\n\\n\\u00a78\\u00a7oTsubasa Ochiai\\nMarc Delcroix\\nTomohiro Nakatani\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.04315\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Jan 2021 06:30:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, submitted to ICASSP 2021\\u00a7r"}']}
{title:'Vygon et al. (§72021§r)', author: 'Roman Vygon; Nikolay Mikhaylovskiy', display:{Lore:['[{"text": "arXiv:2101.04792", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Efficient Representations for Keyword Spotting with Triplet Loss\\u00a7r\\n\\n\\u00a78\\u00a7oRoman Vygon\\nNikolay Mikhaylovskiy\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.04792\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-030-87802-3_69\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIn: Karpov A., Potapova R. (eds) Speech and Computer. SPECOM 2021.\\n  Lecture Notes in Computer Science, vol 12997. Springer, Cham\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 4 Jun 2021 22:20:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to SPECOM 2021\\u00a7r"}']}
{title:'Lam et al. (§72021§r)', author: 'Max W. Y. Lam; Jun Wang; Dan Su; Dong Yu', display:{Lore:['[{"text": "arXiv:2101.05014", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEffective Low-Cost Time-Domain Audio Separation Using Globally Attentive Locally Recurrent Networks\\u00a7r\\n\\n\\u00a78\\u00a7oMax W. Y. Lam\\nJun Wang\\nDan Su\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.05014\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Jan 2021 11:30:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in IEEE SLT 2021\\u00a7r"}']}
{title:'Hu et al. (§72021§r)', author: 'Qiong Hu; Tobias Bleisch; Petko Petkov; Tuomo Raitio; Erik Marchi; Varun Lakshminarasimhan', display:{Lore:['[{"text": "arXiv:2101.05313", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhispered and Lombard Neural Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oQiong Hu\\nTobias Bleisch\\nPetko Petkov\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.05313\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Jan 2021 19:22:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in SLT 2021\\u00a7r"}']}
{title:'Delcroix et al. (§72021§r)', author: 'Marc Delcroix; Katerina Zmolikova; Tsubasa Ochiai; Keisuke Kinoshita; Tomohiro Nakatani', display:{Lore:['[{"text": "arXiv:2101.05516", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker activity driven neural speech extraction\\u00a7r\\n\\n\\u00a78\\u00a7oMarc Delcroix\\nKaterina Zmolikova\\nTsubasa Ochiai\\nKeisuke Kinoshita\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.05516\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 Feb 2021 23:33:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP 2021\\u00a7r"}']}
{title:'Oneata et al. (§72021§r)', author: 'Dan Oneata; Alexandru Caranica; Adriana Stan; Horia Cucu', display:{Lore:['[{"text": "arXiv:2101.05525", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn evaluation of word-level confidence estimation for end-to-end automatic speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDan Oneata\\nAlexandru Caranica\\nAdriana Stan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.05525\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Jan 2021 09:51:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at SLT 2021\\u00a7r"}']}
{title:'Oh et al. (§72021§r)', author: 'Yoo Rhee Oh; Kiyoung Park; Jeon Gyu Park', display:{Lore:['[{"text": "arXiv:2101.05600", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast offline Transformer-based end-to-end automatic speech recognition for real-world applications\\u00a7r\\n\\n\\u00a78\\u00a7oYoo Rhee Oh\\nKiyoung Park\\nJeon Gyu Park\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.05600\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.4218/etrij.2021-0106\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 11 Sep 2021 06:10:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages\\u00a7r"}']}
{title:'Schnell et al. (§72021§r)', author: 'Bastian Schnell; Goeric Huybrechts; Bartek Perz; Thomas Drugman; Jaime Lorenzo-Trueba', display:{Lore:['[{"text": "arXiv:2101.05695", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmoCat: Language-agnostic Emotional Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oBastian Schnell\\nGoeric Huybrechts\\nBartek Perz\\nThomas Drugman\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.05695\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Jan 2021 16:18:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEEICASSP 2021\\u00a7r"}']}
{title:'Arango et al. (§72021§r)', author: 'Javi Arango; Alec DeCaprio; Sunwoo Baik; Luca De Nardis; Stefanie Shattuck-Hufnagel; Maria Gabriella Di Benedetto', display:{Lore:['[{"text": "arXiv:2101.06147", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEstimation of the Frequency of Occurrence of Italian Phonemes in Text\\u00a7r\\n\\n\\u00a78\\u00a7oJavi Arango\\nAlec DeCaprio\\nSunwoo Baik\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.06147\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 18 Jan 2021 11:54:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to Speech Communication\\u00a7r"}']}
{title:'Korzekwa et al. (§72021§r)', author: 'Daniel Korzekwa; Jaime Lorenzo-Trueba; Szymon Zaporowski; Shira Calamaro; Thomas Drugman; Bozena Kostek', display:{Lore:['[{"text": "arXiv:2101.06396", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMispronunciation Detection in Non-native (L2) English with Uncertainty Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Korzekwa\\nJaime Lorenzo-Trueba\\nSzymon Zaporowski\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.06396\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Feb 2021 20:16:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Clayton et al. (§72021§r)', author: 'Michael Clayton; Lin Wang; Andrew McPherson; Andrea Cavallaro', display:{Lore:['[{"text": "arXiv:2101.06795", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn embedded multichannel sound acquisition system for drone audition\\u00a7r\\n\\n\\u00a78\\u00a7oMichael Clayton\\nLin Wang\\nAndrew McPherson\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.06795\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 17 Jan 2021 22:42:36 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Yuekai Zhang; Sining Sun; Long Ma', display:{Lore:['[{"text": "arXiv:2101.06856", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTiny Transducer: A Highly-efficient Speech Recognition Model on Edge Devices\\u00a7r\\n\\n\\u00a78\\u00a7oYuekai Zhang\\nSining Sun\\nLong Ma\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.06856\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 7 Feb 2021 06:11:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2021\\u00a7r"}']}
{title:'Song et al. (§72021§r)', author: 'Eunwoo Song; Ryuichi Yamamoto; Min-Jae Hwang; Jin-Seob Kim; Ohsung Kwon; Jae-Min Kim', display:{Lore:['[{"text": "arXiv:2101.07412", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved parallel WaveGAN vocoder with perceptually weighted spectrogram loss\\u00a7r\\n\\n\\u00a78\\u00a7oEunwoo Song\\nRyuichi Yamamoto\\nMin-Jae Hwang\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.07412\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Jan 2021 01:59:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in SLT 2021\\u00a7r"}']}
{title:'Hussein et al. (§72021§r)', author: 'Amir Hussein; Shinji Watanabe; Ahmed Ali', display:{Lore:['[{"text": "arXiv:2101.08454", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lArabic Speech Recognition by End-to-End, Modular Systems and Human\\u00a7r\\n\\n\\u00a78\\u00a7oAmir Hussein\\nShinji Watanabe\\nAhmed Ali\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.08454\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 29 Jun 2021 16:10:57 GMT)\\u00a7r"}']}
{title:'Champion et al. (§72021§r)', author: 'Pierre Champion; Denis Jouvet; Anthony Larcher', display:{Lore:['[{"text": "arXiv:2101.08478", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Study of F0 Modification for X-Vector Based Speech Pseudonymization Across Gender\\u00a7r\\n\\n\\u00a78\\u00a7oPierre Champion\\nDenis Jouvet\\nAnthony Larcher\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.08478\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nThe Second AAAI Workshop on Privacy-Preserving Artificial\\n  Intelligence, Feb 2021, Nancy, France\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Jan 2021 07:33:07 GMT)\\u00a7r"}']}
{title:'Fujimura et al. (§72021§r)', author: 'Takuya Fujimura; Yuma Koizumi; Kohei Yatabe; Ryoichi Miyazaki', display:{Lore:['[{"text": "arXiv:2101.08625", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoisy-target Training: A Training Strategy for DNN-based Speech Enhancement without Clean Speech\\u00a7r\\n\\n\\u00a78\\u00a7oTakuya Fujimura\\nYuma Koizumi\\nKohei Yatabe\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.08625\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 10 May 2021 05:56:09 GMT)\\u00a7r"}']}
{title:'Joshi et al. (§72021§r)', author: 'Sonal Joshi; Jesús Villalba; Piotr Żelasko; Laureano Moro-Velázquez; Najim Dehak', display:{Lore:['[{"text": "arXiv:2101.08909", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStudy of Pre-processing Defenses against Adversarial Attacks on State-of-the-art Speaker Recognition Systems\\u00a7r\\n\\n\\u00a78\\u00a7oSonal Joshi\\nJes\\u00fas Villalba\\nPiotr \\u017belasko\\nLaureano Moro-Vel\\u00e1zquez\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.08909\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 25 Jun 2021 15:37:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\\u00a7r"}']}
{title:'Wu et al. (§72021§r)', author: 'Peter Wu; Paul Pu Liang; Jiatong Shi; Ruslan Salakhutdinov; Shinji Watanabe; Louis-Philippe Morency', display:{Lore:['[{"text": "arXiv:2101.08919", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnderstanding the Tradeoffs in Client-side Privacy for Downstream Speech Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Wu\\nPaul Pu Liang\\nJiatong Shi\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.08919\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 22 Oct 2021 22:15:42 GMT)\\u00a7r"}']}
{title:'Braun et al. (§72021§r)', author: 'Sebastian Braun; Hannes Gamper; Chandan K. A. Reddy; Ivan Tashev', display:{Lore:['[{"text": "arXiv:2101.09249", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards efficient models for real-time deep noise suppression\\u00a7r\\n\\n\\u00a78\\u00a7oSebastian Braun\\nHannes Gamper\\nChandan K. A. Reddy\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.09249\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 19 May 2021 12:31:27 GMT)\\u00a7r"}']}
{title:'Park et al. (§72021§r)', author: 'Tae Jin Park; Naoyuki Kanda; Dimitrios Dimitriadis; Kyu J. Han; Shinji Watanabe; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2101.09624", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Review of Speaker Diarization: Recent Advances with Deep Learning\\u00a7r\\n\\n\\u00a78\\u00a7oTae Jin Park\\nNaoyuki Kanda\\nDimitrios Dimitriadis\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.09624\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 26 Nov 2021 06:54:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis article is a preprint version of the article published in Computer Speech Language, Volume 72, March 2022, 101317\\u00a7r"}']}
{title:'Shechtman et al. (§72021§r)', author: 'Slava Shechtman; Raul Fernandez; David Haws', display:{Lore:['[{"text": "arXiv:2101.09940", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSupervised and Unsupervised Approaches for Controlling Narrow Lexical Focus in Sequence-to-Sequence Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oSlava Shechtman\\nRaul Fernandez\\nDavid Haws\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.09940\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/SLT48900.2021.9383591\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Jan 2021 08:02:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE Spoken Language Technology Workshop (SLT), 2021\\u00a7r"}']}
{title:'Shi et al. (§72021§r)', author: 'Jiatong Shi; Jonathan D. Amith; Rey Castillo García; Esteban Guadalupe Sierra; Kevin Duh; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2101.10877", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging End-to-End ASR for Endangered Language Documentation: An Empirical Study on Yolox\\u00f3chitl Mixtec\\u00a7r\\n\\n\\u00a78\\u00a7oJiatong Shi\\nJonathan D. Amith\\nRey Castillo Garc\\u00eda\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.10877\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 5 Mar 2021 06:47:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by EACL2021\\u00a7r"}']}
{title:'Lei et al. (§72021§r)', author: 'Jie Lei; Tousif Rahman; Rishad Shafik; Adrian Wheeldon; Alex Yakovlev; Ole-Christoffer Granmo; Fahim Kawsar; Akhil Mathur', display:{Lore:['[{"text": "arXiv:2101.11336", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Power Audio Keyword Spotting using Tsetlin Machines\\u00a7r\\n\\n\\u00a78\\u00a7oJie Lei\\nTousif Rahman\\nRishad Shafik\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.11336\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPre-print of original submission to Journal of Low Power\\n  Electronics and Applications, 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Jan 2021 11:57:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o20 pp\\u00a7r"}']}
{title:'Jang et al. (§72021§r)', author: 'Minsu Jang; Sangwon Seo; Dohyung Kim; Jaeyeon Lee; Jaehong Kim; Jun-Hwan Ahn', display:{Lore:['[{"text": "arXiv:2101.11469", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVOTE400(Voide Of The Elderly 400 Hours): A Speech Dataset to Study Voice Interface for Elderly-Care\\u00a7r\\n\\n\\u00a78\\u00a7oMinsu Jang\\nSangwon Seo\\nDohyung Kim\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.11469\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Jan 2021 05:28:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o3 pages, 7 tables\\u00a7r"}']}
{title:'Kocour et al. (§72021§r)', author: "Martin Kocour; Guillermo Cámbara; Jordi Luque; David Bonet; Mireia Farrús; Martin Karafiát; Karel Veselý; Jan ''Honza'' Ĉernocký", display:{Lore:['[{"text": "arXiv:2101.12729", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBCN2BRNO: ASR System Fusion for Albayzin 2020 Speech to Text Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oMartin Kocour\\nGuillermo C\\u00e1mbara\\nJordi Luque\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.12729\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Jan 2021 18:40:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ofusion,end-to-end model, hybrid model, semisupervised, automatic speech recognition, convolutional neural network\\u00a7r"}']}
{title:'Bonet et al. (§72021§r)', author: 'David Bonet; Guillermo Cámbara; Fernando López; Pablo Gómez; Carlos Segura; Jordi Luque', display:{Lore:['[{"text": "arXiv:2101.12732", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Enhancement for Wake-Up-Word detection in Voice Assistants\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Bonet\\nGuillermo C\\u00e1mbara\\nFernando L\\u00f3pez\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.12732\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Jan 2021 18:44:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7okeyword spotting, speech enhancement, wake-up-word, deep learning, convolutional neural network\\u00a7r"}']}
{title:'Li (§72021§r)', author: 'Xiaofei Li', display:{Lore:['[{"text": "arXiv:2102.00154", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-supervised Sound Event Detection using Random Augmentation and Consistency Regularization\\u00a7r\\n\\n\\u00a78\\u00a7oXiaofei Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.00154\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 30 Jan 2021 05:22:13 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Jie Wang; Jingbei Li; Xintao Zhao; Zhiyong Wu; Shiyin Kang; Helen Meng', display:{Lore:['[{"text": "arXiv:2102.00184", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarially learning disentangled speech representations for robust multi-factor voice conversion\\u00a7r\\n\\n\\u00a78\\u00a7oJie Wang\\nJingbei Li\\nXintao Zhao\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.00184\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 20 Aug 2021 07:20:00 GMT)\\u00a7r"}']}
{title:'Watcharasupat et al. (§72021§r)', author: 'Karn Watcharasupat; Anh H. T. Nguyen; Ching-Hui Ooi; Andy W. H. Khong', display:{Lore:['[{"text": "arXiv:2102.00196", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDirectional Sparse Filtering using Weighted Lehmer Mean for Blind Separation of Unbalanced Speech Mixtures\\u00a7r\\n\\n\\u00a78\\u00a7oKarn Watcharasupat\\nAnh H. T. Nguyen\\nChing-Hui Ooi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.00196\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9414336\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 2021 IEEE International Conference on\\n  Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 4485-4489\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 14 May 2021 15:34:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o(c) 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or "}','{"text": "promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Sudro et al. (§72021§r)', author: 'Protima Nomo Sudro; Rohan Kumar Das; Rohit Sinha; S R Mahadeva Prasanna', display:{Lore:['[{"text": "arXiv:2102.00270", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing the Intelligibility of Cleft Lip and Palate Speech using Cycle-consistent Adversarial Networks\\u00a7r\\n\\n\\u00a78\\u00a7oProtima Nomo Sudro\\nRohan Kumar Das\\nRohit Sinha\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.00270\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 30 Jan 2021 16:49:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 4 figures, IEEE spoken language and technology workshop\\u00a7r"}']}
{title:'N et al. (§72021§r)', author: 'Krishna D N; Ankita Patil', display:{Lore:['[{"text": "arXiv:2102.00306", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Language Identification using Multi-Head Self-Attention and 1D Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oKrishna D N\\nAnkita Patil\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.00306\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 30 Jan 2021 20:32:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure\\u00a7r"}']}
{title:'Sundararaman et al. (§72021§r)', author: 'Mukuntha Narayanan Sundararaman; Ayush Kumar; Jithendra Vepa', display:{Lore:['[{"text": "arXiv:2102.00804", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhoneme-BERT: Joint Language Modelling of Phoneme Sequence and ASR Transcript\\u00a7r\\n\\n\\u00a78\\u00a7oMukuntha Narayanan Sundararaman\\nAyush Kumar\\nJithendra Vepa\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.00804\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 15 Jun 2021 18:19:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021 conference\\u00a7r"}']}
{title:'Borgholt et al. (§72021§r)', author: 'Lasse Borgholt; Tycho Max Sylvester Tax; Jakob Drachmann Havtorn; Lars Maaløe; Christian Igel', display:{Lore:['[{"text": "arXiv:2102.00850", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Scaling Contrastive Representations for Low-Resource Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oLasse Borgholt\\nTycho Max Sylvester Tax\\nJakob Drachmann Havtorn\\nLars Maal\\u00f8e\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.00850\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Feb 2021 13:58:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or "}','{"text": "promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Jiao et al. (§72021§r)', author: 'Yunlong Jiao; Adam Gabrys; Georgi Tinchev; Bartosz Putrycz; Daniel Korzekwa; Viacheslav Klimkov', display:{Lore:['[{"text": "arXiv:2102.01106", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUniversal Neural Vocoding with Parallel WaveNet\\u00a7r\\n\\n\\u00a78\\u00a7oYunlong Jiao\\nAdam Gabrys\\nGeorgi Tinchev\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.01106\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 15 Feb 2021 16:18:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures. Accepted to ICASSP 2021\\u00a7r"}']}
{title:'Sato et al. (§72021§r)', author: 'Hiroshi Sato; Tsubasa Ochiai; Keisuke Kinoshita; Marc Delcroix; Tomohiro Nakatani; Shoko Araki', display:{Lore:['[{"text": "arXiv:2102.01326", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultimodal Attention Fusion for Target Speaker Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oHiroshi Sato\\nTsubasa Ochiai\\nKeisuke Kinoshita\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.01326\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nin IEEE Spoken Language Technology Workshop (SLT), 2021, pp.\\n  778-784\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Feb 2021 05:59:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 5 figures\\u00a7r"}']}
{title:'Horiguchi et al. (§72021§r)', author: 'Shota Horiguchi; Nelson Yalta; Paola Garcia; Yuki Takashima; Yawen Xue; Desh Raj; Zili Huang; Yusuke Fujita; Shinji Watanabe; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:2102.01363", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Hitachi-JHU DIHARD III System: Competitive End-to-End Neural Diarization and X-Vector Clustering Systems Combined by DOVER-Lap\\u00a7r\\n\\n\\u00a78\\u00a7oShota Horiguchi\\nNelson Yalta\\nPaola Garcia\\n+ 6 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.01363\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Feb 2021 07:30:44 GMT)\\u00a7r"}']}
{title:'Meng et al. (§72021§r)', author: 'Zhong Meng; Naoyuki Kanda; Yashesh Gaur; Sarangarajan Parthasarathy; Eric Sun; Liang Lu; Xie Chen; Jinyu Li; Yifan Gong', display:{Lore:['[{"text": "arXiv:2102.01380", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInternal Language Model Training for Domain-Adaptive End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhong Meng\\nNaoyuki Kanda\\nYashesh Gaur\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.01380\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2021 IEEE International Conference on Acoustics, Speech and Signal\\n  Processing (ICASSP), Toronto, Canada\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 22 Apr 2021 19:16:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, ICASSP 2021\\u00a7r"}']}
{title:'Kuruvila et al. (§72021§r)', author: 'Ivine Kuruvila; Kubilay Can Demir; Eghart Fischer; Ulrich Hoppe', display:{Lore:['[{"text": "arXiv:2102.01746", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInference of the Selective Auditory Attention using Sequential LMMSE Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oIvine Kuruvila\\nKubilay Can Demir\\nEghart Fischer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.01746\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Feb 2021 20:39:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 13 figures\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Helin Wang; Yuexian Zou; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2102.01931", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Global-local Attention Framework for Weakly Labelled Audio Tagging\\u00a7r\\n\\n\\u00a78\\u00a7oHelin Wang\\nYuexian Zou\\nWenwu Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.01931\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Feb 2021 08:13:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP2021\\u00a7r"}']}
{title:'Ji et al. (§72021§r)', author: 'Chunyan Ji; Ming Chen; Bin Li; Yi Pan', display:{Lore:['[{"text": "arXiv:2102.02909", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInfant Cry Classification with Graph Convolutional Networks\\u00a7r\\n\\n\\u00a78\\u00a7oChunyan Ji\\nMing Chen\\nBin Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.02909\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 31 Jan 2021 19:23:19 GMT)\\u00a7r"}']}
{title:'Nelus et al. (§72021§r)', author: 'Alexandru Nelus; Rene Glitza; Rainer Martin', display:{Lore:['[{"text": "arXiv:2102.03109", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEstimation of Microphone Clusters in Acoustic Sensor Networks using Unsupervised Federated Learning\\u00a7r\\n\\n\\u00a78\\u00a7oAlexandru Nelus\\nRene Glitza\\nRainer Martin\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.03109\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 15 Feb 2021 19:55:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2021\\u00a7r"}']}
{title:'Di Benedetto et al. (§72021§r)', author: 'Maria Gabriella Di Benedetto; Stefanie Shattuck-Hufnagel; Luca De Nardis; Sara Budoni; Javier Arango; Ian Chan; Alec DeCaprio', display:{Lore:['[{"text": "arXiv:2102.03166", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLexical and syntactic gemination in Italian consonants \\u2013 Does a geminate Italian consonant consist of a repeated or a strengthened consonant?\\u00a7r\\n\\n\\u00a78\\u00a7oMaria Gabriella Di Benedetto\\nStefanie Shattuck-Hufnagel\\nLuca De Nardis\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.03166\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0004987\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 Mar 2021 09:26:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder revision at The Journalof the Acoustical Society of America\\u00a7r"}']}
{title:'Lee et al. (§72021§r)', author: 'Jaesong Lee; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2102.03216", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntermediate Loss Regularization for CTC-based Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJaesong Lee\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.03216\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Feb 2021 15:01:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2021\\u00a7r"}']}
{title:'Ick et al. (§72021§r)', author: 'Christopher Ick; Brian McFee', display:{Lore:['[{"text": "arXiv:2102.03468", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Event Detection in Urban Audio With Single and Multi-Rate PCEN\\u00a7r\\n\\n\\u00a78\\u00a7oChristopher Ick\\nBrian McFee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.03468\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 6 Feb 2021 01:23:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 1 table, accepted for publication in IEEE ICASSP 2021\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Jixuan Wang; Xiong Xiao; Jian Wu; Ranjani Ramamurthy; Frank Rudzicz; Michael Brudno', display:{Lore:['[{"text": "arXiv:2102.03634", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker attribution with voice profiles by graph-based semi-supervised learning\\u00a7r\\n\\n\\u00a78\\u00a7oJixuan Wang\\nXiong Xiao\\nJian Wu\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.03634\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2020-1950\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 6 Feb 2021 18:35:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2020\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Weiqing Wang; Qingjian Lin; Danwei Cai; Lin Yang; Ming Li', display:{Lore:['[{"text": "arXiv:2102.03649", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe DKU-Duke-Lenovo System Description for the Third DIHARD Speech Diarization Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oWeiqing Wang\\nQingjian Lin\\nDanwei Cai\\nLin Yang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.03649\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 6 Feb 2021 19:41:42 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Jisi Zhang; Catalin Zorila; Rama Doddipatla; Jon Barker', display:{Lore:['[{"text": "arXiv:2102.03762", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTime-Domain Speech Extraction with Spatial Information and Multi Speaker Conditioning Mechanism\\u00a7r\\n\\n\\u00a78\\u00a7oJisi Zhang\\nCatalin Zorila\\nRama Doddipatla\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.03762\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9414092\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 7 Feb 2021 10:11:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP 2021\\u00a7r"}']}
{title:'Chen et al. (§72021§r)', author: 'Yu-Wen Chen; Kuo-Hsuan Hung; Shang-Yi Chuang; Jonathan Sherman; Wen-Chin Huang; Xugang Lu; Yu Tsao', display:{Lore:['[{"text": "arXiv:2102.03786", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEMA2S: An End-to-End Multimodal Articulatory-to-Speech System\\u00a7r\\n\\n\\u00a78\\u00a7oYu-Wen Chen\\nKuo-Hsuan Hung\\nShang-Yi Chuang\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.03786\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 9 Jun 2021 05:40:18 GMT)\\u00a7r"}']}
{title:'Chang et al. (§72021§r)', author: 'Feng-Ju Chang; Martin Radfar; Athanasios Mouchtaris; Brian King; Siegfried Kunzmann', display:{Lore:['[{"text": "arXiv:2102.03951", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Multi-Channel Transformer for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oFeng-Ju Chang\\nMartin Radfar\\nAthanasios Mouchtaris\\nBrian King\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.03951\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Feb 2021 00:12:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by 2021 IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP 2021)\\u00a7r"}']}
{title:'Singh et al. (§72021§r)', author: 'Premjeet Singh; Goutam Saha; Md Sahidullah', display:{Lore:['[{"text": "arXiv:2102.04029", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-linear frequency warping using constant-Q transformation for speech emotion recognition\\u00a7r\\n\\n\\u00a78\\u00a7oPremjeet Singh\\nGoutam Saha\\nMd Sahidullah\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.04029\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Feb 2021 06:57:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in 2021 IEEEInternational Conference on Computer Communication and Informatics (IEEE ICCCI2021)\\u00a7r"}']}
{title:'Sadeghi et al. (§72021§r)', author: 'Mostafa Sadeghi; Xavier Alameda-Pineda', display:{Lore:['[{"text": "arXiv:2102.04144", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSwitching Variational Auto-Encoders for Noise-Agnostic Audio-visual Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oMostafa Sadeghi\\nXavier Alameda-Pineda\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.04144\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Feb 2021 11:45:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Qinglong Li; Fei Gao; Haixin Guan; Kaichi Ma', display:{Lore:['[{"text": "arXiv:2102.04629", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-time Monaural Speech Enhancement With Short-time Discrete Cosine Transform\\u00a7r\\n\\n\\u00a78\\u00a7oQinglong Li\\nFei Gao\\nHaixin Guan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.04629\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Feb 2021 03:34:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, Journal submitted\\u00a7r"}']}
{title:'Ikeshita et al. (§72021§r)', author: 'Rintaro Ikeshita; Tomohiro Nakatani', display:{Lore:['[{"text": "arXiv:2102.04696", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIndependent Vector Extraction for Fast Joint Blind Source Separation and Dereverberation\\u00a7r\\n\\n\\u00a78\\u00a7oRintaro Ikeshita\\nTomohiro Nakatani\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.04696\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2021.3074321\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 22 Apr 2021 02:01:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEESignal Processing Letters\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Shucong Zhang; Cong-Thanh Do; Rama Doddipatla; Erfan Loweimi; Peter Bell; Steve Renals', display:{Lore:['[{"text": "arXiv:2102.04697", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTrain your classifier first: Cascade Neural Networks Training from upper layers to lower layers\\u00a7r\\n\\n\\u00a78\\u00a7oShucong Zhang\\nCong-Thanh Do\\nRama Doddipatla\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.04697\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Feb 2021 08:19:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2021\\u00a7r"}']}
{title:'Manocha et al. (§72021§r)', author: 'Pranay Manocha; Zeyu Jin; Richard Zhang; Adam Finkelstein', display:{Lore:['[{"text": "arXiv:2102.05109", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCDPAM: Contrastive learning for perceptual audio similarity\\u00a7r\\n\\n\\u00a78\\u00a7oPranay Manocha\\nZeyu Jin\\nRichard Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.05109\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Feb 2021 20:15:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDataset, code and sound examples can be found at https://github.com/pranaymanocha/PerceptualAudio/tree/master/cdpam\\u00a7r"}']}
{title:'Valin et al. (§72021§r)', author: 'Jean-Marc Valin; Srikanth Tenneti; Karim Helwani; Umut Isik; Arvindh Krishnaswamy', display:{Lore:['[{"text": "arXiv:2102.05245", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Complexity, Real-Time Joint Neural Echo Control and Speech Enhancement Based On PercepNet\\u00a7r\\n\\n\\u00a78\\u00a7oJean-Marc Valin\\nSrikanth Tenneti\\nKarim Helwani\\nUmut Isik\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.05245\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Feb 2021 03:44:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP 2021, 5 pages\\u00a7r"}']}
{title:'Yang et al. (§72021§r)', author: 'Joon-Young Yang; Joon-Hyuk Chang', display:{Lore:['[{"text": "arXiv:2102.05259", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVACE-WPE: Virtual Acoustic Channel Expansion Based On Neural Networks for Weighted Prediction Error-Based Speech Dereverberation\\u00a7r\\n\\n\\u00a78\\u00a7oJoon-Young Yang\\nJoon-Hyuk Chang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.05259\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Feb 2021 04:40:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 12 figures, 10 tables\\u00a7r"}']}
{title:'Nautsch et al. (§72021§r)', author: 'Andreas Nautsch; Xin Wang; Nicholas Evans; Tomi Kinnunen; Ville Vestman; Massimiliano Todisco; Héctor Delgado; Md Sahidullah; Junichi Yamagishi; Kong Aik Lee', display:{Lore:['[{"text": "arXiv:2102.05889", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lASVspoof 2019: spoofing countermeasures for the detection of synthesized, converted and replayed speech\\u00a7r\\n\\n\\u00a78\\u00a7oAndreas Nautsch\\nXin Wang\\nNicholas Evans\\n+ 6 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.05889\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TBIOM.2021.3059479\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Transactions on Biometrics, Behavior, and Identity Science\\n  2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Feb 2021 08:41:42 GMT)\\u00a7r"}']}
{title:'Prasad et al. (§72021§r)', author: 'Archiki Prasad; Preethi Jyothi; Rajbabu Velmurugan', display:{Lore:['[{"text": "arXiv:2102.06237", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Investigation of End-to-End Models for Robust Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oArchiki Prasad\\nPreethi Jyothi\\nRajbabu Velmurugan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.06237\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Feb 2021 19:47:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to appear at ICASSP 2021\\u00a7r"}']}
{title:'Singh et al. (§72021§r)', author: 'Satwinder Singh; Ruili Wang; Yuanhang Qiu', display:{Lore:['[{"text": "arXiv:2102.06306", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDEEPF0: End-To-End Fundamental Frequency Estimation for Music and Speech Signals\\u00a7r\\n\\n\\u00a78\\u00a7oSatwinder Singh\\nRuili Wang\\nYuanhang Qiu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.06306\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Feb 2021 23:11:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ICASSP 2021\\u00a7r"}']}
{title:'Nakashima et al. (§72021§r)', author: 'Taishi Nakashima; Robin Scheibler; Masahito Togami; Nobutaka Ono', display:{Lore:['[{"text": "arXiv:2102.06322", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Dereverberation and Separation with Iterative Source Steering\\u00a7r\\n\\n\\u00a78\\u00a7oTaishi Nakashima\\nRobin Scheibler\\nMasahito Togami\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.06322\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9413478\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 31 May 2021 14:04:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, accepted at ICASSP 2021\\u00a7r"}']}
{title:'Das et al. (§72021§r)', author: 'Rohan Kumar Das; Jichen Yang; Haizhou Li', display:{Lore:['[{"text": "arXiv:2102.06332", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData Augmentation with Signal Companding for Detection of Logical Access Attacks\\u00a7r\\n\\n\\u00a78\\u00a7oRohan Kumar Das\\nJichen Yang\\nHaizhou Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.06332\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Feb 2021 02:51:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, Accepted for publication in International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2021\\u00a7r"}']}
{title:'Carbajal et al. (§72021§r)', author: 'Guillaume Carbajal; Julius Richter; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2102.06454", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGuided Variational Autoencoder for Speech Enhancement With a Supervised Classifier\\u00a7r\\n\\n\\u00a78\\u00a7oGuillaume Carbajal\\nJulius Richter\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.06454\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9414363\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2021 - 2021 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Feb 2021 11:32:48 GMT)\\u00a7r"}']}
{title:'Casebeer et al. (§72021§r)', author: 'Jonah Casebeer; Vinjai Vale; Umut Isik; Jean-Marc Valin; Ritwik Giri; Arvindh Krishnaswamy', display:{Lore:['[{"text": "arXiv:2102.06610", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing into the codec: Noise Robust Speech Coding with Vector-Quantized Autoencoders\\u00a7r\\n\\n\\u00a78\\u00a7oJonah Casebeer\\nVinjai Vale\\nUmut Isik\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.06610\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Feb 2021 16:42:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, ICASSP 2021\\u00a7r"}']}
{title:'Viana-Cámara et al. (§72021§r)', author: 'Rafael Viana-Cámara; Mario Campos-Soberanis; Diego Campos-Sobrino', display:{Lore:['[{"text": "arXiv:2102.06744", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHybrid phonetic-neural model for correction in speech recognition systems\\u00a7r\\n\\n\\u00a78\\u00a7oRafael Viana-C\\u00e1mara\\nMario Campos-Soberanis\\nDiego Campos-Sobrino\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.06744\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Feb 2021 19:57:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 3 figures, presented in COMIA 2020 (http://smia.mx/comia/2020/)\\u00a7r"}']}
{title:'Fan et al. (§72021§r)', author: 'Ruchao Fan; Amber Afshan; Abeer Alwan', display:{Lore:['[{"text": "arXiv:2102.06816", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBi-APC: Bidirectional Autoregressive Predictive Coding for Unsupervised Pre-training and Its Application to Children\'s ASR\\u00a7r\\n\\n\\u00a78\\u00a7oRuchao Fan\\nAmber Afshan\\nAbeer Alwan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.06816\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Feb 2021 23:30:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP2021\\u00a7r"}']}
{title:'Wu et al. (§72021§r)', author: 'Haibin Wu; Xu Li; Andy T. Liu; Zhiyong Wu; Helen Meng; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2102.07047", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial defense for automatic speaker verification by cascaded self-supervised learning models\\u00a7r\\n\\n\\u00a78\\u00a7oHaibin Wu\\nXu Li\\nAndy T. Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.07047\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 14 Feb 2021 01:56:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'S et al. (§72021§r)', author: 'Yashish Maduwantha H. P. E. R. S; Chris Kitchen; Deanna L. Kelly; Carol Espy-Wilson', display:{Lore:['[{"text": "arXiv:2102.07054", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInverted Vocal Tract Variables and Facial Action Units to Quantify Neuromotor Coordination in Schizophrenia\\u00a7r\\n\\n\\u00a78\\u00a7oYashish Maduwantha H. P. E. R. S\\nChris Kitchen\\nDeanna L. Kelly\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.07054\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 14 Feb 2021 02:50:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oConference\\u00a7r"}']}
{title:'Vuong et al. (§72021§r)', author: 'Tyler Vuong; Yangyang Xia; Richard M. Stern', display:{Lore:['[{"text": "arXiv:2102.07330", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Modulation-Domain Loss for Neural-Network-based Real-time Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oTyler Vuong\\nYangyang Xia\\nRichard M. Stern\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.07330\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Feb 2021 04:03:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted IEEE ICASSP 2021\\u00a7r"}']}
{title:'Agrawal et al. (§72021§r)', author: 'Purvi Agrawal; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:2102.07390", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRepresentation Learning For Speech Recognition Using Feedback Based Relevance Weighting\\u00a7r\\n\\n\\u00a78\\u00a7oPurvi Agrawal\\nSriram Ganapathy\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.07390\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE International Conference on Acoustics, Speech, & Signal\\n  Processing (ICASSP) 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Feb 2021 08:20:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2011.00721, arXiv:2011.02136, arXiv:2001.07067\\u00a7r"}']}
{title:'Braun et al. (§72021§r)', author: 'Sebastian Braun; Ivan Tashev', display:{Lore:['[{"text": "arXiv:2102.07445", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn training targets for noise-robust voice activity detection\\u00a7r\\n\\n\\u00a78\\u00a7oSebastian Braun\\nIvan Tashev\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.07445\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n29th European Signal Processing Conference (EUSIPCO), 2021,\\n  Dublin, Ireland\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 19 May 2021 12:30:15 GMT)\\u00a7r"}']}
{title:'Hono et al. (§72021§r)', author: 'Yukiya Hono; Shinji Takaki; Kei Hashimoto; Keiichiro Oura; Yoshihiko Nankaku; Keiichi Tokuda', display:{Lore:['[{"text": "arXiv:2102.07786", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPeriodNet: A non-autoregressive waveform generation model with a structure separating periodic and aperiodic components\\u00a7r\\n\\n\\u00a78\\u00a7oYukiya Hono\\nShinji Takaki\\nKei Hashimoto\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.07786\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Feb 2021 19:00:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted to ICASSP 2021\\u00a7r"}']}
{title:'Subramanian et al. (§72021§r)', author: 'Aswin Shanmugam Subramanian; Chao Weng; Shinji Watanabe; Meng Yu; Dong Yu', display:{Lore:['[{"text": "arXiv:2102.07955", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning based Multi-Source Localization with Source Splitting and its Effectiveness in Multi-Talker Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAswin Shanmugam Subramanian\\nChao Weng\\nShinji Watanabe\\nMeng Yu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.07955\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 28 Nov 2021 10:06:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Computer Speech Language\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Zhepei Wang; Ritwik Giri; Umut Isik; Jean-Marc Valin; Arvindh Krishnaswamy', display:{Lore:['[{"text": "arXiv:2102.07961", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-Supervised Singing Voice Separation with Noisy Self-Training\\u00a7r\\n\\n\\u00a78\\u00a7oZhepei Wang\\nRitwik Giri\\nUmut Isik\\nJean-Marc Valin\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.07961\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Feb 2021 05:06:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at 2021 IEEE InternationalConference on Acoustics, Speech, and Signal Processing (ICASSP 2021)\\u00a7r"}']}
{title:'You et al. (§72021§r)', author: 'Jaeseong You; Gyuhyeon Nam; Dalhyun Kim; Gyeongsu Chae', display:{Lore:['[{"text": "arXiv:2102.08075", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAxial Residual Networks for CycleGAN-based Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oJaeseong You\\nGyuhyeon Nam\\nDalhyun Kim\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.08075\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 24 Aug 2021 10:01:49 GMT)\\u00a7r"}']}
{title:'Morrison et al. (§72021§r)', author: 'Max Morrison; Lucas Rencker; Zeyu Jin; Nicholas J. Bryan; Juan-Pablo Caceres; Bryan Pardo', display:{Lore:['[{"text": "arXiv:2102.08328", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContext-Aware Prosody Correction for Text-Based Speech Editing\\u00a7r\\n\\n\\u00a78\\u00a7oMax Morrison\\nLucas Rencker\\nZeyu Jin\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.08328\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Feb 2021 18:16:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in proceedings of ICASSP 2021\\u00a7r"}']}
{title:'Fang et al. (§72021§r)', author: 'Huajian Fang; Guillaume Carbajal; Stefan Wermter; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2102.08706", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVariational Autoencoder for Speech Enhancement with a Noise-Aware Encoder\\u00a7r\\n\\n\\u00a78\\u00a7oHuajian Fang\\nGuillaume Carbajal\\nStefan Wermter\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.08706\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9414060\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2021 - 2021 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Feb 2021 11:40:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2021. (c) 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for "}','{"text": "advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Yeung et al. (§72021§r)', author: 'Gary Yeung; Ruchao Fan; Abeer Alwan', display:{Lore:['[{"text": "arXiv:2102.09106", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFundamental Frequency Feature Normalization and Data Augmentation for Child Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGary Yeung\\nRuchao Fan\\nAbeer Alwan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.09106\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Feb 2021 01:31:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in IEEE ICASSP\\u00a7r"}']}
{title:'Kashiwagi et al. (§72021§r)', author: 'Yosuke Kashiwagi; Emiru Tsunoo; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2102.09168", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGaussian Kernelized Self-Attention for Long Sequence Data and Its Application to CTC-based Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYosuke Kashiwagi\\nEmiru Tsunoo\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.09168\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Feb 2021 05:51:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP2021\\u00a7r"}']}
{title:'Kleijn et al. (§72021§r)', author: 'W. Bastiaan Kleijn; Andrew Storus; Michael Chinen; Tom Denton; Felicia S. C. Lim; Alejandro Luebs; Jan Skoglund; Hengchin Yeh', display:{Lore:['[{"text": "arXiv:2102.09660", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerative Speech Coding with Predictive Variance Regularization\\u00a7r\\n\\n\\u00a78\\u00a7oW. Bastiaan Kleijn\\nAndrew Storus\\nMichael Chinen\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.09660\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Feb 2021 23:01:13 GMT)\\u00a7r"}']}
{title:'Higuchi et al. (§72021§r)', author: 'Takuya Higuchi; Shreyas Saxena; Mehrez Souden; Tien Dung Tran; Masood Delfarah; Chandra Dhir', display:{Lore:['[{"text": "arXiv:2102.09666", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDynamic curriculum learning via data parameters for noise robust keyword spotting\\u00a7r\\n\\n\\u00a78\\u00a7oTakuya Higuchi\\nShreyas Saxena\\nMehrez Souden\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.09666\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Feb 2021 23:26:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2021\\u00a7r"}']}
{title:'Meng et al. (§72021§r)', author: 'Weixin Meng; Chengshi Zheng; Xiaodong Li', display:{Lore:['[{"text": "arXiv:2102.09838", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Robust Maximum Likelihood Distortionless Response Beamformer based on a Complex Generalized Gaussian Distribution\\u00a7r\\n\\n\\u00a78\\u00a7oWeixin Meng\\nChengshi Zheng\\nXiaodong Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.09838\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Feb 2021 10:12:47 GMT)\\u00a7r"}']}
{title:'Poschadel et al. (§72021§r)', author: 'Nils Poschadel; Robert Hupke; Stephan Preihs; Jürgen Peissig', display:{Lore:['[{"text": "arXiv:2102.09853", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDirection of Arrival Estimation of Noisy Speech Using Convolutional Recurrent Neural Networks with Higher-Order Ambisonics Signals\\u00a7r\\n\\n\\u00a78\\u00a7oNils Poschadel\\nRobert Hupke\\nStephan Preihs\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.09853\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 6 May 2021 09:39:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 6 figures. Accepted to EUSIPCO 2021\\u00a7r"}']}
{title:'Shivakumar et al. (§72021§r)', author: 'Prashanth Gurunath Shivakumar; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2102.09918", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Neural Systems for Automatic Children Speech Recognition: An Empirical Study\\u00a7r\\n\\n\\u00a78\\u00a7oPrashanth Gurunath Shivakumar\\nShrikanth Narayanan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.09918\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Feb 2021 13:18:08 GMT)\\u00a7r"}']}
{title:'Borgholt et al. (§72021§r)', author: 'Lasse Borgholt; Jakob Drachmann Havtorn; Željko Agić; Anders Søgaard; Lars Maaløe; Christian Igel', display:{Lore:['[{"text": "arXiv:2102.09928", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDo End-to-End Speech Recognition Models Care About Context?\\u00a7r\\n\\n\\u00a78\\u00a7oLasse Borgholt\\nJakob Drachmann Havtorn\\n\\u017deljko Agi\\u0107\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.09928\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2020-1750\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Feb 2021 11:13:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in the proceedings of INTERSPEECH 2020, pp. 4352-4356\\u00a7r"}']}
{title:'Kumar et al. (§72021§r)', author: 'A Kishore Kumar; Shefali Waldekar; Goutam Saha; Md Sahidullah', display:{Lore:['[{"text": "arXiv:2102.09939", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lABSP System for The Third DIHARD Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oA Kishore Kumar\\nShefali Waldekar\\nGoutam Saha\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.09939\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Feb 2021 19:11:01 GMT)\\u00a7r"}']}
{title:'Venkatesh et al. (§72021§r)', author: 'Satvik Venkatesh; David Moffat; Alexis Kirke; Gözel Shakeri; Stephen Brewster; Jörg Fachner; Helen Odell-Miller; Alex Street; Nicolas Farina; Sube Banerjee; Eduardo Reck Miranda', display:{Lore:['[{"text": "arXiv:2102.09959", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lArtificially Synthesising Data for Audio Classification and Segmentation to Improve Speech and Music Detection in Radio Broadcast\\u00a7r\\n\\n\\u00a78\\u00a7oSatvik Venkatesh\\nDavid Moffat\\nAlexis Kirke\\n+ 7 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.09959\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Feb 2021 14:47:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, Accepted to ICASSP 2021\\u00a7r"}']}
{title:'Inoue et al. (§72021§r)', author: 'Katsuki Inoue; Sunao Hara; Masanobu Abe; Nobukatsu Hojo; Yusuke Ijima', display:{Lore:['[{"text": "arXiv:2102.10345", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModel architectures to extrapolate emotional expressions in DNN-based text-to-speech\\u00a7r\\n\\n\\u00a78\\u00a7oKatsuki Inoue\\nSunao Hara\\nMasanobu Abe\\nNobukatsu Hojo\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.10345\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.specom.2020.11.004\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 20 Feb 2021 13:29:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis is the author\'s final draft.Accepted by Speech Communication. Please refer to the journal if you want\\u00a7r"}']}
{title:'Dabike et al. (§72021§r)', author: 'Gerardo Roa Dabike; Jon Barker', display:{Lore:['[{"text": "arXiv:2102.10376", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Use of Voice Source Features for Sung Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGerardo Roa Dabike\\nJon Barker\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.10376\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 23 Feb 2021 16:18:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Jassim et al. (§72021§r)', author: 'Wissam A. Jassim; Jan Skoglund; Michael Chinen; Andrew Hines', display:{Lore:['[{"text": "arXiv:2102.10449", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWARP-Q: Quality Prediction For Generative Neural Speech Codecs\\u00a7r\\n\\n\\u00a78\\u00a7oWissam A. Jassim\\nJan Skoglund\\nMichael Chinen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.10449\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 20 Feb 2021 21:25:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for presentation at IEEE ICASSP 2021. Source code and data can be found on https://github.com/wjassim/WARP-Q.git\\u00a7r"}']}
{title:'Zeng et al. (§72021§r)', author: 'Zhen Zeng; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2102.10815", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLVCNet: Efficient Condition-Dependent Modeling Network for Waveform Generation\\u00a7r\\n\\n\\u00a78\\u00a7oZhen Zeng\\nJianzong Wang\\nNing Cheng\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.10815\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Feb 2021 07:55:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021. arXiv admin note: text overlapwith arXiv:2012.01684\\u00a7r"}']}
{title:'Flemotomos et al. (§72021§r)', author: 'Nikolaos Flemotomos; Victor R. Martinez; Zhuohao Chen; Karan Singla; Victor Ardulov; Raghuveer Peri; Derek D. Caperton; James Gibson; Michael J. Tanana; Panayiotis Georgiou; Jake Van Epps; Sarah P. Lord; Tad Hirsch; Zac E. Imel; David C. Atkins; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2102.11265", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomated Evaluation Of Psychotherapy Skills Using Speech And Language Technologies\\u00a7r\\n\\n\\u00a78\\u00a7oNikolaos Flemotomos\\nVictor R. Martinez\\nZhuohao Chen\\n+ 12 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.11265\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3758/s13428-021-01623-4\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 27 Mar 2021 22:24:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7onew version has an updatedtitle\\u00a7r"}']}
{title:'Viana-Cámara et al. (§72021§r)', author: 'Rafael Viana-Cámara; Diego Campos-Sobrino; Mario Campos-Soberanis', display:{Lore:['[{"text": "arXiv:2102.11480", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvolutionary optimization of contexts for phonetic correction in speech recognition systems\\u00a7r\\n\\n\\u00a78\\u00a7oRafael Viana-C\\u00e1mara\\nDiego Campos-Sobrino\\nMario Campos-Soberanis\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.11480\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nResearch in Computing Science Issue 148(8), 2019, pp. 293-306.\\n  ISSN 1870-4069\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Feb 2021 04:14:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 4 figures, This article is a translation of the paper \\"Optimizaci\\u00f3n evolutivade contextos para la correcci\\u00f3n fon\\u00e9tica en sistemas de reconocimiento del habla\\" presented in COMIA 2019\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Wangyou Zhang; Christoph Boeddeker; Shinji Watanabe; Tomohiro Nakatani; Marc Delcroix; Keisuke Kinoshita; Tsubasa Ochiai; Naoyuki Kamo; Reinhold Haeb-Umbach; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2102.11525", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Dereverberation, Beamforming, and Speech Recognition with Improved Numerical Stability and Advanced Frontend\\u00a7r\\n\\n\\u00a78\\u00a7oWangyou Zhang\\nChristoph Boeddeker\\nShinji Watanabe\\n+ 6 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.11525\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9414464\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Feb 2021 07:16:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, accepted by ICASSP 2021\\u00a7r"}']}
{title:'Luo et al. (§72021§r)', author: 'Jian Luo; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2102.11594", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnidirectional Memory-Self-Attention Transducer for Online Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJian Luo\\nJianzong Wang\\nNing Cheng\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.11594\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Feb 2021 10:14:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Chenda Li; Zhuo Chen; Yi Luo; Cong Han; Tianyan Zhou; Keisuke Kinoshita; Marc Delcroix; Shinji Watanabe; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2102.11634", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDual-Path Modeling for Long Recording Speech Separation in Meetings\\u00a7r\\n\\n\\u00a78\\u00a7oChenda Li\\nZhuo Chen\\nYi Luo\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.11634\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Feb 2021 11:23:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2021\\u00a7r"}']}
{title:'Denton et al. (§72021§r)', author: 'Tom Denton; Alejandro Luebs; Felicia S. C. Lim; Andrew Storus; Hengchin Yeh; W. Bastiaan Kleijn; Jan Skoglund', display:{Lore:['[{"text": "arXiv:2102.11906", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHandling Background Noise in Neural Speech Generation\\u00a7r\\n\\n\\u00a78\\u00a7oTom Denton\\nAlejandro Luebs\\nFelicia S. C. Lim\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.11906\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Feb 2021 19:36:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, presented at the Asilomar Conference on Signals, Systems, andComputers 2020\\u00a7r"}']}
{title:'Lin et al. (§72021§r)', author: 'Ju Lin; Adriaan J. van Wijngaarden; Kuang-Ching Wang; Melissa C. Smith', display:{Lore:['[{"text": "arXiv:2102.12078", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Enhancement Using Multi-Stage Self-Attentive Temporal Convolutional Networks\\u00a7r\\n\\n\\u00a78\\u00a7oJu Lin\\nAdriaan J. van Wijngaarden\\nKuang-Ching Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.12078\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Feb 2021 05:48:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint\\u00a7r"}']}
{title:'Lea et al. (§72021§r)', author: 'Colin Lea; Vikramjit Mitra; Aparna Joshi; Sachin Kajarekar; Jeffrey P. Bigham', display:{Lore:['[{"text": "arXiv:2102.12394", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSEP-28k: A Dataset for Stuttering Event Detection From Podcasts With People Who Stutter\\u00a7r\\n\\n\\u00a78\\u00a7oColin Lea\\nVikramjit Mitra\\nAparna Joshi\\nSachin Kajarekar\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.12394\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Feb 2021 16:22:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Schädler (§72021§r)', author: 'Marc René Schädler', display:{Lore:['[{"text": "arXiv:2102.12397", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThoughts on the potential to compensate a hearing loss in noise\\u00a7r\\n\\n\\u00a78\\u00a7oMarc Ren\\u00e9 Sch\\u00e4dler\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.12397\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Feb 2021 16:33:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o26 pages, 22 figures, related code https://doi.org/10.5281/zenodo.4500810\\u00a7r"}']}
{title:'Lux et al. (§72021§r)', author: 'Florian Lux; Ngoc Thang Vu', display:{Lore:['[{"text": "arXiv:2102.12624", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMeta-Learning for improving rare word recognition in end-to-end ASR\\u00a7r\\n\\n\\u00a78\\u00a7oFlorian Lux\\nNgoc Thang Vu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.12624\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 Feb 2021 01:40:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oRevised version to be published in the proceedings of ICASSP 2021\\u00a7r"}']}
{title:'Sebastian et al. (§72021§r)', author: 'Arun Sebastian; Peter A. Cistulli; Gary Cohen; Philip de Chazal', display:{Lore:['[{"text": "arXiv:2102.12829", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Classification of OSA related Snoring Signals from Nocturnal Audio Recordings\\u00a7r\\n\\n\\u00a78\\u00a7oArun Sebastian\\nPeter A. Cistulli\\nGary Cohen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.12829\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 2 Mar 2021 11:22:11 GMT)\\u00a7r"}']}
{title:'Gul et al. (§72021§r)', author: 'Sania Gul; Muhammad Salman Khan; Syed Waqar Shah', display:{Lore:['[{"text": "arXiv:2102.13334", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntegration of deep learning with expectation maximization for spatial cue based speech separation in reverberant conditions\\u00a7r\\n\\n\\u00a78\\u00a7oSania Gul\\nMuhammad Salman Khan\\nSyed Waqar Shah\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.13334\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Feb 2021 07:22:59 GMT)\\u00a7r"}']}
{title:'Lee-Leon et al. (§72021§r)', author: 'Abigail Lee-Leon; Chau Yuen; Dorien Herremans', display:{Lore:['[{"text": "arXiv:2102.13397", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnderwater Acoustic Communication Receiver Using Deep Belief Network\\u00a7r\\n\\n\\u00a78\\u00a7oAbigail Lee-Leon\\nChau Yuen\\nDorien Herremans\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.13397\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Feb 2021 11:18:37 GMT)\\u00a7r"}']}
{title:'Schuller et al. (§72021§r)', author: 'Björn W. Schuller; Anton Batliner; Christian Bergler; Cecilia Mascolo; Jing Han; Iulia Lefter; Heysem Kaya; Shahin Amiriparian; Alice Baird; Lukas Stappen; Sandra Ottl; Maurice Gerczuk; Panagiotis Tzirakis; Chloë Brown; Jagmohan Chauhan; Andreas Grammenos; Apinan Hasthanasombat; Dimitris Spathis; Tong Xia; Pietro Cicuta; Leon J. M. Rothkrantz; Joeri Zwerts; Jelle Treep; Casper Kaandorp', display:{Lore:['[{"text": "arXiv:2102.13468", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe INTERSPEECH 2021 Computational Paralinguistics Challenge: COVID-19 Cough, COVID-19 Speech, Escalation     Primates\\u00a7r\\n\\n\\u00a78\\u00a7oBj\\u00f6rn W. Schuller\\nAnton Batliner\\nChristian Bergler\\n+ 20 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.13468\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Feb 2021 21:39:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Ribeiro et al. (§72021§r)', author: 'Manuel Sam Ribeiro; Joanne Cleland; Aciel Eshky; Korin Richmond; Steve Renals', display:{Lore:['[{"text": "arXiv:2103.00324", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting ultrasound tongue imaging for the automatic detection of speech articulation errors\\u00a7r\\n\\n\\u00a78\\u00a7oManuel Sam Ribeiro\\nJoanne Cleland\\nAciel Eshky\\nKorin Richmond\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.00324\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.specom.2021.02.001\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nSpeech Communication, Volume 128, April 2021, Pages 24-34\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 27 Feb 2021 21:16:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 9 figures, 6 tables\\u00a7r"}']}
{title:'Ribeiro et al. (§72021§r)', author: 'Manuel Sam Ribeiro; Aciel Eshky; Korin Richmond; Steve Renals', display:{Lore:['[{"text": "arXiv:2103.00333", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSilent versus modal multi-speaker speech recognition from ultrasound and video\\u00a7r\\n\\n\\u00a78\\u00a7oManuel Sam Ribeiro\\nAciel Eshky\\nKorin Richmond\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.00333\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 27 Feb 2021 21:34:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, Submitted to Interspeech 2021\\u00a7r"}']}
{title:'Inaguma et al. (§72021§r)', author: 'Hirofumi Inaguma; Tatsuya Kawahara', display:{Lore:['[{"text": "arXiv:2103.00422", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAlignment Knowledge Distillation for Online Streaming Attention-based Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHirofumi Inaguma\\nTatsuya Kawahara\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.00422\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 22 Aug 2021 12:42:44 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Jun Wang; Max W. Y. Lam; Dan Su; Dong Yu', display:{Lore:['[{"text": "arXiv:2103.00816", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContrastive Separative Coding for Self-supervised Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oJun Wang\\nMax W. Y. Lam\\nDan Su\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.00816\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Mar 2021 07:32:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ICASSP 2021\\u00a7r"}']}
{title:'Lam et al. (§72021§r)', author: 'Max W. Y. Lam; Jun Wang; Dan Su; Dong Yu', display:{Lore:['[{"text": "arXiv:2103.00819", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSandglasset: A Light Multi-Granularity Self-attentive Network For Time-Domain Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oMax W. Y. Lam\\nJun Wang\\nDan Su\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.00819\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Mar 2021 07:37:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ICASSP 2021\\u00a7r"}']}
{title:'Chen et al. (§72021§r)', author: 'Mingjian Chen; Xu Tan; Bohan Li; Yanqing Liu; Tao Qin; Sheng Zhao; Tie-Yan Liu', display:{Lore:['[{"text": "arXiv:2103.00993", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaSpeech: Adaptive Text to Speech for Custom Voice\\u00a7r\\n\\n\\u00a78\\u00a7oMingjian Chen\\nXu Tan\\nBohan Li\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.00993\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Mar 2021 13:28:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICLR 2021\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Cong Zhang; Kathleen Jepson; Georg Lohfink; Amalia Arvaniti', display:{Lore:['[{"text": "arXiv:2103.01059", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparing acoustic analyses of speech data collected remotely\\u00a7r\\n\\n\\u00a78\\u00a7oCong Zhang\\nKathleen Jepson\\nGeorg Lohfink\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.01059\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0005132\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nThe Journal of the Acoustical Society of America 149, 3910-3916\\n  (2021)\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 17 Jun 2021 19:56:27 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Jun Wang; Max W. Y. Lam; Dan Su; Dong Yu', display:{Lore:['[{"text": "arXiv:2103.01461", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTune-In: Training Under Negative Environments with Interference for Attention Networks Simulating Cocktail Party Effect\\u00a7r\\n\\n\\u00a78\\u00a7oJun Wang\\nMax W. Y. Lam\\nDan Su\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.01461\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Mar 2021 04:03:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in AAAI 2021\\u00a7r"}']}
{title:'Koo et al. (§72021§r)', author: 'Junghyun Koo; Seungryeol Paik; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2103.02147", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReverb Conversion of Mixed Vocal Tracks Using an End-to-end Convolutional Deep Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oJunghyun Koo\\nSeungryeol Paik\\nKyogu Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.02147\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Mar 2021 03:02:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP 2021\\u00a7r"}']}
{title:'Akbarzadeh et al. (§72021§r)', author: 'Sara Akbarzadeh; Sungmin Lee; Fei Chen; Chin-Tuan Tan', display:{Lore:['[{"text": "arXiv:2103.02421", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe effect of speech and noise levels on the quality perceived by cochlear implant and normal hearing listeners\\u00a7r\\n\\n\\u00a78\\u00a7oSara Akbarzadeh\\nSungmin Lee\\nFei Chen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.02421\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Mar 2021 14:19:02 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Hao Zhang; DeLiang Wang', display:{Lore:['[{"text": "arXiv:2103.02552", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Channel and Multi-Microphone Acoustic Echo Cancellation Using A Deep Learning Based Approach\\u00a7r\\n\\n\\u00a78\\u00a7oHao Zhang\\nDeLiang Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.02552\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Mar 2021 17:47:06 GMT)\\u00a7r"}']}
{title:'Akbarzadeh et al. (§72021§r)', author: 'Sara Akbarzadeh; Sungmin Lee; Chin-Tuan Tan', display:{Lore:['[{"text": "arXiv:2103.02703", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Spatial Selective Auditory Attention of Cochlear Implant Users in Different Conversational Sound Levels\\u00a7r\\n\\n\\u00a78\\u00a7oSara Akbarzadeh\\nSungmin Lee\\nChin-Tuan Tan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.02703\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Mar 2021 21:48:58 GMT)\\u00a7r"}']}
{title:'Kobayashi et al. (§72021§r)', author: 'Kazuhiro Kobayashi; Wen-Chin Huang; Yi-Chiao Wu; Patrick Lumban Tobing; Tomoki Hayashi; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2103.02858", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lcrank: An Open-Source Software for Nonparallel Voice Conversion Based on Vector-Quantized Variational Autoencoder\\u00a7r\\n\\n\\u00a78\\u00a7oKazuhiro Kobayashi\\nWen-Chin Huang\\nYi-Chiao Wu\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.02858\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Mar 2021 06:41:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Gelin et al. (§72021§r)', author: 'Lucile Gelin; Morgane Daniel; Julien Pinquier; Thomas Pellegrini', display:{Lore:['[{"text": "arXiv:2103.02899", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end acoustic modelling for phone recognition of young readers\\u00a7r\\n\\n\\u00a78\\u00a7oLucile Gelin\\nMorgane Daniel\\nJulien Pinquier\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.02899\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Mar 2021 09:03:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages, 8 figures\\u00a7r"}']}
{title:'Yan et al. (§72021§r)', author: 'Bi-Cheng Yan; Berlin Chen', display:{Lore:['[{"text": "arXiv:2103.03023", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Mispronunciation Detection and Diagnosis From Raw Waveforms\\u00a7r\\n\\n\\u00a78\\u00a7oBi-Cheng Yan\\nBerlin Chen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.03023\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 1 Jun 2021 07:10:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint. Under review 5 pages, 3 figures\\u00a7r"}']}
{title:'Bae et al. (§72021§r)', author: 'Hanbin Bae; Jae-Sung Bae; Young-Sun Joo; Young-Ik Kim; Hoon-Young Cho', display:{Lore:['[{"text": "arXiv:2103.03049", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Neural Text-to-Speech Model Utilizing Broadcast Data Mixed with Background Music\\u00a7r\\n\\n\\u00a78\\u00a7oHanbin Bae\\nJae-Sung Bae\\nYoung-Sun Joo\\nYoung-Ik Kim\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.03049\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Mar 2021 14:14:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2021\\u00a7r"}']}
{title:'Dawalatabad et al. (§72021§r)', author: 'Nauman Dawalatabad; Jilt Sebastian; Jom Kuriakose; C. Chandra Sekhar; Shrikanth Narayanan; Hema A. Murthy', display:{Lore:['[{"text": "arXiv:2103.03215", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFront-end Diarization for Percussion Separation in Taniavartanam of Carnatic Music Concerts\\u00a7r\\n\\n\\u00a78\\u00a7oNauman Dawalatabad\\nJilt Sebastian\\nJom Kuriakose\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.03215\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Mar 2021 18:38:39 GMT)\\u00a7r"}']}
{title:'Wilkinson et al. (§72021§r)', author: 'Nicholas Wilkinson; Thomas Niesler', display:{Lore:['[{"text": "arXiv:2103.03529", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Hybrid CNN-BiLSTM Voice Activity Detector\\u00a7r\\n\\n\\u00a78\\u00a7oNicholas Wilkinson\\nThomas Niesler\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.03529\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Mar 2021 08:15:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2021\\u00a7r"}']}
{title:'Rodríguez et al. (§72021§r)', author: 'Demóstenes Z. Rodríguez; Dick Carrillo Melgarejo; Miguel A. Ramírez; Pedro H. J. Nardelli; Sebastian Möller', display:{Lore:['[{"text": "arXiv:2103.03970", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIncorporating Wireless Communication Parameters into the E-Model Algorithm\\u00a7r\\n\\n\\u00a78\\u00a7oDem\\u00f3stenes Z. Rodr\\u00edguez\\nDick Carrillo Melgarejo\\nMiguel A. Ram\\u00edrez\\nPedro H. J. Nardelli\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.03970\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3057955\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING,\\n  VOL. 29, 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Mar 2021 22:45:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages\\u00a7r"}']}
{title:'Chien et al. (§72021§r)', author: 'Chung-Ming Chien; Jheng-Hao Lin; Chien-yu Huang; Po-chun Hsu; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2103.04088", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating on Incorporating Pretrained and Learnable Speaker Representations for Multi-Speaker Multi-Style Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oChung-Ming Chien\\nJheng-Hao Lin\\nChien-yu Huang\\nPo-chun Hsu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.04088\\u00a7r\\n\\nVersion:\\u00a77v5 (Sat, 1 May 2021 08:41:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2021, in the special session of ICASSP 2021 M2VoC Challenge\\u00a7r"}']}
{title:'Garoufis et al. (§72021§r)', author: 'Christos Garoufis; Athanasia Zlatintsi; Petros Maragos', display:{Lore:['[{"text": "arXiv:2103.04336", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHTMD-Net: A Hybrid Masking-Denoising Approach to Time-Domain Monaural Singing Voice Separation\\u00a7r\\n\\n\\u00a78\\u00a7oChristos Garoufis\\nAthanasia Zlatintsi\\nPetros Maragos\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.04336\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 7 Mar 2021 12:24:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted for publication in EUSIPCO 2021\\u00a7r"}']}
{title:'Sabu et al. (§72021§r)', author: 'Kamini Sabu; Syomantak Chaudhuri; Preeti Rao; Mahesh Patil', display:{Lore:['[{"text": "arXiv:2103.04346", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Optimized Signal Processing Pipeline for Syllable Detection and Speech Rate Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oKamini Sabu\\nSyomantak Chaudhuri\\nPreeti Rao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.04346\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 7 Mar 2021 13:06:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 3 figures, accepted in National Conference on Communications (NCC) 2020\\u00a7r"}']}
{title:'Tan et al. (§72021§r)', author: 'Daxin Tan; Hingpang Huang; Guangyan Zhang; Tan Lee', display:{Lore:['[{"text": "arXiv:2103.04699", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCUHK-EE Voice Cloning System for ICASSP 2021 M2VoC Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oDaxin Tan\\nHingpang Huang\\nGuangyan Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.04699\\u00a7r\\n\\nVersion:\\u00a77v5 (Mon, 5 Jul 2021 10:49:41 GMT)\\u00a7r"}']}
{title:'Hardy et al. (§72021§r)', author: 'Emmanuel Hardy; Franck Badets', display:{Lore:['[{"text": "arXiv:2103.04792", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Ultra-low Power RNN Classifier for Always-On Voice Wake-Up Detection Robust to Real-World Scenarios\\u00a7r\\n\\n\\u00a78\\u00a7oEmmanuel Hardy\\nFranck Badets\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.04792\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Mar 2021 14:38:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis is an updated version of the paper submitted at tinyML\'21 that was accepted as a poster (https://openreview.net/group?id=tinyml.org/tinyML/2021/Research_Symposium). The posters are video-onlyand not in the "}','{"text": "conference proceedings, hence this publication on arXiv\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Ke Li; Daniel Povey; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:2103.05081", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Parallelizable Lattice Rescoring Strategy with Neural Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oKe Li\\nDaniel Povey\\nSanjeev Khudanpur\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.05081\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Mar 2021 21:23:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at ICASSP 2021. 5 pages, 1 figure\\u00a7r"}']}
{title:'Fuchs et al. (§72021§r)', author: 'Tzeviya Sylvia Fuchs; Yael Segal; Joseph Keshet', display:{Lore:['[{"text": "arXiv:2103.05468", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCNN-based Spoken Term Detection and Localization without Dynamic Programming\\u00a7r\\n\\n\\u00a78\\u00a7oTzeviya Sylvia Fuchs\\nYael Segal\\nJoseph Keshet\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.05468\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 7 Mar 2021 14:50:58 GMT)\\u00a7r"}']}
{title:'Das et al. (§72021§r)', author: 'Nilaksh Das; Sravan Bodapati; Monica Sunkara; Sundararajan Srinivasan; Duen Horng Chau', display:{Lore:['[{"text": "arXiv:2103.05834", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBest of Both Worlds: Robust Accented Speech Recognition with Adversarial Transfer Learning\\u00a7r\\n\\n\\u00a78\\u00a7oNilaksh Das\\nSravan Bodapati\\nMonica Sunkara\\nSundararajan Srinivasan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.05834\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Mar 2021 02:33:52 GMT)\\u00a7r"}']}
{title:'Katthi et al. (§72021§r)', author: 'Jaswanth Reddy Katthi; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:2103.06478", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Multiway Canonical Correlation Analysis for Multi-Subject EEG Normalization\\u00a7r\\n\\n\\u00a78\\u00a7oJaswanth Reddy Katthi\\nSriram Ganapathy\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.06478\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Mar 2021 05:49:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 2 tables, to be published in ICASSP 2021\\u00a7r"}']}
{title:'Ebbers et al. (§72021§r)', author: 'Janek Ebbers; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2103.06581", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lForward-Backward Convolutional Recurrent Neural Networks and Tag-Conditioned Convolutional Neural Networks for Weakly Labeled Semi-supervised Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJanek Ebbers\\nReinhold Haeb-Umbach\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.06581\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Mar 2021 10:18:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by dcase2020 workshop, the presented system received the reproducible system award for the dcase2020 challenge task 4\\u00a7r"}']}
{title:'Niizumi et al. (§72021§r)', author: 'Daisuke Niizumi; Daiki Takeuchi; Yasunori Ohishi; Noboru Harada; Kunio Kashino', display:{Lore:['[{"text": "arXiv:2103.06695", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBYOL for Audio: Self-Supervised Learning for General-Purpose Audio Representation\\u00a7r\\n\\n\\u00a78\\u00a7oDaisuke Niizumi\\nDaiki Takeuchi\\nYasunori Ohishi\\nNoboru Harada\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.06695\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 21 Apr 2021 01:06:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIJCNN 2021, 8 pages, 4 figures\\u00a7r"}']}
{title:'Qiu et al. (§72021§r)', author: 'David Qiu; Qiujia Li; Yanzhang He; Yu Zhang; Bo Li; Liangliang Cao; Rohit Prabhavalkar; Deepti Bhatia; Wei Li; Ke Hu; Tara N. Sainath; Ian McGraw', display:{Lore:['[{"text": "arXiv:2103.06716", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Word-Level Confidence For Subword End-to-End ASR\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Qiu\\nQiujia Li\\nYanzhang He\\n+ 8 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.06716\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Mar 2021 15:03:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP 2021\\u00a7r"}']}
{title:'Laptev et al. (§72021§r)', author: 'Aleksandr Laptev; Andrei Andrusenko; Ivan Podluzhny; Anton Mitrofanov; Ivan Medennikov; Yuri Matveev', display:{Lore:['[{"text": "arXiv:2103.07186", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDynamic Acoustic Unit Augmentation With BPE-Dropout for Low-Resource End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAleksandr Laptev\\nAndrei Andrusenko\\nIvan Podluzhny\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.07186\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3390/s21093063\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Mar 2021 10:10:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages, 7 figures\\u00a7r"}']}
{title:'Gupta et al. (§72021§r)', author: 'Chitralekha Gupta; Purnima Kamath; Lonce Wyse', display:{Lore:['[{"text": "arXiv:2103.07390", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSignal Representations for Synthesizing Audio Textures with Generative Adversarial Networks\\u00a7r\\n\\n\\u00a78\\u00a7oChitralekha Gupta\\nPurnima Kamath\\nLonce Wyse\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.07390\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5281/zenodo.5054145\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nSound and Music Computing 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Mar 2021 16:31:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Sound and Music Computing Conference (SMC) 2021\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Zi-Qiang Zhang; Yan Song; Ming-Hui Wu; Xin Fang; Li-Rong Dai', display:{Lore:['[{"text": "arXiv:2103.08207", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lXLST: Cross-lingual Self-training to Learn Multilingual Representation for Low Resource Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZi-Qiang Zhang\\nYan Song\\nMing-Hui Wu\\nXin Fang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.08207\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Mar 2021 08:33:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure\\u00a7r"}']}
{title:'Sadhu et al. (§72021§r)', author: 'Samik Sadhu; Di He; Che-Wei Huang; Sri Harish Mallidi; Minhua Wu; Ariya Rastrow; Andreas Stolcke; Jasha Droppo; Roland Maas', display:{Lore:['[{"text": "arXiv:2103.08393", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWav2vec-C: A Self-supervised Model for Speech Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oSamik Sadhu\\nDi He\\nChe-Wei Huang\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.08393\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 23 Jun 2021 22:43:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Interspeech 2021\\u00a7r"}']}
{title:'Nercessian et al. (§72021§r)', author: 'Shahan Nercessian; Andy Sarroff; Kurt James Werner', display:{Lore:['[{"text": "arXiv:2103.08709", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLightweight and interpretable neural modeling of an audio distortion effect using hyperconditioned differentiable biquads\\u00a7r\\n\\n\\u00a78\\u00a7oShahan Nercessian\\nAndy Sarroff\\nKurt James Werner\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.08709\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Mar 2021 20:46:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures. To be published in IEEE International Conference on Acoustics, Speech, Signal Processing (ICASSP) 2021\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Chunlei Zhang; Meng Yu; Chao Weng; Dong Yu', display:{Lore:['[{"text": "arXiv:2103.08781", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Robust Speaker Verification with Target Speaker Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oChunlei Zhang\\nMeng Yu\\nChao Weng\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.08781\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Mar 2021 00:28:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE ICASSP 2021\\u00a7r"}']}
{title:'Dohi et al. (§72021§r)', author: 'Kota Dohi; Takashi Endo; Harsh Purohit; Ryo Tanabe; Yohei Kawaguchi', display:{Lore:['[{"text": "arXiv:2103.08801", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFlow-based Self-supervised Density Estimation for Anomalous Sound Detection\\u00a7r\\n\\n\\u00a78\\u00a7oKota Dohi\\nTakashi Endo\\nHarsh Purohit\\nRyo Tanabe\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.08801\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Mar 2021 01:52:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, accepted in ICASSP 2021\\u00a7r"}']}
{title:'Franzen et al. (§72021§r)', author: 'Jan Franzen; Ernst Seidel; Tim Fingscheidt', display:{Lore:['[{"text": "arXiv:2103.09007", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAEC in a NetShell: On Target and Topology Choices for FCRN Acoustic Echo Cancellation\\u00a7r\\n\\n\\u00a78\\u00a7oJan Franzen\\nErnst Seidel\\nTim Fingscheidt\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.09007\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Mar 2021 12:08:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at IEEEICASSP 2021\\u00a7r"}']}
{title:'Muguli et al. (§72021§r)', author: 'Ananya Muguli; Lancelot Pinto; Nirmala R.; Neeraj Sharma; Prashant Krishnan; Prasanta Kumar Ghosh; Rohit Kumar; Shrirama Bhat; Srikanth Raj Chetupalli; Sriram Ganapathy; Shreyas Ramoji; Viral Nanda', display:{Lore:['[{"text": "arXiv:2103.09148", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiCOVA Challenge: Dataset, task, and baseline system for COVID-19 diagnosis using acoustics\\u00a7r\\n\\n\\u00a78\\u00a7oAnanya Muguli\\nLancelot Pinto\\nNirmala R.\\n+ 8 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.09148\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 17 Jun 2021 05:51:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proceedings of Interspeech, 2021\\u00a7r"}']}
{title:'Yuan et al. (§72021§r)', author: 'Siyang Yuan; Pengyu Cheng; Ruiyi Zhang; Weituo Hao; Zhe Gan; Lawrence Carin', display:{Lore:['[{"text": "arXiv:2103.09420", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Zero-shot Voice Style Transfer via Disentangled Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oSiyang Yuan\\nPengyu Cheng\\nRuiyi Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.09420\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Mar 2021 03:21:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICLR 2021\\u00a7r"}']}
{title:'Lee et al. (§72021§r)', author: 'Keon Lee; Kyumin Park; Daeyoung Kim', display:{Lore:['[{"text": "arXiv:2103.09474", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSTYLER: Style Factor Modeling with Rapidity and Robustness via Speech Decomposition for Expressive and Controllable Neural Text to Speech\\u00a7r\\n\\n\\u00a78\\u00a7oKeon Lee\\nKyumin Park\\nDaeyoung Kim\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.09474\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 25 Jun 2021 01:55:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, Accepted to Interspeech 2021\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Kai Wang; Bengbeng He; Wei-Ping Zhu', display:{Lore:['[{"text": "arXiv:2103.09963", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTSTNN: Two-stage Transformer based Neural Network for Speech Enhancement in the Time Domain\\u00a7r\\n\\n\\u00a78\\u00a7oKai Wang\\nBengbeng He\\nWei-Ping Zhu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.09963\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Mar 2021 00:38:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, accepted by IEEE ICASSP 2021\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Xin Wang; Junich Yamagishi', display:{Lore:['[{"text": "arXiv:2103.11326", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparative Study on Recent Neural Spoofing Countermeasures for Synthetic Speech Detection\\u00a7r\\n\\n\\u00a78\\u00a7oXin Wang\\nJunich Yamagishi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.11326\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 13 Jun 2021 05:44:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2021\\u00a7r"}']}
{title:'Chowdhury et al. (§72021§r)', author: 'Muhammad E. H. Chowdhury; Nabil Ibtehaz; Tawsifur Rahman; Yosra Magdi Salih Mekki; Yazan Qibalwey; Sakib Mahmud; Maymouna Ezeddin; Susu Zughaier; Sumaya Ali S A Al-Maadeed', display:{Lore:['[{"text": "arXiv:2103.12063", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQUCoughScope: An Artificially Intelligent Mobile Application to Detect Asymptomatic COVID-19 Patients using Cough and Breathing Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oMuhammad E. H. Chowdhury\\nNabil Ibtehaz\\nTawsifur Rahman\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.12063\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3390/diagnostics12040920\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nDiagnostics 2022, 12(4), 920\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 20 Mar 2021 18:26:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 page, Table 4, Figure 2\\u00a7r"}']}
{title:'Haidar et al. (§72021§r)', author: 'Md Akmal Haidar; Mehdi Rezagholizadeh', display:{Lore:['[{"text": "arXiv:2103.13329", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFine-tuning of Pre-trained End-to-end Speech Recognition with Generative Adversarial Networks\\u00a7r\\n\\n\\u00a78\\u00a7oMd Akmal Haidar\\nMehdi Rezagholizadeh\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.13329\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Mar 2021 17:40:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ICASSP 2021 conference\\u00a7r"}']}
{title:'Sadhu et al. (§72021§r)', author: 'Samik Sadhu; Hynek Hermansky', display:{Lore:['[{"text": "arXiv:2103.14129", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRadically Old Way of Computing Spectra: Applications in End-to-End ASR\\u00a7r\\n\\n\\u00a78\\u00a7oSamik Sadhu\\nHynek Hermansky\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.14129\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 2 Apr 2021 22:04:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Qiujia Li; Yu Zhang; Bo Li; Liangliang Cao; Philip C. Woodland', display:{Lore:['[{"text": "arXiv:2103.14152", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lResidual Energy-Based Models for End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oQiujia Li\\nYu Zhang\\nBo Li\\nLiangliang Cao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.14152\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 23 Jun 2021 17:26:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proc. Interspeech 2021\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Ju-Chiang Wang; Jordan B. L. Smith; Jitong Chen; Xuchen Song; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2103.14253", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSupervised Chorus Detection for Popular Music Using Convolutional Neural Network and Multi-task Learning\\u00a7r\\n\\n\\u00a78\\u00a7oJu-Chiang Wang\\nJordan B. L. Smith\\nJitong Chen\\nXuchen Song\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.14253\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 21 Apr 2021 06:00:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis version is a preprint ofan accepted paper by ICASSP2021. Please citethe publication in the Proceedings of IEEE International Conference on Acoustics, Speech, Signal Processing\\u00a7r"}']}
{title:'Tang et al. (§72021§r)', author: 'Tiantian Tang; Xinyuan Zhou; Yanhua Long; Yijie Li; Jiaen Liang', display:{Lore:['[{"text": "arXiv:2103.14297", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCNN-based Discriminative Training for Domain Compensation in Acoustic Event Detection with Frame-wise Classifier\\u00a7r\\n\\n\\u00a78\\u00a7oTiantian Tang\\nXinyuan Zhou\\nYanhua Long\\nYijie Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.14297\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Mar 2021 07:17:22 GMT)\\u00a7r"}']}
{title:'Chettri et al. (§72021§r)', author: 'Bhusan Chettri; Rosa González Hautamäki; Md Sahidullah; Tomi Kinnunen', display:{Lore:['[{"text": "arXiv:2103.14602", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData Quality as Predictor of Voice Anti-Spoofing Generalization\\u00a7r\\n\\n\\u00a78\\u00a7oBhusan Chettri\\nRosa Gonz\\u00e1lez Hautam\\u00e4ki\\nMd Sahidullah\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.14602\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 21 Jun 2021 20:53:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2021\\u00a7r"}']}
{title:'Zhen et al. (§72021§r)', author: 'Kai Zhen; Jongmo Sung; Mi Suk Lee; Seungkwon Beak; Minje Kim', display:{Lore:['[{"text": "arXiv:2103.14776", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScalable and Efficient Neural Speech Coding: A Hybrid Design\\u00a7r\\n\\n\\u00a78\\u00a7oKai Zhen\\nJongmo Sung\\nMi Suk Lee\\nSeungkwon Beak\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.14776\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3129353\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 28 Nov 2021 02:17:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE/ACM Transactions on Audio, Speech, and Language Processing (IEEE/ACM TASLP), 2021 (Acceptedfor publication)\\u00a7r"}']}
{title:'Guan et al. (§72021§r)', author: 'Shanzheng Guan; Shupei Liu; Junqi Chen; Wenbo Zhu; Shengqiang Li; Xu Tan; Ziye Yang; Menglong Xu; Yijiang Chen; Jianyu Wang; Xiao-Lei Zhang', display:{Lore:['[{"text": "arXiv:2103.15118", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLibri-adhoc40: A dataset collected from synchronized ad-hoc microphone arrays\\u00a7r\\n\\n\\u00a78\\u00a7oShanzheng Guan\\nShupei Liu\\nJunqi Chen\\n+ 7 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.15118\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 7 Apr 2021 03:31:20 GMT)\\u00a7r"}']}
{title:'Feng et al. (§72021§r)', author: 'Siyuan Feng; Olya Kudina; Bence Mark Halpern; Odette Scharenborg', display:{Lore:['[{"text": "arXiv:2103.15122", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQuantifying Bias in Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSiyuan Feng\\nOlya Kudina\\nBence Mark Halpern\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.15122\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 1 Apr 2021 09:12:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH (IS) 2021. This preprint version differs slightly from the version submitted to IS 2021: Figure 1 is not included in IS 2021\\u00a7r"}']}
{title:'Chen et al. (§72021§r)', author: 'Junqi Chen; Xiao-Lei Zhang', display:{Lore:['[{"text": "arXiv:2103.15305", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScaling sparsemax based channel selection for speech recognition with ad-hoc microphone arrays\\u00a7r\\n\\n\\u00a78\\u00a7oJunqi Chen\\nXiao-Lei Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.15305\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-419\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2021, 291-295\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 1 Apr 2021 15:33:28 GMT)\\u00a7r"}']}
{title:'Kolobov et al. (§72021§r)', author: 'Rostislav Kolobov; Olga Okhapkina; Olga Omelchishina; Andrey Platunov; Roman Bedyakin; Vyacheslav Moshkin; Dmitry Menshikov; Nikolay Mikhaylovskiy', display:{Lore:['[{"text": "arXiv:2103.16193", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMediaSpeech: Multilanguage ASR Benchmark and Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oRostislav Kolobov\\nOlga Okhapkina\\nOlga Omelchishina\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.16193\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Mar 2021 09:21:44 GMT)\\u00a7r"}']}
{title:'Xu et al. (§72021§r)', author: 'Chenglin Xu; Wei Rao; Jibin Wu; Haizhou Li', display:{Lore:['[{"text": "arXiv:2103.16269", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTarget Speaker Verification with Selective Auditory Attention for Single and Multi-talker Speech\\u00a7r\\n\\n\\u00a78\\u00a7oChenglin Xu\\nWei Rao\\nJibin Wu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.16269\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 2 Apr 2021 08:29:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, submitted to IEEE/ACM transaction on Audio, Speech and Language on 10 Jan. 2021\\u00a7r"}']}
{title:'Mao et al. (§72021§r)', author: 'Shuiyang Mao; P. C. Ching; Tan Lee', display:{Lore:['[{"text": "arXiv:2103.16456", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Segment-Based Speech Emotion Recognition by Deep Self-Learning\\u00a7r\\n\\n\\u00a78\\u00a7oShuiyang Mao\\nP. C. Ching\\nTan Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.16456\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Mar 2021 16:02:31 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Pu Wang; Hugo Van hamme', display:{Lore:['[{"text": "arXiv:2103.16674", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPre-training for low resource speech-to-intent applications\\u00a7r\\n\\n\\u00a78\\u00a7oPu Wang\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.16674\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Mar 2021 20:44:29 GMT)\\u00a7r"}']}
{title:'Kanda et al. (§72021§r)', author: 'Naoyuki Kanda; Guoli Ye; Yu Wu; Yashesh Gaur; Xiaofei Wang; Zhong Meng; Zhuo Chen; Takuya Yoshioka', display:{Lore:['[{"text": "arXiv:2103.16776", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLarge-Scale Pre-Training of End-to-End Multi-Talker ASR for Meeting Transcription with Single Distant Microphone\\u00a7r\\n\\n\\u00a78\\u00a7oNaoyuki Kanda\\nGuoli Ye\\nYu Wu\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.16776\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 12 Apr 2021 21:46:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Helin Wang; Bo Wu; Lianwu Chen; Meng Yu; Jianwei Yu; Yong Xu; Shi-Xiong Zhang; Chao Weng; Dan Su; Dong Yu', display:{Lore:['[{"text": "arXiv:2103.16849", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTeCANet: Temporal-Contextual Attention Network for Environment-Aware Speech Dereverberation\\u00a7r\\n\\n\\u00a78\\u00a7oHelin Wang\\nBo Wu\\nLianwu Chen\\n+ 6 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.16849\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 26 Aug 2021 09:57:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2021\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Helin Wang; Yuexian Zou; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2103.16858", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpecAugment++: A Hidden Space Data Augmentation Method for Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oHelin Wang\\nYuexian Zou\\nWenwu Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.16858\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 15 Jun 2021 01:27:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2021\\u00a7r"}']}
{title:'Xu et al. (§72021§r)', author: 'Ziyi Xu; Maximilian Strake; Tim Fingscheidt', display:{Lore:['[{"text": "arXiv:2103.17088", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Noise Suppression With Non-Intrusive PESQNet Supervision Enabling the Use of Real Training Data\\u00a7r\\n\\n\\u00a78\\u00a7oZiyi Xu\\nMaximilian Strake\\nTim Fingscheidt\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.17088\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 Mar 2021 13:58:39 GMT)\\u00a7r"}']}
{title:'Żelasko et al. (§72021§r)', author: 'Piotr Żelasko; Sonal Joshi; Yiwen Shao; Jesus Villalba; Jan Trmal; Najim Dehak; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:2103.17122", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Attacks and Defenses for Speech Recognition Systems\\u00a7r\\n\\n\\u00a78\\u00a7oPiotr \\u017belasko\\nSonal Joshi\\nYiwen Shao\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.17122\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 Mar 2021 14:44:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\\u00a7r"}']}
{title:'Seidel et al. (§72021§r)', author: 'Ernst Seidel; Jan Franzen; Maximilian Strake; Tim Fingscheidt', display:{Lore:['[{"text": "arXiv:2103.17189", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lY^2-Net FCRN for Acoustic Echo and Noise Suppression\\u00a7r\\n\\n\\u00a78\\u00a7oErnst Seidel\\nJan Franzen\\nMaximilian Strake\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.17189\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 18 Jul 2021 16:14:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, accepted for Interspeech 2021\\u00a7r"}']}
{title:'Lohrenz et al. (§72021§r)', author: 'Timo Lohrenz; Zhengyang Li; Tim Fingscheidt', display:{Lore:['[{"text": "arXiv:2104.00120", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Encoder Learning and Stream Fusion for Transformer-Based End-to-End Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oTimo Lohrenz\\nZhengyang Li\\nTim Fingscheidt\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.00120\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 14 Jul 2021 14:10:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at INTERSPEECH 2021\\u00a7r"}']}
{title:'Qi et al. (§72021§r)', author: 'Jiajun Qi; Wu Guo; Bin Gu', display:{Lore:['[{"text": "arXiv:2104.00230", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBidirectional Multiscale Feature Aggregation for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oJiajun Qi\\nWu Guo\\nBin Gu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.00230\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Apr 2021 03:19:10 GMT)\\u00a7r"}']}
{title:'Schädler (§72021§r)', author: 'Marc René Schädler', display:{Lore:['[{"text": "arXiv:2104.00259", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInteractive spatial speech recognition maps based on simulated speech recognition experiments\\u00a7r\\n\\n\\u00a78\\u00a7oMarc Ren\\u00e9 Sch\\u00e4dler\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.00259\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Apr 2021 05:35:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages, 11 figures, related code https://doi.org/10.5281/zenodo.4651595\\u00a7r"}']}
{title:'Barnabò et al. (§72021§r)', author: 'Giorgio Barnabò; Giovanni Trappolini; Lorenzo Lastilla; Cesare Campagnano; Angela Fan; Fabio Petroni; Fabrizio Silvestri', display:{Lore:['[{"text": "arXiv:2104.00353", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCycleDRUMS: Automatic Drum Arrangement For Bass Lines Using CycleGAN\\u00a7r\\n\\n\\u00a78\\u00a7oGiorgio Barnab\\u00f2\\nGiovanni Trappolini\\nLorenzo Lastilla\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.00353\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 9 Apr 2021 10:43:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 5 figures, submitted to IEEE Transactions on Multimedia, the authors contributed equally to this work\\u00a7r"}']}
{title:'Kang et al. (§72021§r)', author: 'Minsu Kang; Jihyun Lee; Simin Kim; Injung Kim', display:{Lore:['[{"text": "arXiv:2104.00624", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast DCTTS: Efficient Deep Convolutional Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oMinsu Kang\\nJihyun Lee\\nSimin Kim\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.00624\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Apr 2021 17:08:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, to be published in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2021\\u00a7r"}']}
{title:'Berg et al. (§72021§r)', author: "Axel Berg; Mark O'Connor; Miguel Tairum Cruz", display:{Lore:['[{"text": "arXiv:2104.00769", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKeyword Transformer: A Self-Attention Model for Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oAxel Berg\\nMark O\'Connor\\nMiguel Tairum Cruz\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.00769\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-1286\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2021, 4249-4253\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 15 Jun 2021 13:06:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH\\u00a7r"}']}
{title:'Kim et al. (§72021§r)', author: 'Kang-wook Kim; Seung-won Park; Junhyeok Lee; Myun-chul Joe', display:{Lore:['[{"text": "arXiv:2104.00931", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAssem-VC: Realistic Voice Conversion by Assembling Modern Speech Synthesis Techniques\\u00a7r\\n\\n\\u00a78\\u00a7oKang-wook Kim\\nSeung-won Park\\nJunhyeok Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.00931\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 11 Oct 2021 15:32:46 GMT)\\u00a7r"}']}
{title:'Rao et al. (§72021§r)', author: 'Wei Rao; Yihui Fu; Yanxin Hu; Xin Xu; Yvkai Jv; Jiangyu Han; Zhongjie Jiang; Lei Xie; Yannan Wang; Shinji Watanabe; Zheng-Hua Tan; Hui Bu; Tao Yu; Shidong Shang', display:{Lore:['[{"text": "arXiv:2104.00960", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lINTERSPEECH 2021 ConferencingSpeech Challenge: Towards Far-field Multi-Channel Speech Enhancement for Video Conferencing\\u00a7r\\n\\n\\u00a78\\u00a7oWei Rao\\nYihui Fu\\nYanxin Hu\\n+ 10 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.00960\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Apr 2021 09:56:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, submitted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Feng et al. (§72021§r)', author: 'Siyuan Feng; Piotr Żelasko; Laureano Moro-Velázquez; Odette Scharenborg', display:{Lore:['[{"text": "arXiv:2104.00994", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Acoustic Unit Discovery by Leveraging a Language-Independent Subword Discriminative Feature Representation\\u00a7r\\n\\n\\u00a78\\u00a7oSiyuan Feng\\nPiotr \\u017belasko\\nLaureano Moro-Vel\\u00e1zquez\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.00994\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 7 Jun 2021 11:21:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in INTERSPEECH 2021\\u00a7r"}']}
{title:'Yu et al. (§72021§r)', author: 'Meng Yu; Chunlei Zhang; Yong Xu; Shixiong Zhang; Dong Yu', display:{Lore:['[{"text": "arXiv:2104.01227", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMetricNet: Towards Improved Modeling For Non-Intrusive Speech Quality Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oMeng Yu\\nChunlei Zhang\\nYong Xu\\nShixiong Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01227\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Apr 2021 20:03:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2021\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'You Zhang; Ge Zhu; Fei Jiang; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2104.01320", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Empirical Study on Channel Effects for Synthetic Voice Spoofing Countermeasure Systems\\u00a7r\\n\\n\\u00a78\\u00a7oYou Zhang\\nGe Zhu\\nFei Jiang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01320\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 10 Oct 2021 21:42:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 6 figures, in Proc. INTERSPEECH 2021\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Yu Wang; Chee Siang Leow; Akio Kobayashi; Takehito Utsuro; Hiromitsu Nishizaki', display:{Lore:['[{"text": "arXiv:2104.01384", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExKaldi-RT: A Real-Time Automatic Speech Recognition Extension Toolkit of Kaldi\\u00a7r\\n\\n\\u00a78\\u00a7oYu Wang\\nChee Siang Leow\\nAkio Kobayashi\\nTakehito Utsuro\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01384\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 8 Aug 2021 22:28:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the IEEE 10th Global Conference on Consumer Electronics\\u00a7r"}']}
{title:'Jeong et al. (§72021§r)', author: 'Myeonghun Jeong; Hyeongju Kim; Sung Jun Cheon; Byoung Jin Choi; Nam Soo Kim', display:{Lore:['[{"text": "arXiv:2104.01409", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiff-TTS: A Denoising Diffusion Model for Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oMyeonghun Jeong\\nHyeongju Kim\\nSung Jun Cheon\\nByoung Jin Choi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01409\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 3 Apr 2021 13:53:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Kataria et al. (§72021§r)', author: 'Saurabh Kataria; Jesús Villalba; Piotr Żelasko; Laureano Moro-Velázquez; Najim Dehak', display:{Lore:['[{"text": "arXiv:2104.01433", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Feature CycleGANs: Speaker Identity Preserving Non-parallel Microphone-Telephone Domain Adaptation for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oSaurabh Kataria\\nJes\\u00fas Villalba\\nPiotr \\u017belasko\\nLaureano Moro-Vel\\u00e1zquez\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01433\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 3 Apr 2021 15:54:00 GMT)\\u00a7r"}']}
{title:'Dawalatabad et al. (§72021§r)', author: 'Nauman Dawalatabad; Mirco Ravanelli; François Grondin; Jenthe Thienpondt; Brecht Desplanques; Hwidong Na', display:{Lore:['[{"text": "arXiv:2104.01466", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lECAPA-TDNN Embeddings for Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oNauman Dawalatabad\\nMirco Ravanelli\\nFran\\u00e7ois Grondin\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01466\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-941\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 3 Apr 2021 19:37:51 GMT)\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Lujun Li; Yikai Kang; Yuchen Shi; Ludwig Kürzinger; Tobias Watzel; Gerhard Rigoll', display:{Lore:['[{"text": "arXiv:2104.01471", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Joint Training with Self-Attention Mechanism for Robust End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oLujun Li\\nYikai Kang\\nYuchen Shi\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01471\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 3 Apr 2021 19:58:51 GMT)\\u00a7r"}']}
{title:'Bakhturina et al. (§72021§r)', author: 'Evelina Bakhturina; Vitaly Lavrukhin; Boris Ginsburg; Yang Zhang', display:{Lore:['[{"text": "arXiv:2104.01497", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHi-Fi Multi-Speaker English TTS Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oEvelina Bakhturina\\nVitaly Lavrukhin\\nBoris Ginsburg\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01497\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 14 Jun 2021 19:20:10 GMT)\\u00a7r"}']}
{title:'Tian et al. (§72021§r)', author: 'Zhengkun Tian; Jiangyan Yi; Jianhua Tao; Ye Bai; Shuai Zhang; Zhengqi Wen; Xuefei Liu', display:{Lore:['[{"text": "arXiv:2104.01522", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTSNAT: Two-Step Non-Autoregressvie Transformer Models for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhengkun Tian\\nJiangyan Yi\\nJianhua Tao\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01522\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2022.3152128\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 4 Apr 2021 02:34:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech2021\\u00a7r"}']}
{title:'Zeng et al. (§72021§r)', author: 'Chang Zeng; Xin Wang; Erica Cooper; Xiaoxiao Miao; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2104.01541", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention Back-end for Automatic Speaker Verification with Multiple Enrollment Utterances\\u00a7r\\n\\n\\u00a78\\u00a7oChang Zeng\\nXin Wang\\nErica Cooper\\nXiaoxiao Miao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01541\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746688\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Oct 2021 01:46:16 GMT)\\u00a7r"}']}
{title:'Majumdar et al. (§72021§r)', author: 'Somshubra Majumdar; Jagadeesh Balam; Oleksii Hrinchuk; Vitaly Lavrukhin; Vahid Noroozi; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2104.01721", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCitrinet: Closing the Gap between Non-Autoregressive and Autoregressive End-to-End Models for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSomshubra Majumdar\\nJagadeesh Balam\\nOleksii Hrinchuk\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01721\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Apr 2021 00:16:27 GMT)\\u00a7r"}']}
{title:'Xie et al. (§72021§r)', author: 'Qicong Xie; Xiaohai Tian; Guanghou Liu; Kun Song; Lei Xie; Zhiyong Wu; Hai Li; Song Shi; Haizhou Li; Fen Hong; Hui Bu; Xin Xu', display:{Lore:['[{"text": "arXiv:2104.01818", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Multi-speaker Multi-style Voice Cloning Challenge 2021\\u00a7r\\n\\n\\u00a78\\u00a7oQicong Xie\\nXiaohai Tian\\nGuanghou Liu\\n+ 8 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01818\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Apr 2021 09:14:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ohas been accepted to ICASSP 2021\\u00a7r"}']}
{title:'Raj et al. (§72021§r)', author: 'Desh Raj; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:2104.01954", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReformulating DOVER-Lap Label Mapping as a Graph Partitioning Problem\\u00a7r\\n\\n\\u00a78\\u00a7oDesh Raj\\nSanjeev Khudanpur\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01954\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 3 Jun 2021 22:59:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures. Acceped at INTERSPEECH 2021\\u00a7r"}']}
{title:'Sivaraman et al. (§72021§r)', author: 'Aswin Sivaraman; Sunwoo Kim; Minje Kim', display:{Lore:['[{"text": "arXiv:2104.02018", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPersonalized Speech Enhancement through Self-Supervised Data Augmentation and Purification\\u00a7r\\n\\n\\u00a78\\u00a7oAswin Sivaraman\\nSunwoo Kim\\nMinje Kim\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02018\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Apr 2021 17:17:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, underreview\\u00a7r"}']}
{title:'Chojnacka et al. (§72021§r)', author: 'Roza Chojnacka; Jason Pelecanos; Quan Wang; Ignacio Lopez Moreno', display:{Lore:['[{"text": "arXiv:2104.02125", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeakerStew: Scaling to Many Languages with a Triaged Multilingual Text-Dependent and Text-Independent Speaker Verification System\\u00a7r\\n\\n\\u00a78\\u00a7oRoza Chojnacka\\nJason Pelecanos\\nQuan Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02125\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 15 Jun 2021 19:39:03 GMT)\\u00a7r"}']}
{title:'Kanda et al. (§72021§r)', author: 'Naoyuki Kanda; Guoli Ye; Yashesh Gaur; Xiaofei Wang; Zhong Meng; Zhuo Chen; Takuya Yoshioka', display:{Lore:['[{"text": "arXiv:2104.02128", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Speaker-Attributed ASR with Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oNaoyuki Kanda\\nGuoli Ye\\nYashesh Gaur\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02128\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Apr 2021 19:54:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Lee et al. (§72021§r)', author: 'Junhyeok Lee; Seungu Han', display:{Lore:['[{"text": "arXiv:2104.02321", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNU-Wave: A Diffusion Probabilistic Model for Neural Audio Upsampling\\u00a7r\\n\\n\\u00a78\\u00a7oJunhyeok Lee\\nSeungu Han\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02321\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-36\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 17 Jun 2021 04:36:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021\\u00a7r"}']}
{title:'Szoke et al. (§72021§r)', author: 'Igor Szoke; Santosh Kesiraju; Ondrej Novotny; Martin Kocour; Karel Vesely; Jan "Honza" Cernocky', display:{Lore:['[{"text": "arXiv:2104.02332", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetecting English Speech in the Air Traffic Control Voice Communication\\u00a7r\\n\\n\\u00a78\\u00a7oIgor Szoke\\nSantosh Kesiraju\\nOndrej Novotny\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02332\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Apr 2021 07:29:09 GMT)\\u00a7r"}']}
{title:'Singh et al. (§72021§r)', author: 'Prachi Singh; Rajat Varma; Venkat Krishnamohan; Srikanth Raj Chetupalli; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:2104.02359", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLEAP Submission for the Third DIHARD Diarization Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oPrachi Singh\\nRajat Varma\\nVenkat Krishnamohan\\nSrikanth Raj Chetupalli\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02359\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 14 Jun 2021 07:47:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in INTERSPEECH 2021\\u00a7r"}']}
{title:'Thienpondt et al. (§72021§r)', author: 'Jenthe Thienpondt; Brecht Desplanques; Kris Demuynck', display:{Lore:['[{"text": "arXiv:2104.02370", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntegrating Frequency Translational Invariance in TDNNs and Frequency Positional Information in 2D ResNets to Enhance Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oJenthe Thienpondt\\nBrecht Desplanques\\nKris Demuynck\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02370\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-1570\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 9 Sep 2021 07:32:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oproceedings of INTERSPEECH 2021\\u00a7r"}']}
{title:'Gerazov et al. (§72021§r)', author: 'Branislav Gerazov; Michael Wagner', display:{Lore:['[{"text": "arXiv:2104.02397", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProsoBeast Prosody Annotation Tool\\u00a7r\\n\\n\\u00a78\\u00a7oBranislav Gerazov\\nMichael Wagner\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02397\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 15 Jun 2021 07:40:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2021\\u00a7r"}']}
{title:'Karra et al. (§72021§r)', author: 'Kiran Karra; Alan McCree', display:{Lore:['[{"text": "arXiv:2104.02469", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Diarization using Two-pass Leave-One-Out Gaussian PLDA Clustering of DNN Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oKiran Karra\\nAlan McCree\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02469\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 14 Jun 2021 22:23:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, accepted at INTERSPEECH 2021\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Lin Zhang; Xin Wang; Erica Cooper; Junichi Yamagishi; Jose Patino; Nicholas Evans', display:{Lore:['[{"text": "arXiv:2104.02518", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Initial Investigation for Detecting Partially Spoofed Audio\\u00a7r\\n\\n\\u00a78\\u00a7oLin Zhang\\nXin Wang\\nErica Cooper\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02518\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 15 Jun 2021 15:41:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2021\\u00a7r"}']}
{title:'Mitrofanov et al. (§72021§r)', author: 'Anton Mitrofanov; Mariya Korenevskaya; Ivan Podluzhny; Yuri Khokhlov; Aleksandr Laptev; Andrei Andrusenko; Aleksei Ilin; Maxim Korenevsky; Ivan Medennikov; Aleksei Romanenko', display:{Lore:['[{"text": "arXiv:2104.02526", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLT-LM: a novel non-autoregressive language model for single-shot lattice rescoring\\u00a7r\\n\\n\\u00a78\\u00a7oAnton Mitrofanov\\nMariya Korenevskaya\\nIvan Podluzhny\\n+ 6 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02526\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Apr 2021 14:06:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to InterSpeech 2021\\u00a7r"}']}
{title:'Stafylakis et al. (§72021§r)', author: 'Themos Stafylakis; Johan Rohdin; Lukas Burget', display:{Lore:['[{"text": "arXiv:2104.02571", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker embeddings by modeling channel-wise correlations\\u00a7r\\n\\n\\u00a78\\u00a7oThemos Stafylakis\\nJohan Rohdin\\nLukas Burget\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02571\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 7 Jul 2021 14:00:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2021\\u00a7r"}']}
{title:'Nozaki et al. (§72021§r)', author: 'Jumon Nozaki; Tatsuya Komatsu', display:{Lore:['[{"text": "arXiv:2104.02724", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRelaxing the Conditional Independence Assumption of CTC-based ASR by Conditioning on Intermediate Predictions\\u00a7r\\n\\n\\u00a78\\u00a7oJumon Nozaki\\nTatsuya Komatsu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02724\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 8 Oct 2021 10:00:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH2021\\u00a7r"}']}
{title:'Lu et al. (§72021§r)', author: 'Zhiyun Lu; Wei Han; Yu Zhang; Liangliang Cao', display:{Lore:['[{"text": "arXiv:2104.02757", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Targeted Universal Adversarial Perturbations to End-to-end ASR Models\\u00a7r\\n\\n\\u00a78\\u00a7oZhiyun Lu\\nWei Han\\nYu Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02757\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Apr 2021 19:39:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Cornell et al. (§72021§r)', author: 'Samuele Cornell; Alessio Brutti; Marco Matassoni; Stefano Squartini', display:{Lore:['[{"text": "arXiv:2104.02819", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning to Rank Microphones for Distant Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSamuele Cornell\\nAlessio Brutti\\nMarco Matassoni\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02819\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 13 Apr 2021 21:19:23 GMT)\\u00a7r"}']}
{title:'Moritz et al. (§72021§r)', author: 'Niko Moritz; Takaaki Hori; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2104.02858", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCapturing Multi-Resolution Context by Dilated Self-Attention\\u00a7r\\n\\n\\u00a78\\u00a7oNiko Moritz\\nTakaaki Hori\\nJonathan Le Roux\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02858\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Apr 2021 02:04:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proc. ICASSP 2021\\u00a7r"}']}
{title:'Jung et al. (§72021§r)', author: 'Jee-weon Jung; Hee-Soo Heo; Youngki Kwon; Joon Son Chung; Bong-Jin Lee', display:{Lore:['[{"text": "arXiv:2104.02878", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThree-class Overlapped Speech Detection using a Convolutional Recurrent Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oJee-weon Jung\\nHee-Soo Heo\\nYoungki Kwon\\nJoon Son Chung\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02878\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Apr 2021 03:01:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 4 tables, submitted to Interspeech as a conference paper\\u00a7r"}']}
{title:'Kwon et al. (§72021§r)', author: 'Youngki Kwon; Jee-weon Jung; Hee-Soo Heo; You Jin Kim; Bong-Jin Lee; Joon Son Chung', display:{Lore:['[{"text": "arXiv:2104.02879", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdapting Speaker Embeddings for Speaker Diarisation\\u00a7r\\n\\n\\u00a78\\u00a7oYoungki Kwon\\nJee-weon Jung\\nHee-Soo Heo\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02879\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Apr 2021 03:04:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 3 tables, submitted to Interspeech as a conference paper\\u00a7r"}']}
{title:'Tian et al. (§72021§r)', author: 'Zhengkun Tian; Jiangyan Yi; Ye Bai; Jianhua Tao; Shuai Zhang; Zhengqi Wen', display:{Lore:['[{"text": "arXiv:2104.02882", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFSR: Accelerating the Inference Process of Transducer-Based Models by Applying Fast-Skip Regularization\\u00a7r\\n\\n\\u00a78\\u00a7oZhengkun Tian\\nJiangyan Yi\\nYe Bai\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02882\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Apr 2021 03:15:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH2021\\u00a7r"}']}
{title:'Lin et al. (§72021§r)', author: 'Jheng-hao Lin; Yist Y. Lin; Chung-Ming Chien; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2104.02901", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lS2VC: A Framework for Any-to-Any Voice Conversion with Self-Supervised Pretrained Representations\\u00a7r\\n\\n\\u00a78\\u00a7oJheng-hao Lin\\nYist Y. Lin\\nChung-Ming Chien\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02901\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 14 Jun 2021 11:10:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2021\\u00a7r"}']}
{title:'Lu et al. (§72021§r)', author: 'Xugang Lu; Peng Shen; Yu Tsao; Hisashi Kawai', display:{Lore:['[{"text": "arXiv:2104.03004", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSiamese Neural Network with Joint Bayesian Model Structure for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oXugang Lu\\nPeng Shen\\nYu Tsao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.03004\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Apr 2021 09:17:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2101.03329\\u00a7r"}']}
{title:'Hu et al. (§72021§r)', author: 'Cheng-Hung Hu; Yi-Chiao Wu; Wen-Chin Huang; Yu-Huai Peng; Yu-Wen Chen; Pin-Jui Ku; Tomoki Toda; Yu Tsao; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:2104.03009", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe AS-NU System for the M2VoC Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oCheng-Hung Hu\\nYi-Chiao Wu\\nWen-Chin Huang\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.03009\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Apr 2021 09:26:20 GMT)\\u00a7r"}']}
{title:'Tseng et al. (§72021§r)', author: 'Wei-Cheng Tseng; Chien-yu Huang; Wei-Tsung Kao; Yist Y. Lin; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2104.03017", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUtilizing Self-supervised Representations for MOS Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oWei-Cheng Tseng\\nChien-yu Huang\\nWei-Tsung Kao\\nYist Y. Lin\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.03017\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 20 Sep 2021 14:12:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of Interspeech 2021. We acknowledge the support of AWS MachineLearning Research Awards program. Source code available at https://github.com/s3prl/s3prl/tree/master/s3prl/downstream/mos_prediction\\u00a7r"}']}
{title:'Záviška et al. (§72021§r)', author: 'Pavel Záviška; Pavel Rajmic; Ondřej Mokrý', display:{Lore:['[{"text": "arXiv:2104.03074", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio declipping performance enhancement via crossfading\\u00a7r\\n\\n\\u00a78\\u00a7oPavel Z\\u00e1vi\\u0161ka\\nPavel Rajmic\\nOnd\\u0159ej Mokr\\u00fd\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.03074\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.sigpro.2021.108365\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nElsevier Signal Processing, vol. 192, March 2022, 108365\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Apr 2021 11:44:15 GMT)\\u00a7r"}']}
{title:'Ng et al. (§72021§r)', author: 'Edwin G. Ng; Chung-Cheng Chiu; Yu Zhang; William Chan', display:{Lore:['[{"text": "arXiv:2104.03416", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPushing the Limits of Non-Autoregressive Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oEdwin G. Ng\\nChung-Cheng Chiu\\nYu Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.03416\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-337\\u00a7r\\n\\nVersion:\\u00a77v4 (Sat, 11 Sep 2021 19:08:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH\\u00a7r"}']}
{title:'Tak et al. (§72021§r)', author: 'Hemlata Tak; Jee-weon Jung; Jose Patino; Massimiliano Todisco; Nicholas Evans', display:{Lore:['[{"text": "arXiv:2104.03654", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGraph Attention Networks for Anti-Spoofing\\u00a7r\\n\\n\\u00a78\\u00a7oHemlata Tak\\nJee-weon Jung\\nJose Patino\\nMassimiliano Todisco\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.03654\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Apr 2021 10:18:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Yajing Liu; Xiulian Peng; Zhiwei Xiong; Yan Lu', display:{Lore:['[{"text": "arXiv:2104.03759", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhoneme-based Distribution Regularization for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oYajing Liu\\nXiulian Peng\\nZhiwei Xiong\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.03759\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Apr 2021 13:21:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2021 (Accepted)\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Haoqi Li; Brian Baucom; Shrikanth Narayanan; Panayiotis Georgiou', display:{Lore:['[{"text": "arXiv:2104.03899", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Speech Representation Learning for Behavior Modeling using Triplet Enhanced Contextualized Networks\\u00a7r\\n\\n\\u00a78\\u00a7oHaoqi Li\\nBrian Baucom\\nShrikanth Narayanan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.03899\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Apr 2021 22:44:23 GMT)\\u00a7r"}']}
{title:'Bredin et al. (§72021§r)', author: 'Hervé Bredin; Antoine Laurent', display:{Lore:['[{"text": "arXiv:2104.04045", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end speaker segmentation for overlap-aware resegmentation\\u00a7r\\n\\n\\u00a78\\u00a7oHerv\\u00e9 Bredin\\nAntoine Laurent\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.04045\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 10 Jun 2021 13:51:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCamera-ready version for Interspeech 2021 with significantly better voice activity detection,overlapped speech detection, and speaker diarization results. Thecode used for results reported in v1 contained a small bug "}','{"text": "that has now been fixed\\u00a7r"}']}
{title:'Seneviratne et al. (§72021§r)', author: 'Nadee Seneviratne; Carol Espy-Wilson', display:{Lore:['[{"text": "arXiv:2104.04195", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech based Depression Severity Level Classification Using a Multi-Stage Dilated CNN-LSTM Model\\u00a7r\\n\\n\\u00a78\\u00a7oNadee Seneviratne\\nCarol Espy-Wilson\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.04195\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Apr 2021 05:10:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, submitted to Interspeech 2021. arXiv admin note: text overlap with arXiv:2011.06739\\u00a7r"}']}
{title:'Martin-Morato et al. (§72021§r)', author: 'Irene Martin-Morato; Annamaria Mesaros', display:{Lore:['[{"text": "arXiv:2104.04214", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhat is the ground truth? Reliability of multi-annotator data for audio tagging\\u00a7r\\n\\n\\u00a78\\u00a7oIrene Martin-Morato\\nAnnamaria Mesaros\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.04214\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Apr 2021 06:58:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to EUSIPCO 2021\\u00a7r"}']}
{title:'Chao et al. (§72021§r)', author: 'Fu-An Chao; Tien-Hong Lo; Shi-Yan Weng; Shih-Hsuan Chiu; Yao-Ting Sung; Berlin Chen', display:{Lore:['[{"text": "arXiv:2104.04221", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe NTNU Taiwanese ASR System for Formosa Speech Recognition Challenge 2020\\u00a7r\\n\\n\\u00a78\\u00a7oFu-An Chao\\nTien-Hong Lo\\nShi-Yan Weng\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.04221\\u00a7r\\n\\nVersion:\\u00a77v4 (Sat, 10 Jul 2021 03:59:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o17 pages, 3 figures, Accepted for publication in IJCLCLP\\u00a7r"}']}
{title:'Sinha et al. (§72021§r)', author: 'Ragini Sinha; Marvin Tammen; Christian Rollwage; Simon Doclo', display:{Lore:['[{"text": "arXiv:2104.04234", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker-conditioned Target Speaker Extraction based on Customized LSTM Cells\\u00a7r\\n\\n\\u00a78\\u00a7oRagini Sinha\\nMarvin Tammen\\nChristian Rollwage\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.04234\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Apr 2021 07:59:36 GMT)\\u00a7r"}']}
{title:'Vieting et al. (§72021§r)', author: 'Peter Vieting; Christoph Lüscher; Wilfried Michel; Ralf Schlüter; Hermann Ney', display:{Lore:['[{"text": "arXiv:2104.04298", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Architectures and Training for Raw Waveform Feature Extraction in ASR\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Vieting\\nChristoph L\\u00fcscher\\nWilfried Michel\\nRalf Schl\\u00fcter\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.04298\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 5 Oct 2021 14:25:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ASRU 2021\\u00a7r"}']}
{title:'Chu et al. (§72021§r)', author: 'Xiangyun Chu; Elizabeth Combs; Amber Wang; Michael Picheny', display:{Lore:['[{"text": "arXiv:2104.04627", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAccented Speech Recognition Inspired by Human Perception\\u00a7r\\n\\n\\u00a78\\u00a7oXiangyun Chu\\nElizabeth Combs\\nAmber Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.04627\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Apr 2021 22:35:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Yechen Wang; Yan Jia; Murong Ma; Zexin Cai; Ming Li', display:{Lore:['[{"text": "arXiv:2104.04993", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe DKU System Description for The Interspeech 2021 Auto-KWS Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oYechen Wang\\nYan Jia\\nMurong Ma\\nZexin Cai\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.04993\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 11 Apr 2021 11:05:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figures, submitted to INTERSPEECH\\u00a7r"}']}
{title:'Udupa et al. (§72021§r)', author: 'Sathvik Udupa; Anwesha Roy; Abhayjeet Singh; Aravind Illa; Prasanta Kumar Ghosh', display:{Lore:['[{"text": "arXiv:2104.05017", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEstimating articulatory movements in speech production with transformer networks\\u00a7r\\n\\n\\u00a78\\u00a7oSathvik Udupa\\nAnwesha Roy\\nAbhayjeet Singh\\nAravind Illa\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.05017\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 12 Jun 2021 08:56:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted for oral presentation at INTERSPEECH 2021\\u00a7r"}']}
{title:'Zhou et al. (§72021§r)', author: 'Liming Zhou; Yongyu Gao; Ziluo Wang; Jiwei Li; Wenbin Zhang', display:{Lore:['[{"text": "arXiv:2104.05267", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComplex Spectral Mapping With Attention Based Convolution Recurrent Neural Network for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oLiming Zhou\\nYongyu Gao\\nZiluo Wang\\nJiwei Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.05267\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 15 Apr 2021 12:32:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech2021 submitted\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Yukun Liu; Ta Li; Pengyuan Zhang; Yonghong Yan', display:{Lore:['[{"text": "arXiv:2104.05390", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Conformer-based End-to-End Speech Recognition Using Neural Architecture Search\\u00a7r\\n\\n\\u00a78\\u00a7oYukun Liu\\nTa Li\\nPengyuan Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.05390\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 13 Apr 2021 01:36:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Væhrens et al. (§72021§r)', author: 'Max Væhrens; Andreas Jonas Fuglsig; Anders Post Jacobsen; Nicolai Almskou Rasmussen; Victor Mølbach Nissen; Joachim Roland Hejslet; Zheng-Hua Tan', display:{Lore:['[{"text": "arXiv:2104.05481", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImprovement of Noise-Robust Single-Channel Voice Activity Detection with Spatial Pre-processing\\u00a7r\\n\\n\\u00a78\\u00a7oMax V\\u00e6hrens\\nAndreas Jonas Fuglsig\\nAnders Post Jacobsen\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.05481\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 12 Apr 2021 14:04:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2021\\u00a7r"}']}
{title:'Guizzo et al. (§72021§r)', author: 'Eric Guizzo; Riccardo F. Gramaccioni; Saeid Jamili; Christian Marinoni; Edoardo Massaro; Claudia Medaglia; Giuseppe Nachira; Leonardo Nucciarelli; Ludovica Paglialunga; Marco Pennese; Sveva Pepe; Enrico Rocchi; Aurelio Uncini; Danilo Comminiello', display:{Lore:['[{"text": "arXiv:2104.05499", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lL3DAS21 Challenge: Machine Learning for 3D Audio Signal Processing\\u00a7r\\n\\n\\u00a78\\u00a7oEric Guizzo\\nRiccardo F. Gramaccioni\\nSaeid Jamili\\n+ 10 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.05499\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/MLSP52302.2021.9596248\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2021 IEEE 31st International Workshop on Machine Learning for\\n  Signal Processing (MLSP), 2021, pp. 1-6\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 29 Apr 2021 18:18:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDocumentation paper for the L3DAS21 Challenge for IEEE MLSP 2021. Further information on www.l3das.com/mlsp2021\\u00a7r"}']}
{title:'Casanova et al. (§72021§r)', author: 'Edresson Casanova; Christopher Shulby; Eren Gölge; Nicolas Michael Müller; Frederico Santos de Oliveira; Arnaldo Candido Junior; Anderson da Silva Soares; Sandra Maria Aluisio; Moacir Antonelli Ponti', display:{Lore:['[{"text": "arXiv:2104.05557", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSC-GlowTTS: an Efficient Zero-Shot Multi-Speaker Text-To-Speech Model\\u00a7r\\n\\n\\u00a78\\u00a7oEdresson Casanova\\nChristopher Shulby\\nEren G\\u00f6lge\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.05557\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 15 Jun 2021 22:19:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted on Interspeech 2021\\u00a7r"}']}
{title:'Kim et al. (§72021§r)', author: 'Ju-ho Kim; Hye-jin Shim; Jee-weon Jung; Ha-Jin Yu', display:{Lore:['[{"text": "arXiv:2104.06604", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Metrics from Mean Teacher: A Supervised Learning Method for Improving the Generalization of Speaker Verification System\\u00a7r\\n\\n\\u00a78\\u00a7oJu-ho Kim\\nHye-jin Shim\\nJee-weon Jung\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.06604\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Apr 2021 03:06:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figures, 5 tables, submitted to 2021 Interspeech as a conference paper\\u00a7r"}']}
{title:'Leamy et al. (§72021§r)', author: 'Paul Leamy; Ted Burke; Dan Barry; David Dorran', display:{Lore:['[{"text": "arXiv:2104.06798", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-based cough counting using independent subspace analysis\\u00a7r\\n\\n\\u00a78\\u00a7oPaul Leamy\\nTed Burke\\nDan Barry\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.06798\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Apr 2021 12:03:28 GMT)\\u00a7r"}']}
{title:'Veillon et al. (§72021§r)', author: 'Clément Le Moine Veillon; Nicolas Obin; Axel Roebel', display:{Lore:['[{"text": "arXiv:2104.07283", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards end-to-end F0 voice conversion based on Dual-GAN with convolutional wavelet kernels\\u00a7r\\n\\n\\u00a78\\u00a7oCl\\u00e9ment Le Moine Veillon\\nNicolas Obin\\nAxel Roebel\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.07283\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Apr 2021 07:42:59 GMT)\\u00a7r"}']}
{title:'Moine et al. (§72021§r)', author: 'Clément Le Moine; Nicolas Obin; Axel Roebel', display:{Lore:['[{"text": "arXiv:2104.07288", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Attentive Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oCl\\u00e9ment Le Moine\\nNicolas Obin\\nAxel Roebel\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.07288\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Apr 2021 07:59:37 GMT)\\u00a7r"}']}
{title:'Neumann et al. (§72021§r)', author: 'Michael Neumann; Oliver Roesler; Jackson Liscombe; Hardik Kothare; David Suendermann-Oeft; David Pautler; Indu Navar; Aria Anvar; Jochen Kumm; Raquel Norel; Ernest Fraenkel; Alexander V. Sherman; James D. Berry; Gary L. Pattee; Jun Wang; Jordan R. Green; Vikram Ramanarayanan', display:{Lore:['[{"text": "arXiv:2104.07310", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating the Utility of Multimodal Conversational Technology and Audiovisual Analytic Measures for the Assessment and Monitoring of Amyotrophic Lateral Sclerosis at Scale\\u00a7r\\n\\n\\u00a78\\u00a7oMichael Neumann\\nOliver Roesler\\nJackson Liscombe\\n+ 13 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.07310\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Apr 2021 08:43:07 GMT)\\u00a7r"}']}
{title:'Madhu et al. (§72021§r)', author: 'Aswathy Madhu; Suresh K', display:{Lore:['[{"text": "arXiv:2104.07326", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnvGAN: Adversarial Synthesis of Environmental Sounds for Data Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oAswathy Madhu\\nSuresh K\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.07326\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Apr 2021 09:26:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEETransactions on Audio, Speech and Language Processing\\u00a7r"}']}
{title:'Zaiem et al. (§72021§r)', author: 'Salah Zaiem; Titouan Parcollet; Slim Essid', display:{Lore:['[{"text": "arXiv:2104.07388", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConditional independence for pretext task selection in Self-supervised speech representation learning\\u00a7r\\n\\n\\u00a78\\u00a7oSalah Zaiem\\nTitouan Parcollet\\nSlim Essid\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.07388\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-1027\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 1 Jul 2021 14:45:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, Accepted for presentation at Interspeech2021\\u00a7r"}']}
{title:'Baskar et al. (§72021§r)', author: 'Murali Karthick Baskar; Lukáš Burget; Shinji Watanabe; Ramon Fernandez Astudillo; Jan "Honza\'\' Černocký', display:{Lore:['[{"text": "arXiv:2104.07474", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEAT: Enhanced ASR-TTS for Self-supervised Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMurali Karthick Baskar\\nLuk\\u00e1\\u0161 Burget\\nShinji Watanabe\\nRamon Fernandez Astudillo\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.07474\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Apr 2021 23:18:25 GMT)\\u00a7r"}']}
{title:'Tura et al. (§72021§r)', author: 'Biel Tura; Santiago Escuder; Ferran Diego; Carlos Segura; Jordi Luque', display:{Lore:['[{"text": "arXiv:2104.08086", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Keyword Spotting by capturing long-range interactions with Temporal Lambda Networks\\u00a7r\\n\\n\\u00a78\\u00a7oBiel Tura\\nSantiago Escuder\\nFerran Diego\\nCarlos Segura\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.08086\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 1 Jul 2021 16:29:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ospeech recognition, keyword spotting, lambda networks\\u00a7r"}']}
{title:'Beliaev et al. (§72021§r)', author: 'Stanislav Beliaev; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2104.08189", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTalkNet 2: Non-Autoregressive Depth-Wise Separable Convolutional Model for Speech Synthesis with Explicit Pitch and Duration Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oStanislav Beliaev\\nBoris Ginsburg\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.08189\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 17 Jun 2021 19:28:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2005.05514\\u00a7r"}']}
{title:'Mussakhojayeva et al. (§72021§r)', author: 'Saida Mussakhojayeva; Aigerim Janaliyeva; Almas Mirzakhmetov; Yerbolat Khassanov; Huseyin Atakan Varol', display:{Lore:['[{"text": "arXiv:2104.08459", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKazakhTTS: An Open-Source Kazakh Text-to-Speech Synthesis Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oSaida Mussakhojayeva\\nAigerim Janaliyeva\\nAlmas Mirzakhmetov\\nYerbolat Khassanov\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.08459\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-2124\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 16 Jun 2021 09:36:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 tables, 2 figures, accepted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Haoyu Li; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2104.08499", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Metric Optimization using Generative Adversarial Networks for Near-End Speech Intelligibility Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oHaoyu Li\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.08499\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 16 Sep 2021 12:06:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEE/ACM Transactionson Audio Speech and Language Processing\\u00a7r"}']}
{title:'A. et al. (§72021§r)', author: 'Rohit M. A.; Preeti Rao', display:{Lore:['[{"text": "arXiv:2104.09064", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Stroke Classification of Tabla Accompaniment in Hindustani Vocal Concert Audio\\u00a7r\\n\\n\\u00a78\\u00a7oRohit M. A.\\nPreeti Rao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.09064\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Apr 2021 05:56:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in the JOURNAL OF ACOUSTICAL SOCIETYOF INDIA, April 2021\\u00a7r"}']}
{title:'Luz et al. (§72021§r)', author: 'Saturnino Luz; Fasih Haider; Sofia de la Fuente; Davida Fromm; Brian MacWhinney', display:{Lore:['[{"text": "arXiv:2104.09356", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetecting cognitive decline using speech only: The ADReSSo Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oSaturnino Luz\\nFasih Haider\\nSofia de la Fuente\\nDavida Fromm\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.09356\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Mar 2021 01:09:38 GMT)\\u00a7r"}']}
{title:'Singh et al. (§72021§r)', author: 'Prachi Singh; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:2104.09456", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervised Representation Learning With Path Integral Clustering For Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oPrachi Singh\\nSriram Ganapathy\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.09456\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3075100\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Apr 2021 17:13:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, Accepted in IEEETransactions on Audio, Speech and Language Processing\\u00a7r"}']}
{title:'Yu et al. (§72021§r)', author: 'Wentao Yu; Steffen Zeiler; Dorothea Kolossa', display:{Lore:['[{"text": "arXiv:2104.09482", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFusing information streams in end-to-end audio-visual speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oWentao Yu\\nSteffen Zeiler\\nDorothea Kolossa\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.09482\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPublished in International Conference on Acoustics, Speech and\\n  Signal Processing (ICASSP), 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Apr 2021 17:42:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Mittag et al. (§72021§r)', author: 'Gabriel Mittag; Babak Naderi; Assmaa Chehadi; Sebastian Möller', display:{Lore:['[{"text": "arXiv:2104.09494", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNISQA: A Deep CNN-Self-Attention Model for Multidimensional Speech Quality Prediction with Crowdsourced Datasets\\u00a7r\\n\\n\\u00a78\\u00a7oGabriel Mittag\\nBabak Naderi\\nAssmaa Chehadi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.09494\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-299\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Apr 2021 17:56:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2021\\u00a7r"}']}
{title:'Carmo et al. (§72021§r)', author: 'Diego M. Carmo; Ricardo Borsoi; Márcio H. Costa', display:{Lore:['[{"text": "arXiv:2104.09615", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust parameter design for Wiener-based binaural noise reduction methods in hearing aids\\u00a7r\\n\\n\\u00a78\\u00a7oDiego M. Carmo\\nRicardo Borsoi\\nM\\u00e1rcio H. Costa\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.09615\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Apr 2021 20:36:08 GMT)\\u00a7r"}']}
{title:'Yamamoto et al. (§72021§r)', author: 'Ayako Yamamoto; Toshio Irino; Kenichi Arai; Shoko Araki; Atsunori Ogawa; Keisuke Kinoshita; Tomohiro Nakatani', display:{Lore:['[{"text": "arXiv:2104.10001", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparison of remote experiments using crowdsourcing and laboratory experiments on speech intelligibility\\u00a7r\\n\\n\\u00a78\\u00a7oAyako Yamamoto\\nToshio Irino\\nKenichi Arai\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.10001\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-174\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 17 Apr 2021 02:00:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper was submitted to Interspeech2021\\u00a7r"}']}
{title:'Mittag et al. (§72021§r)', author: 'Gabriel Mittag; Saman Zadtootaghaj; Thilo Michael; Babak Naderi; Sebastian Möller', display:{Lore:['[{"text": "arXiv:2104.10217", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBias-Aware Loss for Training Image and Speech Quality Prediction Models from Multiple Datasets\\u00a7r\\n\\n\\u00a78\\u00a7oGabriel Mittag\\nSaman Zadtootaghaj\\nThilo Michael\\nBabak Naderi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.10217\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/QoMEX51781.2021.9465384\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Apr 2021 19:20:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at QoMEX 2021\\u00a7r"}']}
{title:'Kida et al. (§72021§r)', author: 'Yusuke Kida; Tatsuya Komatsu; Masahito Togami', display:{Lore:['[{"text": "arXiv:2104.10328", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLabel-Synchronous Speech-to-Text Alignment for ASR Using Forward and Backward Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oYusuke Kida\\nTatsuya Komatsu\\nMasahito Togami\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.10328\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Apr 2021 03:05:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Tang et al. (§72021§r)', author: 'Zhenyu Tang; Dinesh Manocha', display:{Lore:['[{"text": "arXiv:2104.10757", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScene-aware Far-field Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhenyu Tang\\nDinesh Manocha\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.10757\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Apr 2021 20:58:30 GMT)\\u00a7r"}']}
{title:'Huang et al. (§72021§r)', author: 'Lu Huang; Jingyu Sun; Yufeng Tang; Junfeng Hou; Jinkun Chen; Jun Zhang; Zejun Ma', display:{Lore:['[{"text": "arXiv:2104.10764", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHMM-Free Encoder Pre-Training for Streaming RNN Transducer\\u00a7r\\n\\n\\u00a78\\u00a7oLu Huang\\nJingyu Sun\\nYufeng Tang\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.10764\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 11 Jun 2021 03:11:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2021\\u00a7r"}']}
{title:'Yang et al. (§72021§r)', author: 'Yaogen Yang; Haozhe Zhang; Xiaoyi Qin; Shanshan Liang; Huahua Cui; Mingyang Xu; Ming Li', display:{Lore:['[{"text": "arXiv:2104.10832", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBuilding Bilingual and Code-Switched Voice Conversion with Limited Training Data Using Embedding Consistency Loss\\u00a7r\\n\\n\\u00a78\\u00a7oYaogen Yang\\nHaozhe Zhang\\nXiaoyi Qin\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.10832\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Apr 2021 02:43:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2021\\u00a7r"}']}
{title:'Tesch et al. (§72021§r)', author: 'Kristina Tesch; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2104.11033", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNonlinear Spatial Filtering in Multichannel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oKristina Tesch\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.11033\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3076372\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  Vol. 29, 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Apr 2021 13:07:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted version, 11 pages, 6 figures\\u00a7r"}']}
{title:'Niknazar et al. (§72021§r)', author: 'Mohammad Niknazar; Aditya Vempaty; Ravi Kokku', display:{Lore:['[{"text": "arXiv:2104.11038", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Privacy with Smart Digital Assistants in Educational Settings\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Niknazar\\nAditya Vempaty\\nRavi Kokku\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.11038\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Mar 2021 19:58:45 GMT)\\u00a7r"}']}
{title:'Bedyakin et al. (§72021§r)', author: 'Roman Bedyakin; Nikolay Mikhaylovskiy', display:{Lore:['[{"text": "arXiv:2104.11985", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLanguage ID Prediction from Speech Using Self-Attentive Pooling and 1D-Convolutions\\u00a7r\\n\\n\\u00a78\\u00a7oRoman Bedyakin\\nNikolay Mikhaylovskiy\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.11985\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.18653/v1/2021.sigtyp-1.12\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 24 Apr 2021 16:41:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to SYGTYP-2021\\u00a7r"}']}
{title:'Futamata et al. (§72021§r)', author: 'Kosuke Futamata; Byeongseon Park; Ryuichi Yamamoto; Kentaro Tachibana', display:{Lore:['[{"text": "arXiv:2104.12395", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhrase break prediction with bidirectional encoder representations in Japanese text-to-speech synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oKosuke Futamata\\nByeongseon Park\\nRyuichi Yamamoto\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.12395\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Apr 2021 08:29:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Sun et al. (§72021§r)', author: 'Jianwei Sun; Zhiyuan Tang; Hengxin Yin; Wei Wang; Xi Zhao; Shuaijiang Zhao; Xiaoning Lei; Wei Zou; Xiangang Li', display:{Lore:['[{"text": "arXiv:2104.12521", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemantic Data Augmentation for End-to-End Mandarin Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJianwei Sun\\nZhiyuan Tang\\nHengxin Yin\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.12521\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Apr 2021 12:43:46 GMT)\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Mohan Li; Catalin Zorila; Rama Doddipatla', display:{Lore:['[{"text": "arXiv:2104.12631", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHead-synchronous Decoding for Transformer-based Streaming ASR\\u00a7r\\n\\n\\u00a78\\u00a7oMohan Li\\nCatalin Zorila\\nRama Doddipatla\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.12631\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Apr 2021 14:57:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure\\u00a7r"}']}
{title:'Qiu et al. (§72021§r)', author: 'David Qiu; Yanzhang He; Qiujia Li; Yu Zhang; Liangliang Cao; Ian McGraw', display:{Lore:['[{"text": "arXiv:2104.12870", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Task Learning for End-to-End ASR Word and Utterance Confidence with Deletion Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Qiu\\nYanzhang He\\nQiujia Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.12870\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Apr 2021 20:38:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2021\\u00a7r"}']}
{title:'Kentgens et al. (§72021§r)', author: 'Maximilian Kentgens; Peter Jax', display:{Lore:['[{"text": "arXiv:2104.13069", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisualization of Linear Operations in the Spherical Harmonics Domain\\u00a7r\\n\\n\\u00a78\\u00a7oMaximilian Kentgens\\nPeter Jax\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.13069\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/I3DA48870.2021.9610968\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 29 Nov 2021 11:03:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPre-print/author version of paper presented at International Conference on Immersive and 3D Audio (I3DA), Sept. 2021\\u00a7r"}']}
{title:'Di Carlo et al. (§72021§r)', author: 'Diego Di Carlo; Pinchas Tandeitnik; Cédric Foy; Antoine Deleforge; Nancy Bertin; Sharon Gannot', display:{Lore:['[{"text": "arXiv:2104.13168", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7ldEchorate: a Calibrated Room Impulse Response Database for Echo-aware Signal Processing\\u00a7r\\n\\n\\u00a78\\u00a7oDiego Di Carlo\\nPinchas Tandeitnik\\nC\\u00e9dric Foy\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.13168\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Apr 2021 13:23:56 GMT)\\u00a7r"}']}
{title:'Pizzo et al. (§72021§r)', author: 'D. Trejo Pizzo; S. Esteban', display:{Lore:['[{"text": "arXiv:2104.13247", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIATos: AI-powered pre-screening tool for COVID-19 from cough audio samples\\u00a7r\\n\\n\\u00a78\\u00a7oD. Trejo Pizzo\\nS. Esteban\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.13247\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 9 Dec 2021 15:33:55 GMT)\\u00a7r"}']}
{title:'Pujol et al. (§72021§r)', author: 'Hadrien Pujol; Éric Bavu; Alexandre Garcia', display:{Lore:['[{"text": "arXiv:2104.13347", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBeamLearning: an end-to-end Deep Learning approach for the angular localization of sound sources using raw multichannel acoustic pressure data\\u00a7r\\n\\n\\u00a78\\u00a7oHadrien Pujol\\n\\u00c9ric Bavu\\nAlexandre Garcia\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.13347\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0005046\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJ. Acoust. Soc. Am. 149 (6), June 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Apr 2021 17:28:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe following article has been submitted to the special issue on Machine Learning in Acoustics in JASA. Afterit is published, it will be found at http://asa.scitation.org/journal/jas\\u00a7r"}']}
{title:'Copiaco et al. (§72021§r)', author: 'Abigail Copiaco; Christian Ritz; Stefano Fasciani; Nidhal Abdulaziz', display:{Lore:['[{"text": "arXiv:2104.13423", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDASEE A Synthetic Database of Domestic Acoustic Scenes and Events in Dementia Patients Environment\\u00a7r\\n\\n\\u00a78\\u00a7oAbigail Copiaco\\nChristian Ritz\\nStefano Fasciani\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.13423\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 13 May 2021 10:11:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, 6 tables\\u00a7r"}']}
{title:'Choi et al. (§72021§r)', author: 'Woosung Choi; Minseok Kim; Marco A. Martínez Ramírez; Jaehwa Chung; Soonyoung Jung', display:{Lore:['[{"text": "arXiv:2104.13553", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAMSS-Net: Audio Manipulation on User-Specified Sources with Textual Queries\\u00a7r\\n\\n\\u00a78\\u00a7oWoosung Choi\\nMinseok Kim\\nMarco A. Mart\\u00ednez Ram\\u00edrez\\nJaehwa Chung\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.13553\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Apr 2021 03:27:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 8 figures, 3 tables, under reviewingof ACMMM 21\\u00a7r"}']}
{title:'Abeßer et al. (§72021§r)', author: 'Jakob Abeßer; Saichand Gourishetti; András Kátai; Tobias Clauß; Prachi Sharma; Judith Liebetrau', display:{Lore:['[{"text": "arXiv:2104.13620", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIDMT-Traffic: An Open Benchmark Dataset for Acoustic Traffic Monitoring Research\\u00a7r\\n\\n\\u00a78\\u00a7oJakob Abe\\u00dfer\\nSaichand Gourishetti\\nAndr\\u00e1s K\\u00e1tai\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.13620\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Apr 2021 07:58:37 GMT)\\u00a7r"}']}
{title:'Rikhye et al. (§72021§r)', author: 'Rajeev Rikhye; Quan Wang; Qiao Liang; Yanzhang He; Ding Zhao; Yiteng; Huang; Arun Narayanan; Ian McGraw', display:{Lore:['[{"text": "arXiv:2104.13970", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPersonalized Keyphrase Detection using Speaker and Environment Information\\u00a7r\\n\\n\\u00a78\\u00a7oRajeev Rikhye\\nQuan Wang\\nQiao Liang\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.13970\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 15 Jun 2021 15:38:06 GMT)\\u00a7r"}']}
{title:'Saraswat et al. (§72021§r)', author: 'Vivek Saraswat; Ajinkya Gorad; Anand Naik; Aakash Patil; Udayan Ganguly', display:{Lore:['[{"text": "arXiv:2104.14264", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHardware-Friendly Synaptic Orders and Timescales in Liquid State Machines for Speech Classification\\u00a7r\\n\\n\\u00a78\\u00a7oVivek Saraswat\\nAjinkya Gorad\\nAnand Naik\\nAakash Patil\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.14264\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 29 Apr 2021 11:20:39 GMT)\\u00a7r"}']}
{title:'An et al. (§72021§r)', author: 'Keyu An; Yi Zhang; Zhijian Ou', display:{Lore:['[{"text": "arXiv:2104.14791", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeformable TDNN with adaptive receptive fields for speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oKeyu An\\nYi Zhang\\nZhijian Ou\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.14791\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Apr 2021 07:10:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages. submitted to Interspeech 2021\\u00a7r"}']}
{title:'Nguyen et al. (§72021§r)', author: 'Truc Nguyen; Franz Pernkopf', display:{Lore:['[{"text": "arXiv:2104.14921", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCrackle Detection In Lung Sounds Using Transfer Learning And Multi-Input Convolitional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oTruc Nguyen\\nFranz Pernkopf\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.14921\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Apr 2021 11:32:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder Review in Proceeding ofEMBC 2021\\u00a7r"}']}
{title:'Mittags et al. (§72021§r)', author: 'Gabriel Mittags; Sebastian Möller', display:{Lore:['[{"text": "arXiv:2105.00783", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFull-Reference Speech Quality Estimation with Attentional Siamese Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oGabriel Mittags\\nSebastian M\\u00f6ller\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.00783\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP40776.2020.9053951\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 3 May 2021 12:38:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oLate upload, presented at ICASSP 2020\\u00a7r"}']}
{title:'Zhao et al. (§72021§r)', author: 'Rui Zhao; Jian Xue; Jinyu Li; Wenning Wei; Lei He; Yifan Gong', display:{Lore:['[{"text": "arXiv:2105.00858", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Addressing Practical Challenges for RNN-Transducer\\u00a7r\\n\\n\\u00a78\\u00a7oRui Zhao\\nJian Xue\\nJinyu Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.00858\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 18 Jul 2021 18:30:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Hooper et al. (§72021§r)', author: 'Coleman Hooper; Thierry Tambe; Gu-Yeon Wei', display:{Lore:['[{"text": "arXiv:2105.01134", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQuantifying and Maximizing the Benefits of Back-End Noise Adaption on Attention-Based Speech Recognition Models\\u00a7r\\n\\n\\u00a78\\u00a7oColeman Hooper\\nThierry Tambe\\nGu-Yeon Wei\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.01134\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 23 Sep 2021 18:58:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ENLSP 2021\\u00a7r"}']}
{title:'Jaramillo et al. (§72021§r)', author: 'Alfredo Esquivel Jaramillo; Jesper Kjær Nielsen; Mads Græsbøll Christensen', display:{Lore:['[{"text": "arXiv:2105.01302", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Decomposition Based on a Hybrid Speech Model and Optimal Segmentation\\u00a7r\\n\\n\\u00a78\\u00a7oAlfredo Esquivel Jaramillo\\nJesper Kj\\u00e6r Nielsen\\nMads Gr\\u00e6sb\\u00f8ll Christensen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.01302\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 4 May 2021 05:39:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, Interspeech conference\\u00a7r"}']}
{title:'Dehghani et al. (§72021§r)', author: 'Arash Dehghani; Seyyed Ali Seyyedsalehi', display:{Lore:['[{"text": "arXiv:2105.01399", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerformance Evaluation of Deep Convolutional Maxout Neural Network in Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oArash Dehghani\\nSeyyed Ali Seyyedsalehi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.01399\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICBME.2018.8703593\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n25th National and 3rd International Iranian Conference on\\n  Biomedical Engineering (ICBME) (2018), pages: 6, SN: 1538679523, PB: IEEE\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 4 May 2021 10:19:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 2 figures, conference paper submitted to 2018 25th National and 3rd International Iranian Conference on Biomedical Engineering (ICBME)\\u00a7r"}']}
{title:'Williams et al. (§72021§r)', author: 'Jennifer Williams; Jason Fong; Erica Cooper; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2105.01573", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Disentanglement with Multilingual and Monolingual VQ-VAE\\u00a7r\\n\\n\\u00a78\\u00a7oJennifer Williams\\nJason Fong\\nErica Cooper\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.01573\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 28 Jun 2021 16:27:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Speech Synthesis Workshop 2021 (SSW11)\\u00a7r"}']}
{title:'Glarner et al. (§72021§r)', author: 'Thomas Glarner; Janek Ebbers; Reinhold Häb-Umbach', display:{Lore:['[{"text": "arXiv:2105.01786", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Conversion Based Speaker Normalization for Acoustic Unit Discovery\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Glarner\\nJanek Ebbers\\nReinhold H\\u00e4b-Umbach\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.01786\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 4 May 2021 22:40:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2021\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Zhan Zhang; Xi Chen; Yuehai Wang; Jianyi Yang', display:{Lore:['[{"text": "arXiv:2105.01920", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAccent Recognition with Hybrid Phonetic Features\\u00a7r\\n\\n\\u00a78\\u00a7oZhan Zhang\\nXi Chen\\nYuehai Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.01920\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 5 May 2021 08:12:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review for Speech Communication\\u00a7r"}']}
{title:'Das et al. (§72021§r)', author: 'Sneha Das; Nicole Nadine Lønfeldt; Anne Katrine Pagsberg; Line H. Clemmensen', display:{Lore:['[{"text": "arXiv:2105.02055", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Interpretable and Transferable Speech Emotion Recognition: Latent Representation Based Analysis of Features, Methods and Corpora\\u00a7r\\n\\n\\u00a78\\u00a7oSneha Das\\nNicole Nadine L\\u00f8nfeldt\\nAnne Katrine Pagsberg\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.02055\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 5 May 2021 13:47:39 GMT)\\u00a7r"}']}
{title:'Subramani et al. (§72021§r)', author: 'Krishna Subramani; Paris Smaragdis', display:{Lore:['[{"text": "arXiv:2105.02469", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPoint Cloud Audio Processing\\u00a7r\\n\\n\\u00a78\\u00a7oKrishna Subramani\\nParis Smaragdis\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.02469\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/WASPAA52581.2021.9632668\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 29 Jul 2021 06:32:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at WASPAA 2021, Code: https://github.com/SubramaniKrishna/point-cloud-audio\\u00a7r"}']}
{title:'Abeßer (§72021§r)', author: 'Jakob Abeßer', display:{Lore:['[{"text": "arXiv:2105.02592", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUSM-SED - A Dataset for Polyphonic Sound Event Detection in Urban Sound Monitoring Scenarios\\u00a7r\\n\\n\\u00a78\\u00a7oJakob Abe\\u00dfer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.02592\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 6 May 2021 11:45:47 GMT)\\u00a7r"}']}
{title:'Cramer et al. (§72021§r)', author: 'Aurora Cramer; Mark Cartwright; Fatemeh Pishdadian; Juan Pablo Bello', display:{Lore:['[{"text": "arXiv:2105.02911", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeakly Supervised Source-Specific Sound Level Estimation in Noisy Soundscapes\\u00a7r\\n\\n\\u00a78\\u00a7oAurora Cramer\\nMark Cartwright\\nFatemeh Pishdadian\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.02911\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 29 Jul 2021 20:42:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, WASPAA 2021 preprint\\u00a7r"}']}
{title:'Haubner et al. (§72021§r)', author: 'Thomas Haubner; Andreas Brendel; Walter Kellermann', display:{Lore:['[{"text": "arXiv:2105.03337", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOnline Acoustic System Identification Exploiting Kalman Filtering and an Adaptive Impulse Response Subspace Model\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Haubner\\nAndreas Brendel\\nWalter Kellermann\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.03337\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.13140/RG.2.2.23838.46408\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 7 May 2021 15:41:55 GMT)\\u00a7r"}']}
{title:'Sivaraman et al. (§72021§r)', author: 'Aswin Sivaraman; Minje Kim', display:{Lore:['[{"text": "arXiv:2105.03542", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero-Shot Personalized Speech Enhancement through Speaker-Informed Model Selection\\u00a7r\\n\\n\\u00a78\\u00a7oAswin Sivaraman\\nMinje Kim\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.03542\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 8 May 2021 00:15:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, submitted to 2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)\\u00a7r"}']}
{title:'Kim et al. (§72021§r)', author: 'Sunwoo Kim; Minje Kim', display:{Lore:['[{"text": "arXiv:2105.03544", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTest-Time Adaptation Toward Personalized Speech Enhancement: Zero-Shot Learning with Knowledge Distillation\\u00a7r\\n\\n\\u00a78\\u00a7oSunwoo Kim\\nMinje Kim\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.03544\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 8 May 2021 00:42:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, underreview\\u00a7r"}']}
{title:'Lin et al. (§72021§r)', author: 'Ziheng Lin; Yanxiong Li; Zhangjin Huang; Wenhao Zhang; Yufeng Tan; Yichun Chen; Qianhua He', display:{Lore:['[{"text": "arXiv:2105.03583", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDomestic activities clustering from audio recordings using convolutional capsule autoencoder network\\u00a7r\\n\\n\\u00a78\\u00a7oZiheng Lin\\nYanxiong Li\\nZhangjin Huang\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.03583\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 8 May 2021 03:49:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 5 tables, Accepted by IEEE ICASSP 2021\\u00a7r"}']}
{title:'He et al. (§72021§r)', author: 'Liqiang He; Shulin Feng; Dan Su; Dong Yu', display:{Lore:['[{"text": "arXiv:2105.03643", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLatency-Controlled Neural Architecture Search for Streaming Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oLiqiang He\\nShulin Feng\\nDan Su\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.03643\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 14 Sep 2021 03:49:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ASRU 2021\\u00a7r"}']}
{title:'Rouvier et al. (§72021§r)', author: 'Mickael Rouvier; Pierre-Michel Bousquet; Jarod Duret', display:{Lore:['[{"text": "arXiv:2105.04310", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStudy on the temporal pooling used in deep neural networks for speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oMickael Rouvier\\nPierre-Michel Bousquet\\nJarod Duret\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.04310\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 May 2021 12:44:57 GMT)\\u00a7r"}']}
{title:'Dey et al. (§72021§r)', author: 'Spandan Dey; Goutam Saha; Md Sahidullah', display:{Lore:['[{"text": "arXiv:2105.04639", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-Corpora Language Recognition: A Preliminary Investigation with Indian Languages\\u00a7r\\n\\n\\u00a78\\u00a7oSpandan Dey\\nGoutam Saha\\nMd Sahidullah\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.04639\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 12 May 2021 06:07:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in EUSIPCO 2021 : European Signal Processing Conference\\u00a7r"}']}
{title:'Ramírez et al. (§72021§r)', author: 'Marco A. Martínez Ramírez; Oliver Wang; Paris Smaragdis; Nicholas J. Bryan', display:{Lore:['[{"text": "arXiv:2105.04752", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDifferentiable Signal Processing With Black-Box Audio Effects\\u00a7r\\n\\n\\u00a78\\u00a7oMarco A. Mart\\u00ednez Ram\\u00edrez\\nOliver Wang\\nParis Smaragdis\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.04752\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 May 2021 02:20:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at theIEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), June 2021. Source code, demo and audio examples: https://mchijmma.github.io/DeepAFx/\\u00a7r"}']}
{title:'Singh et al. (§72021§r)', author: 'Premjeet Singh; Goutam Saha; Md Sahidullah', display:{Lore:['[{"text": "arXiv:2105.04806", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep scattering network for speech emotion recognition\\u00a7r\\n\\n\\u00a78\\u00a7oPremjeet Singh\\nGoutam Saha\\nMd Sahidullah\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.04806\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 May 2021 06:37:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, Accepted for publication in 2021 European Signal Processing Conference (EUSIPCO 2021)\\u00a7r"}']}
{title:'Kim et al. (§72021§r)', author: 'Jaeyoung Kim; Han Lu; Anshuman Tripathi; Qian Zhang; Hasim Sak', display:{Lore:['[{"text": "arXiv:2105.05005", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReducing Streaming ASR Model Delay with Self Alignment\\u00a7r\\n\\n\\u00a78\\u00a7oJaeyoung Kim\\nHan Lu\\nAnshuman Tripathi\\nQian Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.05005\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 6 May 2021 18:00:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Cámbara et al. (§72021§r)', author: 'Guillermo Cámbara; Alex Peiró-Lilja; Mireia Farrús; Jordi Luque', display:{Lore:['[{"text": "arXiv:2105.05041", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnglish Accent Accuracy Analysis in a State-of-the-Art Automatic Speech Recognition System\\u00a7r\\n\\n\\u00a78\\u00a7oGuillermo C\\u00e1mbara\\nAlex Peir\\u00f3-Lilja\\nMireia Farr\\u00fas\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.05041\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 9 May 2021 08:24:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2 pages, 1 figure, 1 table. To be published in Phonetics and Phonology in Europe 2021\\u00a7r"}']}
{title:'Sheikh et al. (§72021§r)', author: 'Shakeel A. Sheikh; Md Sahidullah; Fabrice Hirsch; Slim Ouni', display:{Lore:['[{"text": "arXiv:2105.05599", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStutterNet: Stuttering Detection Using Time Delay Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oShakeel A. Sheikh\\nMd Sahidullah\\nFabrice Hirsch\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.05599\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 8 Jun 2021 09:44:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in EUSIPCO 2021: European Signal Processing Conference\\u00a7r"}']}
{title:'Pulugundla et al. (§72021§r)', author: 'Bhargav Pulugundla; Yang Gao; Brian King; Gokce Keskin; Harish Mallidi; Minhua Wu; Jasha Droppo; Roland Maas', display:{Lore:['[{"text": "arXiv:2105.05920", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention-based Neural Beamforming Layers for Multi-channel Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oBhargav Pulugundla\\nYang Gao\\nBrian King\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.05920\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 14 May 2021 21:23:51 GMT)\\u00a7r"}']}
{title:'Garg et al. (§72021§r)', author: 'Vineet Garg; Wonil Chang; Siddharth Sigtia; Saurabh Adya; Pramod Simha; Pranay Dighe; Chandra Dhir', display:{Lore:['[{"text": "arXiv:2105.06598", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreaming Transformer for Hardware Efficient Voice Trigger Detection and False Trigger Mitigation\\u00a7r\\n\\n\\u00a78\\u00a7oVineet Garg\\nWonil Chang\\nSiddharth Sigtia\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.06598\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 May 2021 00:41:42 GMT)\\u00a7r"}']}
{title:'Accou et al. (§72021§r)', author: 'Bernd Accou; Mohammad Jalilpour Monesi; Hugo Van hamme; Tom Francart', display:{Lore:['[{"text": "arXiv:2105.06844", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPredicting speech intelligibility from EEG in a non-linear classification paradigm\\u00a7r\\n\\n\\u00a78\\u00a7oBernd Accou\\nMohammad Jalilpour Monesi\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.06844\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1088/1741-2552/ac33e9\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 25 Oct 2021 16:21:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 12 figures\\u00a7r"}']}
{title:'Benois et al. (§72021§r)', author: 'Piero Rivera Benois; Reinhild Roden; Matthias Blau; Simon Doclo', display:{Lore:['[{"text": "arXiv:2105.06894", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7acs.SY\\u00a7r, \\u00a7eeess.SY\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Pressure Minimization at the Ear Drum for In-ear ANC Headphones using a Fixed Feedforward Remote Microphone Technique\\u00a7r\\n\\n\\u00a78\\u00a7oPiero Rivera Benois\\nReinhild Roden\\nMatthias Blau\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.06894\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 May 2021 15:29:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 5 figures, to be published in proceedings of the 27th International Congress on Sound andVibration\\u00a7r"}']}
{title:'Ray et al. (§72021§r)', author: 'Swayambhu Nath Ray; Minhua Wu; Anirudh Raju; Pegah Ghahremani; Raghavendra Bilgi; Milind Rao; Harish Arsikere; Ariya Rastrow; Andreas Stolcke; Jasha Droppo', display:{Lore:['[{"text": "arXiv:2105.07071", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lListen with Intent: Improving Speech Recognition with Audio-to-Intent Front-End\\u00a7r\\n\\n\\u00a78\\u00a7oSwayambhu Nath Ray\\nMinhua Wu\\nAnirudh Raju\\n+ 6 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.07071\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-836\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech, Sept. 2021, pp. 3455-3459\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 16 Jun 2021 19:19:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Interspeech 2021\\u00a7r"}']}
{title:'Kim et al. (§72021§r)', author: 'Myungjong Kim; Vijendra Raj Apsingekar; Divya Neelagiri', display:{Lore:['[{"text": "arXiv:2105.07367", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lX-Vectors with Multi-Scale Aggregation for Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oMyungjong Kim\\nVijendra Raj Apsingekar\\nDivya Neelagiri\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.07367\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 16 May 2021 06:23:36 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72021§r)', author: 'Jun Yang; Nico Brailovsky', display:{Lore:['[{"text": "arXiv:2105.07632", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDual-Stage Low-Complexity Reconfigurable Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oJun Yang\\nNico Brailovsky\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.07632\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 May 2021 06:43:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Ma et al. (§72021§r)', author: 'Fei Ma; Thushara D. Abhayapala; Prasanga N. Samarasinghe', display:{Lore:['[{"text": "arXiv:2105.08219", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA time-domain nearfield frequency-invariant beamforming method\\u00a7r\\n\\n\\u00a78\\u00a7oFei Ma\\nThushara D. Abhayapala\\nPrasanga N. Samarasinghe\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.08219\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 May 2021 01:22:11 GMT)\\u00a7r"}']}
{title:'Katthi et al. (§72021§r)', author: 'Jaswanth Reddy Katthi; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:2105.08492", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Correlation Analysis for Audio-EEG Decoding\\u00a7r\\n\\n\\u00a78\\u00a7oJaswanth Reddy Katthi\\nSriram Ganapathy\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.08492\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 27 Nov 2021 17:31:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oGot accepted to IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING\\u00a7r"}']}
{title:'Brazier et al. (§72021§r)', author: 'Charles Brazier; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2105.08531", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHandling Structural Mismatches in Real-time Opera Tracking\\u00a7r\\n\\n\\u00a78\\u00a7oCharles Brazier\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.08531\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 May 2021 14:00:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, In Proceedings of the 29th European Signal Processing Conference (EUSIPCO 2020), Dublin, Ireland\\u00a7r"}']}
{title:'Carbajal et al. (§72021§r)', author: 'Guillaume Carbajal; Julius Richter; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2105.08970", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDisentanglement Learning for Variational Autoencoders Applied to Audio-Visual Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oGuillaume Carbajal\\nJulius Richter\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.08970\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/WASPAA52581.2021.9632676\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2021 IEEE Workshop on Applications of Signal Processing to Audio\\n  and Acoustics (WASPAA)\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 3 Aug 2021 07:25:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2102.06454\\u00a7r"}']}
{title:'Kinoshita et al. (§72021§r)', author: 'Keisuke Kinoshita; Marc Delcroix; Naohiro Tawara', display:{Lore:['[{"text": "arXiv:2105.09040", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdvances in integration of end-to-end neural and clustering-based diarization for real conversational speech\\u00a7r\\n\\n\\u00a78\\u00a7oKeisuke Kinoshita\\nMarc Delcroix\\nNaohiro Tawara\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.09040\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 31 Aug 2021 10:29:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, Interspeech2021. (Update to include a reference to the code)\\u00a7r"}']}
{title:'Oneata et al. (§72021§r)', author: 'Dan Oneata; Adriana Stan; Horia Cucu', display:{Lore:['[{"text": "arXiv:2105.09652", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker disentanglement in video-to-speech conversion\\u00a7r\\n\\n\\u00a78\\u00a7oDan Oneata\\nAdriana Stan\\nHoria Cucu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.09652\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 20 May 2021 10:31:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proc of EUSIPCO 2021\\u00a7r"}']}
{title:'Saito et al. (§72021§r)', author: 'Koichi Saito; Stefan Uhlich; Giorgio Fabbro; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2105.12315", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTraining Speech Enhancement Systems with Noisy Speech Datasets\\u00a7r\\n\\n\\u00a78\\u00a7oKoichi Saito\\nStefan Uhlich\\nGiorgio Fabbro\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.12315\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 May 2021 03:32:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, submitted to WASPAA2021\\u00a7r"}']}
{title:'Carvalho et al. (§72021§r)', author: 'Luis Carvalho; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2105.12536", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting Temporal Dependencies for Cross-Modal Music Piece Identification\\u00a7r\\n\\n\\u00a78\\u00a7oLuis Carvalho\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.12536\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 29th European Signal Processing Conference\\n  (EUSIPCO 2021), Dublin, Ireland\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 May 2021 13:21:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Torcoli (§72021§r)', author: 'Matteo Torcoli', display:{Lore:['[{"text": "arXiv:2105.13079", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Improved Measure of Musical Noise Based on Spectral Kurtosis\\u00a7r\\n\\n\\u00a78\\u00a7oMatteo Torcoli\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.13079\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/WASPAA.2019.8937195\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2019 IEEE Workshop on Applications of Signal Processing to Audio\\n  and Acoustics (WASPAA), 2019, pp. 90-94\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 May 2021 12:03:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oManuscript accepted for the 2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Shanshan Wang; Toni Heittola; Annamaria Mesaros; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2105.13675", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-visual scene classification: analysis of DCASE 2021 Challenge submissions\\u00a7r\\n\\n\\u00a78\\u00a7oShanshan Wang\\nToni Heittola\\nAnnamaria Mesaros\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.13675\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 20 Jul 2021 11:09:16 GMT)\\u00a7r"}']}
{title:'Martín-Morató et al. (§72021§r)', author: 'Irene Martín-Morató; Toni Heittola; Annamaria Mesaros; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2105.13734", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-complexity acoustic scene classification for multi-device audio: analysis of DCASE 2021 Challenge systems\\u00a7r\\n\\n\\u00a78\\u00a7oIrene Mart\\u00edn-Morat\\u00f3\\nToni Heittola\\nAnnamaria Mesaros\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.13734\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 20 Jul 2021 13:00:41 GMT)\\u00a7r"}']}
{title:'Chinaev et al. (§72021§r)', author: 'Aleksej Chinaev; Sven Wienand; Gerald Enzner', display:{Lore:['[{"text": "arXiv:2105.13743", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SY\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a7eeess.SY\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lControl Architecture of the Double-Cross-Correlation Processor for Sampling-Rate-Offset Estimation in Acoustic Sensor Networks\\u00a7r\\n\\n\\u00a78\\u00a7oAleksej Chinaev\\nSven Wienand\\nGerald Enzner\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.13743\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9413768\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 May 2021 11:13:35 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Songxiang Liu; Yuewen Cao; Dan Su; Helen Meng', display:{Lore:['[{"text": "arXiv:2105.13871", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffSVC: A Diffusion Probabilistic Model for Singing Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oSongxiang Liu\\nYuewen Cao\\nDan Su\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.13871\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 May 2021 14:26:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint. 8 pages, 2 figures and 1 table\\u00a7r"}']}
{title:'Chu et al. (§72021§r)', author: 'Kevin M. Chu; Leslie M. Collins; Boyla O. Mainsah', display:{Lore:['[{"text": "arXiv:2105.14120", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAssessing the intelligibility of vocoded speech using a remote testing framework\\u00a7r\\n\\n\\u00a78\\u00a7oKevin M. Chu\\nLeslie M. Collins\\nBoyla O. Mainsah\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.14120\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 May 2021 21:53:52 GMT)\\u00a7r"}']}
{title:'Chu et al. (§72021§r)', author: 'Kevin M. Chu; Leslie M. Collins; Boyla O. Mainsah', display:{Lore:['[{"text": "arXiv:2105.14135", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhoneme-Based Ratio Mask Estimation for Reverberant Speech Enhancement in Cochlear Implant Processors\\u00a7r\\n\\n\\u00a78\\u00a7oKevin M. Chu\\nLeslie M. Collins\\nBoyla O. Mainsah\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.14135\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 May 2021 23:02:14 GMT)\\u00a7r"}']}
{title:'Manocha et al. (§72021§r)', author: 'Pranay Manocha; Anurag Kumar; Buye Xu; Anjali Menon; Israel D. Gebru; Vamsi K. Ithapu; Paul Calamia', display:{Lore:['[{"text": "arXiv:2105.14180", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDPLM: A Deep Perceptual Spatial-Audio Localization Metric\\u00a7r\\n\\n\\u00a78\\u00a7oPranay Manocha\\nAnurag Kumar\\nBuye Xu\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.14180\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/WASPAA52581.2021.9632781\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 29 May 2021 02:38:31 GMT)\\u00a7r"}']}
{title:'Fang et al. (§72021§r)', author: 'Hao Fang; Chen Gong; Chen Zhang; Yanan Sui; Luming Li', display:{Lore:['[{"text": "arXiv:2105.14704", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParkinsonian Chinese Speech Analysis towards Automatic Classification of Parkinson\'s Disease\\u00a7r\\n\\n\\u00a78\\u00a7oHao Fang\\nChen Gong\\nChen Zhang\\nYanan Sui\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.14704\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 May 2021 04:51:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 5 figures, proceedings of the Machine Learning for Health NeurIPS Workshop, PMLR 136:114-125, 2020\\u00a7r"}']}
{title:'Eshky et al. (§72021§r)', author: 'Aciel Eshky; Joanne Cleland; Manuel Sam Ribeiro; Eleanor Sugden; Korin Richmond; Steve Renals', display:{Lore:['[{"text": "arXiv:2105.15162", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic audiovisual synchronisation for ultrasound tongue imaging\\u00a7r\\n\\n\\u00a78\\u00a7oAciel Eshky\\nJoanne Cleland\\nManuel Sam Ribeiro\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.15162\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.specom.2021.05.008\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 May 2021 17:11:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages, 10 figures. Manuscript accepted atSpeech Communication\\u00a7r"}']}
{title:'Baas et al. (§72021§r)', author: 'Matthew Baas; Herman Kamper', display:{Lore:['[{"text": "arXiv:2106.00043", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStarGAN-ZSVC: Towards Zero-Shot Voice Conversion in Low-Resource Contexts\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew Baas\\nHerman Kamper\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.00043\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-030-66151-9_5\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIn: Springer Communications in Computer and Information Science,\\n  Artificial Intelligence Research (SACAIR 2021), vol. 1342, pp. 69-84, 2020\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 May 2021 18:21:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages, 3 figures. Published in Springer Communications in Computer and Information Science, Artificial Intelligence Research (SACAIR 2021), vol. 1342, pp. 69-84, 2020\\u00a7r"}']}
{title:'Bedyakin et al. (§72021§r)', author: 'Roman Bedyakin; Nikolay Mikhaylovskiy', display:{Lore:['[{"text": "arXiv:2106.00052", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Resource Spoken Language Identification Using Self-Attentive Pooling and Deep 1D Time-Channel Separable Convolutions\\u00a7r\\n\\n\\u00a78\\u00a7oRoman Bedyakin\\nNikolay Mikhaylovskiy\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.00052\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 May 2021 18:35:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Dialog2021. arXiv admin note: text overlap with arXiv:2104.11985\\u00a7r"}']}
{title:'Janbakhshi et al. (§72021§r)', author: 'Parvaneh Janbakhshi; Ina Kodrasi', display:{Lore:['[{"text": "arXiv:2106.00531", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSupervised Speech Representation Learning for Parkinson\'s Disease Classification\\u00a7r\\n\\n\\u00a78\\u00a7oParvaneh Janbakhshi\\nIna Kodrasi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.00531\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 21 Jul 2021 13:12:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ITG Conference on Speech Communication 2021\\u00a7r"}']}
{title:'Chetupalli et al. (§72021§r)', author: 'Srikanth Raj Chetupalli; Prashant Krishnan; Neeraj Sharma; Ananya Muguli; Rohit Kumar; Viral Nanda; Lancelot Mark Pinto; Prasanta Kumar Ghosh; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:2106.00639", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-modal Point-of-Care Diagnostics for COVID-19 Based On Acoustics and Symptoms\\u00a7r\\n\\n\\u00a78\\u00a7oSrikanth Raj Chetupalli\\nPrashant Krishnan\\nNeeraj Sharma\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.00639\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 5 Jun 2021 07:27:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe Manuscript is submitted to IEEE-EMBS Journal of Biomedical and HealthInformatics on June 1, 2021\\u00a7r"}']}
{title:'Wisdom et al. (§72021§r)', author: 'Scott Wisdom; Aren Jansen; Ron J. Weiss; Hakan Erdogan; John R. Hershey', display:{Lore:['[{"text": "arXiv:2106.00847", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSparse, Efficient, and Semantic Mixture Invariant Training: Taming In-the-Wild Unsupervised Sound Separation\\u00a7r\\n\\n\\u00a78\\u00a7oScott Wisdom\\nAren Jansen\\nRon J. Weiss\\nHakan Erdogan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.00847\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 16 Oct 2021 21:55:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure. WASPAA2021\\u00a7r"}']}
{title:'Howard et al. (§72021§r)', author: 'Nathan Howard; Alex Park; Turaj Zakizadeh Shabestary; Alexander Gruenstein; Rohit Prabhavalkar', display:{Lore:['[{"text": "arXiv:2106.00856", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Neural Acoustic Echo Canceller Optimized Using An Automatic Speech Recognizer And Large Scale Synthetic Data\\u00a7r\\n\\n\\u00a78\\u00a7oNathan Howard\\nAlex Park\\nTuraj Zakizadeh Shabestary\\nAlexander Gruenstein\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.00856\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Jun 2021 23:39:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP 2021\\u00a7r"}']}
{title:'Sato et al. (§72021§r)', author: 'Hiroshi Sato; Tsubasa Ochiai; Marc Delcroix; Keisuke Kinoshita; Takafumi Moriya; Naoyuki Kamo', display:{Lore:['[{"text": "arXiv:2106.00949", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lShould We Always Separate?: Switching Between Enhanced and Observed Signals for Overlapping Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHiroshi Sato\\nTsubasa Ochiai\\nMarc Delcroix\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.00949\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-2253\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nin Proc. Interspeech 2021, 1149-1153\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Jun 2021 05:31:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure\\u00a7r"}']}
{title:'Kumar et al. (§72021§r)', author: 'Mari Ganesh Kumar; Jom Kuriakose; Anand Thyagachandran; Arun Kumar A; Ashish Seth; Lodagala Durga Prasad; Saish Jaiswal; Anusha Prakash; Hema Murthy', display:{Lore:['[{"text": "arXiv:2106.01400", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDual Script E2E framework for Multilingual and Code-Switching ASR\\u00a7r\\n\\n\\u00a78\\u00a7oMari Ganesh Kumar\\nJom Kuriakose\\nAnand Thyagachandran\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.01400\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Jun 2021 18:08:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at Interspeech 2021\\u00a7r"}']}
{title:'Lorincz et al. (§72021§r)', author: 'Beata Lorincz; Adriana Stan; Mircea Giurgiu', display:{Lore:['[{"text": "arXiv:2106.01789", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker verification-derived loss and data augmentation for DNN-based multispeaker speech synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oBeata Lorincz\\nAdriana Stan\\nMircea Giurgiu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.01789\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 3 Jun 2021 12:22:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at EUSIPCO 2021\\u00a7r"}']}
{title:'Lorincz et al. (§72021§r)', author: 'Beata Lorincz; Adriana Stan; Mircea Giurgiu', display:{Lore:['[{"text": "arXiv:2106.01812", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn objective evaluation of the effects of recording conditions and speaker characteristics in multi-speaker deep neural speech synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oBeata Lorincz\\nAdriana Stan\\nMircea Giurgiu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.01812\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 3 Jun 2021 13:04:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at 25thInternational Conference on Knowledge-Based and Intelligent Information Engineering Systems (KES 2021)\\u00a7r"}']}
{title:'Gaddy et al. (§72021§r)', author: 'David Gaddy; Dan Klein', display:{Lore:['[{"text": "arXiv:2106.01933", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Improved Model for Voicing Silent Speech\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Gaddy\\nDan Klein\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.01933\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 21 Jun 2021 15:36:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oACL 2021\\u00a7r"}']}
{title:'Bhati et al. (§72021§r)', author: 'Saurabhchand Bhati; Jesús Villalba; Piotr Żelasko; Laureano Moro-Velazquez; Najim Dehak', display:{Lore:['[{"text": "arXiv:2106.02170", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSegmental Contrastive Predictive Coding for Unsupervised Word Segmentation\\u00a7r\\n\\n\\u00a78\\u00a7oSaurabhchand Bhati\\nJes\\u00fas Villalba\\nPiotr \\u017belasko\\nLaureano Moro-Velazquez\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.02170\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 3 Jun 2021 23:12:05 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72021§r)', author: 'Ji-Hoon Kim; Sang-Hoon Lee; Ji-Hyun Lee; Seong-Whan Lee', display:{Lore:['[{"text": "arXiv:2106.02297", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFre-GAN: Adversarial Frequency-consistent Audio Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oJi-Hoon Kim\\nSang-Hoon Lee\\nJi-Hyun Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.02297\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 14 Jun 2021 08:07:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted paper in Interspeech 2021\\u00a7r"}']}
{title:'Meng et al. (§72021§r)', author: 'Zhong Meng; Yu Wu; Naoyuki Kanda; Liang Lu; Xie Chen; Guoli Ye; Eric Sun; Jinyu Li; Yifan Gong', display:{Lore:['[{"text": "arXiv:2106.02302", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMinimum Word Error Rate Training with Language Model Fusion for End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhong Meng\\nYu Wu\\nNaoyuki Kanda\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.02302\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInterspeech 2021, Brno, Czech Republic\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Jun 2021 07:24:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, Interspeech 2021\\u00a7r"}']}
{title:'Banerjee et al. (§72021§r)', author: 'Annesya Banerjee; Achal Nilhani', display:{Lore:['[{"text": "arXiv:2106.02348", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Residual Network based Deep Learning Model for Detection of COVID-19 from Cough Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oAnnesya Banerjee\\nAchal Nilhani\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.02348\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1088/978-0-7503-3795-3ch6\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Jun 2021 08:59:27 GMT)\\u00a7r"}']}
{title:'Harada et al. (§72021§r)', author: 'Noboru Harada; Daisuke Niizumi; Daiki Takeuchi; Yasunori Ohishi; Masahiro Yasuda; Shoichiro Saito', display:{Lore:['[{"text": "arXiv:2106.02369", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lToyADMOS2: Another dataset of miniature-machine operating sounds for anomalous sound detection under domain shift conditions\\u00a7r\\n\\n\\u00a78\\u00a7oNoboru Harada\\nDaisuke Niizumi\\nDaiki Takeuchi\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.02369\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Jun 2021 09:32:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Zhengxiong Wang; Anton Ragni', display:{Lore:['[{"text": "arXiv:2106.02417", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lApproximate Fixed-Points in Recurrent Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oZhengxiong Wang\\nAnton Ragni\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.02417\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Jun 2021 11:33:34 GMT)\\u00a7r"}']}
{title:'Ozan (§72021§r)', author: 'Şükrü Ozan', display:{Lore:['[{"text": "arXiv:2106.02422", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lClassification of Audio Segments in Call Center Recordings using Convolutional Recurrent Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7o\\u015e\\u00fckr\\u00fc Ozan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.02422\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Jun 2021 11:53:48 GMT)\\u00a7r"}']}
{title:'Awasthi et al. (§72021§r)', author: 'Abhijeet Awasthi; Kevin Kilgour; Hassan Rom', display:{Lore:['[{"text": "arXiv:2106.02443", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTeaching keyword spotters to spot new keywords with limited examples\\u00a7r\\n\\n\\u00a78\\u00a7oAbhijeet Awasthi\\nKevin Kilgour\\nHassan Rom\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.02443\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Jun 2021 12:43:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn INTERSPEECH 2021\\u00a7r"}']}
{title:'Keskin et al. (§72021§r)', author: 'Gokce Keskin; Minhua Wu; Brian King; Harish Mallidi; Yang Gao; Jasha Droppo; Ariya Rastrow; Roland Maas', display:{Lore:['[{"text": "arXiv:2106.02750", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDo You Listen with One or Two Microphones? A Unified ASR Model for Single and Multi-Channel Audio\\u00a7r\\n\\n\\u00a78\\u00a7oGokce Keskin\\nMinhua Wu\\nBrian King\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.02750\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 28 Jun 2021 23:56:42 GMT)\\u00a7r"}']}
{title:'Huang et al. (§72021§r)', author: 'Yu-Lin Huang; Bo-Hao Su; Y. -W. Peter Hong; Chi-Chun Lee', display:{Lore:['[{"text": "arXiv:2106.02810", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Attribute-Aligned Strategy for Learning Speech Representation\\u00a7r\\n\\n\\u00a78\\u00a7oYu-Lin Huang\\nBo-Hao Su\\nY. -W. Peter Hong\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.02810\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-1341\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of INTERSPEECH 2021\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 8 Sep 2021 09:47:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures; Accepted in Interspeech 2021\\u00a7r"}']}
{title:'Chung et al. (§72021§r)', author: 'Hyunseung Chung; Sang-Hoon Lee; Seong-Whan Lee', display:{Lore:['[{"text": "arXiv:2106.02830", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReinforce-Aligner: Reinforcement Alignment Search for Robust End-to-End Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oHyunseung Chung\\nSang-Hoon Lee\\nSeong-Whan Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.02830\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 5 Jun 2021 08:08:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in INTERSPEECH 2021\\u00a7r"}']}
{title:'Eskimez et al. (§72021§r)', author: 'Sefik Emre Eskimez; Xiaofei Wang; Min Tang; Hemin Yang; Zirun Zhu; Zhuo Chen; Huaming Wang; Takuya Yoshioka', display:{Lore:['[{"text": "arXiv:2106.02896", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHuman Listening and Live Captioning: Multi-Task Training for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oSefik Emre Eskimez\\nXiaofei Wang\\nMin Tang\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.02896\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 5 Jun 2021 13:40:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH2021\\u00a7r"}']}
{title:'Chowdhury (§72021§r)', author: 'Jatin Chowdhury', display:{Lore:['[{"text": "arXiv:2106.03037", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRTNeural: Fast Neural Inferencing for Real-Time Systems\\u00a7r\\n\\n\\u00a78\\u00a7oJatin Chowdhury\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.03037\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 6 Jun 2021 05:46:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures\\u00a7r"}']}
{title:'Han et al. (§72021§r)', author: 'Jiangyu Han; Wei Rao; Yannan Wang; Yanhua Long', display:{Lore:['[{"text": "arXiv:2106.03113", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Channel Decorrelation for Multi-Channel Target Speech Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oJiangyu Han\\nWei Rao\\nYannan Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.03113\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 6 Jun 2021 13:08:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to Interspeech 2021. arXiv admin note: text overlap with arXiv:2010.09191\\u00a7r"}']}
{title:'Min et al. (§72021§r)', author: 'Dongchan Min; Dong Bok Lee; Eunho Yang; Sung Ju Hwang', display:{Lore:['[{"text": "arXiv:2106.03153", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMeta-StyleSpeech : Multi-Speaker Adaptive Text-to-Speech Generation\\u00a7r\\n\\n\\u00a78\\u00a7oDongchan Min\\nDong Bok Lee\\nEunho Yang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.03153\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 16 Jun 2021 16:57:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICML 2021\\u00a7r"}']}
{title:'Ryu et al. (§72021§r)', author: 'Hyun Gon Ryu; Jeong-Hoon Kim; Simon See', display:{Lore:['[{"text": "arXiv:2106.03167", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMathematical Vocoder Algorithm : Modified Spectral Inversion for Efficient Neural Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oHyun Gon Ryu\\nJeong-Hoon Kim\\nSimon See\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.03167\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 16 Jun 2021 00:51:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osee sample in https://its10041004.github.io/MVA/\\u00a7r"}']}
{title:'Tsunoo et al. (§72021§r)', author: 'Emiru Tsunoo; Kentaro Shibata; Chaitanya Narisetty; Yosuke Kashiwagi; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2106.03419", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData Augmentation Methods for End-to-end Speech Recognition on Distant-Talk Scenarios\\u00a7r\\n\\n\\u00a78\\u00a7oEmiru Tsunoo\\nKentaro Shibata\\nChaitanya Narisetty\\nYosuke Kashiwagi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.03419\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Jun 2021 08:36:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for Interspeech2021\\u00a7r"}']}
{title:'Korzekwa et al. (§72021§r)', author: 'Daniel Korzekwa; Jaime Lorenzo-Trueba; Thomas Drugman; Shira Calamaro; Bozena Kostek', display:{Lore:['[{"text": "arXiv:2106.03494", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeakly-supervised word-level pronunciation error detection in non-native English speech\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Korzekwa\\nJaime Lorenzo-Trueba\\nThomas Drugman\\nShira Calamaro\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.03494\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Jun 2021 10:31:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021\\u00a7r"}']}
{title:'Nelus et al. (§72021§r)', author: 'Alexandru Nelus; Rene Glitza; Rainer Martin', display:{Lore:['[{"text": "arXiv:2106.03671", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Clustered Federated Learning in Complex Multi-source Acoustic Environments\\u00a7r\\n\\n\\u00a78\\u00a7oAlexandru Nelus\\nRene Glitza\\nRainer Martin\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.03671\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Jun 2021 14:51:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at EUSIPCO2021\\u00a7r"}']}
{title:'Takashima et al. (§72021§r)', author: 'Yuki Takashima; Yusuke Fujita; Shinji Watanabe; Shota Horiguchi; Paola García; Kenji Nagamatsu', display:{Lore:['[{"text": "arXiv:2106.04078", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Speaker Diarization Conditioned on Speech Activity and Overlap Detection\\u00a7r\\n\\n\\u00a78\\u00a7oYuki Takashima\\nYusuke Fujita\\nShinji Watanabe\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.04078\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Spoken Language Technology Workshop (SLT), 2021, pp. 849-856\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Jun 2021 03:28:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for SLT 2021\\u00a7r"}']}
{title:'Giri et al. (§72021§r)', author: 'Ritwik Giri; Shrikant Venkataramani; Jean-Marc Valin; Umut Isik; Arvindh Krishnaswamy', display:{Lore:['[{"text": "arXiv:2106.04129", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPersonalized PercepNet: Real-time, Low-complexity Target Voice Separation and Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oRitwik Giri\\nShrikant Venkataramani\\nJean-Marc Valin\\nUmut Isik\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.04129\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Jun 2021 06:35:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2021, 5 pages\\u00a7r"}']}
{title:'Chen et al. (§72021§r)', author: 'Liping Chen; Yan Deng; Xi Wang; Frank K. Soong; Lei He', display:{Lore:['[{"text": "arXiv:2106.04312", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech BERT Embedding For Improving Prosody in Neural TTS\\u00a7r\\n\\n\\u00a78\\u00a7oLiping Chen\\nYan Deng\\nXi Wang\\nFrank K. Soong\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.04312\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2021\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 14 Sep 2021 09:11:42 GMT)\\u00a7r"}']}
{title:'Kawaguchi et al. (§72021§r)', author: 'Yohei Kawaguchi; Keisuke Imoto; Yuma Koizumi; Noboru Harada; Daisuke Niizumi; Kota Dohi; Ryo Tanabe; Harsh Purohit; Takashi Endo', display:{Lore:['[{"text": "arXiv:2106.04492", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDescription and Discussion on DCASE 2021 Challenge Task 2: Unsupervised Anomalous Sound Detection for Machine Condition Monitoring under Domain Shifted Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oYohei Kawaguchi\\nKeisuke Imoto\\nYuma Koizumi\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.04492\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Sep 2021 15:09:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to DCASE 2021 Workshop\\u00a7r"}']}
{title:'Ravanelli et al. (§72021§r)', author: 'Mirco Ravanelli; Titouan Parcollet; Peter Plantinga; Aku Rouhe; Samuele Cornell; Loren Lugosch; Cem Subakan; Nauman Dawalatabad; Abdelwahab Heba; Jianyuan Zhong; Ju-Chieh Chou; Sung-Lin Yeh; Szu-Wei Fu; Chien-Feng Liao; Elena Rastorgueva; François Grondin; William Aris; Hwidong Na; Yan Gao; Renato De Mori; Yoshua Bengio', display:{Lore:['[{"text": "arXiv:2106.04624", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeechBrain: A General-Purpose Speech Toolkit\\u00a7r\\n\\n\\u00a78\\u00a7oMirco Ravanelli\\nTitouan Parcollet\\nPeter Plantinga\\n+ 17 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.04624\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Jun 2021 18:22:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint\\u00a7r"}']}
{title:'Takashima et al. (§72021§r)', author: 'Yuki Takashima; Yusuke Fujita; Shota Horiguchi; Shinji Watanabe; Paola García; Kenji Nagamatsu', display:{Lore:['[{"text": "arXiv:2106.04764", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-Supervised Training with Pseudo-Labeling for End-to-End Neural Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oYuki Takashima\\nYusuke Fujita\\nShota Horiguchi\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.04764\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 9 Jun 2021 01:35:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for Interspeech 2021\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Lu Zhang; Mingjiang Wang; Zehua Zhang; Xuyi Zhuang', display:{Lore:['[{"text": "arXiv:2106.04878", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Interaction between Masking and Mapping Targets for Single-Channel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oLu Zhang\\nMingjiang Wang\\nZehua Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.04878\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 9 Jun 2021 07:59:22 GMT)\\u00a7r"}']}
{title:'Boes et al. (§72021§r)', author: 'Wim Boes; Hugo Van hamme', display:{Lore:['[{"text": "arXiv:2106.05408", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudiovisual transfer learning for audio tagging and sound event detection\\u00a7r\\n\\n\\u00a78\\u00a7oWim Boes\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.05408\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-695\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings Interspeech 2021; 2021; pp. 2401 - 2405\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 9 Jun 2021 21:55:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Wu et al. (§72021§r)', author: 'Yi-Chiao Wu; Cheng-Hung Hu; Hung-Shin Lee; Yu-Huai Peng; Wen-Chin Huang; Yu Tsao; Hsin-Min Wang; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2106.05629", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRelational Data Selection for Data Augmentation of Speaker-dependent Multi-band MelGAN Vocoder\\u00a7r\\n\\n\\u00a78\\u00a7oYi-Chiao Wu\\nCheng-Hung Hu\\nHung-Shin Lee\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.05629\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Jun 2021 10:11:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, 3 tables, Proc. Interspeech, 2021\\u00a7r"}']}
{title:'Seyfarth et al. (§72021§r)', author: 'Scott Seyfarth; Sundararajan Srinivasan; Katrin Kirchhoff', display:{Lore:['[{"text": "arXiv:2106.05792", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker-conversation factorial designs for diarization error analysis\\u00a7r\\n\\n\\u00a78\\u00a7oScott Seyfarth\\nSundararajan Srinivasan\\nKatrin Kirchhoff\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.05792\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Jun 2021 14:48:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, Interspeech 2021\\u00a7r"}']}
{title:'Adiga et al. (§72021§r)', author: 'Devaraja Adiga; Rishabh Kumar; Amrith Krishna; Preethi Jyothi; Ganesh Ramakrishnan; Pawan Goyal', display:{Lore:['[{"text": "arXiv:2106.05852", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Speech Recognition in Sanskrit: A New Speech Corpus and Modelling Insights\\u00a7r\\n\\n\\u00a78\\u00a7oDevaraja Adiga\\nRishabh Kumar\\nAmrith Krishna\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.05852\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 23 Jul 2021 07:16:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted paper at the 59th Annual Meeting of the Association for Computational Linguistics (ACL 2021 Findings)\\u00a7r"}']}
{title:'Ray et al. (§72021§r)', author: 'Swayambhu Nath Ray; Soumyajit Mitra; Raghavendra Bilgi; Sri Garimella', display:{Lore:['[{"text": "arXiv:2106.06183", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving RNN-T ASR Performance with Date-Time and Location Awareness\\u00a7r\\n\\n\\u00a78\\u00a7oSwayambhu Nath Ray\\nSoumyajit Mitra\\nRaghavendra Bilgi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.06183\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 16 Jun 2021 05:47:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in TSD 2021\\u00a7r"}']}
{title:'Hua et al. (§72021§r)', author: 'Guang Hua; Andrew Beng Jin Teoh; Haijian Zhang', display:{Lore:['[{"text": "arXiv:2106.06341", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards End-to-End Synthetic Speech Detection\\u00a7r\\n\\n\\u00a78\\u00a7oGuang Hua\\nAndrew Beng Jin Teoh\\nHaijian Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.06341\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2021.3089437\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Jun 2021 12:25:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in IEEE Signal Processing Letters 2021\\u00a7r"}']}
{title:'Deshmukh et al. (§72021§r)', author: 'Soham Deshmukh; Bhiksha Raj; Rita Singh', display:{Lore:['[{"text": "arXiv:2106.06858", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving weakly supervised sound event detection with self-supervised auxiliary tasks\\u00a7r\\n\\n\\u00a78\\u00a7oSoham Deshmukh\\nBhiksha Raj\\nRita Singh\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.06858\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 12 Jun 2021 20:28:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 21\\u00a7r"}']}
{title:'Politis et al. (§72021§r)', author: 'Archontis Politis; Sharath Adavanne; Daniel Krause; Antoine Deleforge; Prerak Srivastava; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2106.06999", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Dataset of Dynamic Reverberant Sound Scenes with Directional Interferers for Sound Event Localization and Detection\\u00a7r\\n\\n\\u00a78\\u00a7oArchontis Politis\\nSharath Adavanne\\nDaniel Krause\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.06999\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 4 Jul 2021 19:30:58 GMT)\\u00a7r"}']}
{title:'Hao et al. (§72021§r)', author: 'Yunzhe Hao; Jiaming Xu; Peng Zhang; Bo Xu', display:{Lore:['[{"text": "arXiv:2106.07016", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWASE: Learning When to Attend for Speaker Extraction in Cocktail Party Environments\\u00a7r\\n\\n\\u00a78\\u00a7oYunzhe Hao\\nJiaming Xu\\nPeng Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.07016\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 13 Jun 2021 14:56:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2021\\u00a7r"}']}
{title:'Delcroix et al. (§72021§r)', author: 'Marc Delcroix; Jorge Bennasar Vázquez; Tsubasa Ochiai; Keisuke Kinoshita; Shoko Araki', display:{Lore:['[{"text": "arXiv:2106.07144", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFew-shot learning of new sound classes for target sound extraction\\u00a7r\\n\\n\\u00a78\\u00a7oMarc Delcroix\\nJorge Bennasar V\\u00e1zquez\\nTsubasa Ochiai\\nKeisuke Kinoshita\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.07144\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Jun 2021 03:13:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Interspeech 2021\\u00a7r"}']}
{title:'Qi et al. (§72021§r)', author: 'Jinzi Qi; Hugo Van hamme', display:{Lore:['[{"text": "arXiv:2106.07337", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Disorder Classification Using Extended Factorized Hierarchical Variational Auto-encoders\\u00a7r\\n\\n\\u00a78\\u00a7oJinzi Qi\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.07337\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Jun 2021 12:29:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, submitted to INTERSPEECH2021\\u00a7r"}']}
{title:'Manohar et al. (§72021§r)', author: 'Vimal Manohar; Tatiana Likhomanenko; Qiantong Xu; Wei-Ning Hsu; Ronan Collobert; Yatharth Saraf; Geoffrey Zweig; Abdelrahman Mohamed', display:{Lore:['[{"text": "arXiv:2106.07759", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKaizen: Continuously improving teacher using Exponential Moving Average for semi-supervised speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oVimal Manohar\\nTatiana Likhomanenko\\nQiantong Xu\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.07759\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 27 Oct 2021 15:55:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUpdated with camera ready version\\u00a7r"}']}
{title:'Jang et al. (§72021§r)', author: 'Won Jang; Dan Lim; Jaesam Yoon; Bongwan Kim; Juntae Kim', display:{Lore:['[{"text": "arXiv:2106.07889", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation\\u00a7r\\n\\n\\u00a78\\u00a7oWon Jang\\nDan Lim\\nJaesam Yoon\\nBongwan Kim\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.07889\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Jun 2021 05:35:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Singh et al. (§72021§r)', author: 'Vishwanath Pratap Singh; Shashi Kumar; Ravi Shekhar Jha; Abhishek Pandey', display:{Lore:['[{"text": "arXiv:2106.07972", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSRIB Submission to Interspeech 2021 DiCOVA Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oVishwanath Pratap Singh\\nShashi Kumar\\nRavi Shekhar Jha\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.07972\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Jun 2021 08:50:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures\\u00a7r"}']}
{title:'Drude et al. (§72021§r)', author: 'Lukas Drude; Jahn Heymann; Andreas Schwarz; Jean-Marc Valin', display:{Lore:['[{"text": "arXiv:2106.07994", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-channel Opus compression for far-field automatic speech recognition with a fixed bitrate budget\\u00a7r\\n\\n\\u00a78\\u00a7oLukas Drude\\nJahn Heymann\\nAndreas Schwarz\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.07994\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Jun 2021 09:16:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2021\\u00a7r"}']}
{title:'Arabskyy et al. (§72021§r)', author: 'Yuriy Arabskyy; Aashish Agarwal; Subhadeep Dey; Oscar Koller', display:{Lore:['[{"text": "arXiv:2106.08126", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDialectal Speech Recognition and Translation of Swiss German Speech to Standard German Text: Microsoft\'s Submission to SwissText 2021\\u00a7r\\n\\n\\u00a78\\u00a7oYuriy Arabskyy\\nAashish Agarwal\\nSubhadeep Dey\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.08126\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 1 Jul 2021 10:58:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oto be published in SwissText 2021\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Jicheng Zhang; Yizhou Peng; Pham Van Tung; Haihua Xu; Hao Huang; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2106.08211", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lE2E-based Multi-task Learning Approach to Joint Speech and Accent Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJicheng Zhang\\nYizhou Peng\\nPham Van Tung\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.08211\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Jun 2021 15:17:08 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Pu Wang; Bagher BabaAli; Hugo Van hamme', display:{Lore:['[{"text": "arXiv:2106.08313", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Study into Pre-training Strategies for Spoken Language Understanding on Dysarthric Speech\\u00a7r\\n\\n\\u00a78\\u00a7oPu Wang\\nBagher BabaAli\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.08313\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Jun 2021 17:43:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2021\\u00a7r"}']}
{title:'Torresquintero et al. (§72021§r)', author: 'Alexandra Torresquintero; Tian Huey Teh; Christopher G. R. Wallis; Marlene Staib; Devang S Ram Mohan; Vivian Hu; Lorenzo Foglianti; Jiameng Gao; Simon King', display:{Lore:['[{"text": "arXiv:2106.08321", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lADEPT: A Dataset for Evaluating Prosody Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oAlexandra Torresquintero\\nTian Huey Teh\\nChristopher G. R. Wallis\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.08321\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 21 Jul 2021 09:31:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, accepted to Interspeech 2021\\u00a7r"}']}
{title:'Mohan et al. (§72021§r)', author: 'Devang S Ram Mohan; Vivian Hu; Tian Huey Teh; Alexandra Torresquintero; Christopher G. R. Wallis; Marlene Staib; Lorenzo Foglianti; Jiameng Gao; Simon King', display:{Lore:['[{"text": "arXiv:2106.08352", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCtrl-P: Temporal Control of Prosodic Variation for Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oDevang S Ram Mohan\\nVivian Hu\\nTian Huey Teh\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.08352\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Jun 2021 18:03:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in Interspeech 2021. 5 pages, 4 figures\\u00a7r"}']}
{title:'Qian et al. (§72021§r)', author: 'Kaizhi Qian; Yang Zhang; Shiyu Chang; Jinjun Xiong; Chuang Gan; David Cox; Mark Hasegawa-Johnson', display:{Lore:['[{"text": "arXiv:2106.08519", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGlobal Rhythm Style Transfer Without Text Transcriptions\\u00a7r\\n\\n\\u00a78\\u00a7oKaizhi Qian\\nYang Zhang\\nShiyu Chang\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.08519\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Jun 2021 02:21:00 GMT)\\u00a7r"}']}
{title:'Ng et al. (§72021§r)', author: 'Si-Ioi Ng; Cymie Wing-Yee Ng; Jingyu Li; Tan Lee', display:{Lore:['[{"text": "arXiv:2106.08536", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetection of Consonant Errors in Disordered Speech Based on Consonant-vowel Segment Embedding\\u00a7r\\n\\n\\u00a78\\u00a7oSi-Ioi Ng\\nCymie Wing-Yee Ng\\nJingyu Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.08536\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Jun 2021 03:25:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Guo et al. (§72021§r)', author: 'Pengcheng Guo; Xuankai Chang; Shinji Watanabe; Lei Xie', display:{Lore:['[{"text": "arXiv:2106.08595", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Speaker ASR Combining Non-Autoregressive Conformer CTC and Conditional Speaker Chain\\u00a7r\\n\\n\\u00a78\\u00a7oPengcheng Guo\\nXuankai Chang\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.08595\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Jun 2021 07:41:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2021\\u00a7r"}']}
{title:'Gabryś et al. (§72021§r)', author: 'Adam Gabryś; Yunlong Jiao; Viacheslav Klimkov; Daniel Korzekwa; Roberto Barra-Chicote', display:{Lore:['[{"text": "arXiv:2106.08649", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving the expressiveness of neural vocoding with non-affine Normalizing Flows\\u00a7r\\n\\n\\u00a78\\u00a7oAdam Gabry\\u015b\\nYunlong Jiao\\nViacheslav Klimkov\\nDaniel Korzekwa\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.08649\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-1555\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Jun 2021 09:25:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021, 5 pages,3 figures\\u00a7r"}']}
{title:'Lv et al. (§72021§r)', author: 'Shubo Lv; Yanxin Hu; Shimin Zhang; Lei Xie', display:{Lore:['[{"text": "arXiv:2106.08672", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDCCRN+: Channel-wise Subband DCCRN with SNR Estimation for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oShubo Lv\\nYanxin Hu\\nShimin Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.08672\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Jun 2021 10:16:35 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Zhichao Wang; Xinyong Zhou; Fengyu Yang; Tao Li; Hongqiang Du; Lei Xie; Wendong Gan; Haitao Chen; Hai Li', display:{Lore:['[{"text": "arXiv:2106.08741", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnriching Source Style Transfer in Recognition-Synthesis based Non-Parallel Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oZhichao Wang\\nXinyong Zhou\\nFengyu Yang\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.08741\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 26 Jun 2021 10:50:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2021\\u00a7r"}']}
{title:'Higuchi et al. (§72021§r)', author: 'Yosuke Higuchi; Niko Moritz; Jonathan Le Roux; Takaaki Hori', display:{Lore:['[{"text": "arXiv:2106.08922", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMomentum Pseudo-Labeling for Semi-Supervised Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYosuke Higuchi\\nNiko Moritz\\nJonathan Le Roux\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.08922\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Jun 2021 16:24:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021\\u00a7r"}']}
{title:'Strauss et al. (§72021§r)', author: 'Martin Strauss; Bernd Edler', display:{Lore:['[{"text": "arXiv:2106.09008", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Flow-Based Neural Network for Time Domain Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oMartin Strauss\\nBernd Edler\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.09008\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9413999\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Jun 2021 17:56:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Strauss et al. (§72021§r)', author: 'Martin Strauss; Jouni Paulus; Matteo Torcoli; Bernd Edler', display:{Lore:['[{"text": "arXiv:2106.09093", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Hands-on Comparison of DNNs for Dialog Separation Using Transfer Learning from Music Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oMartin Strauss\\nJouni Paulus\\nMatteo Torcoli\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.09093\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 22 Jun 2021 08:44:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted in INTERSPEECH 2021\\u00a7r"}']}
{title:'Lee et al. (§72021§r)', author: 'Jaesong Lee; Jingu Kang; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2106.09216", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLayer Pruning on Demand with Intermediate CTC\\u00a7r\\n\\n\\u00a78\\u00a7oJaesong Lee\\nJingu Kang\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.09216\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Jun 2021 02:40:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2021\\u00a7r"}']}
{title:'Droppo et al. (§72021§r)', author: 'Jasha Droppo; Oguz Elibol', display:{Lore:['[{"text": "arXiv:2106.09488", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScaling Laws for Acoustic Models\\u00a7r\\n\\n\\u00a78\\u00a7oJasha Droppo\\nOguz Elibol\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.09488\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Jun 2021 18:59:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2021\\u00a7r"}']}
{title:'Shenoy et al. (§72021§r)', author: 'Ashish Shenoy; Sravan Bodapati; Katrin Kirchhoff', display:{Lore:['[{"text": "arXiv:2106.09532", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lASR Adaptation for E-commerce Chatbots using Cross-Utterance Context and Multi-Task Language Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oAshish Shenoy\\nSravan Bodapati\\nKatrin Kirchhoff\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.09532\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.18653/v1/2021.ecnlp-1.3\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Jun 2021 21:27:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ACL-IJCNLP 2021 Workshop on e-Commerce and NLP (ECNLP)\\u00a7r"}']}
{title:'Vaaras et al. (§72021§r)', author: 'Einari Vaaras; Sari Ahlqvist-Björkroth; Konstantinos Drossos; Okko Räsänen', display:{Lore:['[{"text": "arXiv:2106.09539", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Analysis of the Emotional Content of Speech in Daylong Child-Centered Recordings from a Neonatal Intensive Care Unit\\u00a7r\\n\\n\\u00a78\\u00a7oEinari Vaaras\\nSari Ahlqvist-Bj\\u00f6rkroth\\nKonstantinos Drossos\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.09539\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Jun 2021 11:17:52 GMT)\\u00a7r"}']}
{title:'Bayerl et al. (§72021§r)', author: 'Sebastian P. Bayerl; Marc Wenninger; Jochen Schmidt; Alexander Wolff von Gudenberg; Korbinian Riedhammer', display:{Lore:['[{"text": "arXiv:2106.09545", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSTAN: A stuttering therapy analysis helper\\u00a7r\\n\\n\\u00a78\\u00a7oSebastian P. Bayerl\\nMarc Wenninger\\nJochen Schmidt\\nAlexander Wolff von Gudenberg\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.09545\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nDemo presented at 2021 IEEE Spoken Language Technology Workshop\\n  (SLT)\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Jun 2021 13:48:12 GMT)\\u00a7r"}']}
{title:'Calis et al. (§72021§r)', author: 'Metin Calis; Steven van de Par; Richard Heusdens; Richard C. Hendriks', display:{Lore:['[{"text": "arXiv:2106.09574", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLocalization based on enhanced low frequency interaural level difference\\u00a7r\\n\\n\\u00a78\\u00a7oMetin Calis\\nSteven van de Par\\nRichard Heusdens\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.09574\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Jun 2021 14:59:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 8 figures, preprint for a journal submission, paper in review, not yet accepted\\u00a7r"}']}
{title:'Monesi et al. (§72021§r)', author: 'Mohammad Jalilpour Monesi; Bernd Accou; Tom Francart; Hugo Van Hamme', display:{Lore:['[{"text": "arXiv:2106.09622", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExtracting Different Levels of Speech Information from EEG Using an LSTM-Based Model\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Jalilpour Monesi\\nBernd Accou\\nTom Francart\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.09622\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Jun 2021 15:59:18 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72021§r)', author: 'Nanxin Chen; Yu Zhang; Heiga Zen; Ron J. Weiss; Mohammad Norouzi; Najim Dehak; William Chan', display:{Lore:['[{"text": "arXiv:2106.09660", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oNanxin Chen\\nYu Zhang\\nHeiga Zen\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.09660\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 19 Jun 2021 01:47:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH\\u00a7r"}']}
{title:'Kim et al. (§72021§r)', author: 'Kwangyoun Kim; Felix Wu; Prashant Sridhar; Kyu J. Han; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2106.09760", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-mode Transformer Transducer with Stochastic Future Context\\u00a7r\\n\\n\\u00a78\\u00a7oKwangyoun Kim\\nFelix Wu\\nPrashant Sridhar\\nKyu J. Han\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.09760\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Jun 2021 18:42:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021\\u00a7r"}']}
{title:'Fan et al. (§72021§r)', author: 'Ruchao Fan; Wei Chu; Peng Chang; Jing Xiao; Abeer Alwan', display:{Lore:['[{"text": "arXiv:2106.09885", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Improved Single Step Non-autoregressive Transformer for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oRuchao Fan\\nWei Chu\\nPeng Chang\\nJing Xiao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.09885\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 22 Jul 2021 00:56:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech2021\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Jinhan Wang; Yunzheng Zhu; Ruchao Fan; Wei Chu; Abeer Alwan', display:{Lore:['[{"text": "arXiv:2106.09963", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow Resource German ASR with Untranscribed Data Spoken by Non-native Children \\u2013 INTERSPEECH 2021 Shared Task SPAPL System\\u00a7r\\n\\n\\u00a78\\u00a7oJinhan Wang\\nYunzheng Zhu\\nRuchao Fan\\nWei Chu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.09963\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Jun 2021 07:36:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Disong Wang; Liqun Deng; Yu Ting Yeung; Xiao Chen; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2106.10127", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Domain Adaptation for Dysarthric Speech Detection via Domain Adversarial Training and Mutual Information Minimization\\u00a7r\\n\\n\\u00a78\\u00a7oDisong Wang\\nLiqun Deng\\nYu Ting Yeung\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.10127\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Jun 2021 13:34:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Disong Wang; Liqun Deng; Yu Ting Yeung; Xiao Chen; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2106.10132", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-shot Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oDisong Wang\\nLiqun Deng\\nYu Ting Yeung\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.10132\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Jun 2021 13:50:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021. Code, pre-trained models and demo are available at https://github.com/Wendison/VQMIVC\\u00a7r"}']}
{title:'Karpov et al. (§72021§r)', author: 'Nikolay Karpov; Alexander Denisenko; Fedor Minkin', display:{Lore:['[{"text": "arXiv:2106.10161", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGolos: Russian Dataset for Speech Research\\u00a7r\\n\\n\\u00a78\\u00a7oNikolay Karpov\\nAlexander Denisenko\\nFedor Minkin\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.10161\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Jun 2021 14:55:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted to Interspeech2021\\u00a7r"}']}
{title:'Karanasou et al. (§72021§r)', author: 'Penny Karanasou; Sri Karlapati; Alexis Moinet; Arnaud Joly; Ammar Abbas; Simon Slangen; Jaime Lorenzo Trueba; Thomas Drugman', display:{Lore:['[{"text": "arXiv:2106.10229", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA learned conditional prior for the VAE acoustic space of a TTS system\\u00a7r\\n\\n\\u00a78\\u00a7oPenny Karanasou\\nSri Karlapati\\nAlexis Moinet\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.10229\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Jun 2021 15:36:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oin Proceedings of Interspeech 2021\\u00a7r"}']}
{title:'Tomanek et al. (§72021§r)', author: 'Katrin Tomanek; Françoise Beaufays; Julie Cattiau; Angad Chandorkar; Khe Chai Sim', display:{Lore:['[{"text": "arXiv:2106.10259", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn-Device Personalization of Automatic Speech Recognition Models for Disordered Speech\\u00a7r\\n\\n\\u00a78\\u00a7oKatrin Tomanek\\nFran\\u00e7oise Beaufays\\nJulie Cattiau\\nAngad Chandorkar\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.10259\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Jun 2021 17:48:08 GMT)\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Jie Li; Lizhong Yao', display:{Lore:['[{"text": "arXiv:2106.10277", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGPLA-12: An Acoustic Signal Dataset of Gas Pipeline Leakage\\u00a7r\\n\\n\\u00a78\\u00a7oJie Li\\nLizhong Yao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.10277\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 19 Jun 2021 04:37:39 GMT)\\u00a7r"}']}
{title:'Koyama et al. (§72021§r)', author: 'Shoichi Koyama; Tomoya Nishida; Keisuke Kimura; Takumi Abe; Natsuki Ueno; Jesper Brunnström', display:{Lore:['[{"text": "arXiv:2106.10801", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMeshRIR: A Dataset of Room Impulse Responses on Meshed Grid Points For Evaluating Sound Field Analysis and Synthesis Methods\\u00a7r\\n\\n\\u00a78\\u00a7oShoichi Koyama\\nTomoya Nishida\\nKeisuke Kimura\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.10801\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 23 Jul 2021 16:01:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEWorkshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)2021\\u00a7r"}']}
{title:'Shimada et al. (§72021§r)', author: 'Kazuki Shimada; Naoya Takahashi; Yuichiro Koyama; Shusuke Takahashi; Emiru Tsunoo; Masafumi Takahashi; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2106.10806", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnsemble of ACCDOA- and EINV2-based Systems with D3Nets and Impulse Response Simulation for Sound Event Localization and Detection\\u00a7r\\n\\n\\u00a78\\u00a7oKazuki Shimada\\nNaoya Takahashi\\nYuichiro Koyama\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.10806\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Jun 2021 01:52:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, submitted to DCASE2021task3\\u00a7r"}']}
{title:'Cong et al. (§72021§r)', author: 'Jian Cong; Shan Yang; Na Hu; Guangzhi Li; Lei Xie; Dan Su', display:{Lore:['[{"text": "arXiv:2106.10828", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lControllable Context-aware Conversational Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oJian Cong\\nShan Yang\\nNa Hu\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.10828\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Jun 2021 03:36:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Cong et al. (§72021§r)', author: 'Jian Cong; Shan Yang; Lei Xie; Dan Su', display:{Lore:['[{"text": "arXiv:2106.10831", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGlow-WaveGAN: Learning Speech Representations from GAN-based Variational Auto-Encoder For High Fidelity Flow-based Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oJian Cong\\nShan Yang\\nLei Xie\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.10831\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 22 Jun 2021 01:48:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Baby et al. (§72021§r)', author: 'Arun Baby; Pranav Jawale; Saranya Vinnaitherthan; Sumukh Badam; Nagaraj Adiga; Sharath Adavanne', display:{Lore:['[{"text": "arXiv:2106.10870", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-native English lexicon creation for bilingual speech synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oArun Baby\\nPranav Jawale\\nSaranya Vinnaitherthan\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.10870\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Jun 2021 06:07:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for Presentation at SpeechSynthesis Workshop (SSW), 2021 (August 2021)\\u00a7r"}']}
{title:'Magistro (§72021§r)', author: 'Giuseppe Magistro', display:{Lore:['[{"text": "arXiv:2106.10915", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech prosody and remote experiments: a technical report\\u00a7r\\n\\n\\u00a78\\u00a7oGiuseppe Magistro\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.10915\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Jun 2021 08:26:47 GMT)\\u00a7r"}']}
{title:'Sharma et al. (§72021§r)', author: 'Neeraj Kumar Sharma; Ananya Muguli; Prashant Krishnan; Rohit Kumar; Srikanth Raj Chetupalli; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:2106.10997", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards sound based testing of COVID-19 \\u2013 Summary of the first Diagnostics of COVID-19 using Acoustics (DiCOVA) Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oNeeraj Kumar Sharma\\nAnanya Muguli\\nPrashant Krishnan\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.10997\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Jun 2021 11:30:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oManuscript in review in the Elsevier Computer Speech and Language journal\\u00a7r"}']}
{title:'Mitra et al. (§72021§r)', author: 'Vikramjit Mitra; Zifang Huang; Colin Lea; Lauren Tooley; Sarah Wu; Darren Botten; Ashwini Palekar; Shrinath Thelapurath; Panayiotis Georgiou; Sachin Kajarekar; Jefferey Bigham', display:{Lore:['[{"text": "arXiv:2106.11759", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalysis and Tuning of a Voice Assistant System for Dysfluent Speech\\u00a7r\\n\\n\\u00a78\\u00a7oVikramjit Mitra\\nZifang Huang\\nColin Lea\\n+ 7 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.11759\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Jun 2021 20:58:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 page reference, 2 figures\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Haiyang Liu; Jihan Zhang', display:{Lore:['[{"text": "arXiv:2106.11769", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Ultrasound Tongue Image Reconstruction from Lip Images Using Self-supervised Learning and Attention Mechanism\\u00a7r\\n\\n\\u00a78\\u00a7oHaiyang Liu\\nJihan Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.11769\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 20 Jun 2021 10:51:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in KDD Workshop (BIOKDD 2021)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Shanshan Wang; Gaurav Naithani; Archontis Politis; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2106.11794", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep neural network Based Low-latency Speech Separation with Asymmetric analysis-Synthesis Window Pair\\u00a7r\\n\\n\\u00a78\\u00a7oShanshan Wang\\nGaurav Naithani\\nArchontis Politis\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.11794\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Jun 2021 14:05:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to EUSIPCO-2021\\u00a7r"}']}
{title:'Raj et al. (§72021§r)', author: 'R G Prithvi Raj; Rohit Kumar; M K Jayesh; Anurenjan Purushothaman; Sriram Ganapathy; M A Basha Shaik', display:{Lore:['[{"text": "arXiv:2106.12763", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSRIB-LEAP submission to Far-field Multi-Channel Speech Enhancement Challenge for Video Conferencing\\u00a7r\\n\\n\\u00a78\\u00a7oR G Prithvi Raj\\nRohit Kumar\\nM K Jayesh\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.12763\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Jun 2021 04:15:31 GMT)\\u00a7r"}']}
{title:'Kabeli et al. (§72021§r)', author: 'Ori Kabeli; Yossi Adi; Zhenyu Tang; Buye Xu; Anurag Kumar', display:{Lore:['[{"text": "arXiv:2106.13493", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOnline Self-Attentive Gated RNNs for Real-Time Speaker Separation\\u00a7r\\n\\n\\u00a78\\u00a7oOri Kabeli\\nYossi Adi\\nZhenyu Tang\\nBuye Xu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.13493\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 27 Jul 2021 14:07:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAppears at the Workshop on Machine Learning in Speech and Language Processing 2021\\u00a7r"}']}
{title:'Pillay (§72021§r)', author: 'Ashwin Pillay', display:{Lore:['[{"text": "arXiv:2106.13966", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Audio Envelope Generator Derived from Industrial Process Control\\u00a7r\\n\\n\\u00a78\\u00a7oAshwin Pillay\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.13966\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 26 Jun 2021 08:33:39 GMT)\\u00a7r"}']}
{title:'Pertilä et al. (§72021§r)', author: 'Pasi Pertilä; Emre Cakir; Aapo Hakala; Eemi Fagerlund; Tuomas Virtanen; Archontis Politis; Antti Eronen', display:{Lore:['[{"text": "arXiv:2106.14787", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMobile Microphone Array Speech Detection and Localization in Diverse Everyday Environments\\u00a7r\\n\\n\\u00a78\\u00a7oPasi Pertil\\u00e4\\nEmre Cakir\\nAapo Hakala\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.14787\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Jun 2021 15:07:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oto be published in the proceedings of the 29th European Signal Processing Conference, EUSIPCO 2021\\u00a7r"}']}
{title:'Bak et al. (§72021§r)', author: 'Taejun Bak; Jae-Sung Bae; Hanbin Bae; Young-Ik Kim; Hoon-Young Cho', display:{Lore:['[{"text": "arXiv:2106.15123", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFastPitchFormant: Source-filter based Decomposed Modeling for Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oTaejun Bak\\nJae-Sung Bae\\nHanbin Bae\\nYoung-Ik Kim\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.15123\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Jun 2021 07:06:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Bae et al. (§72021§r)', author: 'Jae-Sung Bae; Tae-Jun Bak; Young-Sun Joo; Hoon-Young Cho', display:{Lore:['[{"text": "arXiv:2106.15144", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierarchical Context-Aware Transformers for Non-Autoregressive Text to Speech\\u00a7r\\n\\n\\u00a78\\u00a7oJae-Sung Bae\\nTae-Jun Bak\\nYoung-Sun Joo\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.15144\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Jun 2021 08:05:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Yang et al. (§72021§r)', author: 'Jinhyeok Yang; Jae-Sung Bae; Taejun Bak; Youngik Kim; Hoon-Young Cho', display:{Lore:['[{"text": "arXiv:2106.15153", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGANSpeech: Adversarial Training for High-Fidelity Multi-Speaker Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oJinhyeok Yang\\nJae-Sung Bae\\nTaejun Bak\\nYoungik Kim\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.15153\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Jun 2021 08:15:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Nguyen et al. (§72021§r)', author: 'Thi Ngoc Tho Nguyen; Karn Watcharasupat; Ngoc Khanh Nguyen; Douglas L. Jones; Woon Seng Gan', display:{Lore:['[{"text": "arXiv:2106.15190", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDCASE 2021 Task 3: Spectrotemporally-aligned Features for Polyphonic Sound Event Localization and Detection\\u00a7r\\n\\n\\u00a78\\u00a7oThi Ngoc Tho Nguyen\\nKarn Watcharasupat\\nNgoc Khanh Nguyen\\nDouglas L. Jones\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.15190\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5281/zenodo.5031836\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Jun 2021 09:18:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, Technical Report for DCASE 2021 Challenge Task 3. arXiv admin note text overlapwith arXiv:2110.00275\\u00a7r"}']}
{title:'Tan et al. (§72021§r)', author: 'Xu Tan; Tao Qin; Frank Soong; Tie-Yan Liu', display:{Lore:['[{"text": "arXiv:2106.15561", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Survey on Neural Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oXu Tan\\nTao Qin\\nFrank Soong\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.15561\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 23 Jul 2021 12:32:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oA comprehensive survey on TTS, 63 pages, 18 tables, 7 figures, 457 references\\u00a7r"}']}
{title:'Abbas et al. (§72021§r)', author: 'Ammar Abbas; Bajibabu Bollepalli; Alexis Moinet; Arnaud Joly; Penny Karanasou; Peter Makarov; Simon Slangens; Sri Karlapati; Thomas Drugman', display:{Lore:['[{"text": "arXiv:2106.15649", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Scale Spectrogram Modelling for Neural Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oAmmar Abbas\\nBajibabu Bollepalli\\nAlexis Moinet\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.15649\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Jun 2021 18:01:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for the11th ISCA Speech Synthesis Workshop (SSW11)\\u00a7r"}']}
{title:'Biberger et al. (§72021§r)', author: 'Thomas Biberger; Stephan D. Ewert', display:{Lore:['[{"text": "arXiv:2106.15659", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards a generalized monaural and binaural auditory model for psychoacoustics and speech intelligibility\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Biberger\\nStephan D. Ewert\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.15659\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Jun 2021 18:12:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ActaAcustica\\u00a7r"}']}
{title:'Koizumi et al. (§72021§r)', author: 'Yuma Koizumi; Shigeki Karita; Scott Wisdom; Hakan Erdogan; John R. Hershey; Llion Jones; Michiel Bacchiani', display:{Lore:['[{"text": "arXiv:2106.15813", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDF-Conformer: Integrated architecture of Conv-TasNet and Conformer using linear complexity self-attention for speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oYuma Koizumi\\nShigeki Karita\\nScott Wisdom\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.15813\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 5 Aug 2021 08:26:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figure. accepted for WASPAA 2021\\u00a7r"}']}
{title:'Kirsch et al. (§72021§r)', author: 'Christoph Kirsch; Josef Poppitz; Torben Wendt; Steven van de Par; Stephan D. Ewert', display:{Lore:['[{"text": "arXiv:2106.15888", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatial resolution of late reverberation in virtual acoustic environments\\u00a7r\\n\\n\\u00a78\\u00a7oChristoph Kirsch\\nJosef Poppitz\\nTorben Wendt\\nSteven van de Par\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.15888\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Jun 2021 08:25:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Trends in Hearing\\u00a7r"}']}
{title:'Fichna et al. (§72021§r)', author: 'Stefan Fichna; Thomas Biberger; Bernhard U. Seeber; Stephan D. Ewert', display:{Lore:['[{"text": "arXiv:2106.15909", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEffect of acoustic scene complexity and visual scene representation on auditory perception in virtual audio-visual environments\\u00a7r\\n\\n\\u00a78\\u00a7oStefan Fichna\\nThomas Biberger\\nBernhard U. Seeber\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.15909\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/I3DA48870.2021.9610916\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 7 Nov 2021 14:29:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted publication in Proceedings of 3DA 2021 International Conference on Immersive and 3D Audio\\u00a7r"}']}
{title:'Liao et al. (§72021§r)', author: 'Dexin Liao; Jing Li; Yiming Zhi; Song Li; Qingyang Hong; Lin Li', display:{Lore:['[{"text": "arXiv:2106.15950", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Integrated Framework for Two-pass Personalized Voice Trigger\\u00a7r\\n\\n\\u00a78\\u00a7oDexin Liao\\nJing Li\\nYiming Zhi\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.15950\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Jun 2021 09:58:55 GMT)\\u00a7r"}']}
{title:'Kolotzek et al. (§72021§r)', author: 'Norbert Kolotzek; Pierre G. Aublin; Bernhard U. Seeber', display:{Lore:['[{"text": "arXiv:2106.16024", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast processing explains the effect of sound reflection on binaural unmasking\\u00a7r\\n\\n\\u00a78\\u00a7oNorbert Kolotzek\\nPierre G. Aublin\\nBernhard U. Seeber\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.16024\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Jun 2021 12:45:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint from June 2nd , 2021\\u00a7r"}']}
{title:'Kirsch et al. (§72021§r)', author: 'Christoph Kirsch; Josef Poppitz; Torben Wendt; Steven van de Par; Stephan D. Ewert', display:{Lore:['[{"text": "arXiv:2107.00004", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComputationally efficient spatial rendering of late reverberation in virtual acoustic environments\\u00a7r\\n\\n\\u00a78\\u00a7oChristoph Kirsch\\nJosef Poppitz\\nTorben Wendt\\nSteven van de Par\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.00004\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/I3DA48870.2021.9610896\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2021 Immersive and 3D Audio: from Architecture to Automotive\\n  (I3DA)\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Jun 2021 08:33:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to the I3DA 2021 InternationalConference(IEEE Xplore Digital Library). arXiv admin note: text overlap with arXiv:2106.15888\\u00a7r"}']}
{title:'Afshan et al. (§72021§r)', author: 'Amber Afshan; Kshitiz Kumar; Jian Wu', display:{Lore:['[{"text": "arXiv:2107.00099", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSequence-level Confidence Classifier for ASR Utterance Accuracy and Application to Acoustic Models\\u00a7r\\n\\n\\u00a78\\u00a7oAmber Afshan\\nKshitiz Kumar\\nJian Wu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.00099\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Jun 2021 20:50:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021\\u00a7r"}']}
{title:'Mendonça et al. (§72021§r)', author: 'John Mendonça; Rubén Solera-Ureña; Alberto Abad; Isabel Trancoso', display:{Lore:['[{"text": "arXiv:2107.00112", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing Self-Supervised Feature Extractors with Attention for Automatic COVID-19 Detection from Speech\\u00a7r\\n\\n\\u00a78\\u00a7oJohn Mendon\\u00e7a\\nRub\\u00e9n Solera-Ure\\u00f1a\\nAlberto Abad\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.00112\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Jun 2021 21:35:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech2021\\u00a7r"}']}
{title:'Dietz et al. (§72021§r)', author: 'Mathias Dietz; Jörg Encke; Kristin I. Bracklo; Stephan D. Ewert', display:{Lore:['[{"text": "arXiv:2107.00320", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrediction of tone detection thresholds in interaurally delayed noise based on interaural phase difference fluctuations\\u00a7r\\n\\n\\u00a78\\u00a7oMathias Dietz\\nJ\\u00f6rg Encke\\nKristin I. Bracklo\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.00320\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1051/aacus/2021054\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nActa Acustica, 5, 60 (2021)\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jul 2021 09:21:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to Acta Acustica for possible publication\\u00a7r"}']}
{title:'Inaguma et al. (§72021§r)', author: 'Hirofumi Inaguma; Tatsuya Kawahara', display:{Lore:['[{"text": "arXiv:2107.00635", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStableEmit: Selection Probability Discount for Reducing Emission Latency of Streaming Monotonic Attention ASR\\u00a7r\\n\\n\\u00a78\\u00a7oHirofumi Inaguma\\nTatsuya Kawahara\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.00635\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 15 Jul 2021 17:58:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2021\\u00a7r"}']}
{title:'Inaguma et al. (§72021§r)', author: 'Hirofumi Inaguma; Brian Yan; Siddharth Dalmia; Pengcheng Guo; Jiatong Shi; Kevin Duh; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2107.00636", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lESPnet-ST IWSLT 2021 Offline Speech Translation System\\u00a7r\\n\\n\\u00a78\\u00a7oHirofumi Inaguma\\nBrian Yan\\nSiddharth Dalmia\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.00636\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 6 Jul 2021 15:43:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIWSLT 2021\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Qiujia Li; Chao Zhang; Philip C. Woodland', display:{Lore:['[{"text": "arXiv:2107.00764", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCombining Frame-Synchronous and Label-Synchronous Systems for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oQiujia Li\\nChao Zhang\\nPhilip C. Woodland\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.00764\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jul 2021 22:27:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEE/ACM Transactionson Audio Speech and Language Processing\\u00a7r"}']}
{title:'Rikhye et al. (§72021§r)', author: 'Rajeev Rikhye; Quan Wang; Qiao Liang; Yanzhang He; Ian McGraw', display:{Lore:['[{"text": "arXiv:2107.01201", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-user VoiceFilter-Lite via Attentive Speaker Embedding\\u00a7r\\n\\n\\u00a78\\u00a7oRajeev Rikhye\\nQuan Wang\\nQiao Liang\\nYanzhang He\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.01201\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Nov 2021 16:52:37 GMT)\\u00a7r"}']}
{title:'Moritz et al. (§72021§r)', author: 'Niko Moritz; Takaaki Hori; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2107.01269", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDual Causal/Non-Causal Self-Attention for Streaming End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oNiko Moritz\\nTakaaki Hori\\nJonathan Le Roux\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.01269\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Jul 2021 20:56:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021\\u00a7r"}']}
{title:'Lohrenz et al. (§72021§r)', author: 'Timo Lohrenz; Patrick Schwarz; Zhengyang Li; Tim Fingscheidt', display:{Lore:['[{"text": "arXiv:2107.01275", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRelaxed Attention: A Simple Method to Boost Performance of End-to-End Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oTimo Lohrenz\\nPatrick Schwarz\\nZhengyang Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.01275\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 15 Dec 2021 11:10:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ASRU 2021, code contributed to http://github.com/freewym/espresso\\u00a7r"}']}
{title:'Chao et al. (§72021§r)', author: 'Fu-An Chao; Shao-Wei Fan Jiang; Bi-Cheng Yan; Jeih-weih Hung; Berlin Chen', display:{Lore:['[{"text": "arXiv:2107.01531", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTENET: A Time-reversal Enhancement Network for Noise-robust ASR\\u00a7r\\n\\n\\u00a78\\u00a7oFu-An Chao\\nShao-Wei Fan Jiang\\nBi-Cheng Yan\\nJeih-weih Hung\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.01531\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 14 Sep 2021 08:43:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ASRU 2021\\u00a7r"}']}
{title:'Horiguchi et al. (§72021§r)', author: 'Shota Horiguchi; Shinji Watanabe; Paola Garcia; Yawen Xue; Yuki Takashima; Yohei Kawaguchi', display:{Lore:['[{"text": "arXiv:2107.01545", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Neural Diarization for Unlimited Numbers of Speakers Using Global and Local Attractors\\u00a7r\\n\\n\\u00a78\\u00a7oShota Horiguchi\\nShinji Watanabe\\nPaola Garcia\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.01545\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 23 Sep 2021 16:06:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ASRU 2021\\u00a7r"}']}
{title:'Tan et al. (§72021§r)', author: 'Daxin Tan; Liqun Deng; Yu Ting Yeung; Xin Jiang; Xiao Chen; Tan Lee', display:{Lore:['[{"text": "arXiv:2107.01554", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEditSpeech: A Text Based Speech Editing System Using Partial Inference and Bidirectional Fusion\\u00a7r\\n\\n\\u00a78\\u00a7oDaxin Tan\\nLiqun Deng\\nYu Ting Yeung\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.01554\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 8 Oct 2021 03:06:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ASRU 2021\\u00a7r"}']}
{title:'Wu et al. (§72021§r)', author: 'Jian Wu; Zhuo Chen; Sanyuan Chen; Yu Wu; Takuya Yoshioka; Naoyuki Kanda; Shujie Liu; Jinyu Li', display:{Lore:['[{"text": "arXiv:2107.01922", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigation of Practical Aspects of Single Channel Speech Separation for ASR\\u00a7r\\n\\n\\u00a78\\u00a7oJian Wu\\nZhuo Chen\\nSanyuan Chen\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.01922\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Jul 2021 10:35:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2021\\u00a7r"}']}
{title:'Csapó et al. (§72021§r)', author: 'Tamás Gábor Csapó; László Tóth; Gábor Gosztolya; Alexandra Markó', display:{Lore:['[{"text": "arXiv:2107.02003", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Synthesis from Text and Ultrasound Tongue Image-based Articulatory Input\\u00a7r\\n\\n\\u00a78\\u00a7oTam\\u00e1s G\\u00e1bor Csap\\u00f3\\nL\\u00e1szl\\u00f3 T\\u00f3th\\nG\\u00e1bor Gosztolya\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.02003\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Jul 2021 13:21:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at SSW11 (11th Speech Synthesis Workshop)\\u00a7r"}']}
{title:'Niu et al. (§72021§r)', author: 'Shu-Tong Niu; Jun Du; Lei Sun; Chin-Hui Lee', display:{Lore:['[{"text": "arXiv:2107.02357", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSeparation Guided Speaker Diarization in Realistic Mismatched Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oShu-Tong Niu\\nJun Du\\nLei Sun\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.02357\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Jul 2021 02:39:32 GMT)\\u00a7r"}']}
{title:'Gutierrez et al. (§72021§r)', author: 'Elijah Gutierrez; Pilar Oplustil-Gallegos; Catherine Lai', display:{Lore:['[{"text": "arXiv:2107.02527", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLocation, Location: Enhancing the Evaluation of Text-to-Speech Synthesis Using the Rapid Prosody Transcription Paradigm\\u00a7r\\n\\n\\u00a78\\u00a7oElijah Gutierrez\\nPilar Oplustil-Gallegos\\nCatherine Lai\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.02527\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Jul 2021 10:36:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Speech Synthesis Workshop 2019: https://ssw11.hte.hu/en/\\u00a7r"}']}
{title:'An et al. (§72021§r)', author: 'Keyu An; Zhijian Ou', display:{Lore:['[{"text": "arXiv:2107.02670", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting Single-Channel Speech For Multi-channel End-to-end Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oKeyu An\\nZhijian Ou\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.02670\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Jul 2021 15:21:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ASRU 2021\\u00a7r"}']}
{title:'Di Benedetto et al. (§72021§r)', author: 'Maria-Gabriella Di Benedetto; Stefanie Shattuck-Hufnagel; Jeung-Yoon Choi; Luca De Nardis; Javier Arango; Ian Chan; Alec DeCaprio', display:{Lore:['[{"text": "arXiv:2107.02720", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLexical Access Model for Italian \\u2013 Modeling human speech processing: identification of words in running speech toward lexical access based on the detection of landmarks and other acoustic cues to features\\u00a7r\\n\\n\\u00a78\\u00a7oMaria-Gabriella Di Benedetto\\nStefanie Shattuck-Hufnagel\\nJeung-Yoon Choi\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.02720\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Jun 2021 10:54:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Language and Speech, 2021\\u00a7r"}']}
{title:'Kanda et al. (§72021§r)', author: 'Naoyuki Kanda; Xiong Xiao; Jian Wu; Tianyan Zhou; Yashesh Gaur; Xiaofei Wang; Zhong Meng; Zhuo Chen; Takuya Yoshioka', display:{Lore:['[{"text": "arXiv:2107.02852", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparative Study of Modular and Joint Approaches for Speaker-Attributed ASR on Monaural Long-Form Audio\\u00a7r\\n\\n\\u00a78\\u00a7oNaoyuki Kanda\\nXiong Xiao\\nJian Wu\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.02852\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 17 Sep 2021 23:57:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ASRU 2021\\u00a7r"}']}
{title:'Zheng et al. (§72021§r)', author: 'Huahuan Zheng; Wenjie Peng; Zhijian Ou; Jinsong Zhang', display:{Lore:['[{"text": "arXiv:2107.03007", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdvancing CTC-CRF Based End-to-End Speech Recognition with Wordpieces and Conformers\\u00a7r\\n\\n\\u00a78\\u00a7oHuahuan Zheng\\nWenjie Peng\\nZhijian Ou\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.03007\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 8 Jul 2021 14:04:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ASRU 2021\\u00a7r"}']}
{title:'Cao et al. (§72021§r)', author: 'Songjun Cao; Yike Zhang; Xiaobing Feng; Long Ma', display:{Lore:['[{"text": "arXiv:2107.03165", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Speech Recognition Accuracy of Local POI Using Geographical Models\\u00a7r\\n\\n\\u00a78\\u00a7oSongjun Cao\\nYike Zhang\\nXiaobing Feng\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.03165\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Jul 2021 11:46:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by SLT 2021\\u00a7r"}']}
{title:'Nam et al. (§72021§r)', author: 'Hyeonuk Nam; Byeong-Yun Ko; Gyeong-Tae Lee; Seong-Hu Kim; Won-Ho Jung; Sang-Min Choi; Yong-Hwa Park', display:{Lore:['[{"text": "arXiv:2107.03649", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHeavily Augmented Sound Event Detection utilizing Weak Predictions\\u00a7r\\n\\n\\u00a78\\u00a7oHyeonuk Nam\\nByeong-Yun Ko\\nGyeong-Tae Lee\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.03649\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 14 Sep 2021 05:14:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWon 3rd place on IEEE DCASE 2021 Task 4\\u00a7r"}']}
{title:'Du et al. (§72021§r)', author: 'Zongyang Du; Berrak Sisman; Kun Zhou; Haizhou Li', display:{Lore:['[{"text": "arXiv:2107.03748", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExpressive Voice Conversion: A Joint Framework for Speaker Identity and Emotional Style Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oZongyang Du\\nBerrak Sisman\\nKun Zhou\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.03748\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Oct 2021 00:39:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ASRU 2021\\u00a7r"}']}
{title:'Venugopalan et al. (§72021§r)', author: 'Subhashini Venugopalan; Joel Shor; Manoj Plakal; Jimmy Tobin; Katrin Tomanek; Jordan R. Green; Michael P. Brenner', display:{Lore:['[{"text": "arXiv:2107.03985", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparing Supervised Models And Learned Speech Representations For Classifying Intelligibility Of Disordered Speech On Selected Phrases\\u00a7r\\n\\n\\u00a78\\u00a7oSubhashini Venugopalan\\nJoel Shor\\nManoj Plakal\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.03985\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Jul 2021 17:24:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2021\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Xiaohui Zhang; Vimal Manohar; David Zhang; Frank Zhang; Yangyang Shi; Nayan Singhal; Julian Chan; Fuchun Peng; Yatharth Saraf; Mike Seltzer', display:{Lore:['[{"text": "arXiv:2107.04154", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn lattice-free boosted MMI training of HMM and CTC-based full-context ASR models\\u00a7r\\n\\n\\u00a78\\u00a7oXiaohui Zhang\\nVimal Manohar\\nDavid Zhang\\n+ 6 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.04154\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Sep 2021 02:49:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by ASRU 2021\\u00a7r"}']}
{title:'Luo et al. (§72021§r)', author: 'Jian Luo; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2107.04227", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDropout Regularization for Self-Supervised Learning of Transformer Encoder Speech Representation\\u00a7r\\n\\n\\u00a78\\u00a7oJian Luo\\nJianzong Wang\\nNing Cheng\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.04227\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Jul 2021 05:57:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7owill be presented in INTERSPEECH 2021\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Lu Zhang; Mingjiang Wang; Andong Li; Zehua Zhang; Xuyi Zhuang', display:{Lore:['[{"text": "arXiv:2107.04232", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIncorporating Multi-Target in Multi-Stage Speech Enhancement Model for Better Generalization\\u00a7r\\n\\n\\u00a78\\u00a7oLu Zhang\\nMingjiang Wang\\nAndong Li\\nZehua Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.04232\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Jul 2021 06:12:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to APSIPA-ASC 2021\\u00a7r"}']}
{title:'Schulze et al. (§72021§r)', author: 'Sören Schulze; Johannes Leuschner; Emily J. King', display:{Lore:['[{"text": "arXiv:2107.04235", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBlind Source Separation in Polyphonic Music Recordings Using Deep Neural Networks Trained via Policy Gradients\\u00a7r\\n\\n\\u00a78\\u00a7oS\\u00f6ren Schulze\\nJohannes Leuschner\\nEmily J. King\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.04235\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 9 Aug 2021 11:00:04 GMT)\\u00a7r"}']}
{title:'Luo et al. (§72021§r)', author: 'Jian Luo; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2107.04289", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLoss Prediction: End-to-End Active Learning Approach For Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJian Luo\\nJianzong Wang\\nNing Cheng\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.04289\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Jul 2021 08:03:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IJCNN 2021\\u00a7r"}']}
{title:'Villalba et al. (§72021§r)', author: 'Jesús Villalba; Sonal Joshi; Piotr Żelasko; Najim Dehak', display:{Lore:['[{"text": "arXiv:2107.04448", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRepresentation Learning to Classify and Detect Adversarial Attacks against Speaker and Speech Recognition Systems\\u00a7r\\n\\n\\u00a78\\u00a7oJes\\u00fas Villalba\\nSonal Joshi\\nPiotr \\u017belasko\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.04448\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Jul 2021 13:55:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2021\\u00a7r"}']}
{title:'Shankar et al. (§72021§r)', author: 'Ravi Shankar; Archana Venkataraman', display:{Lore:['[{"text": "arXiv:2107.04973", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Deep-Bayesian Framework for Adaptive Speech Duration Modification\\u00a7r\\n\\n\\u00a78\\u00a7oRavi Shankar\\nArchana Venkataraman\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.04973\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 11 Jul 2021 05:53:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 7 figures\\u00a7r"}']}
{title:'Sreeram et al. (§72021§r)', author: 'Anirudh Sreeram; Nicholas Mehlman; Raghuveer Peri; Dillon Knox; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2107.05222", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerceptual-based deep-learning denoiser as a defense against adversarial attacks on ASR systems\\u00a7r\\n\\n\\u00a78\\u00a7oAnirudh Sreeram\\nNicholas Mehlman\\nRaghuveer Peri\\nDillon Knox\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.05222\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 12 Jul 2021 07:00:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures submitted to ASRU 2021\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Chengyi Wang; Yu Wu; Shujie Liu; Jinyu Li; Yao Qian; Kenichi Kumatani; Furu Wei', display:{Lore:['[{"text": "arXiv:2107.05233", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUniSpeech at scale: An Empirical Study of Pre-training Method on Large-Scale Speech Recognition Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oChengyi Wang\\nYu Wu\\nShujie Liu\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.05233\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 12 Jul 2021 07:30:17 GMT)\\u00a7r"}']}
{title:'Mesaros et al. (§72021§r)', author: 'Annamaria Mesaros; Toni Heittola; Tuomas Virtanen; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:2107.05463", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Event Detection: A Tutorial\\u00a7r\\n\\n\\u00a78\\u00a7oAnnamaria Mesaros\\nToni Heittola\\nTuomas Virtanen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.05463\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/MSP.2021.3090678\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 12 Jul 2021 14:30:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oto appear in IEEE Signal Processing Magazine, Volume 38, Issue 5\\u00a7r"}']}
{title:'Csapó (§72021§r)', author: 'Tamás Gábor Csapó', display:{Lore:['[{"text": "arXiv:2107.05550", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExtending Text-to-Speech Synthesis with Articulatory Movement Prediction using Ultrasound Tongue Imaging\\u00a7r\\n\\n\\u00a78\\u00a7oTam\\u00e1s G\\u00e1bor Csap\\u00f3\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.05550\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 12 Jul 2021 16:19:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at SSW11 (11th Speech Synthesis Workshop). arXiv admin note: text overlap with arXiv:2107.02003\\u00a7r"}']}
{title:'Xu et al. (§72021§r)', author: 'Menglong Xu; Shengqiang Li; Chengdong Liang; Xiao-Lei Zhang', display:{Lore:['[{"text": "arXiv:2107.05859", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAUC Optimization for Robust Small-footprint Keyword Spotting with Limited Training Data\\u00a7r\\n\\n\\u00a78\\u00a7oMenglong Xu\\nShengqiang Li\\nChengdong Liang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.05859\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Jul 2021 05:44:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ASRU2021\\u00a7r"}']}
{title:'Zhou et al. (§72021§r)', author: 'Long Zhou; Jinyu Li; Eric Sun; Shujie Liu', display:{Lore:['[{"text": "arXiv:2107.05876", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Configurable Multilingual Model is All You Need to Recognize All Languages\\u00a7r\\n\\n\\u00a78\\u00a7oLong Zhou\\nJinyu Li\\nEric Sun\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.05876\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Jul 2021 06:52:41 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Lu Zhang; Chenxing Li; Feng Deng; Xiaorui Wang', display:{Lore:['[{"text": "arXiv:2107.06467", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Task Audio Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oLu Zhang\\nChenxing Li\\nFeng Deng\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.06467\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Jul 2021 03:04:33 GMT)\\u00a7r"}']}
{title:'Tao et al. (§72021§r)', author: 'Ruijie Tao; Zexu Pan; Rohan Kumar Das; Xinyuan Qian; Mike Zheng Shou; Haizhou Li', display:{Lore:['[{"text": "arXiv:2107.06592", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIs Someone Speaking? Exploring Long-term Temporal Features for Audio-visual Active Speaker Detection\\u00a7r\\n\\n\\u00a78\\u00a7oRuijie Tao\\nZexu Pan\\nRohan Kumar Das\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.06592\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3474085.3475587\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 25 Jul 2021 15:00:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oACM Multimedia 2021\\u00a7r"}']}
{title:'Luong et al. (§72021§r)', author: 'Manh Luong; Viet Anh Tran', display:{Lore:['[{"text": "arXiv:2107.06642", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMany-to-Many Voice Conversion based Feature Disentanglement using Variational Autoencoder\\u00a7r\\n\\n\\u00a78\\u00a7oManh Luong\\nViet Anh Tran\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.06642\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nINTERSPEECH 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 11 Jul 2021 13:31:16 GMT)\\u00a7r"}']}
{title:'Braun et al. (§72021§r)', author: 'Sebastian Braun; Ivan Tashev', display:{Lore:['[{"text": "arXiv:2107.06775", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow complexity online convolutional beamforming\\u00a7r\\n\\n\\u00a78\\u00a7oSebastian Braun\\nIvan Tashev\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.06775\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nWASPAA 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Jul 2021 15:35:23 GMT)\\u00a7r"}']}
{title:'Steinmetz et al. (§72021§r)', author: 'Christian J. Steinmetz; Vamsi Krishna Ithapu; Paul Calamia', display:{Lore:['[{"text": "arXiv:2107.07503", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFiltered Noise Shaping for Time Domain Room Impulse Response Estimation From Reverberant Speech\\u00a7r\\n\\n\\u00a78\\u00a7oChristian J. Steinmetz\\nVamsi Krishna Ithapu\\nPaul Calamia\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.07503\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Jul 2021 17:56:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to WASPAA 2021. See details at https://facebookresearch.github.io/FiNS/\\u00a7r"}']}
{title:'Inaguma et al. (§72021§r)', author: 'Hirofumi Inaguma; Tatsuya Kawahara', display:{Lore:['[{"text": "arXiv:2107.07509", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVAD-free Streaming Hybrid CTC/Attention ASR for Unsegmented Recording\\u00a7r\\n\\n\\u00a78\\u00a7oHirofumi Inaguma\\nTatsuya Kawahara\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.07509\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Jul 2021 17:59:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2021\\u00a7r"}']}
{title:'Higuchi et al. (§72021§r)', author: 'Takuya Higuchi; Anmol Gupta; Chandra Dhir', display:{Lore:['[{"text": "arXiv:2107.07634", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-task Learning with Cross Attention for Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oTakuya Higuchi\\nAnmol Gupta\\nChandra Dhir\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.07634\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 22 Sep 2021 17:45:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ASRU 2021\\u00a7r"}']}
{title:'Sun et al. (§72021§r)', author: 'Xiaoheng Sun; Yiliang Jiang; Wei Li', display:{Lore:['[{"text": "arXiv:2107.08425", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lResidual Attention Based Network for Automatic Classification of Phonation Modes\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoheng Sun\\nYiliang Jiang\\nWei Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.08425\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 18 Jul 2021 12:37:00 GMT)\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Xu Li; Xixin Wu; Hui Lu; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2107.08803", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChannel-wise Gated Res2Net: Towards Robust Detection of Synthetic Speech Attacks\\u00a7r\\n\\n\\u00a78\\u00a7oXu Li\\nXixin Wu\\nHui Lu\\nXunying Liu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.08803\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Jul 2021 12:27:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Praher et al. (§72021§r)', author: 'Verena Praher; Katharina Prinz; Arthur Flexer; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2107.09045", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Veracity of Local, Model-agnostic Explanations in Audio Classification: Targeted Investigations with Adversarial Examples\\u00a7r\\n\\n\\u00a78\\u00a7oVerena Praher\\nKatharina Prinz\\nArthur Flexer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.09045\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 6 Sep 2021 14:51:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 4 figures, to be published in Proceedings of the International Society for Music Information Retrieval Conference 2021 (ISMIR 2021)\\u00a7r"}']}
{title:'Aralikatti et al. (§72021§r)', author: 'Rohith Aralikatti; Anton Ratnarajah; Zhenyu Tang; Dinesh Manocha', display:{Lore:['[{"text": "arXiv:2107.09177", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Reverberant Speech Separation with Multi-stage Training and Curriculum Learning\\u00a7r\\n\\n\\u00a78\\u00a7oRohith Aralikatti\\nAnton Ratnarajah\\nZhenyu Tang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.09177\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Jul 2021 22:11:19 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Tianzi Wang; Yuya Fujita; Xuankai Chang; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2107.09428", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreaming End-to-End ASR based on Blockwise Non-Autoregressive Models\\u00a7r\\n\\n\\u00a78\\u00a7oTianzi Wang\\nYuya Fujita\\nXuankai Chang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.09428\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Jul 2021 11:42:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figures, Interspeech21 conference\\u00a7r"}']}
{title:'Mei et al. (§72021§r)', author: 'Xinhao Mei; Xubo Liu; Qiushi Huang; Mark D. Plumbley; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2107.09817", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Captioning Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oXinhao Mei\\nXubo Liu\\nQiushi Huang\\nMark D. Plumbley\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.09817\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Jul 2021 00:31:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Xubo Liu; Qiushi Huang; Xinhao Mei; Tom Ko; H Lilian Tang; Mark D. Plumbley; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2107.09990", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCL4AC: A Contrastive Loss for Audio Captioning\\u00a7r\\n\\n\\u00a78\\u00a7oXubo Liu\\nQiushi Huang\\nXinhao Mei\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.09990\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 22 Nov 2021 10:16:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe first two authors contributed equally, 5 pages, 3 figures, accepted by DCASE2021 Workshop\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Xubo Liu; Turab Iqbal; Jinzheng Zhao; Qiushi Huang; Mark D. Plumbley; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2107.09998", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConditional Sound Generation Using Neural Discrete Time-Frequency Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oXubo Liu\\nTurab Iqbal\\nJinzheng Zhao\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.09998\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 6 Oct 2021 13:44:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE 31st InternationalWorlshop on Machine Learning for Signal Processing (MLSP) 2021, 6 pages, 1 figure\\u00a7r"}']}
{title:'Torcoli et al. (§72021§r)', author: 'Matteo Torcoli; Jouni Paulus; Thorsten Kastner; Christian Uhle', display:{Lore:['[{"text": "arXiv:2107.10151", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lControlling the Remixing of Separated Dialogue with a Non-Intrusive Quality Estimate\\u00a7r\\n\\n\\u00a78\\u00a7oMatteo Torcoli\\nJouni Paulus\\nThorsten Kastner\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.10151\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/WASPAA52581.2021.9632756\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Jul 2021 15:26:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oManuscript accepted for the 2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics\\u00a7r"}']}
{title:'Nguyen et al. (§72021§r)', author: 'Thi Ngoc Tho Nguyen; Karn N. Watcharasupat; Zhen Jian Lee; Ngoc Khanh Nguyen; Douglas L. Jones; Woon Seng Gan', display:{Lore:['[{"text": "arXiv:2107.10469", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhat Makes Sound Event Localization and Detection Difficult? Insights from Error Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oThi Ngoc Tho Nguyen\\nKarn N. Watcharasupat\\nZhen Jian Lee\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.10469\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the Detection and Classification of Acoustic Scenes\\n  and Events 2021 Workshop, pp. 120-124\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 2 Oct 2021 05:29:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for the6th Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE), 2021\\u00a7r"}']}
{title:'Watcharasupat et al. (§72021§r)', author: 'Karn N. Watcharasupat; Thi Ngoc Tho Nguyen; Ngoc Khanh Nguyen; Zhen Jian Lee; Douglas L. Jones; Woon Seng Gan', display:{Lore:['[{"text": "arXiv:2107.10471", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Polyphonic Sound Event Detection on Multichannel Recordings with the S\\u00f8rensen-Dice Coefficient Loss and Transfer Learning\\u00a7r\\n\\n\\u00a78\\u00a7oKarn N. Watcharasupat\\nThi Ngoc Tho Nguyen\\nNgoc Khanh Nguyen\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.10471\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 2 Oct 2021 07:38:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to the 6th Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE), 2021\\u00a7r"}']}
{title:'Uhle et al. (§72021§r)', author: 'Christian Uhle; Matteo Torcoli; Jouni Paulus', display:{Lore:['[{"text": "arXiv:2107.10562", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lControlling the Perceived Sound Quality for Dialogue Enhancement with Deep Learning\\u00a7r\\n\\n\\u00a78\\u00a7oChristian Uhle\\nMatteo Torcoli\\nJouni Paulus\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.10562\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP40776.2020.9053789\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2020 - 2020 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Jul 2021 10:30:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted paper at ICASSP 2020\\u00a7r"}']}
{title:'Salimzianov (§72021§r)', author: 'Ilnar Salimzianov', display:{Lore:['[{"text": "arXiv:2107.10637", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA baseline model for computationally inexpensive speech recognition for Kazakh using the Coqui STT framework\\u00a7r\\n\\n\\u00a78\\u00a7oIlnar Salimzianov\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.10637\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 28 Nov 2021 12:30:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 2 tables\\u00a7r"}']}
{title:'Rownicka et al. (§72021§r)', author: 'Joanna Rownicka; Kilian Sprenkamp; Antonio Tripiana; Volodymyr Gromoglasov; Timo P Kunz', display:{Lore:['[{"text": "arXiv:2107.10658", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDigital Einstein Experience: Fast Text-to-Speech for Conversational AI\\u00a7r\\n\\n\\u00a78\\u00a7oJoanna Rownicka\\nKilian Sprenkamp\\nAntonio Tripiana\\nVolodymyr Gromoglasov\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.10658\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Jul 2021 12:03:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at Interspeech 2021\\u00a7r"}']}
{title:'Greysukh (§72021§r)', author: 'Alexander Greysukh', display:{Lore:['[{"text": "arXiv:2107.10676", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCNN Classifier for Just-in-Time Woodpeckers Detection and Deterrent\\u00a7r\\n\\n\\u00a78\\u00a7oAlexander Greysukh\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.10676\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Jul 2021 14:32:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 4 figures\\u00a7r"}']}
{title:'Ma et al. (§72021§r)', author: 'Duo Ma; Nana Hou; Van Tung Pham; Haihua Xu; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2107.10701", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultitask-Based Joint Learning Approach To Robust ASR For Radio Communication Speech\\u00a7r\\n\\n\\u00a78\\u00a7oDuo Ma\\nNana Hou\\nVan Tung Pham\\nHaihua Xu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.10701\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Jul 2021 14:11:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7pages,3figures,Submitted to APSIPA2021\\u00a7r"}']}
{title:'Kalinov et al. (§72021§r)', author: 'Aleksei Kalinov; Somshubra Majumdar; Jagadeesh Balam; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2107.10708", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCarneliNet: Neural Mixture Model for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAleksei Kalinov\\nSomshubra Majumdar\\nJagadeesh Balam\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.10708\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Jul 2021 14:29:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ASRU 2021\\u00a7r"}']}
{title:'Petermann et al. (§72021§r)', author: 'Darius Petermann; Seungkwon Beack; Minje Kim', display:{Lore:['[{"text": "arXiv:2107.10843", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHARP-Net: Hyper-Autoencoded Reconstruction Propagation for Scalable Neural Audio Coding\\u00a7r\\n\\n\\u00a78\\u00a7oDarius Petermann\\nSeungkwon Beack\\nMinje Kim\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.10843\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 23 Jul 2021 14:33:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)2021, Mohonk Mountain House, New Paltz, NY\\u00a7r"}']}
{title:'Thaler et al. (§72021§r)', author: 'Fabian Thaler; Stefan Faußer; Heiko Gewald', display:{Lore:['[{"text": "arXiv:2107.11175", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing NLP to analyze whether customer statements comply with their inner belief\\u00a7r\\n\\n\\u00a78\\u00a7oFabian Thaler\\nStefan Fau\\u00dfer\\nHeiko Gewald\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.11175\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 11 Aug 2021 08:55:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o26 pages, 2 figures, 4 tables\\u00a7r"}']}
{title:'Shi et al. (§72021§r)', author: 'Xuan Shi; Erica Cooper; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2107.11506", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUse of speaker recognition approaches for learning and evaluating embedding representations of musical instrument sounds\\u00a7r\\n\\n\\u00a78\\u00a7oXuan Shi\\nErica Cooper\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.11506\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 24 Dec 2021 05:40:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the IEEE/ACM Transactionson Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Lu et al. (§72021§r)', author: 'Yen-Ju Lu; Yu Tsao; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2107.11876", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Study on Speech Enhancement Based on Diffusion Probabilistic Model\\u00a7r\\n\\n\\u00a78\\u00a7oYen-Ju Lu\\nYu Tsao\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.11876\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 21 Nov 2021 19:37:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in APSIPA 2021\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Jinjiang Liu; Xueliang Zhang', display:{Lore:['[{"text": "arXiv:2107.11968", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInplace Gated Convolutional Recurrent Neural Network For Dual-channel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oJinjiang Liu\\nXueliang Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.11968\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Jul 2021 05:49:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH2021\\u00a7r"}']}
{title:'Chen et al. (§72021§r)', author: 'Xinhui Chen; You Zhang; Ge Zhu; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2107.12018", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUR Channel-Robust Synthetic Speech Detection System for ASVspoof 2021\\u00a7r\\n\\n\\u00a78\\u00a7oXinhui Chen\\nYou Zhang\\nGe Zhu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.12018\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 23 Aug 2021 19:42:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proc. ASVspoof 2021 Workshop\\u00a7r"}']}
{title:'Zainkó et al. (§72021§r)', author: 'Csaba Zainkó; László Tóth; Amin Honarmandi Shandiz; Gábor Gosztolya; Alexandra Markó; Géza Németh; Tamás Gábor Csapó', display:{Lore:['[{"text": "arXiv:2107.12051", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptation of Tacotron2-based Text-To-Speech for Articulatory-to-Acoustic Mapping using Ultrasound Tongue Imaging\\u00a7r\\n\\n\\u00a78\\u00a7oCsaba Zaink\\u00f3\\nL\\u00e1szl\\u00f3 T\\u00f3th\\nAmin Honarmandi Shandiz\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.12051\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Jul 2021 09:19:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at SSW11. arXiv admin note:text overlap with arXiv:2008.03152\\u00a7r"}']}
{title:'Martín-Morató et al. (§72021§r)', author: 'Irene Martín-Morató; Manu Harju; Annamaria Mesaros', display:{Lore:['[{"text": "arXiv:2107.12089", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCrowdsourcing strong labels for sound event detection\\u00a7r\\n\\n\\u00a78\\u00a7oIrene Mart\\u00edn-Morat\\u00f3\\nManu Harju\\nAnnamaria Mesaros\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.12089\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Jul 2021 10:11:24 GMT)\\u00a7r"}']}
{title:'Ge et al. (§72021§r)', author: 'Wanying Ge; Jose Patino; Massimiliano Todisco; Nicholas Evans', display:{Lore:['[{"text": "arXiv:2107.12212", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRaw Differentiable Architecture Search for Speech Deepfake and Spoofing Detection\\u00a7r\\n\\n\\u00a78\\u00a7oWanying Ge\\nJose Patino\\nMassimiliano Todisco\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.12212\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Oct 2021 14:07:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ASVspoof 2021 Workshop\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Siyuan Zhang; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2107.12601", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMicrophone Array Generalization for Multichannel Narrowband Deep Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oSiyuan Zhang\\nXiaofei Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.12601\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Jul 2021 05:03:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech Conference 2021\\u00a7r"}']}
{title:'Tak et al. (§72021§r)', author: 'Hemlata Tak; Jee-weon Jung; Jose Patino; Madhu Kamble; Massimiliano Todisco; Nicholas Evans', display:{Lore:['[{"text": "arXiv:2107.12710", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Spectro-Temporal Graph Attention Networks for Speaker Verification Anti-Spoofing and Speech Deepfake Detection\\u00a7r\\n\\n\\u00a78\\u00a7oHemlata Tak\\nJee-weon Jung\\nJose Patino\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.12710\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 23 Aug 2021 17:06:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ASVspoof 2021 Workshop\\u00a7r"}']}
{title:'Mukherjee et al. (§72021§r)', author: 'Uddipan Mukherjee; Sidharth Pancholi', display:{Lore:['[{"text": "arXiv:2107.13237", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Visual Domain Transfer Learning Approach for Heartbeat Sound Classification\\u00a7r\\n\\n\\u00a78\\u00a7oUddipan Mukherjee\\nSidharth Pancholi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.13237\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 4 Oct 2021 07:05:36 GMT)\\u00a7r"}']}
{title:'Devi et al. (§72021§r)', author: 'Thangjam Clarinda Devi; Kabita Thaoroijam', display:{Lore:['[{"text": "arXiv:2107.13419", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVowel-based Meeteilon dialect identification using a Random Forest classifier\\u00a7r\\n\\n\\u00a78\\u00a7oThangjam Clarinda Devi\\nKabita Thaoroijam\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.13419\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Jul 2021 04:09:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, double coulumn, 8 Figures, 1 table. Already presented as poster presentation at OCOCOSDA 2020 but not yet published\\u00a7r"}']}
{title:'Yang et al. (§72021§r)', author: 'Haici Yang; Shivani Firodiya; Nicholas J. Bryan; Minje Kim', display:{Lore:['[{"text": "arXiv:2107.13634", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDon\'t Separate, Learn to Remix: End-to-End Neural Remixing with Joint Optimization\\u00a7r\\n\\n\\u00a78\\u00a7oHaici Yang\\nShivani Firodiya\\nNicholas J. Bryan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.13634\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 22 Oct 2021 17:56:37 GMT)\\u00a7r"}']}
{title:'Slyman et al. (§72021§r)', author: 'Eric Slyman; Chris Daw; Morgan Skrabut; Ana Usenko; Brian Hutchinson', display:{Lore:['[{"text": "arXiv:2107.14369", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFine-Grained Classroom Activity Detection from Audio with Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oEric Slyman\\nChris Daw\\nMorgan Skrabut\\nAna Usenko\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.14369\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 Nov 2021 21:59:30 GMT)\\u00a7r"}']}
{title:'Musaev et al. (§72021§r)', author: 'Muhammadjon Musaev; Saida Mussakhojayeva; Ilyos Khujayorov; Yerbolat Khassanov; Mannon Ochilov; Huseyin Atakan Varol', display:{Lore:['[{"text": "arXiv:2107.14419", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUSC: An Open-Source Uzbek Speech Corpus and Initial Speech Recognition Experiments\\u00a7r\\n\\n\\u00a78\\u00a7oMuhammadjon Musaev\\nSaida Mussakhojayeva\\nIlyos Khujayorov\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.14419\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Jul 2021 03:39:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 2 figures, 2 tables, accepted to SPECOM 2021\\u00a7r"}']}
{title:'von Neumann et al. (§72021§r)', author: 'Thilo von Neumann; Christoph Boeddeker; Keisuke Kinoshita; Marc Delcroix; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2107.14445", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeeding Up Permutation Invariant Training for Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oThilo von Neumann\\nChristoph Boeddeker\\nKeisuke Kinoshita\\nMarc Delcroix\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.14445\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Jul 2021 06:31:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at 14thITG Conference on Speech Communication\\u00a7r"}']}
{title:'von Neumann et al. (§72021§r)', author: 'Thilo von Neumann; Keisuke Kinoshita; Christoph Boeddeker; Marc Delcroix; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2107.14446", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGraph-PIT: Generalized permutation invariant training for continuous separation of arbitrary numbers of speakers\\u00a7r\\n\\n\\u00a78\\u00a7oThilo von Neumann\\nKeisuke Kinoshita\\nChristoph Boeddeker\\nMarc Delcroix\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.14446\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-1177\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 20 Sep 2021 06:43:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2021\\u00a7r"}']}
{title:'Brazier et al. (§72021§r)', author: 'Charles Brazier; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2107.14496", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn-Line Audio-to-Lyrics Alignment Based on a Reference Performance\\u00a7r\\n\\n\\u00a78\\u00a7oCharles Brazier\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.14496\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Jul 2021 08:57:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 1 figure, In Proceedings of the 22nd International Society for Music Information Retrieval (ISMIR) Conference, Online, 2021\\u00a7r"}']}
{title:'Dutta et al. (§72021§r)', author: 'Debottam Dutta; Purvi Agrawal; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:2107.14793", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Multi-Head Relevance Weighting Framework For Learning Raw Waveform Audio Representations\\u00a7r\\n\\n\\u00a78\\u00a7oDebottam Dutta\\nPurvi Agrawal\\nSriram Ganapathy\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.14793\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Jul 2021 17:51:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to 2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics(WASPAA 2021)\\u00a7r"}']}
{title:'Shi (§72021§r)', author: 'Zhaofeng Shi', display:{Lore:['[{"text": "arXiv:2108.00443", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Survey on Audio Synthesis and Audio-Visual Multimodal Processing\\u00a7r\\n\\n\\u00a78\\u00a7oZhaofeng Shi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.00443\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 1 Aug 2021 12:35:16 GMT)\\u00a7r"}']}
{title:'Papapanagiotou et al. (§72021§r)', author: 'Vasileios Papapanagiotou; Christos Diou; Anastasios Delopoulos', display:{Lore:['[{"text": "arXiv:2108.00769", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Feature Learning of 1D Convolutional Neural Networks with Contrastive Loss for Eating Detection Using an In-Ear Microphone\\u00a7r\\n\\n\\u00a78\\u00a7oVasileios Papapanagiotou\\nChristos Diou\\nAnastasios Delopoulos\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.00769\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 3 Aug 2021 10:02:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, IEEE EMBC 2021\\u00a7r"}']}
{title:'Papapanagiotou et al. (§72021§r)', author: 'Vasileios Papapanagiotou; Stefanos Ganotakis; Anastasios Delopoulos', display:{Lore:['[{"text": "arXiv:2108.00771", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBite-Weight Estimation Using Commercial Ear Buds\\u00a7r\\n\\n\\u00a78\\u00a7oVasileios Papapanagiotou\\nStefanos Ganotakis\\nAnastasios Delopoulos\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.00771\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 2 Aug 2021 10:34:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 2 figures, IEEE EMBC 2021\\u00a7r"}']}
{title:'Jin et al. (§72021§r)', author: 'Zengrui Jin; Mengzhe Geng; Xurong Xie; Jianwei Yu; Shansong Liu; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2108.00899", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Data Augmentation for Disordered Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZengrui Jin\\nMengzhe Geng\\nXurong Xie\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.00899\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 2 Aug 2021 13:44:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, INTERSPEECH 2021\\u00a7r"}']}
{title:'Song et al. (§72021§r)', author: 'Siyuan Song; Brecht Desplanques; Celest De Moor; Kris Demuynck; Nilesh Madhu', display:{Lore:['[{"text": "arXiv:2108.00912", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Acoustic Scene Classification in the Presence of Active Foreground Speech\\u00a7r\\n\\n\\u00a78\\u00a7oSiyuan Song\\nBrecht Desplanques\\nCelest De Moor\\nKris Demuynck\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.00912\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 2 Aug 2021 14:03:50 GMT)\\u00a7r"}']}
{title:'van Niekerk et al. (§72021§r)', author: 'Benjamin van Niekerk; Leanne Nortje; Matthew Baas; Herman Kamper', display:{Lore:['[{"text": "arXiv:2108.00917", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalyzing Speaker Information in Self-Supervised Models to Improve Zero-Resource Speech Processing\\u00a7r\\n\\n\\u00a78\\u00a7oBenjamin van Niekerk\\nLeanne Nortje\\nMatthew Baas\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.00917\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 2 Aug 2021 14:10:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2021\\u00a7r"}']}
{title:'Mussakhojayeva et al. (§72021§r)', author: 'Saida Mussakhojayeva; Yerbolat Khassanov; Huseyin Atakan Varol', display:{Lore:['[{"text": "arXiv:2108.01280", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Study of Multilingual End-to-End Speech Recognition for Kazakh, Russian, and English\\u00a7r\\n\\n\\u00a78\\u00a7oSaida Mussakhojayeva\\nYerbolat Khassanov\\nHuseyin Atakan Varol\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.01280\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 Aug 2021 04:04:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 3 tables, accepted to SPECOM 2021\\u00a7r"}']}
{title:'Barnekow et al. (§72021§r)', author: 'Vanessa Barnekow; Dominik Binder; Niclas Kromrey; Pascal Munaretto; Andreas Schaad; Felix Schmieder', display:{Lore:['[{"text": "arXiv:2108.01469", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCreation and Detection of German Voice Deepfakes\\u00a7r\\n\\n\\u00a78\\u00a7oVanessa Barnekow\\nDominik Binder\\nNiclas Kromrey\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.01469\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 2 Aug 2021 06:17:25 GMT)\\u00a7r"}']}
{title:'Macoskey et al. (§72021§r)', author: 'Jonathan Macoskey; Grant P. Strimel; Jinru Su; Ariya Rastrow', display:{Lore:['[{"text": "arXiv:2108.01553", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAmortized Neural Networks for Low-Latency Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJonathan Macoskey\\nGrant P. Strimel\\nJinru Su\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.01553\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 Aug 2021 15:05:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2021\\u00a7r"}']}
{title:'Macoskey et al. (§72021§r)', author: 'Jonathan Macoskey; Grant P. Strimel; Ariya Rastrow', display:{Lore:['[{"text": "arXiv:2108.01561", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning a Neural Diff for Speech Models\\u00a7r\\n\\n\\u00a78\\u00a7oJonathan Macoskey\\nGrant P. Strimel\\nAriya Rastrow\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.01561\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 17 Aug 2021 12:05:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2021\\u00a7r"}']}
{title:'Macoskey et al. (§72021§r)', author: 'Jonathan Macoskey; Grant P. Strimel; Ariya Rastrow', display:{Lore:['[{"text": "arXiv:2108.01704", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBifocal Neural ASR: Exploiting Keyword Spotting for Inference Optimization\\u00a7r\\n\\n\\u00a78\\u00a7oJonathan Macoskey\\nGrant P. Strimel\\nAriya Rastrow\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.01704\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 Aug 2021 18:58:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2021\\u00a7r"}']}
{title:'Nakatani et al. (§72021§r)', author: 'Tomohiro Nakatani; Rintaro Ikeshita; Keisuke Kinoshita; Hiroshi Sawada; Shoko Araki', display:{Lore:['[{"text": "arXiv:2108.01836", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBlind and neural network-guided convolutional beamformer for joint denoising, dereverberation, and source separation\\u00a7r\\n\\n\\u00a78\\u00a7oTomohiro Nakatani\\nRintaro Ikeshita\\nKeisuke Kinoshita\\nHiroshi Sawada\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.01836\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP39728.2021.9414264\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 4 Aug 2021 04:03:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE ICASSP 2021\\u00a7r"}']}
{title:'Nguyen et al. (§72021§r)', author: 'Truc Nguyen; Franz Pernkopf', display:{Lore:['[{"text": "arXiv:2108.01991", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLung Sound Classification Using Co-tuning and Stochastic Normalization\\u00a7r\\n\\n\\u00a78\\u00a7oTruc Nguyen\\nFranz Pernkopf\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.01991\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 4 Aug 2021 12:16:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEEBE Transaction\\u00a7r"}']}
{title:'Mei et al. (§72021§r)', author: 'Xinhao Mei; Qiushi Huang; Xubo Liu; Gengyun Chen; Jingqian Wu; Yusong Wu; Jinzheng Zhao; Shengchen Li; Tom Ko; H Lilian Tang; Xi Shao; Mark D. Plumbley; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2108.02752", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Encoder-Decoder Based Audio Captioning System With Transfer and Reinforcement Learning\\u00a7r\\n\\n\\u00a78\\u00a7oXinhao Mei\\nQiushi Huang\\nXubo Liu\\n+ 9 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.02752\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 5 Aug 2021 17:34:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, submitted to DCASE 2021 workshop\\u00a7r"}']}
{title:'Hono et al. (§72021§r)', author: 'Yukiya Hono; Kei Hashimoto; Keiichiro Oura; Yoshihiko Nankaku; Keiichi Tokuda', display:{Lore:['[{"text": "arXiv:2108.02776", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSinsy: A Deep Neural Network-Based Singing Voice Synthesis System\\u00a7r\\n\\n\\u00a78\\u00a7oYukiya Hono\\nKei Hashimoto\\nKeiichiro Oura\\nYoshihiko Nankaku\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.02776\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3104165\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol. 29, pp. 2803-2815, 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 5 Aug 2021 17:59:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages, 11 figures, 3 tables, Accepted to IEEE/ACM Transactionson Audio, Speech and Language Processing\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Guangyan Zhang; Ying Qin; Daxin Tan; Tan Lee', display:{Lore:['[{"text": "arXiv:2108.02821", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lApplying the Information Bottleneck Principle to Prosodic Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oGuangyan Zhang\\nYing Qin\\nDaxin Tan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.02821\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 5 Aug 2021 19:20:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be appeared in Interspeech 2021\\u00a7r"}']}
{title:'Ojha et al. (§72021§r)', author: 'Rupam Ojha; C Chandra Sekhar', display:{Lore:['[{"text": "arXiv:2108.02850", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Domain Adaptation in Speech Recognition using Phonetic Features\\u00a7r\\n\\n\\u00a78\\u00a7oRupam Ojha\\nC Chandra Sekhar\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.02850\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 4 Aug 2021 06:22:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Halimeh et al. (§72021§r)', author: 'Mhd Modar Halimeh; Walter Kellermann', display:{Lore:['[{"text": "arXiv:2108.03130", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComplex-valued Spatial Autoencoders for Multichannel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oMhd Modar Halimeh\\nWalter Kellermann\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.03130\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 Aug 2021 14:03:20 GMT)\\u00a7r"}']}
{title:'He et al. (§72021§r)', author: 'Maokui He; Desh Raj; Zili Huang; Jun Du; Zhuo Chen; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2108.03342", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTarget-speaker Voice Activity Detection with Improved I-Vector Estimation for Unknown Number of Speaker\\u00a7r\\n\\n\\u00a78\\u00a7oMaokui He\\nDesh Raj\\nZili Huang\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.03342\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 7 Aug 2021 01:29:37 GMT)\\u00a7r"}']}
{title:'Heydari et al. (§72021§r)', author: 'Mojtaba Heydari; Frank Cwitkowitz; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2108.03576", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBeatNet: CRNN and Particle Filtering for Online Joint Beat Downbeat and Meter Tracking\\u00a7r\\n\\n\\u00a78\\u00a7oMojtaba Heydari\\nFrank Cwitkowitz\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.03576\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 8 Aug 2021 06:07:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o22nd International Society for Music Information Retrieval (ISMIR) Conference Paper, Fall2021. 8 Pages (Total), 3 Figures, 2 Tables, 1 Algorithm\\u00a7r"}']}
{title:'Mustafa et al. (§72021§r)', author: 'Ahmed Mustafa; Jan Büthe; Srikanth Korse; Kishan Gupta; Guillaume Fuchs; Nicola Pia', display:{Lore:['[{"text": "arXiv:2108.04051", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Streamwise GAN Vocoder for Wideband Speech Coding at Very Low Bit Rate\\u00a7r\\n\\n\\u00a78\\u00a7oAhmed Mustafa\\nJan B\\u00fcthe\\nSrikanth Korse\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.04051\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 9 Aug 2021 14:03:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the 2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA 2021)\\u00a7r"}']}
{title:'Purushothaman et al. (§72021§r)', author: 'Anurenjan Purushothaman; Anirudh Sreeram; Rohit Kumar; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:2108.05520", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDereverberation of Autoregressive Envelopes for Far-field Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAnurenjan Purushothaman\\nAnirudh Sreeram\\nRohit Kumar\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.05520\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 13 Aug 2021 09:50:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2008.03339\\u00a7r"}']}
{title:'Lee et al. (§72021§r)', author: 'Kong Aik Lee; Qiongqiong Wang; Takafumi Koshinaka', display:{Lore:['[{"text": "arXiv:2108.05679", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lXi-Vector Embedding for Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oKong Aik Lee\\nQiongqiong Wang\\nTakafumi Koshinaka\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.05679\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 12 Aug 2021 11:54:56 GMT)\\u00a7r"}']}
{title:'Dang et al. (§72021§r)', author: 'T. Dang; V. Sethu; E. Ambikairajah; J. Epps; H. Li', display:{Lore:['[{"text": "arXiv:2108.05993", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Spatio-Temporal Discretisation of Nonlinear Active Cochlear Models\\u00a7r\\n\\n\\u00a78\\u00a7oT. Dang\\nV. Sethu\\nE. Ambikairajah\\nJ. Epps\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.05993\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 12 Aug 2021 23:03:13 GMT)\\u00a7r"}']}
{title:'van der Westhuizen et al. (§72021§r)', author: 'Ewald van der Westhuizen; Trideba Padhi; Thomas Niesler', display:{Lore:['[{"text": "arXiv:2108.06164", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultilingual training set selection for ASR in under-resourced Malian languages\\u00a7r\\n\\n\\u00a78\\u00a7oEwald van der Westhuizen\\nTrideba Padhi\\nThomas Niesler\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.06164\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 13 Aug 2021 10:36:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 4 figures, Accepted for presentation at SPECOM2021\\u00a7r"}']}
{title:'van der Westhuizen et al. (§72021§r)', author: 'Ewald van der Westhuizen; Herman Kamper; Raghav Menon; John Quinn; Thomas Niesler', display:{Lore:['[{"text": "arXiv:2108.06174", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFeature learning for efficient ASR-free keyword spotting in low-resource languages\\u00a7r\\n\\n\\u00a78\\u00a7oEwald van der Westhuizen\\nHerman Kamper\\nRaghav Menon\\nJohn Quinn\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.06174\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.csl.2021.101275\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 13 Aug 2021 11:39:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o37 pages, 14 figures, Preprint accepted for publication in Computer Speech and Language\\u00a7r"}']}
{title:'Ezzerg et al. (§72021§r)', author: 'Abdelhamid Ezzerg; Adam Gabrys; Bartosz Putrycz; Daniel Korzekwa; Daniel Saez-Trigueros; David McHardy; Kamil Pokora; Jakub Lachowicz; Jaime Lorenzo-Trueba; Viacheslav Klimkov', display:{Lore:['[{"text": "arXiv:2108.06270", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing audio quality for expressive Neural Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oAbdelhamid Ezzerg\\nAdam Gabrys\\nBartosz Putrycz\\n+ 6 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.06270\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 13 Aug 2021 14:32:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 4 figures, 2 tables, SSW 2021\\u00a7r"}']}
{title:'Kim et al. (§72021§r)', author: 'Ji-Hoon Kim; Sang-Hoon Lee; Ji-Hyun Lee; Hong-Gyu Jung; Seong-Whan Lee', display:{Lore:['[{"text": "arXiv:2108.06890", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGC-TTS: Few-shot Speaker Adaptation with Geometric Constraints\\u00a7r\\n\\n\\u00a78\\u00a7oJi-Hoon Kim\\nSang-Hoon Lee\\nJi-Hyun Lee\\nHong-Gyu Jung\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.06890\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 16 Aug 2021 04:25:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted paper in IEEE International Conference on Systems, Man, and Cybernetics (SMC 2021)\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Yuanyuan Liu; Nelly Penttilä; Tiina Ihalainen; Juulia Lintula; Rachel Convey; Okko Räsänen', display:{Lore:['[{"text": "arXiv:2108.06943", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLanguage-Independent Approach for Automatic Computation of Vowel Articulation Features in Dysarthric Speech Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oYuanyuan Liu\\nNelly Penttil\\u00e4\\nTiina Ihalainen\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.06943\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3090973\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech and Language Processing,\\n  2021\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 17 Aug 2021 07:10:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages\\u00a7r"}']}
{title:'Calhoun et al. (§72021§r)', author: 'Robert B. Calhoun; Clark Dunson; Murphey L. Johnson; Scott R. Lamkin; William R. Lewis; Robert L. Showen; Mark A. Sompel; Lester P. Wollman', display:{Lore:['[{"text": "arXiv:2108.07377", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrecision and accuracy of acoustic gunshot location in an urban environment\\u00a7r\\n\\n\\u00a78\\u00a7oRobert B. Calhoun\\nClark Dunson\\nMurphey L. Johnson\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.07377\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 16 Aug 2021 23:54:57 GMT)\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Jin Li; Nan Yan; Lan Wang', display:{Lore:['[{"text": "arXiv:2108.07974", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFDN: Finite Difference Network with Hierarchical Convolutional Features for Text-independent Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oJin Li\\nNan Yan\\nLan Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.07974\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 7 Oct 2021 12:57:22 GMT)\\u00a7r"}']}
{title:'Gong et al. (§72021§r)', author: 'Xia Gong; Yuxiang Zhu; Haidi Zhu; Haoran Wei', display:{Lore:['[{"text": "arXiv:2108.08470", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChMusic: A Traditional Chinese Music Dataset for Evaluation of Instrument Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXia Gong\\nYuxiang Zhu\\nHaidi Zhu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.08470\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 11 Dec 2021 22:56:28 GMT)\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Jin Li; Nan Yan; Lan Wang', display:{Lore:['[{"text": "arXiv:2108.08663", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Cross-Lingual Speech Emotion Recognition Using Pseudo Multilabel\\u00a7r\\n\\n\\u00a78\\u00a7oJin Li\\nNan Yan\\nLan Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.08663\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 7 Oct 2021 13:09:36 GMT)\\u00a7r"}']}
{title:'Ragano et al. (§72021§r)', author: 'Alessandro Ragano; Emmanouil Benetos; Andrew Hines', display:{Lore:['[{"text": "arXiv:2108.08745", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMore for Less: Non-Intrusive Speech Quality Assessment with Limited Annotations\\u00a7r\\n\\n\\u00a78\\u00a7oAlessandro Ragano\\nEmmanouil Benetos\\nAndrew Hines\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.08745\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/QoMEX51781.2021.9465410\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 19 Aug 2021 15:20:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in 202113th International Conference on Quality of Multimedia Experience (QoMEX)\\u00a7r"}']}
{title:'Anders et al. (§72021§r)', author: 'Franz Anders; Mario Hlawitschka; Mirco Fuchs', display:{Lore:['[{"text": "arXiv:2108.09205", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigation of the Assessment of Infant Vocalizations by Laypersons\\u00a7r\\n\\n\\u00a78\\u00a7oFranz Anders\\nMario Hlawitschka\\nMirco Fuchs\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.09205\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 20 Aug 2021 14:52:23 GMT)\\u00a7r"}']}
{title:'N (§72021§r)', author: 'Krishna D N', display:{Lore:['[{"text": "arXiv:2108.09669", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing Large Pre-Trained Models with Cross-Modal Attention for Multi-Modal Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oKrishna D N\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.09669\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 22 Aug 2021 09:01:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 Pages, REJECTED FROM INTERSPEECH 2021\\u00a7r"}']}
{title:'Yusuf et al. (§72021§r)', author: 'Bolaji Yusuf; Alican Gok; Batuhan Gundogdu; Murat Saraclar', display:{Lore:['[{"text": "arXiv:2108.10357", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Open Vocabulary Keyword Search\\u00a7r\\n\\n\\u00a78\\u00a7oBolaji Yusuf\\nAlican Gok\\nBatuhan Gundogdu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.10357\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 23 Aug 2021 18:34:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2021\\u00a7r"}']}
{title:'Kumar et al. (§72021§r)', author: 'Sanjay Kumar; Jie Wei Aow; Wong Dexuan; Heow Pueh Lee', display:{Lore:['[{"text": "arXiv:2108.10683", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigation of lightweight acoustic curtains for mid-to-high frequency noise insulations\\u00a7r\\n\\n\\u00a78\\u00a7oSanjay Kumar\\nJie Wei Aow\\nWong Dexuan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.10683\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 16 Aug 2021 05:35:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages, 7 figures. arXiv admin note: text overlapwith arXiv:2008.06690\\u00a7r"}']}
{title:'Sofronievski et al. (§72021§r)', author: 'Bojan Sofronievski; Branislav Gerazov', display:{Lore:['[{"text": "arXiv:2108.10689", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScorpiano \\u2013 A System for Automatic Music Transcription for Monophonic Piano Music\\u00a7r\\n\\n\\u00a78\\u00a7oBojan Sofronievski\\nBranislav Gerazov\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.10689\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 24 Aug 2021 12:47:03 GMT)\\u00a7r"}']}
{title:'Chowdhury et al. (§72021§r)', author: 'Labib Chowdhury; Mustafa Kamal; Najia Hasan; Nabeel Mohammed', display:{Lore:['[{"text": "arXiv:2108.10714", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCurricular SincNet: Towards Robust Deep Speaker Recognition by Emphasizing Hard Samples in Latent Space\\u00a7r\\n\\n\\u00a78\\u00a7oLabib Chowdhury\\nMustafa Kamal\\nNajia Hasan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.10714\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/BIOSIG52210.2021.9548296\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 21 Aug 2021 09:13:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at 20thInternational Conference of the Biometrics Special Interest Group (BIOSIG 2021)\\u00a7r"}']}
{title:'Kodrasi (§72021§r)', author: 'Ina Kodrasi', display:{Lore:['[{"text": "arXiv:2108.11153", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTemporal envelope and fine structure cues for dysarthric speech detection using CNNs\\u00a7r\\n\\n\\u00a78\\u00a7oIna Kodrasi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.11153\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2021.3108509\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 25 Aug 2021 10:13:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in IEEE Signal Processing Letters, 2021\\u00a7r"}']}
{title:'Poran et al. (§72021§r)', author: 'Shachaf Poran; Gil Amsalem; Amit Beka; Dmitri Goldenberg', display:{Lore:['[{"text": "arXiv:2108.11463", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWith One Voice: Composing a Travel Voice Assistant from Re-purposed Models\\u00a7r\\n\\n\\u00a78\\u00a7oShachaf Poran\\nGil Amsalem\\nAmit Beka\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.11463\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2nd International Workshop on Industrial Recommendation Systems @\\n  KDD 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 4 Aug 2021 10:34:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2nd International Workshopon Industrial Recommendation Systems @ KDD 2021\\u00a7r"}']}
{title:'Chao et al. (§72021§r)', author: 'Fu-An Chao; Jeih-weih Hung; Berlin Chen', display:{Lore:['[{"text": "arXiv:2108.11598", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-domain Single-channel Speech Enhancement Model with Bi-projection Fusion Module for Noise-robust ASR\\u00a7r\\n\\n\\u00a78\\u00a7oFu-An Chao\\nJeih-weih Hung\\nBerlin Chen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.11598\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 26 Aug 2021 06:29:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 3 figures, Accepted by ICME 2021\\u00a7r"}']}
{title:'Durante et al. (§72021§r)', author: 'Zane Durante; Leena Mathur; Eric Ye; Sichong Zhao; Tejas Ramdas; Khalil Iskarous', display:{Lore:['[{"text": "arXiv:2108.12531", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Representations and Phoneme Classification for Preserving the Endangered Language of Ladin\\u00a7r\\n\\n\\u00a78\\u00a7oZane Durante\\nLeena Mathur\\nEric Ye\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.12531\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 27 Aug 2021 23:51:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICSAMLSLP 2021 (held with Interspeech 2021)\\u00a7r"}']}
{title:'Chang et al. (§72021§r)', author: 'Feng-Ju Chang; Martin Radfar; Athanasios Mouchtaris; Maurizio Omologo', display:{Lore:['[{"text": "arXiv:2108.12953", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Channel Transformer Transducer for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oFeng-Ju Chang\\nMartin Radfar\\nAthanasios Mouchtaris\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.12953\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPublished in INTERSPEECH 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 Aug 2021 01:50:51 GMT)\\u00a7r"}']}
{title:'Jiang et al. (§72021§r)', author: 'Guanxin Jiang; Arijit Biswas; Christian Bergler; Andreas Maier', display:{Lore:['[{"text": "arXiv:2108.13087", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInSE-NET: A Perceptually Coded Audio Quality Model based on CNN\\u00a7r\\n\\n\\u00a78\\u00a7oGuanxin Jiang\\nArijit Biswas\\nChristian Bergler\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.13087\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 Aug 2021 09:51:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to 151st Audio Engineering Society (AES), Las Vegas, NV, USA, October 2021\\u00a7r"}']}
{title:'Gupta et al. (§72021§r)', author: 'Deepika Gupta; Hanumant Singh Shekhawat', display:{Lore:['[{"text": "arXiv:2108.13326", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lArtificial bandwidth extension using deep neural network and H^\\u221e sampled-data control theory\\u00a7r\\n\\n\\u00a78\\u00a7oDeepika Gupta\\nHanumant Singh Shekhawat\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.13326\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 Aug 2021 15:45:39 GMT)\\u00a7r"}']}
{title:'Nankaku et al. (§72021§r)', author: 'Yoshihiko Nankaku; Kenta Sumiya; Takenori Yoshimura; Shinji Takaki; Kei Hashimoto; Keiichiro Oura; Keiichi Tokuda', display:{Lore:['[{"text": "arXiv:2108.13985", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Sequence-to-Sequence Speech Synthesis Using a Hidden Semi-Markov Model Based Structured Attention Mechanism\\u00a7r\\n\\n\\u00a78\\u00a7oYoshihiko Nankaku\\nKenta Sumiya\\nTakenori Yoshimura\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.13985\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 31 Aug 2021 17:12:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Delgado et al. (§72021§r)', author: 'Héctor Delgado; Nicholas Evans; Tomi Kinnunen; Kong Aik Lee; Xuechen Liu; Andreas Nautsch; Jose Patino; Md Sahidullah; Massimiliano Todisco; Xin Wang; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2109.00535", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lASVspoof 2021: Automatic Speaker Verification Spoofing and Countermeasures Challenge Evaluation Plan\\u00a7r\\n\\n\\u00a78\\u00a7oH\\u00e9ctor Delgado\\nNicholas Evans\\nTomi Kinnunen\\n+ 7 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.00535\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Sep 2021 15:32:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ohttp://www.asvspoof.org\\u00a7r"}']}
{title:'Yamagishi et al. (§72021§r)', author: 'Junichi Yamagishi; Xin Wang; Massimiliano Todisco; Md Sahidullah; Jose Patino; Andreas Nautsch; Xuechen Liu; Kong Aik Lee; Tomi Kinnunen; Nicholas Evans; Héctor Delgado', display:{Lore:['[{"text": "arXiv:2109.00537", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lASVspoof 2021: accelerating progress in spoofed and deepfake speech detection\\u00a7r\\n\\n\\u00a78\\u00a7oJunichi Yamagishi\\nXin Wang\\nMassimiliano Todisco\\n+ 7 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.00537\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Sep 2021 16:17:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the ASVspoof 2021 Workshop\\u00a7r"}']}
{title:'Xue et al. (§72021§r)', author: 'Junxiao Xue; Hao Zhou; Yabo Wang', display:{Lore:['[{"text": "arXiv:2109.00913", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhysiological-Physical Feature Fusion for Automatic Voice Spoofing Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJunxiao Xue\\nHao Zhou\\nYabo Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.00913\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Sep 2021 03:32:22 GMT)\\u00a7r"}']}
{title:'Huang et al. (§72021§r)', author: 'Zhen Huang; Xiaodan Zhuang; Daben Liu; Xiaoqiang Xiao; Yuchen Zhang; Sabato Marco Siniscalchi', display:{Lore:['[{"text": "arXiv:2109.00921", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Retraining-Free Speech Recognition for Intra-sentential Code-Switching\\u00a7r\\n\\n\\u00a78\\u00a7oZhen Huang\\nXiaodan Zhuang\\nDaben Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.00921\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8682478\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP2019 12-17 May 2019\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 27 Aug 2021 19:15:16 GMT)\\u00a7r"}']}
{title:'Singla et al. (§72021§r)', author: 'Yaman Kumar Singla; Avykat Gupta; Shaurya Bagga; Changyou Chen; Balaji Krishnamurthy; Rajiv Ratn Shah', display:{Lore:['[{"text": "arXiv:2109.00928", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker-Conditioned Hierarchical Modeling for Automated Speech Scoring\\u00a7r\\n\\n\\u00a78\\u00a7oYaman Kumar Singla\\nAvykat Gupta\\nShaurya Bagga\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.00928\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3459637.3482395\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 Aug 2021 07:00:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in CIKM2021\\u00a7r"}']}
{title:'Burchi et al. (§72021§r)', author: 'Maxime Burchi; Valentin Vielzeuf', display:{Lore:['[{"text": "arXiv:2109.01163", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient conformer: Progressive downsampling and grouped attention for automatic speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMaxime Burchi\\nValentin Vielzeuf\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.01163\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nASRU 2021, Dec 2021, Cartagena, Colombia\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 8 Sep 2021 11:47:52 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Mingkuan Liu; Chi Zhang; Hua Xing; Chao Feng; Monchu Chen; Judith Bishop; Grace Ngapo', display:{Lore:['[{"text": "arXiv:2109.01164", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScalable Data Annotation Pipeline for High-Quality Large Speech Datasets Development\\u00a7r\\n\\n\\u00a78\\u00a7oMingkuan Liu\\nChi Zhang\\nHua Xing\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.01164\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Sep 2021 17:54:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to NeurIPS 2021 Datasets and Benchmarks Track (Round 2)\\u00a7r"}']}
{title:'Shivakumar et al. (§72021§r)', author: 'Prashanth Gurunath Shivakumar; Somer Bishop; Catherine Lord; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2109.01568", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhone Duration Modeling for Speaker Age Estimation in Children\\u00a7r\\n\\n\\u00a78\\u00a7oPrashanth Gurunath Shivakumar\\nSomer Bishop\\nCatherine Lord\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.01568\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 3 Sep 2021 14:43:39 GMT)\\u00a7r"}']}
{title:'Sun et al. (§72021§r)', author: 'Xiaoheng Sun; Qiqi He; Yongwei Gao; Wei Li', display:{Lore:['[{"text": "arXiv:2109.01607", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusical Tempo Estimation Using a Multi-scale Network\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoheng Sun\\nQiqi He\\nYongwei Gao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.01607\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 3 Sep 2021 16:32:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ISMIR 2021\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Weiqing Wang; Danwei Cai; Qingjian Lin; Lin Yang; Junjie Wang; Jin Wang; Ming Li', display:{Lore:['[{"text": "arXiv:2109.02002", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe DKU-DukeECE-Lenovo System for the Diarization Task of the 2021 VoxCeleb Speaker Recognition Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oWeiqing Wang\\nDanwei Cai\\nQingjian Lin\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.02002\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 7 Sep 2021 03:59:50 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Jie Wang; Fuchuang Tong; Zhicong Chen; Lin Li; Qingyang Hong; Haodong Zhou', display:{Lore:['[{"text": "arXiv:2109.02549", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lXMUSPEECH System for VoxCeleb Speaker Recognition Challenge 2021\\u00a7r\\n\\n\\u00a78\\u00a7oJie Wang\\nFuchuang Tong\\nZhicong Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.02549\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 Sep 2021 15:36:09 GMT)\\u00a7r"}']}
{title:'Tan et al. (§72021§r)', author: 'Zhenning Tan; Yuguang Yang; Eunjung Han; Andreas Stolcke', display:{Lore:['[{"text": "arXiv:2109.02576", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Speaker Identification for Shared Devices by Adapting Embeddings to Speaker Subsets\\u00a7r\\n\\n\\u00a78\\u00a7oZhenning Tan\\nYuguang Yang\\nEunjung Han\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.02576\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ASRU51503.2021.9687975\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. IEEE Automatic Speech Recognition and Understanding\\n  Workshop, Dec. 2021, pp. 1124-1131\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 Sep 2021 16:22:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ASRU 2021\\u00a7r"}']}
{title:'Cai et al. (§72021§r)', author: 'Danwei Cai; Ming Li', display:{Lore:['[{"text": "arXiv:2109.02853", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe DKU-DukeECE System for the Self-Supervision Speaker Verification Task of the 2021 VoxCeleb Speaker Recognition Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oDanwei Cai\\nMing Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.02853\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Sep 2021 04:13:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2010.14751\\u00a7r"}']}
{title:'Grooby et al. (§72021§r)', author: 'Ethan Grooby; Jinyuan He; Davood Fattahi; Lindsay Zhou; Arrabella King; Ashwin Ramanathan; Atul Malhotra; Guy A. Dumont; Faezeh Marzbanrad', display:{Lore:['[{"text": "arXiv:2109.03275", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA New Non-Negative Matrix Co-Factorisation Approach for Noisy Neonatal Chest Sound Separation\\u00a7r\\n\\n\\u00a78\\u00a7oEthan Grooby\\nJinyuan He\\nDavood Fattahi\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.03275\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/EMBC46164.2021.9630256\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2021 43rd Annual International Conference of the IEEE Engineering\\n  in Medicine Biology Society (EMBC)\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 4 Sep 2021 02:48:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 2 figures. To appear as conference paper at 43rd Annual International Conference of the IEEEEngineering in Medicine and Biology Society, 1st-5th November 2021\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Songxiang Liu; Shan Yang; Dan Su; Dong Yu', display:{Lore:['[{"text": "arXiv:2109.03439", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReferee: Towards reference-free cross-speaker style transfer with low-quality data for expressive speech synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oSongxiang Liu\\nShan Yang\\nDan Su\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.03439\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 8 Sep 2021 05:39:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, preprint\\u00a7r"}']}
{title:'Thienpondt et al. (§72021§r)', author: 'Jenthe Thienpondt; Brecht Desplanques; Kris Demuynck', display:{Lore:['[{"text": "arXiv:2109.04070", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe IDLAB VoxCeleb Speaker Recognition Challenge 2021 System Description\\u00a7r\\n\\n\\u00a78\\u00a7oJenthe Thienpondt\\nBrecht Desplanques\\nKris Demuynck\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.04070\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Sep 2021 07:23:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2104.02370\\u00a7r"}']}
{title:'Schepker et al. (§72021§r)', author: 'Henning Schepker; Florian Denk; Birger Kollmeier; Simon Doclo', display:{Lore:['[{"text": "arXiv:2109.04241", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust single- and multi-loudspeaker least-squares-based equalization for hearing devices\\u00a7r\\n\\n\\u00a78\\u00a7oHenning Schepker\\nFlorian Denk\\nBirger Kollmeier\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.04241\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Sep 2021 12:59:23 GMT)\\u00a7r"}']}
{title:'Inaguma et al. (§72021§r)', author: 'Hirofumi Inaguma; Yosuke Higuchi; Kevin Duh; Tatsuya Kawahara; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2109.04411", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-autoregressive End-to-end Speech Translation with Parallel Autoregressive Rescoring\\u00a7r\\n\\n\\u00a78\\u00a7oHirofumi Inaguma\\nYosuke Higuchi\\nKevin Duh\\nTatsuya Kawahara\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.04411\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Sep 2021 16:50:16 GMT)\\u00a7r"}']}
{title:'Chetupalli et al. (§72021§r)', author: 'Srikanth Raj Chetupalli; Thippur V. Sreenivas', display:{Lore:['[{"text": "arXiv:2109.04544", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDirectional MCLP Analysis and Reconstruction for Spatial Speech Communication\\u00a7r\\n\\n\\u00a78\\u00a7oSrikanth Raj Chetupalli\\nThippur V. Sreenivas\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.04544\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Sep 2021 20:16:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe manuscript is submitted as a full paper to IEEE/ACM Transactionson Audio, Speech and Language Processing\\u00a7r"}']}
{title:'Yu et al. (§72021§r)', author: 'Wentao Yu; Steffen Zeiler; Dorothea Kolossa', display:{Lore:['[{"text": "arXiv:2109.04894", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLarge-vocabulary Audio-visual Speech Recognition in Noisy Environments\\u00a7r\\n\\n\\u00a78\\u00a7oWentao Yu\\nSteffen Zeiler\\nDorothea Kolossa\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.04894\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nThe IEEE 23rd International Workshop on Multimedia Signal\\n  Processing (MMSP), 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 10 Sep 2021 14:05:32 GMT)\\u00a7r"}']}
{title:'Bekal et al. (§72021§r)', author: 'Dhanush Bekal; Ashish Shenoy; Monica Sunkara; Sravan Bodapati; Katrin Kirchhoff', display:{Lore:['[{"text": "arXiv:2109.05092", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRemember the context! ASR slot error correction through memorization\\u00a7r\\n\\n\\u00a78\\u00a7oDhanush Bekal\\nAshish Shenoy\\nMonica Sunkara\\nSravan Bodapati\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.05092\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 18 Sep 2021 00:35:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 3 figures, 4 tables, Accepted to ASRU 2021\\u00a7r"}']}
{title:'Xia et al. (§72021§r)', author: 'Yangyang Xia; Buye Xu; Anurag Kumar', display:{Lore:['[{"text": "arXiv:2109.05172", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIncorporating Real-world Noisy Speech in Neural-network-based Speech Enhancement Systems\\u00a7r\\n\\n\\u00a78\\u00a7oYangyang Xia\\nBuye Xu\\nAnurag Kumar\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.05172\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 21 Sep 2021 05:11:33 GMT)\\u00a7r"}']}
{title:'Rouvier et al. (§72021§r)', author: 'Mickael Rouvier; Pierre-Michel Bousquet', display:{Lore:['[{"text": "arXiv:2109.05977", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStudying squeeze-and-excitation used in CNN for speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oMickael Rouvier\\nPierre-Michel Bousquet\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.05977\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Sep 2021 13:54:02 GMT)\\u00a7r"}']}
{title:'Nair et al. (§72021§r)', author: 'Abhishek Ramdas Nair; Shantanu Chakrabartty; Chetan Singh Thakur', display:{Lore:['[{"text": "arXiv:2109.06171", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7acs.SY\\u00a7r, \\u00a7eeess.SY\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIn-filter Computing For Designing Ultra-light Acoustic Pattern Recognizers\\u00a7r\\n\\n\\u00a78\\u00a7oAbhishek Ramdas Nair\\nShantanu Chakrabartty\\nChetan Singh Thakur\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.06171\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JIOT.2021.3109739\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 11 Sep 2021 08:16:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oin IEEEInternet of Things Journal\\u00a7r"}']}
{title:'Coria et al. (§72021§r)', author: 'Juan M. Coria; Hervé Bredin; Sahar Ghannay; Sophie Rosset', display:{Lore:['[{"text": "arXiv:2109.06483", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOverlap-aware low-latency online speaker diarization based on end-to-end local segmentation\\u00a7r\\n\\n\\u00a78\\u00a7oJuan M. Coria\\nHerv\\u00e9 Bredin\\nSahar Ghannay\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.06483\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Sep 2021 07:27:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ASRU 2021. Code available at https://github.com/juanmc2005/StreamingSpeakerDiarization/\\u00a7r"}']}
{title:'Singh et al. (§72021§r)', author: 'Prachi Singh; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:2109.06824", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Metric Learning With Graph Clustering For Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oPrachi Singh\\nSriram Ganapathy\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.06824\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Sep 2021 17:07:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, Accepted in ASRU 2021\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Changhan Wang; Wei-Ning Hsu; Yossi Adi; Adam Polyak; Ann Lee; Peng-Jen Chen; Jiatao Gu; Juan Pino', display:{Lore:['[{"text": "arXiv:2109.06912", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lfairseq S^2: A Scalable and Integrable Speech Synthesis Toolkit\\u00a7r\\n\\n\\u00a78\\u00a7oChanghan Wang\\nWei-Ning Hsu\\nYossi Adi\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.06912\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Sep 2021 18:20:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to EMNLP 2021 Demo\\u00a7r"}']}
{title:'Iijima et al. (§72021§r)', author: 'Naoto Iijima; Shoichi Koyama; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2109.07274", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBinaural rendering from microphone array signals of arbitrary geometry\\u00a7r\\n\\n\\u00a78\\u00a7oNaoto Iijima\\nShoichi Koyama\\nHiroshi Saruwatari\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.07274\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Sep 2021 13:23:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe following article has been accepted by Journal of the Acoustical Society of America (JASA). After it is published, it will be found at http://asa.scitation.org/journal/jas\\u00a7r"}']}
{title:'Cao et al. (§72021§r)', author: 'Songjun Cao; Yueteng Kang; Yanzhe Fu; Xiaoshuo Xu; Sining Sun; Yike Zhang; Long Ma', display:{Lore:['[{"text": "arXiv:2109.07327", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Streaming Transformer Based ASR Under a Framework of Self-supervised Learning\\u00a7r\\n\\n\\u00a78\\u00a7oSongjun Cao\\nYueteng Kang\\nYanzhe Fu\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.07327\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Sep 2021 14:35:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH2021\\u00a7r"}']}
{title:'Deng et al. (§72021§r)', author: 'Keqi Deng; Songjun Cao; Long Ma', display:{Lore:['[{"text": "arXiv:2109.07349", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Accent Identification and Accented Speech Recognition Under a Framework of Self-supervised Learning\\u00a7r\\n\\n\\u00a78\\u00a7oKeqi Deng\\nSongjun Cao\\nLong Ma\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.07349\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Sep 2021 15:01:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH2021\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Wei Liu; Tan Lee', display:{Lore:['[{"text": "arXiv:2109.07750", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUtterance-level neural confidence measure for end-to-end children speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oWei Liu\\nTan Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.07750\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Sep 2021 06:49:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by ASRU 2021\\u00a7r"}']}
{title:'Dossou et al. (§72021§r)', author: 'Bonaventure F. P. Dossou; Yeno K. S. Gbenou', display:{Lore:['[{"text": "arXiv:2109.07916", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.HC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFSER: Deep Convolutional Neural Networks for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oBonaventure F. P. Dossou\\nYeno K. S. Gbenou\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.07916\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Sep 2021 05:03:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oABAW Workshop, ICCV 2021\\u00a7r"}']}
{title:'Mohanty et al. (§72021§r)', author: 'Anwesh Mohanty; Adrian Frischknecht; Christoph Gerum; Oliver Bringmann', display:{Lore:['[{"text": "arXiv:2109.07930", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBehavior of Keyword Spotting Networks Under Noisy Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oAnwesh Mohanty\\nAdrian Frischknecht\\nChristoph Gerum\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.07930\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-030-86362-3_30\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICANN 2021. Lecture Notes in Computer Science, vol 12891, pp\\n  369-378. Springer\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Sep 2021 10:02:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 5 figures, Published in Lecture Notes in Computer Science book series (LNCS, volume 12891)\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Chen Zhang; Jiaxing Yu; LuChin Chang; Xu Tan; Jiawei Chen; Tao Qin; Kejun Zhang', display:{Lore:['[{"text": "arXiv:2109.07940", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPDAugment: Data Augmentation by Pitch and Duration Adjustments for Automatic Lyrics Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oChen Zhang\\nJiaxing Yu\\nLuChin Chang\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.07940\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 17 Sep 2021 05:53:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages\\u00a7r"}']}
{title:'Manocha et al. (§72021§r)', author: 'Pranay Manocha; Buye Xu; Anurag Kumar', display:{Lore:['[{"text": "arXiv:2109.08125", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNORESQA: A Framework for Speech Quality Assessment using Non-Matching References\\u00a7r\\n\\n\\u00a78\\u00a7oPranay Manocha\\nBuye Xu\\nAnurag Kumar\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.08125\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 18 Oct 2021 17:00:52 GMT)\\u00a7r"}']}
{title:'Achanta et al. (§72021§r)', author: 'Sivanand Achanta; Albert Antony; Ladan Golipour; Jiangchuan Li; Tuomo Raitio; Ramya Rasipuram; Francesco Rossi; Jennifer Shi; Jaimin Upadhyay; David Winarsky; Hepeng Zhang', display:{Lore:['[{"text": "arXiv:2109.08710", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.PF\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn-device neural speech synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oSivanand Achanta\\nAlbert Antony\\nLadan Golipour\\n+ 7 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.08710\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 17 Sep 2021 18:31:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages 2 figures, accepted to ASRU 2021\\u00a7r"}']}
{title:'Weninger et al. (§72021§r)', author: 'Felix Weninger; Marco Gaudesi; Ralf Leibold; Roberto Gemello; Puming Zhan', display:{Lore:['[{"text": "arXiv:2109.08744", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDual-Encoder Architecture with Encoder Selection for Joint Close-Talk and Far-Talk Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oFelix Weninger\\nMarco Gaudesi\\nRalf Leibold\\nRoberto Gemello\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.08744\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 17 Sep 2021 19:52:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ASRU 2021\\u00a7r"}']}
{title:'Yang et al. (§72021§r)', author: 'Yuguang Yang; Yu Pan; Xin Dong; Minqiang Xu', display:{Lore:['[{"text": "arXiv:2109.08870", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast query-by-example speech search using separable model\\u00a7r\\n\\n\\u00a78\\u00a7oYuguang Yang\\nYu Pan\\nXin Dong\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.08870\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 18 Sep 2021 07:57:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8pages, 8 figures\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Jingyu Li; Si-Ioi Ng; Tan Lee', display:{Lore:['[{"text": "arXiv:2109.09674", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Text-Independent Speaker Verification with Auxiliary Speakers Using Graph\\u00a7r\\n\\n\\u00a78\\u00a7oJingyu Li\\nSi-Ioi Ng\\nTan Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.09674\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Sep 2021 16:43:49 GMT)\\u00a7r"}']}
{title:'Silva-Rodríguez et al. (§72021§r)', author: 'J. Silva-Rodríguez; M. F. Dolz; M. Ferrer; A. Castelló; V. Naranjo; G. Piñero', display:{Lore:['[{"text": "arXiv:2109.09686", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic Echo Cancellation using Residual U-Nets\\u00a7r\\n\\n\\u00a78\\u00a7oJ. Silva-Rodr\\u00edguez\\nM. F. Dolz\\nM. Ferrer\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.09686\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Sep 2021 16:57:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 2 figures, submitted to the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing on October 2020\\u00a7r"}']}
{title:'Svoboda et al. (§72021§r)', author: 'Emil Svoboda; Tomáš Bořil; Jan Rusz; Tereza Tykalová; Dana Horáková; Charles R. G. Guttman; Krastan B. Blagoev; Hiroto Hatabu; Vlad I. Valtchinov', display:{Lore:['[{"text": "arXiv:2109.09844", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAssessing clinical utility of Machine Learning and Artificial Intelligence approaches to analyze speech recordings in Multiple Sclerosis: A Pilot Study\\u00a7r\\n\\n\\u00a78\\u00a7oEmil Svoboda\\nTom\\u00e1\\u0161 Bo\\u0159il\\nJan Rusz\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.09844\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Sep 2021 15:22:13 GMT)\\u00a7r"}']}
{title:'Zhou et al. (§72021§r)', author: 'Liangchen Zhou; Wenbin Jiang; Jingyan Xu; Fei Wen; Peilin Liu', display:{Lore:['[{"text": "arXiv:2109.11164", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMasks Fusion with Multi-Target Learning For Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oLiangchen Zhou\\nWenbin Jiang\\nJingyan Xu\\nFei Wen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.11164\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 28 Sep 2021 03:14:29 GMT)\\u00a7r"}']}
{title:'Gaudesi et al. (§72021§r)', author: 'Marco Gaudesi; Felix Weninger; Dushyant Sharma; Puming Zhan', display:{Lore:['[{"text": "arXiv:2109.11225", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChannelAugment: Improving generalization of multi-channel ASR by training with input channel randomization\\u00a7r\\n\\n\\u00a78\\u00a7oMarco Gaudesi\\nFelix Weninger\\nDushyant Sharma\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.11225\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Sep 2021 09:13:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ASRU 2021\\u00a7r"}']}
{title:'Inaguma et al. (§72021§r)', author: 'Hirofumi Inaguma; Siddharth Dalmia; Brian Yan; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2109.12804", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast-MD: Fast Multi-Decoder End-to-End Speech Translation with Non-Autoregressive Hidden Intermediates\\u00a7r\\n\\n\\u00a78\\u00a7oHirofumi Inaguma\\nSiddharth Dalmia\\nBrian Yan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.12804\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Sep 2021 05:21:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at IEEEASRU 2021\\u00a7r"}']}
{title:'Cho et al. (§72021§r)', author: 'Jejin Cho; Jesus Villalba; Najim Dehak', display:{Lore:['[{"text": "arXiv:2109.13425", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe JHU submission to VoxSRC-21: Track 3\\u00a7r\\n\\n\\u00a78\\u00a7oJejin Cho\\nJesus Villalba\\nNajim Dehak\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.13425\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Sep 2021 01:30:10 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72021§r)', author: 'Myungjong Kim; Taeyeon Ki; Aviral Anshu; Vijendra Raj Apsingekar', display:{Lore:['[{"text": "arXiv:2109.13518", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNorth America Bixby Speaker Diarization System for the VoxCeleb Speaker Recognition Challenge 2021\\u00a7r\\n\\n\\u00a78\\u00a7oMyungjong Kim\\nTaeyeon Ki\\nAviral Anshu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.13518\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Sep 2021 06:45:01 GMT)\\u00a7r"}']}
{title:'Perez et al. (§72021§r)', author: 'Matthew Perez; Amrit Romana; Angela Roberts; Noelle Carlozzi; Jennifer Ann Miner; Praveen Dayalu; Emily Mower Provost', display:{Lore:['[{"text": "arXiv:2109.13815", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lArticulatory Coordination for Speech Motor Tracking in Huntington Disease\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew Perez\\nAmrit Romana\\nAngela Roberts\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.13815\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-688\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Sep 2021 15:39:49 GMT)\\u00a7r"}']}
{title:'Ronchini et al. (§72021§r)', author: 'Francesca Ronchini; Romain Serizel; Nicolas Turpault; Samuele Cornell', display:{Lore:['[{"text": "arXiv:2109.14061", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe impact of non-target events in synthetic soundscapes for sound event detection\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesca Ronchini\\nRomain Serizel\\nNicolas Turpault\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.14061\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5281/zenodo.5770113\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 6th Detection and Classification of Acoustic\\n  Scenes and Events 2021 Workshop (DCASE2021)\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Sep 2021 21:46:19 GMT)\\u00a7r"}']}
{title:'Poncelet et al. (§72021§r)', author: 'Jakob Poncelet; Hugo Van hamme', display:{Lore:['[{"text": "arXiv:2109.14357", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparison of Self-Supervised Speech Pre-Training Methods on Flemish Dutch\\u00a7r\\n\\n\\u00a78\\u00a7oJakob Poncelet\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.14357\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 29 Sep 2021 11:38:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in the 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2021)\\u00a7r"}']}
{title:'Szwajcowski (§72021§r)', author: 'Adam Szwajcowski', display:{Lore:['[{"text": "arXiv:2109.14370", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lObjective-oriented method for uniformation of various directivity representations\\u00a7r\\n\\n\\u00a78\\u00a7oAdam Szwajcowski\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.14370\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 29 Sep 2021 11:57:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAuthor\'s Accepted Manuscript from 151st AES Convention\\u00a7r"}']}
{title:'López et al. (§72021§r)', author: 'Paula Sánchez López; Paul Callens; Milos Cernak', display:{Lore:['[{"text": "arXiv:2109.14436", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Universal Deep Room Acoustics Estimator\\u00a7r\\n\\n\\u00a78\\u00a7oPaula S\\u00e1nchez L\\u00f3pez\\nPaul Callens\\nMilos Cernak\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.14436\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/WASPAA52581.2021.9632738\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Workshop on Applications of Signal Processing to Audio and\\n  Acoustics (WASPAA) 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 29 Sep 2021 14:19:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oRoom acoustics, Convolutional Recurrent Neural Network, RT60, C50, DRR,STI, SNR\\u00a7r"}']}
{title:'King et al. (§72021§r)', author: 'James King; Ramon Viñas Torné; Alexander Campbell; Pietro Liò', display:{Lore:['[{"text": "arXiv:2109.14994", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn investigation of pre-upsampling generative modelling and Generative Adversarial Networks in audio super resolution\\u00a7r\\n\\n\\u00a78\\u00a7oJames King\\nRamon Vi\\u00f1as Torn\\u00e9\\nAlexander Campbell\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.14994\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Sep 2021 10:46:23 GMT)\\u00a7r"}']}
{title:'Yu et al. (§72021§r)', author: 'Wentao Yu; Jan Freiwald; Sören Tewes; Fabien Huennemeyer; Dorothea Kolossa', display:{Lore:['[{"text": "arXiv:2109.15108", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFederated Learning in ASR: Not as Easy as You Think\\u00a7r\\n\\n\\u00a78\\u00a7oWentao Yu\\nJan Freiwald\\nS\\u00f6ren Tewes\\nFabien Huennemeyer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.15108\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nITG Conference on Speech Communication, 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Sep 2021 13:21:10 GMT)\\u00a7r"}']}
{title:'Grooby et al. (§72021§r)', author: 'Ethan Grooby; Chiranjibi Sitaula; Davood Fattahi; Reza Sameni; Kenneth Tan; Lindsay Zhou; Arrabella King; Ashwin Ramanathan; Atul Malhotra; Guy A. Dumont; Faezeh Marzbanrad', display:{Lore:['[{"text": "arXiv:2109.15127", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-Time Multi-Level Neonatal Heart and Lung Sound Quality Assessment for Telehealth Applications\\u00a7r\\n\\n\\u00a78\\u00a7oEthan Grooby\\nChiranjibi Sitaula\\nDavood Fattahi\\n+ 7 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.15127\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ACCESS.2022.3144355\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Access, 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 29 Sep 2021 01:08:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 8 figures, 3 tables. Paper submittedand under review in IEEE Access\\u00a7r"}']}
{title:'Shibano et al. (§72021§r)', author: 'Toshiko Shibano; Xinyi Zhang; Mia Taige Li; Haejin Cho; Peter Sullivan; Muhammad Abdul-Mageed', display:{Lore:['[{"text": "arXiv:2110.00678", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Technology for Everyone: Automatic Speech Recognition for Non-Native English with Transfer Learning\\u00a7r\\n\\n\\u00a78\\u00a7oToshiko Shibano\\nXinyi Zhang\\nMia Taige Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.00678\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 15 Oct 2021 02:43:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAll authors contributed equally. Paper accepted to International Conference on Natural Language and Speech Processing 2021 (ICNLSP 2021)\\u00a7r"}']}
{title:'Sudro et al. (§72021§r)', author: 'Protima Nomo Sudro; Rohan Kumar Das; Rohit Sinha; S. R. Mahadeva Prasanna', display:{Lore:['[{"text": "arXiv:2110.00797", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSignificance of Data Augmentation for Improving Cleft Lip and Palate Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oProtima Nomo Sudro\\nRohan Kumar Das\\nRohit Sinha\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.00797\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Oct 2021 13:03:44 GMT)\\u00a7r"}']}
{title:'Luo et al. (§72021§r)', author: 'Zhaojie Luo; Shoufeng Lin; Rui Liu; Jun Baba; Yuichiro Yoshikawa; Ishiguro Hiroshi', display:{Lore:['[{"text": "arXiv:2110.01164", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDecoupling Speaker-Independent Emotions for Voice Conversion Via Source-Filter Networks\\u00a7r\\n\\n\\u00a78\\u00a7oZhaojie Luo\\nShoufeng Lin\\nRui Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.01164\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Oct 2021 03:14:48 GMT)\\u00a7r"}']}
{title:'Sharma et al. (§72021§r)', author: 'Neeraj Kumar Sharma; Srikanth Raj Chetupalli; Debarpan Bhattacharya; Debottam Dutta; Pravin Mote; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:2110.01177", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Second DiCOVA Challenge: Dataset and performance analysis for COVID-19 diagnosis using acoustics\\u00a7r\\n\\n\\u00a78\\u00a7oNeeraj Kumar Sharma\\nSrikanth Raj Chetupalli\\nDebarpan Bhattacharya\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.01177\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 11 Oct 2021 12:28:06 GMT)\\u00a7r"}']}
{title:'Jung et al. (§72021§r)', author: 'Jee-weon Jung; Hee-Soo Heo; Hemlata Tak; Hye-jin Shim; Joon Son Chung; Bong-Jin Lee; Ha-Jin Yu; Nicholas Evans', display:{Lore:['[{"text": "arXiv:2110.01200", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAASIST: Audio Anti-Spoofing using Integrated Spectro-Temporal Graph Attention Networks\\u00a7r\\n\\n\\u00a78\\u00a7oJee-weon Jung\\nHee-Soo Heo\\nHemlata Tak\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.01200\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Oct 2021 05:48:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, 3 tables, submitted to ICASSP2022\\u00a7r"}']}
{title:'Schepker et al. (§72021§r)', author: 'Henning Schepker; Reinhild Rohden; Florian Denk; Birger Kollmeier; Matthias Blau; Simon Doclo', display:{Lore:['[{"text": "arXiv:2110.01422", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIndividualized sound pressure equalization in hearing devices exploiting an electro-acoustic model\\u00a7r\\n\\n\\u00a78\\u00a7oHenning Schepker\\nReinhild Rohden\\nFlorian Denk\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.01422\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Oct 2021 12:56:42 GMT)\\u00a7r"}']}
{title:'Steinmetz et al. (§72021§r)', author: 'Christian J. Steinmetz; Joshua D. Reiss', display:{Lore:['[{"text": "arXiv:2110.01436", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWaveBeat: End-to-end beat and downbeat tracking in the time domain\\u00a7r\\n\\n\\u00a78\\u00a7oChristian J. Steinmetz\\nJoshua D. Reiss\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.01436\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Oct 2021 13:31:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at the 151st AES Convention\\u00a7r"}']}
{title:'Qin et al. (§72021§r)', author: 'Ying Qin; Wei Liu; Zhiyuan Peng; Si-Ioi Ng; Jingyu Li; Haibo Hu; Tan Lee', display:{Lore:['[{"text": "arXiv:2110.01493", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting Pre-Trained ASR Models for Alzheimer\'s Disease Recognition Through Spontaneous Speech\\u00a7r\\n\\n\\u00a78\\u00a7oYing Qin\\nWei Liu\\nZhiyuan Peng\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.01493\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Oct 2021 15:02:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by NCMMSC2021\\u00a7r"}']}
{title:'Pepe et al. (§72021§r)', author: 'Giovanni Pepe; Leonardo Gabrielli; Stefano Squartini; Carlo Tripodi; Nicolò Strozzi', display:{Lore:['[{"text": "arXiv:2110.02077", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Optimization of Parametric IIR Filters for Audio Equalization\\u00a7r\\n\\n\\u00a78\\u00a7oGiovanni Pepe\\nLeonardo Gabrielli\\nStefano Squartini\\nCarlo Tripodi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02077\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Oct 2021 14:19:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to IEEE/ACM TASLP on 12 May 2021\\u00a7r"}']}
{title:'León et al. (§72021§r)', author: 'Diego León; Felipe Tobar', display:{Lore:['[{"text": "arXiv:2110.02144", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLate reverberation suppression using U-nets\\u00a7r\\n\\n\\u00a78\\u00a7oDiego Le\\u00f3n\\nFelipe Tobar\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02144\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Oct 2021 16:18:09 GMT)\\u00a7r"}']}
{title:'Sagredo et al. (§72021§r)', author: 'Bryan Sagredo; Sonia Español-Jiménez; Felipe Tobar', display:{Lore:['[{"text": "arXiv:2110.02151", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetection of blue whale vocalisations using a temporal-domain convolutional neural network\\u00a7r\\n\\n\\u00a78\\u00a7oBryan Sagredo\\nSonia Espa\\u00f1ol-Jim\\u00e9nez\\nFelipe Tobar\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02151\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Oct 2021 16:32:16 GMT)\\u00a7r"}']}
{title:'Brendel et al. (§72021§r)', author: 'Andreas Brendel; Johannes Zeitler; Walter Kellermann', display:{Lore:['[{"text": "arXiv:2110.02189", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lManifold learning-supported estimation of relative transfer functions for spatial filtering\\u00a7r\\n\\n\\u00a78\\u00a7oAndreas Brendel\\nJohannes Zeitler\\nWalter Kellermann\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02189\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Oct 2021 17:28:38 GMT)\\u00a7r"}']}
{title:'Munkhdalai et al. (§72021§r)', author: 'Tsendsuren Munkhdalai; Khe Chai Sim; Angad Chandorkar; Fan Gao; Mason Chua; Trevor Strohman; Françoise Beaufays', display:{Lore:['[{"text": "arXiv:2110.02220", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast Contextual Adaptation with Neural Associative Memory for On-Device Personalized Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oTsendsuren Munkhdalai\\nKhe Chai Sim\\nAngad Chandorkar\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02220\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 7 Oct 2021 00:12:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 3 tables\\u00a7r"}']}
{title:'Fenton (§72021§r)', author: 'Steven Fenton', display:{Lore:['[{"text": "arXiv:2110.02285", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModelling of the Fender Bassman 5F6-A Tone Stack\\u00a7r\\n\\n\\u00a78\\u00a7oSteven Fenton\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02285\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Oct 2021 18:47:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 6 figues. General Reference Paper\\u00a7r"}']}
{title:'Bhati et al. (§72021§r)', author: 'Saurabhchand Bhati; Jesús Villalba; Piotr Żelasko; Laureano Moro-Velazquez; Najim Dehak', display:{Lore:['[{"text": "arXiv:2110.02345", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Speech Segmentation and Variable Rate Representation Learning using Segmental Contrastive Predictive Coding\\u00a7r\\n\\n\\u00a78\\u00a7oSaurabhchand Bhati\\nJes\\u00fas Villalba\\nPiotr \\u017belasko\\nLaureano Moro-Velazquez\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02345\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 8 Oct 2021 21:42:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2106.02170\\u00a7r"}']}
{title:'Morrison et al. (§72021§r)', author: 'Max Morrison; Zeyu Jin; Nicholas J. Bryan; Juan-Pablo Caceres; Bryan Pardo', display:{Lore:['[{"text": "arXiv:2110.02360", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Pitch-Shifting and Time-Stretching with Controllable LPCNet\\u00a7r\\n\\n\\u00a78\\u00a7oMax Morrison\\nZeyu Jin\\nNicholas J. Bryan\\nJuan-Pablo Caceres\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02360\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Oct 2021 21:04:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Gu et al. (§72021§r)', author: 'Yewei Gu; Zhenyu Zhang; Xiaowei Yi; Xianfeng Zhao', display:{Lore:['[{"text": "arXiv:2110.02500", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMediumVC: Any-to-any voice conversion using synthetic specific-speaker speeches as intermedium features\\u00a7r\\n\\n\\u00a78\\u00a7oYewei Gu\\nZhenyu Zhang\\nXiaowei Yi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02500\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Oct 2021 04:29:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures\\u00a7r"}']}
{title:'Cho et al. (§72021§r)', author: 'Yin-Ping Cho; Fu-Rong Yang; Yung-Chuan Chang; Ching-Ting Cheng; Xiao-Han Wang; Yi-Wen Liu', display:{Lore:['[{"text": "arXiv:2110.02511", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Survey on Recent Deep Learning-driven Singing Voice Synthesis Systems\\u00a7r\\n\\n\\u00a78\\u00a7oYin-Ping Cho\\nFu-Rong Yang\\nYung-Chuan Chang\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02511\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Oct 2021 05:27:23 GMT)\\u00a7r"}']}
{title:'Brazier et al. (§72021§r)', author: 'Charles Brazier; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2110.02592", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Real-time Score Following in Opera by Combining Music with Lyrics Tracking\\u00a7r\\n\\n\\u00a78\\u00a7oCharles Brazier\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02592\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Oct 2021 08:58:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, In Proceedings of the 2nd Workshop on NLP for Music and Audio(NLP4MusA), Online, 2021\\u00a7r"}']}
{title:'Bohnstingl et al. (§72021§r)', author: 'Thomas Bohnstingl; Ayush Garg; Stanisław Woźniak; George Saon; Evangelos Eleftheriou; Angeliki Pantazi', display:{Lore:['[{"text": "arXiv:2110.02743", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards efficient end-to-end speech recognition with biologically-inspired neural networks\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Bohnstingl\\nAyush Garg\\nStanis\\u0142aw Wo\\u017aniak\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02743\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 4 Nov 2021 10:41:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the Efficient Natural Language and Speech Processing workshop at NeurIPS 2021\\u00a7r"}']}
{title:'Pamisetty et al. (§72021§r)', author: 'Giridhar Pamisetty; K. Sri Rama Murty', display:{Lore:['[{"text": "arXiv:2110.02854", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProsody-TTS: An end-to-end speech synthesis system with prosody control\\u00a7r\\n\\n\\u00a78\\u00a7oGiridhar Pamisetty\\nK. Sri Rama Murty\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02854\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Oct 2021 15:29:35 GMT)\\u00a7r"}']}
{title:'Bernard et al. (§72021§r)', author: 'Thomas Bernard; François Grondin', display:{Lore:['[{"text": "arXiv:2110.03103", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLightweight Speech Enhancement in Unseen Noisy and Reverberant Conditions using KISS-GEV Beamforming\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Bernard\\nFran\\u00e7ois Grondin\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03103\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 10 Oct 2021 20:24:34 GMT)\\u00a7r"}']}
{title:'Sack et al. (§72021§r)', author: 'Andrew Sack; Wenzhao Jiang; Michael Perlmutter; Palina Salanevich; Deanna Needell', display:{Lore:['[{"text": "arXiv:2110.03114", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn audio enhancement via online non-negative matrix factorization\\u00a7r\\n\\n\\u00a78\\u00a7oAndrew Sack\\nWenzhao Jiang\\nMichael Perlmutter\\nPalina Salanevich\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03114\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Oct 2021 00:10:54 GMT)\\u00a7r"}']}
{title:'Denton et al. (§72021§r)', author: 'Tom Denton; Scott Wisdom; John R. Hershey', display:{Lore:['[{"text": "arXiv:2110.03209", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Bird Classification with Unsupervised Sound Separation\\u00a7r\\n\\n\\u00a78\\u00a7oTom Denton\\nScott Wisdom\\nJohn R. Hershey\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03209\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Oct 2021 06:48:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures. Examples available at https://bird-mixit.github.io\\u00a7r"}']}
{title:'Roebel et al. (§72021§r)', author: 'Axel Roebel; Frederik Bous', display:{Lore:['[{"text": "arXiv:2110.03329", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Universal Neural Vocoding with a Multi-band Excited WaveNet\\u00a7r\\n\\n\\u00a78\\u00a7oAxel Roebel\\nFrederik Bous\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03329\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Oct 2021 10:47:03 GMT)\\u00a7r"}']}
{title:'Dai et al. (§72021§r)', author: 'Dongyang Dai; Yuanzhe Chen; Li Chen; Ming Tu; Lu Liu; Rui Xia; Qiao Tian; Yuping Wang; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2110.03347", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCloning one\'s voice using very limited data in the wild\\u00a7r\\n\\n\\u00a78\\u00a7oDongyang Dai\\nYuanzhe Chen\\nLi Chen\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03347\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 8 Oct 2021 06:00:58 GMT)\\u00a7r"}']}
{title:'Kwon et al. (§72021§r)', author: 'Youngki Kwon; Hee-Soo Heo; Jee-weon Jung; You Jin Kim; Bong-Jin Lee; Joon Son Chung', display:{Lore:['[{"text": "arXiv:2110.03361", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-scale speaker embedding-based graph attention networks for speaker diarisation\\u00a7r\\n\\n\\u00a78\\u00a7oYoungki Kwon\\nHee-Soo Heo\\nJee-weon Jung\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03361\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Oct 2021 11:59:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, submitted to ICASSP as a conference paper\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Jin Li; Haibin Liu; Nan Yan; Lan Wang', display:{Lore:['[{"text": "arXiv:2110.03392", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhanced Memory Network: The novel network structure for Symbolic Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oJin Li\\nHaibin Liu\\nNan Yan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03392\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Oct 2021 12:30:01 GMT)\\u00a7r"}']}
{title:'Aftab et al. (§72021§r)', author: 'Arya Aftab; Alireza Morsali; Shahrokh Ghaemmaghami; Benoit Champagne', display:{Lore:['[{"text": "arXiv:2110.03435", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLight-SERNet: A lightweight fully convolutional neural network for speech emotion recognition\\u00a7r\\n\\n\\u00a78\\u00a7oArya Aftab\\nAlireza Morsali\\nShahrokh Ghaemmaghami\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03435\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Oct 2021 13:16:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2022 submitted, 5 pages, 2 figures, 4 tables\\u00a7r"}']}
{title:'Endo et al. (§72021§r)', author: 'Hayato Endo; Hiromitsu Nishizaki', display:{Lore:['[{"text": "arXiv:2110.03511", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPeer Collaborative Learning for Polyphonic Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oHayato Endo\\nHiromitsu Nishizaki\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03511\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.48550/arXiv.2110.03511\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Oct 2021 14:47:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Li et al. (§72021§r)', author: 'Jialu Li; Vimal Manohar; Pooja Chitkara; Andros Tjandra; Michael Picheny; Frank Zhang; Xiaohui Zhang; Yatharth Saraf', display:{Lore:['[{"text": "arXiv:2110.03520", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAccent-Robust Automatic Speech Recognition Using Supervised and Unsupervised Wav2vec Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oJialu Li\\nVimal Manohar\\nPooja Chitkara\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03520\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 8 Oct 2021 14:44:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Peng et al. (§72021§r)', author: 'Yizhou Peng; Jicheng Zhang; Haihua Xu; Hao Huang; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2110.03573", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMinimum word error training for non-autoregressive Transformer-based code-switching ASR\\u00a7r\\n\\n\\u00a78\\u00a7oYizhou Peng\\nJicheng Zhang\\nHaihua Xu\\nHao Huang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03573\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Oct 2021 15:53:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmit to ICASSP 2021\\u00a7r"}']}
{title:'Tatanov et al. (§72021§r)', author: 'Oktai Tatanov; Stanislav Beliaev; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2110.03584", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMixer-TTS: non-autoregressive, fast and compact text-to-speech model conditioned on language model embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oOktai Tatanov\\nStanislav Beliaev\\nBoris Ginsburg\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03584\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 22 Oct 2021 16:51:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint. Submitted to ICASSP-22\\u00a7r"}']}
{title:'Benois et al. (§72021§r)', author: 'Piero Rivera Benois; Reinhild Roden; Matthias Blau; Simon Doclo', display:{Lore:['[{"text": "arXiv:2110.03586", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimization of a Fixed Virtual Sensing Feedback ANC Controller for In-Ear Headphones with Multiple Loudspeakers\\u00a7r\\n\\n\\u00a78\\u00a7oPiero Rivera Benois\\nReinhild Roden\\nMatthias Blau\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03586\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Oct 2021 16:09:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 pages, Submitted to IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2022)\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Guangyan Zhang; Yichong Leng; Daxin Tan; Ying Qin; Kaitao Song; Xu Tan; Sheng Zhao; Tan Lee', display:{Lore:['[{"text": "arXiv:2110.03857", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA study on the efficacy of model pre-training in developing neural text-to-speech system\\u00a7r\\n\\n\\u00a78\\u00a7oGuangyan Zhang\\nYichong Leng\\nDaxin Tan\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03857\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Oct 2021 02:09:28 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Changhong Wang; Emmanouil Benetos; Shuge Wang; Elisabetta Versace', display:{Lore:['[{"text": "arXiv:2110.03965", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Scattering for Automatic Chick Call Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oChanghong Wang\\nEmmanouil Benetos\\nShuge Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03965\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Oct 2021 08:31:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, submitted to ICASSP 2022\\u00a7r"}']}
{title:'Liao et al. (§72021§r)', author: 'Chien-Feng Liao; Jen-Yu Liu; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2110.04005", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKaraSinger: Score-Free Singing Voice Synthesis with VQ-VAE using Mel-spectrograms\\u00a7r\\n\\n\\u00a78\\u00a7oChien-Feng Liao\\nJen-Yu Liu\\nYi-Hsuan Yang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04005\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Oct 2021 10:00:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Ling et al. (§72021§r)', author: 'Shaoshi Ling; Chen Shen; Meng Cai; Zejun Ma', display:{Lore:['[{"text": "arXiv:2110.04056", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Pseudo-label Training For End-to-end Speech Recognition Using Gradient Mask\\u00a7r\\n\\n\\u00a78\\u00a7oShaoshi Ling\\nChen Shen\\nMeng Cai\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04056\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Oct 2021 12:05:25 GMT)\\u00a7r"}']}
{title:'Alary et al. (§72021§r)', author: 'Benoit Alary; Vesa Välimäki', display:{Lore:['[{"text": "arXiv:2110.04082", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Method for Capturing and Reproducing Directional Reverberation in Six Degrees of Freedom\\u00a7r\\n\\n\\u00a78\\u00a7oBenoit Alary\\nVesa V\\u00e4lim\\u00e4ki\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04082\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Oct 2021 12:18:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been accepted for the I3DA 2021 International Conference and will be submitted to IEEE Xplore Digital Library for possible publication. Copyright may be transferred without notice, after which this version"}','{"text": "may no longer be accessible\\u00a7r"}']}
{title:'Wu et al. (§72021§r)', author: 'Pengfei Wu; Junjie Pan; Chenchang Xu; Junhui Zhang; Lin Wu; Xiang Yin; Zejun Ma', display:{Lore:['[{"text": "arXiv:2110.04153", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-speaker Emotion Transfer Based on Speaker Condition Layer Normalization and Semi-Supervised Training in Text-To-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oPengfei Wu\\nJunjie Pan\\nChenchang Xu\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04153\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 11 Oct 2021 02:30:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022, 5 pages,2 figures\\u00a7r"}']}
{title:'Lotfidereshgi et al. (§72021§r)', author: 'Reza Lotfidereshgi; Philippe Gournay', display:{Lore:['[{"text": "arXiv:2110.04241", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCognitive Coding of Speech\\u00a7r\\n\\n\\u00a78\\u00a7oReza Lotfidereshgi\\nPhilippe Gournay\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04241\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Oct 2021 16:49:16 GMT)\\u00a7r"}']}
{title:'Zhu et al. (§72021§r)', author: 'Ge Zhu; Frank Cwitkowitz; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2110.04265", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA study of the robustness of raw waveform based speaker embeddings under mismatched conditions\\u00a7r\\n\\n\\u00a78\\u00a7oGe Zhu\\nFrank Cwitkowitz\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04265\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 11 Oct 2021 20:50:09 GMT)\\u00a7r"}']}
{title:'Taherian et al. (§72021§r)', author: 'Hassan Taherian; Ke Tan; DeLiang Wang', display:{Lore:['[{"text": "arXiv:2110.04289", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLocation-based training for multi-channel talker-independent speaker separation\\u00a7r\\n\\n\\u00a78\\u00a7oHassan Taherian\\nKe Tan\\nDeLiang Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04289\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Oct 2021 17:56:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 22\\u00a7r"}']}
{title:'Chee et al. (§72021§r)', author: 'Jerry Chee; Sebastian Braun; Vishak Gopal; Ross Cutler', display:{Lore:['[{"text": "arXiv:2110.04378", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerformance optimizations on deep noise suppression models\\u00a7r\\n\\n\\u00a78\\u00a7oJerry Chee\\nSebastian Braun\\nVishak Gopal\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04378\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Oct 2021 21:00:01 GMT)\\u00a7r"}']}
{title:'Koluguri et al. (§72021§r)', author: 'Nithin Rao Koluguri; Taejin Park; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2110.04410", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTitaNet: Neural Model for speaker representation with 1D Depth-wise separable convolutions and global context\\u00a7r\\n\\n\\u00a78\\u00a7oNithin Rao Koluguri\\nTaejin Park\\nBoris Ginsburg\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04410\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Oct 2021 23:49:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7opreprint. Submitted to ICASSP 2022\\u00a7r"}']}
{title:'Siriwardena et al. (§72021§r)', author: 'Yashish M. Siriwardena; Chris Kitchen; Deanna L. Kelly; Carol Espy-Wilson', display:{Lore:['[{"text": "arXiv:2110.04440", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultimodal Approach for Assessing Neuromotor Coordination in Schizophrenia Using Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oYashish M. Siriwardena\\nChris Kitchen\\nDeanna L. Kelly\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04440\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3462244.3479967\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 2021 International Conference on Multimodal\\n  Interaction\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 9 Oct 2021 03:11:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages. arXiv admin note:text overlap with arXiv:2102.07054\\u00a7r"}']}
{title:'Ng et al. (§72021§r)', author: 'Si-Ioi Ng; Tan Lee', display:{Lore:['[{"text": "arXiv:2110.04511", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData Augmentation with Locally-time Reversed Speech for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSi-Ioi Ng\\nTan Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04511\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 9 Oct 2021 09:00:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Heise et al. (§72021§r)', author: 'David Heise; Helen L. Bear', display:{Lore:['[{"text": "arXiv:2110.04584", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisually Exploring Multi-Purpose Audio Data\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Heise\\nHelen L. Bear\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04584\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 9 Oct 2021 14:46:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at MMSP 2021\\u00a7r"}']}
{title:'Bear et al. (§72021§r)', author: 'Helen L. Bear; Veronica Morfi; Emmanouil Benetos', display:{Lore:['[{"text": "arXiv:2110.04585", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn evaluation of data augmentation methods for sound scene geotagging\\u00a7r\\n\\n\\u00a78\\u00a7oHelen L. Bear\\nVeronica Morfi\\nEmmanouil Benetos\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04585\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 9 Oct 2021 14:50:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at Interspeech 2021\\u00a7r"}']}
{title:'Tobin et al. (§72021§r)', author: 'Jimmy Tobin; Katrin Tomanek', display:{Lore:['[{"text": "arXiv:2110.04612", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPersonalized Automatic Speech Recognition Trained on Small Disordered Speech Datasets\\u00a7r\\n\\n\\u00a78\\u00a7oJimmy Tobin\\nKatrin Tomanek\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04612\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 9 Oct 2021 17:11:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Pimenta-Zanon et al. (§72021§r)', author: 'Matheus Henrique Pimenta-Zanon; Glaucia Maria Bressan; Fabrício Martins Lopes', display:{Lore:['[{"text": "arXiv:2110.04654", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComplex Network-Based Approach for Feature Extraction and Classification of Musical Genres\\u00a7r\\n\\n\\u00a78\\u00a7oMatheus Henrique Pimenta-Zanon\\nGlaucia Maria Bressan\\nFabr\\u00edcio Martins Lopes\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04654\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 9 Oct 2021 22:23:33 GMT)\\u00a7r"}']}
{title:'Ma et al. (§72021§r)', author: 'Yufeng Ma; Yiwei Ding; Miao Zhao; Yu Zheng; Min Liu; Minqiang Xu', display:{Lore:['[{"text": "arXiv:2110.04692", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPoformer: A simple pooling transformer for speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oYufeng Ma\\nYiwei Ding\\nMiao Zhao\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04692\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 10 Oct 2021 03:12:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Gao et al. (§72021§r)', author: 'Shan Gao; Xihong Wu; Tianshu Qu', display:{Lore:['[{"text": "arXiv:2110.04850", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDirect source and early reflections localization using deep deconvolution network under reverberant environment\\u00a7r\\n\\n\\u00a78\\u00a7oShan Gao\\nXihong Wu\\nTianshu Qu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04850\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 22 Oct 2021 17:08:12 GMT)\\u00a7r"}']}
{title:'Wiesner et al. (§72021§r)', author: 'Matthew Wiesner; Desh Raj; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:2110.04863", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInjecting Text and Cross-lingual Supervision in Few-shot Learning from Self-Supervised Models\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew Wiesner\\nDesh Raj\\nSanjeev Khudanpur\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04863\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 10 Oct 2021 17:33:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oo\\u0327p\\u0327y\\u0327\\u0157i\\u0327\\u0123\\u1e29\\u0163 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for "}','{"text": "advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Higuchi et al. (§72021§r)', author: 'Yosuke Higuchi; Niko Moritz; Jonathan Le Roux; Takaaki Hori', display:{Lore:['[{"text": "arXiv:2110.04948", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdvancing Momentum Pseudo-Labeling with Conformer and Initialization Strategy\\u00a7r\\n\\n\\u00a78\\u00a7oYosuke Higuchi\\nNiko Moritz\\nJonathan Le Roux\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04948\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Oct 2021 00:52:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP2022\\u00a7r"}']}
{title:'Shi et al. (§72021§r)', author: 'Yangyang Shi; Chunyang Wu; Dilin Wang; Alex Xiao; Jay Mahadeokar; Xiaohui Zhang; Chunxi Liu; Ke Li; Yuan Shangguan; Varun Nagaraja; Ozlem Kalinli; Mike Seltzer', display:{Lore:['[{"text": "arXiv:2110.05241", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreaming Transformer Transducer Based Speech Recognition Using Non-Causal Convolution\\u00a7r\\n\\n\\u00a78\\u00a7oYangyang Shi\\nChunyang Wu\\nDilin Wang\\n+ 8 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05241\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Oct 2021 21:36:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, submitto ICASSP 2022\\u00a7r"}']}
{title:'Higuchi et al. (§72021§r)', author: 'Yosuke Higuchi; Nanxin Chen; Yuya Fujita; Hirofumi Inaguma; Tatsuya Komatsu; Jaesong Lee; Jumon Nozaki; Tianzi Wang; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2110.05249", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparative Study on Non-Autoregressive Modelings for Speech-to-Text Generation\\u00a7r\\n\\n\\u00a78\\u00a7oYosuke Higuchi\\nNanxin Chen\\nYuya Fujita\\n+ 5 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05249\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Oct 2021 13:05:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ASRU2021\\u00a7r"}']}
{title:'Champion et al. (§72021§r)', author: 'Pierre Champion; Thomas Thebaud; Gaël Le Lan; Anthony Larcher; Denis Jouvet', display:{Lore:['[{"text": "arXiv:2110.05431", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the invertibility of a voice privacy system using embedding alignement\\u00a7r\\n\\n\\u00a78\\u00a7oPierre Champion\\nThomas Thebaud\\nGa\\u00ebl Le Lan\\nAnthony Larcher\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05431\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nASRU 2021 - IEEE Automatic Speech Recognition and Understanding\\n  Workshop, Dec 2021, Cartagena, Colombia\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Oct 2021 14:43:47 GMT)\\u00a7r"}']}
{title:'Pan et al. (§72021§r)', author: 'Jing Pan; Tao Lei; Kwangyoun Kim; Kyu Han; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2110.05571", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSRU++: Pioneering Fast Recurrence with Attention for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJing Pan\\nTao Lei\\nKwangyoun Kim\\nKyu Han\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05571\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Oct 2021 19:23:50 GMT)\\u00a7r"}']}
{title:'Yoshioka et al. (§72021§r)', author: 'Takuya Yoshioka; Xiaofei Wang; Dongmei Wang; Min Tang; Zirun Zhu; Zhuo Chen; Naoyuki Kanda', display:{Lore:['[{"text": "arXiv:2110.05745", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVarArray: Array-Geometry-Agnostic Continuous Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oTakuya Yoshioka\\nXiaofei Wang\\nDongmei Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05745\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 26 Oct 2021 06:57:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, 3 tables, submitted to ICASSP 2022; updated reference information of [33]\\u00a7r"}']}
{title:'Pratap et al. (§72021§r)', author: 'Vineel Pratap; Qiantong Xu; Tatiana Likhomanenko; Gabriel Synnaeve; Ronan Collobert', display:{Lore:['[{"text": "arXiv:2110.05994", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWord Order Does Not Matter For Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oVineel Pratap\\nQiantong Xu\\nTatiana Likhomanenko\\nGabriel Synnaeve\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05994\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 18 Oct 2021 19:04:13 GMT)\\u00a7r"}']}
{title:'Falcon-Perez et al. (§72021§r)', author: 'Ricardo Falcon-Perez; Kazuki Shimada; Yuichiro Koyama; Shusuke Takahashi; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2110.06126", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatial mixup: Directional loudness modification as data augmentation for sound event localization and detection\\u00a7r\\n\\n\\u00a78\\u00a7oRicardo Falcon-Perez\\nKazuki Shimada\\nYuichiro Koyama\\nShusuke Takahashi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06126\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747312\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Oct 2021 16:16:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 4 tables. Submitted to the 2022 International Conference on Acoustics, Speech, Signal Processing (ICASSP)\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Zhuohuang Zhang; Takuya Yoshioka; Naoyuki Kanda; Zhuo Chen; Xiaofei Wang; Dongmei Wang; Sefik Emre Eskimez', display:{Lore:['[{"text": "arXiv:2110.06428", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAll-neural beamformer for continuous speech separation\\u00a7r\\n\\n\\u00a78\\u00a7oZhuohuang Zhang\\nTakuya Yoshioka\\nNaoyuki Kanda\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06428\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Oct 2021 01:24:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 2 tables\\u00a7r"}']}
{title:'Nikonorov et al. (§72021§r)', author: 'Sergey Nikonorov; Berrak Sisman; Mingyang Zhang; Haizhou Li', display:{Lore:['[{"text": "arXiv:2110.06434", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeepA: A Deep Neural Analyzer For Speech And Singing Vocoding\\u00a7r\\n\\n\\u00a78\\u00a7oSergey Nikonorov\\nBerrak Sisman\\nMingyang Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06434\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Oct 2021 01:39:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ASRU 2021\\u00a7r"}']}
{title:'Scheibler (§72021§r)', author: 'Robin Scheibler', display:{Lore:['[{"text": "arXiv:2110.06440", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSDR \\u2013 Medium Rare with Fast Computations\\u00a7r\\n\\n\\u00a78\\u00a7oRobin Scheibler\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06440\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Oct 2021 01:57:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 2 tables. Submitted to ICASSP 2022\\u00a7r"}']}
{title:'Williams et al. (§72021§r)', author: 'Jennifer Williams; Junichi Yamagishi; Paul-Gauthier Noe; Cassia Valentini Botinhao; Jean-Francois Bonastre', display:{Lore:['[{"text": "arXiv:2110.06760", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRevisiting Speech Content Privacy\\u00a7r\\n\\n\\u00a78\\u00a7oJennifer Williams\\nJunichi Yamagishi\\nPaul-Gauthier Noe\\nCassia Valentini Botinhao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06760\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Oct 2021 14:45:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ISCASecurity and Privacy in Speech Communication (1st SPSC Symposium)\\u00a7r"}']}
{title:'Gaznepoglu et al. (§72021§r)', author: 'Ünal Ege Gaznepoglu; Nils Peters', display:{Lore:['[{"text": "arXiv:2110.06887", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring the Importance of F0 Trajectories for Speaker Anonymization using X-vectors and Neural Waveform Models\\u00a7r\\n\\n\\u00a78\\u00a7o\\u00dcnal Ege Gaznepoglu\\nNils Peters\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06887\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Oct 2021 17:19:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 figures, 2 tables, accepted to Workshop on Machine Learning in Speech and Language Processing 2021 Example audio files and the poster available https://www.audiolabs-erlangen.de/fau/ professor/peters/publication"}','{"text": "s/MLSLP2021\\u00a7r"}']}
{title:'Hadian et al. (§72021§r)', author: 'Hossein Hadian; Arseniy Gorin', display:{Lore:['[{"text": "arXiv:2110.07055", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContinual learning using lattice-free MMI for speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHossein Hadian\\nArseniy Gorin\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.07055\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Oct 2021 22:11:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022 - IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) Copyright 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for "}','{"text": "all other uses\\u00a7r"}']}
{title:'Gupta et al. (§72021§r)', author: 'Udhav Gupta; Avi; Bhavesh Jain', display:{Lore:['[{"text": "arXiv:2110.07419", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStudent-t Networks for Melody Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oUdhav Gupta\\nAvi\\nBhavesh Jain\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.07419\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 29 Nov 2021 01:43:18 GMT)\\u00a7r"}']}
{title:'Zhao et al. (§72021§r)', author: 'Yingzhu Zhao; Chongjia Ni; Cheung-Chi Leung; Shafiq Joty; Eng Siong Chng; Bin Ma', display:{Lore:['[{"text": "arXiv:2110.08545", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Unified Speaker Adaptation Approach for ASR\\u00a7r\\n\\n\\u00a78\\u00a7oYingzhu Zhao\\nChongjia Ni\\nCheung-Chi Leung\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.08545\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 16 Oct 2021 10:48:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by EMNLP 2021\\u00a7r"}']}
{title:'Riviere et al. (§72021§r)', author: 'Morgane Riviere; Jade Copet; Gabriel Synnaeve', display:{Lore:['[{"text": "arXiv:2110.08583", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lASR4REAL: An extended benchmark for speech models\\u00a7r\\n\\n\\u00a78\\u00a7oMorgane Riviere\\nJade Copet\\nGabriel Synnaeve\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.08583\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 16 Oct 2021 14:34:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Hsu et al. (§72021§r)', author: 'Wei-Han Hsu; Bo-Yu Chen; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2110.08862", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning Based EDM Subgenre Classification using Mel-Spectrogram and Tempogram Features\\u00a7r\\n\\n\\u00a78\\u00a7oWei-Han Hsu\\nBo-Yu Chen\\nYi-Hsuan Yang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.08862\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 17 Oct 2021 16:25:33 GMT)\\u00a7r"}']}
{title:'Hiroe (§72021§r)', author: 'Atsuo Hiroe', display:{Lore:['[{"text": "arXiv:2110.09019", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSimilarity-and-Independence-Aware Beamformer with Iterative Casting and Boost Start for Target Source Extraction Using Reference\\u00a7r\\n\\n\\u00a78\\u00a7oAtsuo Hiroe\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09019\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/OJSP.2021.3120938\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nA. Hiroe, \\"Similarity-and-Independence-Aware Beam-former with\\n  Iterative Casting and Boost Start for Target Source Extraction Using\\n  Reference,\\" in IEEE Open Journal of Signal Processing, 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Oct 2021 05:23:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication as a regular paper in the IEEE Open Journal of Signal Processing (2021)\\u00a7r"}']}
{title:'Eskimez et al. (§72021§r)', author: 'Sefik Emre Eskimez; Takuya Yoshioka; Huaming Wang; Xiaofei Wang; Zhuo Chen; Xuedong Huang', display:{Lore:['[{"text": "arXiv:2110.09625", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPersonalized Speech Enhancement: New Models and Comprehensive Evaluation\\u00a7r\\n\\n\\u00a78\\u00a7oSefik Emre Eskimez\\nTakuya Yoshioka\\nHuaming Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09625\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Oct 2021 21:21:23 GMT)\\u00a7r"}']}
{title:'Sun et al. (§72021§r)', author: 'Haoran Sun; Chen Chen; Lantian Li; Dong Wang', display:{Lore:['[{"text": "arXiv:2110.09928", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCycleFlow: Purify Information Factors by Cycle Loss\\u00a7r\\n\\n\\u00a78\\u00a7oHaoran Sun\\nChen Chen\\nLantian Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09928\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Oct 2021 03:06:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Chen et al. (§72021§r)', author: 'Yi-Chen Chen; Shu-wen Yang; Cheng-Kuang Lee; Simon See; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2110.09930", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Representation Learning Through Self-supervised Pretraining And Multi-task Finetuning\\u00a7r\\n\\n\\u00a78\\u00a7oYi-Chen Chen\\nShu-wen Yang\\nCheng-Kuang Lee\\nSimon See\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09930\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Oct 2021 07:16:04 GMT)\\u00a7r"}']}
{title:'Taherian et al. (§72021§r)', author: 'Hassan Taherian; Sefik Emre Eskimez; Takuya Yoshioka; Huaming Wang; Zhuo Chen; Xuedong Huang', display:{Lore:['[{"text": "arXiv:2110.10330", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOne model to enhance them all: array geometry agnostic multi-channel personalized speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oHassan Taherian\\nSefik Emre Eskimez\\nTakuya Yoshioka\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.10330\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Oct 2021 01:03:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Subakan et al. (§72021§r)', author: 'Cem Subakan; Mirco Ravanelli; Samuele Cornell; François Grondin', display:{Lore:['[{"text": "arXiv:2110.10812", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lREAL-M: Towards Speech Separation on Real Mixtures\\u00a7r\\n\\n\\u00a78\\u00a7oCem Subakan\\nMirco Ravanelli\\nSamuele Cornell\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.10812\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Oct 2021 22:39:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Torcoli et al. (§72021§r)', author: 'Matteo Torcoli; Thorsten Kastner; Jürgen Herre', display:{Lore:['[{"text": "arXiv:2110.11438", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lObjective Measures of Perceptual Audio Quality Reviewed: An Evaluation of Their Application Domain Dependence\\u00a7r\\n\\n\\u00a78\\u00a7oMatteo Torcoli\\nThorsten Kastner\\nJ\\u00fcrgen Herre\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.11438\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3069302\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING,\\n  VOL. 29, 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Oct 2021 19:15:36 GMT)\\u00a7r"}']}
{title:'Hu et al. (§72021§r)', author: 'Ting-Yao Hu; Mohammadreza Armandpour; Ashish Shrivastava; Jen-Hao Rick Chang; Hema Koppula; Oncel Tuzel', display:{Lore:['[{"text": "arXiv:2110.11479", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynt++: Utilizing Imperfect Synthetic Data to Improve Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oTing-Yao Hu\\nMohammadreza Armandpour\\nAshish Shrivastava\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.11479\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Oct 2021 21:11:42 GMT)\\u00a7r"}']}
{title:'Benhafid et al. (§72021§r)', author: 'Zhor Benhafid; Kawthar Yasmine Zergat; Abderrahmane Amrouche', display:{Lore:['[{"text": "arXiv:2110.12304", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Study of Acoustic Features in Arabic Speaker Identification under Noisy Environmental Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oZhor Benhafid\\nKawthar Yasmine Zergat\\nAbderrahmane Amrouche\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.12304\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 23 Oct 2021 21:50:50 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72021§r)', author: 'Kang-wook Kim; Junhyeok Lee', display:{Lore:['[{"text": "arXiv:2110.12676", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lControllable and Interpretable Singing Voice Decomposition via Assem-VC\\u00a7r\\n\\n\\u00a78\\u00a7oKang-wook Kim\\nJunhyeok Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.12676\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Oct 2021 06:52:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to NeurIPS Workshop on ML for Creativity and Design 2021 (Oral)\\u00a7r"}']}
{title:'Gburrek et al. (§72021§r)', author: 'Tobias Gburrek; Joerg Schmalenstroeer; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2110.12820", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Synchronization of Wireless Acoustic Sensor Networks in the Presence of Time-varying Sampling Rate Offsets and Speaker Changes\\u00a7r\\n\\n\\u00a78\\u00a7oTobias Gburrek\\nJoerg Schmalenstroeer\\nReinhold Haeb-Umbach\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.12820\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Oct 2021 11:38:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Feng et al. (§72021§r)', author: 'Jinglun Feng; Hua Xiao; Ejup Hoxha; Yifeng Song; Liang Yang; Jizhong Xiao', display:{Lore:['[{"text": "arXiv:2110.13125", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Impact-sounding Acoustic Inspection of Concrete Structure\\u00a7r\\n\\n\\u00a78\\u00a7oJinglun Feng\\nHua Xiao\\nEjup Hoxha\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.13125\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n10th International Conference on Structural Health Monitoring of\\n  Intelligent Infrastructure, SHMII 10, 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Oct 2021 17:39:36 GMT)\\u00a7r"}']}
{title:'Abeßer et al. (§72021§r)', author: 'Jakob Abeßer; Meinard Müller', display:{Lore:['[{"text": "arXiv:2110.13586", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Audio Domain Adaptation for Acoustic Scene Classification using Disentanglement Learning\\u00a7r\\n\\n\\u00a78\\u00a7oJakob Abe\\u00dfer\\nMeinard M\\u00fcller\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.13586\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Oct 2021 11:39:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Rajaa et al. (§72021§r)', author: 'Shangeth Rajaa; Pham Van Tung; Chng Eng Siong', display:{Lore:['[{"text": "arXiv:2110.13653", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Speaker Representation with Semi-supervised Learning approach for Speaker Profiling\\u00a7r\\n\\n\\u00a78\\u00a7oShangeth Rajaa\\nPham Van Tung\\nChng Eng Siong\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.13653\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 24 Oct 2021 20:44:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Wangyou Zhang; Jing Shi; Chenda Li; Shinji Watanabe; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2110.14139", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lClosing the Gap Between Time-Domain Multi-Channel Speech Enhancement on Real and Simulation Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oWangyou Zhang\\nJing Shi\\nChenda Li\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.14139\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Oct 2021 03:01:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted by IEEE WASPAA 2021\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Wangyou Zhang; Zhuo Chen; Naoyuki Kanda; Shujie Liu; Jinyu Li; Sefik Emre Eskimez; Takuya Yoshioka; Xiong Xiao; Zhong Meng; Yanmin Qian; Furu Wei', display:{Lore:['[{"text": "arXiv:2110.14142", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSeparating Long-Form Speech with Group-Wise Permutation Invariant Training\\u00a7r\\n\\n\\u00a78\\u00a7oWangyou Zhang\\nZhuo Chen\\nNaoyuki Kanda\\n+ 7 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.14142\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 17 Nov 2021 05:10:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 3 tables, submitted to IEEE ICASSP 2022\\u00a7r"}']}
{title:'Zhang et al. (§72021§r)', author: 'Yixuan Zhang; Zhuo Chen; Jian Wu; Takuya Yoshioka; Peidong Wang; Zhong Meng; Jinyu Li', display:{Lore:['[{"text": "arXiv:2110.14838", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContinuous Speech Separation with Recurrent Selective Attention Network\\u00a7r\\n\\n\\u00a78\\u00a7oYixuan Zhang\\nZhuo Chen\\nJian Wu\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.14838\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Oct 2021 01:34:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Adavanne et al. (§72021§r)', author: 'Sharath Adavanne; Archontis Politis; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2111.00030", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDifferentiable Tracking-Based Training of Deep Learning Sound Source Localizers\\u00a7r\\n\\n\\u00a78\\u00a7oSharath Adavanne\\nArchontis Politis\\nTuomas Virtanen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.00030\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Oct 2021 18:04:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEEWorkshop on Applications of Signal Processing to Audio and Acoustics (WASPAA2021)\\u00a7r"}']}
{title:'Narayanan et al. (§72021§r)', author: "Arun Narayanan; Chung-Cheng Chiu; Tom O'Malley; Quan Wang; Yanzhang He", display:{Lore:['[{"text": "arXiv:2111.00127", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-attention conformer for context modeling in speech enhancement for ASR\\u00a7r\\n\\n\\u00a78\\u00a7oArun Narayanan\\nChung-Cheng Chiu\\nTom O\'Malley\\nQuan Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.00127\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 30 Oct 2021 00:15:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWill appear in IEEE-ASRU 2021\\u00a7r"}']}
{title:'Yousefi et al. (§72021§r)', author: 'Midia Yousefi; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:2111.00316", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-time Speaker counting in a cocktail party scenario using Attention-guided Convolutional Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oMidia Yousefi\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.00316\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 30 Oct 2021 19:24:57 GMT)\\u00a7r"}']}
{title:'Yousefi et al. (§72021§r)', author: 'Midia Yousefi; John H. L. Hanse', display:{Lore:['[{"text": "arXiv:2111.00320", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker conditioning of acoustic models using affine transformation for multi-speaker speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMidia Yousefi\\nJohn H. L. Hanse\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.00320\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 30 Oct 2021 19:49:52 GMT)\\u00a7r"}']}
{title:'Hung et al. (§72021§r)', author: 'Yun-Ning Hung; Karn N. Watcharasupat; Chih-Wei Wu; Iroro Orife; Kelian Li; Pavan Seshadri; Junyoung Lee', display:{Lore:['[{"text": "arXiv:2111.01320", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAVASpeech-SMAD: A Strongly Labelled Speech and Music Activity Detection Dataset with Label Co-Occurrence\\u00a7r\\n\\n\\u00a78\\u00a7oYun-Ning Hung\\nKarn N. Watcharasupat\\nChih-Wei Wu\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.01320\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Nov 2021 01:40:32 GMT)\\u00a7r"}']}
{title:'Wu et al. (§72021§r)', author: 'Peter Wu; Jiatong Shi; Yifan Zhong; Shinji Watanabe; Alan W Black', display:{Lore:['[{"text": "arXiv:2111.01326", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-lingual Transfer for Speech Processing using Acoustic Language Similarity\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Wu\\nJiatong Shi\\nYifan Zhong\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.01326\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Nov 2021 01:55:17 GMT)\\u00a7r"}']}
{title:'Wen et al. (§72021§r)', author: 'Shulin Wen; Duy Hai Nguyen; Miqing Wang; Woon-Seng Gan', display:{Lore:['[{"text": "arXiv:2111.01652", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7acs.SY\\u00a7r, \\u00a7eeess.SY\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDesign and Evaluation of Active Noise Control on Machinery Noise\\u00a7r\\n\\n\\u00a78\\u00a7oShulin Wen\\nDuy Hai Nguyen\\nMiqing Wang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.01652\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nAPSIPA 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Nov 2021 15:11:20 GMT)\\u00a7r"}']}
{title:'Ribecky et al. (§72021§r)', author: 'Sebastian Ribecky; Jakob Abeßer; Hanna Lukashevich', display:{Lore:['[{"text": "arXiv:2111.01710", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-input Architecture and Disentangled Representation Learning for Multi-dimensional Modeling of Music Similarity\\u00a7r\\n\\n\\u00a78\\u00a7oSebastian Ribecky\\nJakob Abe\\u00dfer\\nHanna Lukashevich\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.01710\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Nov 2021 16:23:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Westhausen et al. (§72021§r)', author: 'Nils L. Westhausen; Rainer Huber; Hannah Baumgartner; Ragini Sinha; Jan Rennies; Bernd T. Meyer', display:{Lore:['[{"text": "arXiv:2111.01914", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReduction of Subjective Listening Effort for TV Broadcast Signals with Recurrent Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oNils L. Westhausen\\nRainer Huber\\nHannah Baumgartner\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.01914\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Nov 2021 22:07:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE/ACM Transactionson Audio, Speech and Language Processing. This version is the authors\' version and may vary from the final publication in details\\u00a7r"}']}
{title:'Oji et al. (§72021§r)', author: 'Romina Oji; Seyedeh Fatemeh Razavi; Sajjad Abdi Dehsorkh; Alireza Hariri; Hadi Asheri; Reshad Hosseini', display:{Lore:['[{"text": "arXiv:2111.03470", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParsiNorm: A Persian Toolkit for Speech Processing Normalization\\u00a7r\\n\\n\\u00a78\\u00a7oRomina Oji\\nSeyedeh Fatemeh Razavi\\nSajjad Abdi Dehsorkh\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.03470\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 15 Dec 2021 15:07:30 GMT)\\u00a7r"}']}
{title:'Xu et al. (§72021§r)', author: 'Ziyi Xu; Maximilian Strake; Tim Fingscheidt', display:{Lore:['[{"text": "arXiv:2111.03847", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Noise Suppression Maximizing Non-Differentiable PESQ Mediated by a Non-Intrusive PESQNet\\u00a7r\\n\\n\\u00a78\\u00a7oZiyi Xu\\nMaximilian Strake\\nTim Fingscheidt\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.03847\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 6 Nov 2021 10:19:24 GMT)\\u00a7r"}']}
{title:'Lee et al. (§72021§r)', author: 'Dongheon Lee; Seongrae Kim; Jung-Woo Choi', display:{Lore:['[{"text": "arXiv:2111.04312", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInter-channel Conv-TasNet for multichannel speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oDongheon Lee\\nSeongrae Kim\\nJung-Woo Choi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.04312\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Nov 2021 07:52:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, this work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\\u00a7r"}']}
{title:'Sivaprasad et al. (§72021§r)', author: 'Sarath Sivaprasad; Saiteja Kosgi; Vineet Gandhi', display:{Lore:['[{"text": "arXiv:2111.04730", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotional Prosody Control for Speech Generation\\u00a7r\\n\\n\\u00a78\\u00a7oSarath Sivaprasad\\nSaiteja Kosgi\\nVineet Gandhi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.04730\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-307\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 7 Nov 2021 08:52:04 GMT)\\u00a7r"}']}
{title:'Chiang et al. (§72021§r)', author: 'Hsin-Tien Chiang; Yi-Chiao Wu; Cheng Yu; Tomoki Toda; Hsin-Min Wang; Yih-Chun Hu; Yu Tsao', display:{Lore:['[{"text": "arXiv:2111.05691", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHASA-net: A non-intrusive hearing-aid speech assessment network\\u00a7r\\n\\n\\u00a78\\u00a7oHsin-Tien Chiang\\nYi-Chiao Wu\\nCheng Yu\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.05691\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Nov 2021 14:10:13 GMT)\\u00a7r"}']}
{title:'Yu et al. (§72021§r)', author: 'Cheng Yu; Szu-Wei Fu; Tsun-An Hsieh; Yu Tsao; Mirco Ravanelli', display:{Lore:['[{"text": "arXiv:2111.05703", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOSSEM: one-shot speaker adaptive speech enhancement using meta learning\\u00a7r\\n\\n\\u00a78\\u00a7oCheng Yu\\nSzu-Wei Fu\\nTsun-An Hsieh\\nYu Tsao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.05703\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Nov 2021 14:21:28 GMT)\\u00a7r"}']}
{title:'Mošner et al. (§72021§r)', author: 'Ladislav Mošner; Oldřich Plchot; Lukáš Burget; Jan Černocký', display:{Lore:['[{"text": "arXiv:2111.06458", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultiSV: Dataset for Far-Field Multi-Channel Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oLadislav Mo\\u0161ner\\nOld\\u0159ich Plchot\\nLuk\\u00e1\\u0161 Burget\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.06458\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Nov 2021 20:55:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Dohi et al. (§72021§r)', author: 'Kota Dohi; Takashi Endo; Yohei Kawaguchi', display:{Lore:['[{"text": "arXiv:2111.06539", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDisentangling Physical Parameters for Anomalous Sound Detection Under Domain Shifts\\u00a7r\\n\\n\\u00a78\\u00a7oKota Dohi\\nTakashi Endo\\nYohei Kawaguchi\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.06539\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Nov 2021 02:32:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 4 figures\\u00a7r"}']}
{title:'Ronssin et al. (§72021§r)', author: 'Damien Ronssin; Milos Cernak', display:{Lore:['[{"text": "arXiv:2111.06601", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAC-VC: Non-parallel Low Latency Phonetic Posteriorgrams Based Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oDamien Ronssin\\nMilos Cernak\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.06601\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Nov 2021 08:21:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oASRU 2021\\u00a7r"}']}
{title:'Das et al. (§72021§r)', author: 'Rohan Kumar Das; Ruijie Tao; Haizhou Li', display:{Lore:['[{"text": "arXiv:2111.06671", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHLT-NUS SUBMISSION FOR 2020 NIST Conversational Telephone Speech SRE\\u00a7r\\n\\n\\u00a78\\u00a7oRohan Kumar Das\\nRuijie Tao\\nHaizhou Li\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.06671\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Nov 2021 11:47:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o3 pages\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Songxiang Liu; Dan Su; Dong Yu', display:{Lore:['[{"text": "arXiv:2111.07218", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMeta-Voice: Fast few-shot style transfer for expressive voice cloning using meta learning\\u00a7r\\n\\n\\u00a78\\u00a7oSongxiang Liu\\nDan Su\\nDong Yu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.07218\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 14 Nov 2021 01:30:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPre-print technical report, 6 pages, 6 figures\\u00a7r"}']}
{title:'Fuglsig et al. (§72021§r)', author: 'Andreas Jonas Fuglsig; Jan Østergaard; Jesper Jensen; Lars Søndergaard Bertelsen; Peter Mariager; Zheng-Hua Tan', display:{Lore:['[{"text": "arXiv:2111.07759", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Far- and Near-End Speech Intelligibility Enhancement based on the Approximated Speech Intelligibility Index\\u00a7r\\n\\n\\u00a78\\u00a7oAndreas Jonas Fuglsig\\nJan \\u00d8stergaard\\nJesper Jensen\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.07759\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746170\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Nov 2021 13:57:44 GMT)\\u00a7r"}']}
{title:'Lotfidereshgi et al. (§72021§r)', author: 'Reza Lotfidereshgi; Philippe Gournay', display:{Lore:['[{"text": "arXiv:2111.08112", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBiologically inspired speech emotion recognition\\u00a7r\\n\\n\\u00a78\\u00a7oReza Lotfidereshgi\\nPhilippe Gournay\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.08112\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2017.7953135\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Nov 2021 22:28:05 GMT)\\u00a7r"}']}
{title:'Lotfidereshgi et al. (§72021§r)', author: 'Reza Lotfidereshgi; Philippe Gournay', display:{Lore:['[{"text": "arXiv:2111.08116", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Prediction using an Adaptive Recurrent Neural Network with Application to Packet Loss Concealment\\u00a7r\\n\\n\\u00a78\\u00a7oReza Lotfidereshgi\\nPhilippe Gournay\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.08116\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2018.8462185\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Nov 2021 22:33:48 GMT)\\u00a7r"}']}
{title:'Kano et al. (§72021§r)', author: 'Takatomo Kano; Atsunori Ogawa; Marc Delcroix; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2111.08201", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention-based Multi-hypothesis Fusion for Speech Summarization\\u00a7r\\n\\n\\u00a78\\u00a7oTakatomo Kano\\nAtsunori Ogawa\\nMarc Delcroix\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.08201\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Nov 2021 03:00:29 GMT)\\u00a7r"}']}
{title:'Lv et al. (§72021§r)', author: 'Shubo Lv; Yihui Fu; Mengtao Xing; Jiayao Sun; Lei Xie; Jun Huang; Yannan Wang; Tao Yu', display:{Lore:['[{"text": "arXiv:2111.08387", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lS-DCCRN: Super Wide Band DCCRN with learnable complex feature for speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oShubo Lv\\nYihui Fu\\nMengtao Xing\\n+ 4 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.08387\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Nov 2021 11:34:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Yousefi et al. (§72021§r)', author: 'Midia Yousefi; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:2111.08635", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSingle-channel speech separation using Soft-minimum Permutation Invariant Training\\u00a7r\\n\\n\\u00a78\\u00a7oMidia Yousefi\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.08635\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Nov 2021 17:25:05 GMT)\\u00a7r"}']}
{title:"O'Malley et al. (§72021§r)", author: "Tom O'Malley; Arun Narayanan; Quan Wang; Alex Park; James Walker; Nathan Howard", display:{Lore:['[{"text": "arXiv:2111.09935", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Conformer-based ASR Frontend for Joint Acoustic Echo Cancellation, Speech Enhancement and Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oTom O\'Malley\\nArun Narayanan\\nQuan Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.09935\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Nov 2021 20:15:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWill appear in IEEE-ASRU 2021\\u00a7r"}']}
{title:'Liu et al. (§72021§r)', author: 'Chunxi Liu; Michael Picheny; Leda Sarı; Pooja Chitkara; Alex Xiao; Xiaohui Zhang; Mark Chou; Andres Alvarado; Caner Hazirbas; Yatharth Saraf', display:{Lore:['[{"text": "arXiv:2111.09983", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Measuring Fairness in Speech Recognition: Casual Conversations Dataset Transcriptions\\u00a7r\\n\\n\\u00a78\\u00a7oChunxi Liu\\nMichael Picheny\\nLeda Sar\\u0131\\n+ 6 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.09983\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Nov 2021 23:54:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022. Our dataset will be publicly available at (https://ai.facebook.com/datasets/casual-conversations-downloads) for general use. We also would like to note that considering the limitations of our "}','{"text": "dataset, we limit the use of it for only evaluationpurposes (see license agreement)\\u00a7r"}']}
{title:'Kim et al. (§72021§r)', author: 'Jiyeon Kim; Mehul Kumar; Dhananjaya Gowda; Abhinav Garg; Chanwoo Kim', display:{Lore:['[{"text": "arXiv:2111.10043", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA comparison of streaming models and data augmentation methods for robust speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJiyeon Kim\\nMehul Kumar\\nDhananjaya Gowda\\nAbhinav Garg\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.10043\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Nov 2021 04:49:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted as a conference paper at ASRU 2021\\u00a7r"}']}
{title:'Kim et al. (§72021§r)', author: 'Jiyeon Kim; Mehul Kumar; Dhananjaya Gowda; Abhinav Garg; Chanwoo Kim', display:{Lore:['[{"text": "arXiv:2111.10047", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-supervised transfer learning for language expansion of end-to-end speech recognition models to low-resource languages\\u00a7r\\n\\n\\u00a78\\u00a7oJiyeon Kim\\nMehul Kumar\\nDhananjaya Gowda\\nAbhinav Garg\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.10047\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Nov 2021 05:09:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted as a conference paper at ASRU 2021\\u00a7r"}']}
{title:'Makiuchi et al. (§72021§r)', author: 'Mariana Rodrigues Makiuchi; Kuniaki Uto; Koichi Shinoda', display:{Lore:['[{"text": "arXiv:2111.10202", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultimodal Emotion Recognition with High-level Speech and Text Features\\u00a7r\\n\\n\\u00a78\\u00a7oMariana Rodrigues Makiuchi\\nKuniaki Uto\\nKoichi Shinoda\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.10202\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 29 Sep 2021 07:08:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ASRU 2021. Code available at https://github.com/mmakiuchi/multimodal_emotion_recognition\\u00a7r"}']}
{title:'Toye et al. (§72021§r)', author: 'Adedolapo Aishat Toye; Suryaprakash Kompalli', display:{Lore:['[{"text": "arXiv:2111.10207", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparative Study of Speech Analysis Methods to Predict Parkinson\'s Disease\\u00a7r\\n\\n\\u00a78\\u00a7oAdedolapo Aishat Toye\\nSuryaprakash Kompalli\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.10207\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Nov 2021 04:29:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMachine Learning for Health (ML4H) - Extended Abstract\\u00a7r"}']}
{title:'Joshi et al. (§72021§r)', author: 'Raviraj Joshi; Venkateshan Kannan', display:{Lore:['[{"text": "arXiv:2111.10208", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention based end to end Speech Recognition for Voice Search in Hindi and English\\u00a7r\\n\\n\\u00a78\\u00a7oRaviraj Joshi\\nVenkateshan Kannan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.10208\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3503162.3503173\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Nov 2021 18:08:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Forum for Information Retrieval Evaluation (FIRE) 2021\\u00a7r"}']}
{title:'Koyama et al. (§72021§r)', author: 'Shoichi Koyama; Keisuke Kimura; Natsuki Ueno', display:{Lore:['[{"text": "arXiv:2111.11045", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Field Reproduction With Weighted Mode Matching and Infinite-Dimensional Harmonic Analysis: An Experimental Evaluation\\u00a7r\\n\\n\\u00a78\\u00a7oShoichi Koyama\\nKeisuke Kimura\\nNatsuki Ueno\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.11045\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Nov 2021 08:21:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to International Conference on Immersive and 3D Audio (I3DA) 2021\\u00a7r"}']}
{title:'Braun et al. (§72021§r)', author: 'Sebastian Braun; Hannes Gamper', display:{Lore:['[{"text": "arXiv:2111.11606", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEffect of noise suppression losses on speech distortion and ASR performance\\u00a7r\\n\\n\\u00a78\\u00a7oSebastian Braun\\nHannes Gamper\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.11606\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Nov 2021 02:08:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2022\\u00a7r"}']}
{title:'You et al. (§72021§r)', author: 'Zhao You; Shulin Feng; Dan Su; Dong Yu', display:{Lore:['[{"text": "arXiv:2111.11831", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeechMoE2: Mixture-of-Experts Model with Improved Routing\\u00a7r\\n\\n\\u00a78\\u00a7oZhao You\\nShulin Feng\\nDan Su\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.11831\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Nov 2021 12:53:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure. Submitted to ICASSP 2022\\u00a7r"}']}
{title:'McKenzie et al. (§72021§r)', author: 'Thomas McKenzie; Leo McCormack; Christoph Hold', display:{Lore:['[{"text": "arXiv:2111.11882", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDataset of Spatial Room Impulse Responses in a Variable Acoustics Room for Six Degrees-of-Freedom Rendering and Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oThomas McKenzie\\nLeo McCormack\\nChristoph Hold\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.11882\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Nov 2021 13:51:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o3 pages, 3 figures, 2 tables\\u00a7r"}']}
{title:'Kim et al. (§72021§r)', author: 'Minseok Kim; Woosung Choi; Jaehwa Chung; Daewon Lee; Soonyoung Jung', display:{Lore:['[{"text": "arXiv:2111.12203", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKUIELab-MDX-Net: A Two-Stream Neural Network for Music Demixing\\u00a7r\\n\\n\\u00a78\\u00a7oMinseok Kim\\nWoosung Choi\\nJaehwa Chung\\nDaewon Lee\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.12203\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Nov 2021 00:10:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMDX Workshop @ ISMIR 2021, 7 pages, 3 figures\\u00a7r"}']}
{title:'Kim et al. (§72021§r)', author: 'Jinsung Kim; Yeong-Seok Jeong; Woosung Choi; Jaehwa Chung; Soonyoung Jung', display:{Lore:['[{"text": "arXiv:2111.13321", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning source-aware representations of music in a discrete latent space\\u00a7r\\n\\n\\u00a78\\u00a7oJinsung Kim\\nYeong-Seok Jeong\\nWoosung Choi\\nJaehwa Chung\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.13321\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Nov 2021 05:57:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMDX Workshop @ ISMIR 2021, 7 pages, 2 figure\\u00a7r"}']}
{title:'Borgholt et al. (§72021§r)', author: 'Lasse Borgholt; Jakob Drachmann Havtorn; Mostafa Abdou; Joakim Edin; Lars Maaløe; Anders Søgaard; Christian Igel', display:{Lore:['[{"text": "arXiv:2111.14842", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDo We Still Need Automatic Speech Recognition for Spoken Language Understanding?\\u00a7r\\n\\n\\u00a78\\u00a7oLasse Borgholt\\nJakob Drachmann Havtorn\\nMostafa Abdou\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.14842\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 Nov 2021 15:13:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review as a conferencepaper at ICASSP 2022\\u00a7r"}']}
{title:'Vitthal et al. (§72021§r)', author: 'Charvi Vitthal; Shreeharsha B S; Kamini Sabu; Preeti Rao', display:{Lore:['[{"text": "arXiv:2112.00635", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPredicting lexical skills from oral reading with acoustic measures\\u00a7r\\n\\n\\u00a78\\u00a7oCharvi Vitthal\\nShreeharsha B S\\nKamini Sabu\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.00635\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Dec 2021 16:39:31 GMT)\\u00a7r"}']}
{title:'Singh et al. (§72021§r)', author: 'Vishwanath Pratap Singh; Shakti P. Rath; Abhishek Pandey', display:{Lore:['[{"text": "arXiv:2112.01023", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA higher order Minkowski loss for improved prediction ability of acoustic model in ASR\\u00a7r\\n\\n\\u00a78\\u00a7oVishwanath Pratap Singh\\nShakti P. Rath\\nAbhishek Pandey\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.01023\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 Dec 2021 07:20:08 GMT)\\u00a7r"}']}
{title:'Singh et al. (§72021§r)', author: 'Vishwanath Pratap Singh; Shakti P. Rath; Abhishek Pandey', display:{Lore:['[{"text": "arXiv:2112.01025", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Mixture of Expert Based Deep Neural Network for Improved ASR\\u00a7r\\n\\n\\u00a78\\u00a7oVishwanath Pratap Singh\\nShakti P. Rath\\nAbhishek Pandey\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.01025\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 Dec 2021 07:26:34 GMT)\\u00a7r"}']}
{title:'Steinmetz et al. (§72021§r)', author: 'Christian J. Steinmetz; Joshua D. Reiss', display:{Lore:['[{"text": "arXiv:2112.02926", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSteerable discovery of neural audio effects\\u00a7r\\n\\n\\u00a78\\u00a7oChristian J. Steinmetz\\nJoshua D. Reiss\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.02926\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 Dec 2021 10:56:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to NeurIPS 2021 Workshopon Machine Learning for Creativity and Design\\u00a7r"}']}
{title:'Kang et al. (§72021§r)', author: 'Woo Hyun Kang; Jahangir Alam; Abderrahim Fathan', display:{Lore:['[{"text": "arXiv:2112.03454", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Speech Representation Learning via Flow-based Embedding Regularization\\u00a7r\\n\\n\\u00a78\\u00a7oWoo Hyun Kang\\nJahangir Alam\\nAbderrahim Fathan\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.03454\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Dec 2021 02:30:37 GMT)\\u00a7r"}']}
{title:'Yu et al. (§72021§r)', author: 'Chin-Yun Yu; Kin-Wai Cheuk', display:{Lore:['[{"text": "arXiv:2112.03752", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDanna-Sep: Unite to separate them all\\u00a7r\\n\\n\\u00a78\\u00a7oChin-Yun Yu\\nKin-Wai Cheuk\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.03752\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nISMIR 2021 Workshop on Music Source Separation (2021)\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Dec 2021 15:07:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o3 pages, 1 figure, accepted at MDX workshop, ISMIR 2021\\u00a7r"}']}
{title:'S et al. (§72021§r)', author: 'Zitha S; Raghavendra Rao Suresh; Pooja Rao; T. V. Prabhakar', display:{Lore:['[{"text": "arXiv:2112.03871", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTraining end-to-end speech-to-text models on mobile phones\\u00a7r\\n\\n\\u00a78\\u00a7oZitha S\\nRaghavendra Rao Suresh\\nPooja Rao\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.03871\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Dec 2021 18:08:19 GMT)\\u00a7r"}']}
{title:'Singh et al. (§72021§r)', author: 'Abhayjeet Singh; Achuth Rao MV; Rakesh Vaideeswaran; Chiranjeevi Yarra; Prasanta Kumar Ghosh', display:{Lore:['[{"text": "arXiv:2112.04151", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA study on native American English speech recognition by Indian listeners with varying word familiarity level\\u00a7r\\n\\n\\u00a78\\u00a7oAbhayjeet Singh\\nAchuth Rao MV\\nRakesh Vaideeswaran\\nChiranjeevi Yarra\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.04151\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 8 Dec 2021 07:43:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 5 figues, COCOSDA 2021\\u00a7r"}']}
{title:'Rao et al. (§72021§r)', author: 'Nagashree K. S. Rao; Nils Peters', display:{Lore:['[{"text": "arXiv:2112.04841", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn The Effect Of Coding Artifacts On Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oNagashree K. S. Rao\\nNils Peters\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.04841\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Dec 2021 11:20:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7opaper presented at the 2021 Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE)\\u00a7r"}']}
{title:'Zucatelli et al. (§72021§r)', author: 'G. Zucatelli; R. Coelho', display:{Lore:['[{"text": "arXiv:2112.04949", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHarmonic and non-Harmonic Based Noisy Reverberant Speech Enhancement in Time Domain\\u00a7r\\n\\n\\u00a78\\u00a7oG. Zucatelli\\nR. Coelho\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.04949\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Dec 2021 14:26:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages\\u00a7r"}']}
{title:'Ogura et al. (§72021§r)', author: 'Misa Ogura; Matt Haynes', display:{Lore:['[{"text": "arXiv:2112.05016", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lX-Vector based voice activity detection for multi-genre broadcast speech-to-text\\u00a7r\\n\\n\\u00a78\\u00a7oMisa Ogura\\nMatt Haynes\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.05016\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Dec 2021 16:14:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 3 figures, 4 tables\\u00a7r"}']}
{title:'S et al. (§72021§r)', author: 'Arthi S; V Sreenivas T', display:{Lore:['[{"text": "arXiv:2112.07216", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatiogram: A phase based directional angular measure and perceptual weighting for ensemble source width\\u00a7r\\n\\n\\u00a78\\u00a7oArthi S\\nV Sreenivas T\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.07216\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Dec 2021 08:00:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 11 figures\\u00a7r"}']}
{title:'Deng et al. (§72021§r)', author: 'Keqi Deng; Songjun Cao; Yike Zhang; Long Ma', display:{Lore:['[{"text": "arXiv:2112.07254", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Hybrid CTC/Attention End-to-end Speech Recognition with Pretrained Acoustic and Language Model\\u00a7r\\n\\n\\u00a78\\u00a7oKeqi Deng\\nSongjun Cao\\nYike Zhang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.07254\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Dec 2021 09:38:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oASRU2021\\u00a7r"}']}
{title:'Pizarro et al. (§72021§r)', author: 'Matias Pizarro; Dorothea Kolossa; Asja Fischer', display:{Lore:['[{"text": "arXiv:2112.07400", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobustifying automatic speech recognition by extracting slowly varying features\\u00a7r\\n\\n\\u00a78\\u00a7oMatias Pizarro\\nDorothea Kolossa\\nAsja Fischer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.07400\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/SPSC.2021-8\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Dec 2021 13:50:23 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72021§r)', author: 'Chengyi Wang; Yu Wu; Sanyuan Chen; Shujie Liu; Jinyu Li; Yao Qian; Zhenglu Yang', display:{Lore:['[{"text": "arXiv:2112.08778", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Learning for speech recognition with Intermediate layer supervision\\u00a7r\\n\\n\\u00a78\\u00a7oChengyi Wang\\nYu Wu\\nSanyuan Chen\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.08778\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Dec 2021 10:45:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Mun et al. (§72021§r)', author: 'Sung Hwan Mun; Min Hyun Han; Dongjune Lee; Jihwan Kim; Nam Soo Kim', display:{Lore:['[{"text": "arXiv:2112.08929", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBootstrap Equilibrium and Probabilistic Speaker Representation Learning for Self-supervised Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oSung Hwan Mun\\nMin Hyun Han\\nDongjune Lee\\nJihwan Kim\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.08929\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ACCESS.2021.3137190\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 24 Dec 2021 10:30:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE Access\\u00a7r"}']}
{title:'Agarwal et al. (§72021§r)', author: 'Vinayak Agarwal; Maddie Cusimano; James Traer; Josh McDermott', display:{Lore:['[{"text": "arXiv:2112.08984", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a75physics.app-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lObject-based synthesis of scraping and rolling sounds based on non-linear physical constraints\\u00a7r\\n\\n\\u00a78\\u00a7oVinayak Agarwal\\nMaddie Cusimano\\nJames Traer\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.08984\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceeding of the 24th International Conference on Digital Audio\\n  Effects (DAFx-20in21), 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Dec 2021 15:58:02 GMT)\\u00a7r"}']}
{title:'Anderson et al. (§72021§r)', author: 'Mark Anderson; Naomi Harte', display:{Lore:['[{"text": "arXiv:2112.09006", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBioacoustic Event Detection with prototypical networks and data augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oMark Anderson\\nNaomi Harte\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.09006\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Dec 2021 16:40:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 Figures, 3 Tables, Technical Report for DCASE2021 Challenge Task 5, June 2021\\u00a7r"}']}
{title:'Anderson et al. (§72021§r)', author: 'Mark Anderson; John Kennedy; Naomi Harte', display:{Lore:['[{"text": "arXiv:2112.09042", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow Resource Species Agnostic Bird Activity Detection\\u00a7r\\n\\n\\u00a78\\u00a7oMark Anderson\\nJohn Kennedy\\nNaomi Harte\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.09042\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/SiPS52927.2021.00015\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Workshop on Signal Processing Systems (SiPS), 2021, pp. 34-39\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Dec 2021 17:34:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper is accepted and presented at the IEEE Workshop on Signal Processing Systems (SiPS) October 2021, 3 Figures, 5 Tables\\u00a7r"}']}
{title:'Torcoli et al. (§72021§r)', author: 'Matteo Torcoli; Christian Simon; Jouni Paulus; Davide Straninger; Alfred Riedel; Volker Koch; Stefan Wits; Daniela Rieger; Harald Fuchs; Christian Uhle; Stefan Meltzer; Adrian Murtaza', display:{Lore:['[{"text": "arXiv:2112.09494", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDialog+ in Broadcasting: First Field Tests Using Deep-Learning-Based Dialogue Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oMatteo Torcoli\\nChristian Simon\\nJouni Paulus\\n+ 8 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.09494\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 17 Dec 2021 13:04:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at IBC2021 (International Broadcasting Convention)\\u00a7r"}']}
{title:'Queiroz et al. (§72021§r)', author: 'A. Queiroz; R. Coelho', display:{Lore:['[{"text": "arXiv:2112.09896", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoisy Speech Based Temporal Decomposition to Improve Fundamental Frequency Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oA. Queiroz\\nR. Coelho\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.09896\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 18 Dec 2021 10:31:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages\\u00a7r"}']}
{title:'Huang et al. (§72021§r)', author: 'Rongjie Huang; Feiyang Chen; Yi Ren; Jinglin Liu; Chenye Cui; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2112.10358", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Singer: Fast Multi-Singer Singing Voice Vocoder With A Large-Scale Corpus\\u00a7r\\n\\n\\u00a78\\u00a7oRongjie Huang\\nFeiyang Chen\\nYi Ren\\n+ 2 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.10358\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Dec 2021 06:41:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ACM Multimedia 2021\\u00a7r"}']}
{title:'Emami et al. (§72021§r)', author: 'Melikasadat Emami; Dung Tran; Kazuhito Koishida', display:{Lore:['[{"text": "arXiv:2112.10950", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAugmented Contrastive Self-Supervised Learning for Audio Invariant Representations\\u00a7r\\n\\n\\u00a78\\u00a7oMelikasadat Emami\\nDung Tran\\nKazuhito Koishida\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.10950\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Dec 2021 02:50:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 4 figures\\u00a7r"}']}
{title:'Klumpp et al. (§72021§r)', author: 'Philipp Klumpp; Tomás Arias-Vergara; Juan Camilo Vásquez-Correa; Paula Andrea Pérez-Toro; Juan Rafael Orozco-Arroyave; Anton Batliner; Elmar Nöth', display:{Lore:['[{"text": "arXiv:2112.11514", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Phonetic Footprint of Parkinson\'s Disease\\u00a7r\\n\\n\\u00a78\\u00a7oPhilipp Klumpp\\nTom\\u00e1s Arias-Vergara\\nJuan Camilo V\\u00e1squez-Correa\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.11514\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.csl.2021.101321\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nElsevier Computer Speech and Language, Volume 72, March 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Dec 2021 20:44:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ohttps://www.sciencedirect.com/science/article/abs/pii/S0885230821001169\\u00a7r"}']}
{title:'Muñoz-Romero et al. (§72021§r)', author: 'Sergio Muñoz-Romero; Jerónimo Arenas García; Vanessa Gómez-Verdejo', display:{Lore:['[{"text": "arXiv:2112.12280", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNonnegative OPLS for Supervised Design of Filter Banks: Application to Image and Audio Feature Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oSergio Mu\\u00f1oz-Romero\\nJer\\u00f3nimo Arenas Garc\\u00eda\\nVanessa G\\u00f3mez-Verdejo\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.12280\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TMM.2017.2778568\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Transactions on Multimedia, vol. 20, July 2018\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 Dec 2021 23:58:25 GMT)\\u00a7r"}']}
{title:'Xie et al. (§72021§r)', author: 'Qicong Xie; Tao Li; Xinsheng Wang; Zhichao Wang; Lei Xie; Guoqiao Yu; Guanglu Wan', display:{Lore:['[{"text": "arXiv:2112.12743", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-speaker Multi-style Text-to-speech Synthesis With Single-speaker Single-style Training Data Scenarios\\u00a7r\\n\\n\\u00a78\\u00a7oQicong Xie\\nTao Li\\nXinsheng Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.12743\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Dec 2021 17:54:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to icassp2022\\u00a7r"}']}
{title:'Yang et al. (§72021§r)', author: 'Joon-Young Yang; Joon-Hyuk Chang', display:{Lore:['[{"text": "arXiv:2112.13569", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTask-specific Optimization of Virtual Channel Linear Prediction-based Speech Dereverberation Front-End for Far-Field Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oJoon-Young Yang\\nJoon-Hyuk Chang\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.13569\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Dec 2021 08:44:48 GMT)\\u00a7r"}']}
{title:'Ali (§72021§r)', author: 'Abbas Raza Ali', display:{Lore:['[{"text": "arXiv:2112.14678", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Dialect Arabic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAbbas Raza Ali\\u00a7r\\n\\n\\u00a772021\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.14678\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/IJCNN48605.2020.9206658\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 25 Dec 2021 20:55:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2020 International Joint Conference on Neural Networks (IJCNN)\\u00a7r"}']}
