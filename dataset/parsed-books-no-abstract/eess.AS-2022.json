{title:'Wang et al. (§72022§r)', author: 'Quan Wang; Carlton Downey; Li Wan; Philip Andrew Mansfield; Ignacio Lopez Moreno', display:{Lore:['[{"text": "arXiv:1710.10468", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Diarization with LSTM\\u00a7r\\n\\n\\u00a78\\u00a7oQuan Wang\\nCarlton Downey\\nLi Wan\\nPhilip Andrew Mansfield\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1710.10468\\u00a7r\\n\\nVersion:\\u00a77v7 (Sun, 23 Jan 2022 23:10:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at ICASSP 2018\\u00a7r"}']}
{title:'Tammen et al. (§72022§r)', author: 'Marvin Tammen; Dörte Fischer; Bernd T. Meyer; Simon Doclo', display:{Lore:['[{"text": "arXiv:1905.08492", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDNN-Based Speech Presence Probability Estimation for Multi-Frame Single-Microphone Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oMarvin Tammen\\nD\\u00f6rte Fischer\\nBernd T. Meyer\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1905.08492\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP40776.2020.9054196\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 14 Nov 2022 09:40:54 GMT)\\u00a7r"}']}
{title:'Aldeneh et al. (§72022§r)', author: 'Zakaria Aldeneh; Mimansa Jaiswal; Michael Picheny; Melvin McInnis; Emily Mower Provost', display:{Lore:['[{"text": "arXiv:1910.05115", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIdentifying Mood Episodes Using Dialogue Features from Clinical Interviews\\u00a7r\\n\\n\\u00a78\\u00a7oZakaria Aldeneh\\nMimansa Jaiswal\\nMichael Picheny\\nMelvin McInnis\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1910.05115\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 24 Mar 2022 21:01:05 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Taejun Kim; Juhan Nam', display:{Lore:['[{"text": "arXiv:1911.01803", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTemporal Feedback Convolutional Recurrent Neural Networks for Speech Command Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oTaejun Kim\\nJuhan Nam\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1911.01803\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 18 Sep 2022 07:12:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper is accepted to APSIPA ASC 2022\\u00a7r"}']}
{title:'Maguolo et al. (§72022§r)', author: 'Gianluca Maguolo; Michelangelo Paci; Loris Nanni; Ludovico Bonan', display:{Lore:['[{"text": "arXiv:1912.05472", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudiogmenter: a MATLAB Toolbox for Audio Data Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oGianluca Maguolo\\nMichelangelo Paci\\nLoris Nanni\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.05472\\u00a7r\\n\\nVersion:\\u00a77v4 (Sat, 1 Jan 2022 18:01:02 GMT)\\u00a7r"}']}
{title:'Cutler et al. (§72022§r)', author: 'Ross Cutler; Ramin Mehran; Sam Johnson; Cha Zhang; Adam Kirk; Oliver Whyte; Adarsh Kowdle', display:{Lore:['[{"text": "arXiv:2002.03977", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultimodal active speaker detection and virtual cinematography for video conferencing\\u00a7r\\n\\n\\u00a78\\u00a7oRoss Cutler\\nRamin Mehran\\nSam Johnson\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2002.03977\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 24 May 2022 22:55:20 GMT)\\u00a7r"}']}
{title:'Rituerto-González et al. (§72022§r)', author: 'Esther Rituerto-González; Carmen Peláez-Moreno', display:{Lore:['[{"text": "arXiv:2003.07688", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end Recurrent Denoising Autoencoder Embeddings for Speaker Identification\\u00a7r\\n\\n\\u00a78\\u00a7oEsther Rituerto-Gonz\\u00e1lez\\nCarmen Pel\\u00e1ez-Moreno\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2003.07688\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s00521-021-06083-7\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nOnline, Neural Comput & Applic (2021), pp. 1-11\\u00a7r\\n\\nVersion:\\u00a77v5 (Mon, 16 May 2022 12:59:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished on Monday 10th of May 2021 in Neural Computing and Applications, Springer\\u00a7r"}']}
{title:'Lepori et al. (§72022§r)', author: 'Michael A Lepori; Chaz Firestone', display:{Lore:['[{"text": "arXiv:2003.12362", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCan you hear me now? Sensitive comparisons of human and machine perception\\u00a7r\\n\\n\\u00a78\\u00a7oMichael A Lepori\\nChaz Firestone\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2003.12362\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 3 Aug 2022 01:55:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o24 pages; 4 figures\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Chao-Han Huck Yang; Jun Qi; Pin-Yu Chen; Xiaoli Ma; Chin-Hui Lee', display:{Lore:['[{"text": "arXiv:2003.13917", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCharacterizing Speech Adversarial Examples Using Self-Attention U-Net Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oChao-Han Huck Yang\\nJun Qi\\nPin-Yu Chen\\nXiaoli Ma\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2003.13917\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP40776.2020.9053288\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2020 IEEE International Conference on Acoustics, Speech and Signal\\n  Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 1 Jan 2022 04:47:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe authors have revised some annotations in Table 4 to improve the clarity. The authors thank reading feedbacks from Jonathan Le Roux. The first draft was finishedin August 2019. Accepted to IEEE ICASSP 2020\\u00a7r"}']}
{title:'Sun et al. (§72022§r)', author: 'Yifu Sun; Xulong Zhang; Yi Yu; Xi Chen; Wei Li', display:{Lore:['[{"text": "arXiv:2004.04040", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigation of Singing Voice Separation for Singing Voice Detection in Polyphonic Music\\u00a7r\\n\\n\\u00a78\\u00a7oYifu Sun\\nXulong Zhang\\nYi Yu\\nXi Chen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2004.04040\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 5 May 2022 11:01:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by CSMT (The 9th Conference on Sound and Music Technology)\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Xulong Zhang; Jianzong Wang; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2004.04371", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMDCNN-SID: Multi-scale Dilated Convolution Network for Singer Identification\\u00a7r\\n\\n\\u00a78\\u00a7oXulong Zhang\\nJianzong Wang\\nNing Cheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2004.04371\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 23 May 2022 09:34:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IJCNN2022 (The 2022 International Joint Conference on Neural Networks)\\u00a7r"}']}
{title:'Casanova et al. (§72022§r)', author: 'Edresson Casanova; Arnaldo Candido Junior; Christopher Shulby; Frederico Santos de Oliveira; João Paulo Teixeira; Moacir Antonelli Ponti; Sandra Maria Aluisio', display:{Lore:['[{"text": "arXiv:2005.05144", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTTS-Portuguese Corpus: a corpus for speech synthesis in Brazilian Portuguese\\u00a7r\\n\\n\\u00a78\\u00a7oEdresson Casanova\\nArnaldo Candido Junior\\nChristopher Shulby\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2005.05144\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s10579-021-09570-4\\u00a7r\\n\\nVersion:\\u00a77v4 (Sat, 29 Jan 2022 19:28:54 GMT)\\u00a7r"}']}
{title:'Yao et al. (§72022§r)', author: 'Xuewen Yao; Megan Micheletti; Mckensey Johnson; Edison Thomaz; Kaya de Barbaro', display:{Lore:['[{"text": "arXiv:2005.07036", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInfant Crying Detection in Real-World Environments\\u00a7r\\n\\n\\u00a78\\u00a7oXuewen Yao\\nMegan Micheletti\\nMckensey Johnson\\nEdison Thomaz\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2005.07036\\u00a7r\\n\\nVersion:\\u00a77v6 (Wed, 16 Feb 2022 22:01:27 GMT)\\u00a7r"}']}
{title:'Ren et al. (§72022§r)', author: 'Yi Ren; Chenxu Hu; Xu Tan; Tao Qin; Sheng Zhao; Zhou Zhao; Tie-Yan Liu', display:{Lore:['[{"text": "arXiv:2006.04558", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFastSpeech 2: Fast and High-Quality End-to-End Text to Speech\\u00a7r\\n\\n\\u00a78\\u00a7oYi Ren\\nChenxu Hu\\nXu Tan\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2006.04558\\u00a7r\\n\\nVersion:\\u00a77v8 (Mon, 8 Aug 2022 01:53:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICLR 2021\\u00a7r"}']}
{title:'Sun et al. (§72022§r)', author: 'Yifan Sun; Xihong Wu', display:{Lore:['[{"text": "arXiv:2006.13350", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmbodied Self-supervised Learning by Coordinated Sampling and Training\\u00a7r\\n\\n\\u00a78\\u00a7oYifan Sun\\nXihong Wu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2006.13350\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 16 Jan 2022 09:33:33 GMT)\\u00a7r"}']}
{title:'Jeon (§72022§r)', author: 'Woojay Jeon', display:{Lore:['[{"text": "arXiv:2007.10329", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic Neighbor Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oWoojay Jeon\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.10329\\u00a7r\\n\\nVersion:\\u00a77v5 (Thu, 6 Jan 2022 23:14:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAnonymized version submitted to ICLR 2021\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Yunpeng Li; Beat Gfeller; Marco Tagliasacchi; Dominik Roblek', display:{Lore:['[{"text": "arXiv:2008.02027", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning to Denoise Historical Music\\u00a7r\\n\\n\\u00a78\\u00a7oYunpeng Li\\nBeat Gfeller\\nMarco Tagliasacchi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.02027\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 16 Jun 2022 11:18:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oISMIR 2020\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Yu-Wen Chen; Kuo-Hsuan Hung; You-Jin Li; Alexander Chao-Fu Kang; Ya-Hsin Lai; Kai-Chun Liu; Szu-Wei Fu; Syu-Siang Wang; Yu Tsao', display:{Lore:['[{"text": "arXiv:2008.09264", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCITISEN: A Deep Learning-Based Speech Signal-Processing Mobile Application\\u00a7r\\n\\n\\u00a78\\u00a7oYu-Wen Chen\\nKuo-Hsuan Hung\\nYou-Jin Li\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.09264\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ACCESS.2022.3153469\\u00a7r\\n\\nVersion:\\u00a77v5 (Mon, 25 Apr 2022 14:23:41 GMT)\\u00a7r"}']}
{title:'Chowdhury et al. (§72022§r)', author: 'Anurag Chowdhury; Arun Ross', display:{Lore:['[{"text": "arXiv:2008.11668", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeepVOX: Discovering Features from Raw Audio for Speaker Recognition in Non-ideal Audio Signals\\u00a7r\\n\\n\\u00a78\\u00a7oAnurag Chowdhury\\nArun Ross\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.11668\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 13 Jun 2022 03:39:05 GMT)\\u00a7r"}']}
{title:'Nistal et al. (§72022§r)', author: 'J. Nistal; S. Lattner; G. Richard', display:{Lore:['[{"text": "arXiv:2008.12073", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDrumGAN: Synthesis of Drum Sounds With Timbral Feature Conditioning Using Generative Adversarial Networks\\u00a7r\\n\\n\\u00a78\\u00a7oJ. Nistal\\nS. Lattner\\nG. Richard\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.12073\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 28 Jun 2022 17:45:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 1 figure, 3 tables, accepted in Proc. of the 21st International Society for Music Information Retrieval (ISMIR2020)\\u00a7r"}']}
{title:'Chuang et al. (§72022§r)', author: 'Shang-Yi Chuang; Hsin-Min Wang; Yu Tsao', display:{Lore:['[{"text": "arXiv:2008.13222", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Lite Audio-Visual Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oShang-Yi Chuang\\nHsin-Min Wang\\nYu Tsao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2008.13222\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 31 Jan 2022 19:57:05 GMT)\\u00a7r"}']}
{title:'Abdallah et al. (§72022§r)', author: 'Habib Ben Abdallah; Christopher J. Henry; Sheela Ramanna', display:{Lore:['[{"text": "arXiv:2009.04077", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l1-Dimensional polynomial neural networks for audio signal related problems\\u00a7r\\n\\n\\u00a78\\u00a7oHabib Ben Abdallah\\nChristopher J. Henry\\nSheela Ramanna\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2009.04077\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.knosys.2022.108174\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 12 Jan 2022 19:07:17 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Yu Zhang; James Qin; Daniel S. Park; Wei Han; Chung-Cheng Chiu; Ruoming Pang; Quoc V. Le; Yonghui Wu', display:{Lore:['[{"text": "arXiv:2010.10504", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYu Zhang\\nJames Qin\\nDaniel S. Park\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.10504\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Jul 2022 22:31:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 3 figures, 5 tables. Accepted to NeurIPS SAS 2020 Workshop; v2: minor errors corrected\\u00a7r"}']}
{title:'Cutler et al. (§72022§r)', author: 'Ross Cutler; Babak Naderi; Markus Loide; Sten Sootla; Ando Saabas', display:{Lore:['[{"text": "arXiv:2010.13063", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCrowdsourcing approach for subjective evaluation of echo impairment\\u00a7r\\n\\n\\u00a78\\u00a7oRoss Cutler\\nBabak Naderi\\nMarkus Loide\\nSten Sootla\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.13063\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 27 Feb 2022 08:19:09 GMT)\\u00a7r"}']}
{title:'Bai et al. (§72022§r)', author: 'Jisheng Bai; Jianfeng Chen; Mou Wang', display:{Lore:['[{"text": "arXiv:2011.00175", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultimodal Urban Sound Tagging with Spatiotemporal Context\\u00a7r\\n\\n\\u00a78\\u00a7oJisheng Bai\\nJianfeng Chen\\nMou Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.00175\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TCDS.2022.3160168\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 15 Mar 2022 03:00:04 GMT)\\u00a7r"}']}
{title:'Choi et al. (§72022§r)', author: 'Yeunju Choi; Youngmoon Jung; Youngjoo Suh; Hoirin Kim', display:{Lore:['[{"text": "arXiv:2011.01174", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning to Maximize Speech Quality Directly Using MOS Prediction for Neural Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oYeunju Choi\\nYoungmoon Jung\\nYoungjoo Suh\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.01174\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ACCESS.2022.3175810\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Access, vol. 10, pp. 52621 - 52629, 2022\\u00a7r\\n\\nVersion:\\u00a77v5 (Wed, 25 May 2022 07:07:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 5 figures, 4 tables\\u00a7r"}']}
{title:'Deng et al. (§72022§r)', author: 'Chengyun Deng; Shiqian Ma; Yi Zhang; Yongtao Sha; Hui Zhang; Hui Song; Xiangang Li', display:{Lore:['[{"text": "arXiv:2011.02102", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Speaker Extraction Network Based on Iterative Refined Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oChengyun Deng\\nShiqian Ma\\nYi Zhang\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.02102\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 11 Aug 2022 08:55:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2021\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Yi Zhang; Chengyun Deng; Shiqian Ma; Yongtao Sha; Hui Song', display:{Lore:['[{"text": "arXiv:2011.02109", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Multi-task Network for Delay Estimation and Echo Cancellation\\u00a7r\\n\\n\\u00a78\\u00a7oYi Zhang\\nChengyun Deng\\nShiqian Ma\\nYongtao Sha\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.02109\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 11 Aug 2022 08:51:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2020\\u00a7r"}']}
{title:'Sivaraman et al. (§72022§r)', author: 'Aswin Sivaraman; Minje Kim', display:{Lore:['[{"text": "arXiv:2011.03426", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Learning from Contrastive Mixtures for Personalized Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oAswin Sivaraman\\nMinje Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.03426\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 Aug 2022 18:24:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been superseded by article 2104.02017\\u00a7r"}']}
{title:'Arasteh (§72022§r)', author: 'Soroosh Tayebi Arasteh', display:{Lore:['[{"text": "arXiv:2011.04896", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Empirical Study on Text-Independent Speaker Verification based on the GE2E Method\\u00a7r\\n\\n\\u00a78\\u00a7oSoroosh Tayebi Arasteh\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2011.04896\\u00a7r\\n\\nVersion:\\u00a77v4 (Sun, 27 Feb 2022 12:23:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 7 tables, 2 figures, 4 algorithms. An empirical study on the paper arXiv:1710.10467 by Wan et al. (2017)\\u00a7r"}']}
{title:'Haubner et al. (§72022§r)', author: 'Thomas Haubner; Mhd. Modar Halimeh; Andreas Brendel; Walter Kellermann', display:{Lore:['[{"text": "arXiv:2012.08867", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Synergistic Kalman- and Deep Postfiltering Approach to Acoustic Echo Cancellation\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Haubner\\nMhd. Modar Halimeh\\nAndreas Brendel\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2012.08867\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.23919/EUSIPCO54536.2021.9616295\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 4 Mar 2022 10:45:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for European Signal Processing Conference (EUSIPCO), Dublin, Ireland, August 2021\\u00a7r"}']}
{title:'Xu et al. (§72022§r)', author: 'Xinmeng Xu; Jianjun Hao', display:{Lore:['[{"text": "arXiv:2101.05975", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-layer Feature Fusion Convolution Network for Audio-visual Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oXinmeng Xu\\nJianjun Hao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.05975\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 23 May 2022 10:04:15 GMT)\\u00a7r"}']}
{title:'Xu et al. (§72022§r)', author: 'Xinmeng Xu; Jianjun Hao', display:{Lore:['[{"text": "arXiv:2101.06268", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAMFFCN: Attentional Multi-layer Feature Fusion Convolution Network for Audio-visual Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oXinmeng Xu\\nJianjun Hao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2101.06268\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 27 Sep 2022 01:01:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2101.05975\\u00a7r"}']}
{title:'Xu et al. (§72022§r)', author: 'Xinmeng Xu; Yang Wang; Dongxiang Xu; Yiyuan Peng; Cong Zhang; Jie Jia; Binbin Chen', display:{Lore:['[{"text": "arXiv:2102.02599", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVSEGAN: Visual Speech Enhancement Generative Adversarial Network\\u00a7r\\n\\n\\u00a78\\u00a7oXinmeng Xu\\nYang Wang\\nDongxiang Xu\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.02599\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Apr 2022 01:10:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Hangting Chen; Yang Yi; Dang Feng; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2102.02998", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBeam-Guided TasNet: An Iterative Speech Separation Framework with Multi-Channel Output\\u00a7r\\n\\n\\u00a78\\u00a7oHangting Chen\\nYang Yi\\nDang Feng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.02998\\u00a7r\\n\\nVersion:\\u00a77v6 (Tue, 12 Apr 2022 08:34:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Inerspeech 2022\\u00a7r"}']}
{title:'Steinmetz et al. (§72022§r)', author: 'Christian J. Steinmetz; Joshua D. Reiss', display:{Lore:['[{"text": "arXiv:2102.06200", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient neural networks for real-time modeling of analog dynamic range compression\\u00a7r\\n\\n\\u00a78\\u00a7oChristian J. Steinmetz\\nJoshua D. Reiss\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2102.06200\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 15 Apr 2022 10:48:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUpdated and will appear at 152nd AES Convention (note title change)\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Meng Li; Xia Yan; Feng Lin', display:{Lore:['[{"text": "arXiv:2103.01661", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIncorporating VAD into ASR System by Multi-task Learning\\u00a7r\\n\\n\\u00a78\\u00a7oMeng Li\\nXia Yan\\nFeng Lin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.01661\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 30 Sep 2022 06:36:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures\\u00a7r"}']}
{title:'Kayser et al. (§72022§r)', author: 'Hendrik Kayser; Tobias Herzke; Paul Maanen; Max Zimmermann; Giso Grimm; Volker Hohmann', display:{Lore:['[{"text": "arXiv:2103.02313", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOpen community platform for hearing aid algorithm research: open Master Hearing Aid (openMHA)\\u00a7r\\n\\n\\u00a78\\u00a7oHendrik Kayser\\nTobias Herzke\\nPaul Maanen\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.02313\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.softx.2021.100953\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nSoftwareX, Volume 17, 2022, 100953, ISSN 2352-7110\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 24 Jan 2022 10:38:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 5 figures\\u00a7r"}']}
{title:'Grondin et al. (§72022§r)', author: 'François Grondin; Dominic Létourneau; Cédric Godin; Jean-Samuel Lauzon; Jonathan Vincent; Simon Michaud; Samuel Faucher; François Michaud', display:{Lore:['[{"text": "arXiv:2103.03954", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lODAS: Open embeddeD Audition System\\u00a7r\\n\\n\\u00a78\\u00a7oFran\\u00e7ois Grondin\\nDominic L\\u00e9tourneau\\nC\\u00e9dric Godin\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.03954\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 11 May 2022 17:22:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper was published in Frontiers Robotics and AI\\u00a7r"}']}
{title:'Liang et al. (§72022§r)', author: 'Yunhao Liang; Yanhua Long; Yijie Li; Jiaen Liang; Yuping Wang', display:{Lore:['[{"text": "arXiv:2103.12388", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint framework with deep feature distillation and adaptive focal loss for weakly supervised audio tagging and acoustic event detection\\u00a7r\\n\\n\\u00a78\\u00a7oYunhao Liang\\nYanhua Long\\nYijie Li\\nJiaen Liang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.12388\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.dsp.2022.103446\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 12 Feb 2022 09:00:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUpdated, please refer to \\"https://sciencedirect.53yu.com/science/article/abs/pii/S105120042200063X\\"\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Rui Wang; Zhihua Wei; Haoran Duan; Shouling Ji; Yang Long; Zhen Hong', display:{Lore:['[{"text": "arXiv:2103.13581", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficientTDNN: Efficient Architecture Search for Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oRui Wang\\nZhihua Wei\\nHaoran Duan\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.13581\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2022.3182856\\u00a7r\\n\\nVersion:\\u00a77v5 (Sat, 18 Jun 2022 09:35:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 12 figures, accepted to TASLP\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Sehoon Kim; Amir Gholami; Zhewei Yao; Nicholas Lee; Patrick Wang; Aniruddha Nrusimha; Bohan Zhai; Tianren Gao; Michael W. Mahoney; Kurt Keutzer', display:{Lore:['[{"text": "arXiv:2103.16827", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInteger-only Zero-shot Quantization for Efficient Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSehoon Kim\\nAmir Gholami\\nZhewei Yao\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.16827\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2022\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 30 Jan 2022 22:10:56 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Minchan Kim; Sung Jun Cheon; Byoung Jin Choi; Jong Jin Kim; Nam Soo Kim', display:{Lore:['[{"text": "arXiv:2104.00436", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExpressive Text-to-Speech using Style Tag\\u00a7r\\n\\n\\u00a78\\u00a7oMinchan Kim\\nSung Jun Cheon\\nByoung Jin Choi\\nJong Jin Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.00436\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-465\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 6 Oct 2022 07:47:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2021\\u00a7r"}']}
{title:'Chetupalli et al. (§72022§r)', author: 'Srikanth Raj Chetupalli; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:2104.01882", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker conditioned acoustic modeling for multi-speaker conversational ASR\\u00a7r\\n\\n\\u00a78\\u00a7oSrikanth Raj Chetupalli\\nSriram Ganapathy\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.01882\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 29 Aug 2022 10:34:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oManuscript accepted for presentation at Interspeech 2022\\u00a7r"}']}
{title:'Sivaraman et al. (§72022§r)', author: 'Aswin Sivaraman; Minje Kim', display:{Lore:['[{"text": "arXiv:2104.02017", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Personalized Speech Enhancement through Self-Supervised Learning\\u00a7r\\n\\n\\u00a78\\u00a7oAswin Sivaraman\\nMinje Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.02017\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JSTSP.2022.3181782\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 27 Jul 2022 04:48:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 9 figures, published in IEEE JSTSP2022\\u00a7r"}']}
{title:'Bakhturina et al. (§72022§r)', author: 'Evelina Bakhturina; Vitaly Lavrukhin; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2104.04896", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Toolbox for Construction and Analysis of Speech Datasets\\u00a7r\\n\\n\\u00a78\\u00a7oEvelina Bakhturina\\nVitaly Lavrukhin\\nBoris Ginsburg\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.04896\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 6 Jan 2022 21:53:39 GMT)\\u00a7r"}']}
{title:'Fejgin et al. (§72022§r)', author: 'Daniel Fejgin; Simon Doclo', display:{Lore:['[{"text": "arXiv:2104.05079", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparison of Binaural RTF-Vector-Based Direction of Arrival Estimation Methods Exploiting an External Microphone\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Fejgin\\nSimon Doclo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2104.05079\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.23919/EUSIPCO54536.2021.9616327\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 18 May 2022 14:34:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oo\\u0327p\\u0327y\\u0327\\u0157i\\u0327\\u0123\\u1e29\\u0163 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for "}','{"text": "advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Jinglin Liu; Chengxi Li; Yi Ren; Feiyang Chen; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2105.02446", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism\\u00a7r\\n\\n\\u00a78\\u00a7oJinglin Liu\\nChengxi Li\\nYi Ren\\nFeiyang Chen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.02446\\u00a7r\\n\\nVersion:\\u00a77v6 (Tue, 22 Mar 2022 04:53:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSVS (DiffSinger), TTS (DiffSpeech), Shallow Diffusion Mechanism; Submitted to arxiv on 6 May 2021; Accepted by AAAI 2022\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Wencheng Li; Zhenhua Tan; Jingyu Ning; Zhenche Xia; Danke Wu', display:{Lore:['[{"text": "arXiv:2105.14826", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPF-Net: Personalized Filter for Speaker Recognition from Raw Waveform\\u00a7r\\n\\n\\u00a78\\u00a7oWencheng Li\\nZhenhua Tan\\nJingyu Ning\\nZhenche Xia\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2105.14826\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 18 Jun 2022 16:30:29 GMT)\\u00a7r"}']}
{title:'Haubner et al. (§72022§r)', author: 'Thomas Haubner; Andreas Brendel; Walter Kellermann', display:{Lore:['[{"text": "arXiv:2106.01262", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-To-End Deep Learning-Based Adaptation Control for Frequency-Domain Adaptive System Identification\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Haubner\\nAndreas Brendel\\nWalter Kellermann\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.01262\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 4 Mar 2022 10:49:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2022, Singapore, Singapore\\u00a7r"}']}
{title:'Pan et al. (§72022§r)', author: 'Zexu Pan; Ruijie Tao; Chenglin Xu; Haizhou Li', display:{Lore:['[{"text": "arXiv:2106.07150", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelective Listening by Synchronizing Speech with Lips\\u00a7r\\n\\n\\u00a78\\u00a7oZexu Pan\\nRuijie Tao\\nChenglin Xu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.07150\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 24 Jan 2022 06:17:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by TASLP\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Fan-Lin Wang; Yu-Huai Peng; Hung-Shin Lee; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:2106.07579", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDual-Path Filter Network: Speaker-Aware Modeling for Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oFan-Lin Wang\\nYu-Huai Peng\\nHung-Shin Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.07579\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 30 Mar 2022 09:05:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech2021\\u00a7r"}']}
{title:'Horiguchi et al. (§72022§r)', author: 'Shota Horiguchi; Yusuke Fujita; Shinji Watanabe; Yawen Xue; Paola Garcia', display:{Lore:['[{"text": "arXiv:2106.10654", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEncoder-Decoder Based Attractors for End-to-End Neural Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oShota Horiguchi\\nYusuke Fujita\\nShinji Watanabe\\nYawen Xue\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.10654\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2022.3162080\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 28 Mar 2022 10:39:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEE/ACM TASLP. This article is based on our previous conference paper arxiv:2005.09921\\u00a7r"}']}
{title:'Kang et al. (§72022§r)', author: 'Minsu Kang; Sungjae Kim; Injung Kim', display:{Lore:['[{"text": "arXiv:2106.11171", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUniTTS: Residual Learning of Unified Embedding Space for Speech Style Control\\u00a7r\\n\\n\\u00a78\\u00a7oMinsu Kang\\nSungjae Kim\\nInjung Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.11171\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 28 Feb 2022 19:37:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o20 pages, 11 figures\\u00a7r"}']}
{title:'Lee et al. (§72022§r)', author: 'Gyeong-Hoon Lee; Tae-Woo Kim; Hanbin Bae; Min-Ji Lee; Young-Ik Kim; Hoon-Young Cho', display:{Lore:['[{"text": "arXiv:2106.15205", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lN-Singer: A Non-Autoregressive Korean Singing Voice Synthesis System for Pronunciation Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oGyeong-Hoon Lee\\nTae-Woo Kim\\nHanbin Bae\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.15205\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 21 Feb 2022 13:58:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2021\\u00a7r"}']}
{title:'Zaiem et al. (§72022§r)', author: 'Salah Zaiem; Titouan Parcollet; Slim Essid; Abdel Heba', display:{Lore:['[{"text": "arXiv:2107.00594", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPretext Tasks selection for multitask self-supervised speech representation learning\\u00a7r\\n\\n\\u00a78\\u00a7oSalah Zaiem\\nTitouan Parcollet\\nSlim Essid\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.00594\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JSTSP.2022.3195430\\u00a7r\\n\\nVersion:\\u00a77v5 (Fri, 11 Nov 2022 09:46:36 GMT)\\u00a7r"}']}
{title:'Vecchi et al. (§72022§r)', author: 'Alejandro Osses Vecchi; Léo Varnet; Laurel H. Carney; Torsten Dau; Ian C. Bruce; Sarah Verhulst; Piotr Majdak', display:{Lore:['[{"text": "arXiv:2107.01753", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA comparative study of eight human auditory models of monaural processing\\u00a7r\\n\\n\\u00a78\\u00a7oAlejandro Osses Vecchi\\nL\\u00e9o Varnet\\nLaurel H. Carney\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.01753\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1051/aacus/2022008\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 9 Jan 2022 11:36:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oRevision 1 of the manuscript\\u00a7r"}']}
{title:'Hu et al. (§72022§r)', author: 'Cheng-Hung Hu; Yu-Huai Peng; Junichi Yamagishi; Yu Tsao; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:2107.09392", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSVSNet: An End-to-end Speaker Voice Similarity Assessment Model\\u00a7r\\n\\n\\u00a78\\u00a7oCheng-Hung Hu\\nYu-Huai Peng\\nJunichi Yamagishi\\nYu Tsao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.09392\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2022.3152672\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 17 Feb 2022 02:35:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in IEEE Signal Processing Letters (SPL)\\u00a7r"}']}
{title:'Lee et al. (§72022§r)', author: 'Gyeong-Tae Lee; Hyeonuk Nam; Seong-Hu Kim; Sang-Min Choi; Youngkey Kim; Yong-Hwa Park', display:{Lore:['[{"text": "arXiv:2107.13260", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep learning based cough detection camera using enhanced features\\u00a7r\\n\\n\\u00a78\\u00a7oGyeong-Tae Lee\\nHyeonuk Nam\\nSeong-Hu Kim\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.13260\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.eswa.2022.117811\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nExpert Systems With Applications, Vol. 206, No. 15, pp. 1-20, 2022\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 24 May 2022 17:01:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o28 pages, 20 figures, and 14 tables\\u00a7r"}']}
{title:'Kessler et al. (§72022§r)', author: 'Samuel Kessler; Bethan Thomas; Salah Karout', display:{Lore:['[{"text": "arXiv:2107.13530", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Adapter Based Pre-Training for Efficient and Scalable Self-Supervised Speech Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oSamuel Kessler\\nBethan Thomas\\nSalah Karout\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.13530\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 7 Feb 2022 11:30:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 6 figures. Accepted at ICASSP 2022. This version replaces an earlier version of paper accepted at an ICML 2021 workshop\\u00a7r"}']}
{title:'Franzen et al. (§72022§r)', author: 'Jan Franzen; Tim Fingscheidt', display:{Lore:['[{"text": "arXiv:2108.03051", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Residual Echo Suppression and Noise Reduction: A Multi-Input FCRN Approach in a Hybrid Speech Enhancement System\\u00a7r\\n\\n\\u00a78\\u00a7oJan Franzen\\nTim Fingscheidt\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.03051\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 23 Mar 2022 08:10:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at IEEEICASSP 2022\\u00a7r"}']}
{title:'Kumar et al. (§72022§r)', author: 'Rohit Kumar; Anurenjan Purushothaman; Anirudh Sreeram; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:2108.03975", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Speech Recognition With Joint Dereverberation Of Sub-Band Autoregressive Envelopes\\u00a7r\\n\\n\\u00a78\\u00a7oRohit Kumar\\nAnurenjan Purushothaman\\nAnirudh Sreeram\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.03975\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 18 Feb 2022 03:53:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages with refrences, e2e asr\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Zhan Zhang; Yuehai Wang; Jianyi Yang', display:{Lore:['[{"text": "arXiv:2108.05517", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMasked Acoustic Unit for Mispronunciation Detection and Correction\\u00a7r\\n\\n\\u00a78\\u00a7oZhan Zhang\\nYuehai Wang\\nJianyi Yang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.05517\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 4 May 2022 10:30:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Jin Li; Rongfeng Su; Xurong Xie; Nan Yan; Lan Wang', display:{Lore:['[{"text": "arXiv:2108.07980", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Multi-level Acoustic Feature Extraction Framework for Transformer Based End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJin Li\\nRongfeng Su\\nXurong Xie\\nNan Yan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.07980\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 8 Jul 2022 03:06:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Cwitkowitz et al. (§72022§r)', author: 'Frank Cwitkowitz; Mojtaba Heydari; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2108.10382", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Sparse Analytic Filters for Piano Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oFrank Cwitkowitz\\nMojtaba Heydari\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.10382\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 10 Nov 2022 22:10:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSound and Music Computing Conference (SMC) 2022\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Juntae Kim; Jeehye Lee', display:{Lore:['[{"text": "arXiv:2108.10752", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeneralizing RNN-Transducer to Out-Domain Audio via Sparse Self-Attention Layers\\u00a7r\\n\\n\\u00a78\\u00a7oJuntae Kim\\nJeehye Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.10752\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 17 Jun 2022 12:14:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in INTERSPEECH 2022\\u00a7r"}']}
{title:'Mehta et al. (§72022§r)', author: 'Shivam Mehta; Éva Székely; Jonas Beskow; Gustav Eje Henter', display:{Lore:['[{"text": "arXiv:2108.13320", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural HMMs are all you need (for high-quality attention-free TTS)\\u00a7r\\n\\n\\u00a78\\u00a7oShivam Mehta\\n\\u00c9va Sz\\u00e9kely\\nJonas Beskow\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.13320\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746686\\u00a7r\\n\\nVersion:\\u00a77v6 (Wed, 16 Feb 2022 16:56:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures; final version for ICASSP 2022\\u00a7r"}']}
{title:'Mitsufuji et al. (§72022§r)', author: 'Yuki Mitsufuji; Giorgio Fabbro; Stefan Uhlich; Fabian-Robert Stöter; Alexandre Défossez; Minseok Kim; Woosung Choi; Chin-Yun Yu; Kin-Wai Cheuk', display:{Lore:['[{"text": "arXiv:2108.13559", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Demixing Challenge 2021\\u00a7r\\n\\n\\u00a78\\u00a7oYuki Mitsufuji\\nGiorgio Fabbro\\nStefan Uhlich\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.13559\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3389/frsip.2021.808395\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nFrontiers in Signal Processing, 28 January 2022\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 23 May 2022 09:47:14 GMT)\\u00a7r"}']}
{title:'Yan et al. (§72022§r)', author: 'Bi-Cheng Yan; Shao-Wei Fan Jiang; Fu-An Chao; Berlin Chen', display:{Lore:['[{"text": "arXiv:2108.13816", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMaximum F1-score training for end-to-end mispronunciation detection and diagnosis of L2 English speech\\u00a7r\\n\\n\\u00a78\\u00a7oBi-Cheng Yan\\nShao-Wei Fan Jiang\\nFu-An Chao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.13816\\u00a7r\\n\\nVersion:\\u00a77v4 (Sat, 9 Jul 2022 16:45:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE International Conference on Multimedia and Expo (ICME 2022)\\u00a7r"}']}
{title:'Venkatesh et al. (§72022§r)', author: 'Satvik Venkatesh; David Moffat; Eduardo Reck Miranda', display:{Lore:['[{"text": "arXiv:2109.00962", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lYou Only Hear Once: A YOLO-like Algorithm for Audio Segmentation and Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oSatvik Venkatesh\\nDavid Moffat\\nEduardo Reck Miranda\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.00962\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3390/app12073293\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nAppl.Sci. 12 (2022) 3293\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 18 Sep 2022 17:05:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o19 pages, 4 figures, 8 tables. Added more experimental validation and background information. Published in Applied Sciences\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Haoyu Li; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2109.07931", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDDS: A new device-degraded speech dataset for speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oHaoyu Li\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.07931\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 22 Mar 2022 06:44:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Peng et al. (§72022§r)', author: 'Puyuan Peng; David Harwath', display:{Lore:['[{"text": "arXiv:2109.08186", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.IR\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast-Slow Transformer for Visually Grounding Speech\\u00a7r\\n\\n\\u00a78\\u00a7oPuyuan Peng\\nDavid Harwath\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.08186\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 2 Mar 2022 18:03:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2022, code and model weights are available at https://github.com/jasonppy/FaST-VGS-Family\\u00a7r"}']}
{title:'Raj et al. (§72022§r)', author: 'Desh Raj; Liang Lu; Zhuo Chen; Yashesh Gaur; Jinyu Li', display:{Lore:['[{"text": "arXiv:2109.08555", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContinuous Streaming Multi-Talker ASR with Dual-path Transducers\\u00a7r\\n\\n\\u00a78\\u00a7oDesh Raj\\nLiang Lu\\nZhuo Chen\\nYashesh Gaur\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.08555\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 22 Jan 2022 12:43:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at IEEE ICASSP 2022\\u00a7r"}']}
{title:'Xia et al. (§72022§r)', author: 'Wei Xia; Han Lu; Quan Wang; Anshuman Tripathi; Yiling Huang; Ignacio Lopez Moreno; Hasim Sak', display:{Lore:['[{"text": "arXiv:2109.11641", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTurn-to-Diarize: Online Speaker Diarization Constrained by Transformer Transducer Speaker Turn Detection\\u00a7r\\n\\n\\u00a78\\u00a7oWei Xia\\nHan Lu\\nQuan Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.11641\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 25 Jan 2022 18:48:38 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Yu Zhang; Daniel S. Park; Wei Han; James Qin; Anmol Gulati; Joel Shor; Aren Jansen; Yuanzhong Xu; Yanping Huang; Shibo Wang; Zongwei Zhou; Bo Li; Min Ma; William Chan; Jiahui Yu; Yongqiang Wang; Liangliang Cao; Khe Chai Sim; Bhuvana Ramabhadran; Tara N. Sainath; Françoise Beaufays; Zhifeng Chen; Quoc V. Le; Chung-Cheng Chiu; Ruoming Pang; Yonghui Wu', display:{Lore:['[{"text": "arXiv:2109.13226", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYu Zhang\\nDaniel S. Park\\nWei Han\\n+ 22 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.13226\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JSTSP.2022.3182537\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 21 Jul 2022 18:43:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages, 7 figures, 13 tables; v2: minor corrections, reference baselines and bibliography updated; v3: corrections based on reviewer feedback, bibliography updated\\u00a7r"}']}
{title:'Mitsui et al. (§72022§r)', author: 'Kentaro Mitsui; Kei Sawada', display:{Lore:['[{"text": "arXiv:2109.13714", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMSR-NV: Neural Vocoder Using Multiple Sampling Rates\\u00a7r\\n\\n\\u00a78\\u00a7oKentaro Mitsui\\nKei Sawada\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.13714\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 24 Jun 2022 01:57:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages including supplement, 3 figures, accepted for INTERSPEECH 2022. Audio samples: https://rinnakk.github.io/research/publications/MSR-NV/\\u00a7r"}']}
{title:'Pan et al. (§72022§r)', author: 'Zexu Pan; Meng Ge; Haizhou Li', display:{Lore:['[{"text": "arXiv:2109.14831", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUSEV: Universal Speaker Extraction with Visual Cue\\u00a7r\\n\\n\\u00a78\\u00a7oZexu Pan\\nMeng Ge\\nHaizhou Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.14831\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 Aug 2022 18:41:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by TASLP\\u00a7r"}']}
{title:'Ren et al. (§72022§r)', author: 'Yi Ren; Jinglin Liu; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2109.15166", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPortaSpeech: Portable and High-Quality Generative Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oYi Ren\\nJinglin Liu\\nZhou Zhao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.15166\\u00a7r\\n\\nVersion:\\u00a77v5 (Sun, 13 Feb 2022 09:00:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by NeurIPS 2021. Source code: https://github.com/NATSpeech/NATSpeech\\u00a7r"}']}
{title:'Hwang et al. (§72022§r)', author: 'Dongseong Hwang; Ananya Misra; Zhouyuan Huo; Nikhil Siddhartha; Shefali Garg; David Qiu; Khe Chai Sim; Trevor Strohman; Françoise Beaufays; Yanzhang He', display:{Lore:['[{"text": "arXiv:2110.00165", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLarge-scale ASR Domain Adaptation using Self- and Semi-supervised Learning\\u00a7r\\n\\n\\u00a78\\u00a7oDongseong Hwang\\nAnanya Misra\\nZhouyuan Huo\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.00165\\u00a7r\\n\\nVersion:\\u00a77v5 (Wed, 16 Feb 2022 03:01:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2022 accepted, 5 pages, 2 figures, 5 tables\\u00a7r"}']}
{title:'Nguyen et al. (§72022§r)', author: 'Thi Ngoc Tho Nguyen; Karn N. Watcharasupat; Ngoc Khanh Nguyen; Douglas L. Jones; Woon-Seng Gan', display:{Lore:['[{"text": "arXiv:2110.00275", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSALSA: Spatial Cue-Augmented Log-Spectrogram Features for Polyphonic Sound Event Localization and Detection\\u00a7r\\n\\n\\u00a78\\u00a7oThi Ngoc Tho Nguyen\\nKarn N. Watcharasupat\\nNgoc Khanh Nguyen\\nDouglas L. Jones\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.00275\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2022.3173054\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol. 30, pp. 1749-1762, 2022\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 6 Jun 2022 05:07:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o(c) 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or "}','{"text": "promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Watcharasupat et al. (§72022§r)', author: 'Karn N. Watcharasupat; Thi Ngoc Tho Nguyen; Woon-Seng Gan; Shengkui Zhao; Bin Ma', display:{Lore:['[{"text": "arXiv:2110.00745", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Complex-Valued Multidilated Convolutional Neural Network for Joint Acoustic Echo Cancellation and Noise Suppression\\u00a7r\\n\\n\\u00a78\\u00a7oKarn N. Watcharasupat\\nThi Ngoc Tho Nguyen\\nWoon-Seng Gan\\nShengkui Zhao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.00745\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747034\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 2022 IEEE International Conference on\\n  Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 656-660\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 22 Jan 2022 11:50:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be presented at the 2022 International Conference on Acoustics, Speech, Signal Processing (ICASSP)\\u00a7r"}']}
{title:'Hussain et al. (§72022§r)', author: 'Shehzeen Hussain; Van Nguyen; Shuhua Zhang; Erik Visser', display:{Lore:['[{"text": "arXiv:2110.01077", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-task Voice Activated Framework using Self-supervised Learning\\u00a7r\\n\\n\\u00a78\\u00a7oShehzeen Hussain\\nVan Nguyen\\nShuhua Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.01077\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 19 Mar 2022 04:28:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2022\\u00a7r"}']}
{title:'Reddy et al. (§72022§r)', author: 'Chandan K A Reddy; Vishak Gopal; Ross Cutler', display:{Lore:['[{"text": "arXiv:2110.01763", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDNSMOS P.835: A Non-Intrusive Perceptual Objective Speech Quality Metric to Evaluate Noise Suppressors\\u00a7r\\n\\n\\u00a78\\u00a7oChandan K A Reddy\\nVishak Gopal\\nRoss Cutler\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.01763\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 4 Feb 2022 06:56:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2010.15258\\u00a7r"}']}
{title:'Cooper et al. (§72022§r)', author: 'Erica Cooper; Wen-Chin Huang; Tomoki Toda; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2110.02635", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeneralization Ability of MOS Prediction Networks\\u00a7r\\n\\n\\u00a78\\u00a7oErica Cooper\\nWen-Chin Huang\\nTomoki Toda\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02635\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 14 Feb 2022 07:34:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oo\\u0327p\\u0327y\\u0327\\u0157i\\u0327\\u0123\\u1e29\\u0163 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for "}','{"text": "advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Eurich et al. (§72022§r)', author: 'Bernhard Eurich; Jörg Encke; Stephan D. Ewert; Mathias Dietz', display:{Lore:['[{"text": "arXiv:2110.02695", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLower Interaural Coherence in Off-Signal Bands Impairs Binaural Detection\\u00a7r\\n\\n\\u00a78\\u00a7oBernhard Eurich\\nJ\\u00f6rg Encke\\nStephan D. Ewert\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02695\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0011673\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJ. Acoust. Soc. Am. 151(6), 2022, 3927-3936\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 7 Mar 2022 11:09:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages, 5 figures\\u00a7r"}']}
{title:'Xie et al. (§72022§r)', author: 'Huang Xie; Okko Räsänen; Konstantinos Drossos; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2110.02939", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Audio-Caption Aligning Learns Correspondences between Individual Sound Events and Textual Phrases\\u00a7r\\n\\n\\u00a78\\u00a7oHuang Xie\\nOkko R\\u00e4s\\u00e4nen\\nKonstantinos Drossos\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02939\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 21 Feb 2022 06:27:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2022\\u00a7r"}']}
{title:'Raitio et al. (§72022§r)', author: 'Tuomo Raitio; Jiangchuan Li; Shreyas Seshadri', display:{Lore:['[{"text": "arXiv:2110.02952", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierarchical prosody modeling and control in non-autoregressive parallel neural TTS\\u00a7r\\n\\n\\u00a78\\u00a7oTuomo Raitio\\nJiangchuan Li\\nShreyas Seshadri\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.02952\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 22 Mar 2022 19:09:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, preprint accepted to ICASSP 2022. arXiv admin note: text overlapwith arXiv:2009.06775\\u00a7r"}']}
{title:'Purin et al. (§72022§r)', author: 'Marju Purin; Sten Sootla; Mateja Sponza; Ando Saabas; Ross Cutler', display:{Lore:['[{"text": "arXiv:2110.03010", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAECMOS: A speech quality assessment metric for echo impairment\\u00a7r\\n\\n\\u00a78\\u00a7oMarju Purin\\nSten Sootla\\nMateja Sponza\\nAndo Saabas\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03010\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 27 Jan 2022 07:29:11 GMT)\\u00a7r"}']}
{title:'Seshadri et al. (§72022§r)', author: 'Shreyas Seshadri; Tuomo Raitio; Dan Castellani; Jiangchuan Li', display:{Lore:['[{"text": "arXiv:2110.03012", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmphasis control for parallel neural TTS\\u00a7r\\n\\n\\u00a78\\u00a7oShreyas Seshadri\\nTuomo Raitio\\nDan Castellani\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03012\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 29 Mar 2022 16:12:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, submitted to Interspeech 2022\\u00a7r"}']}
{title:'Laptev et al. (§72022§r)', author: 'Aleksandr Laptev; Somshubra Majumdar; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2110.03098", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCTC Variations Through New WFST Topologies\\u00a7r\\n\\n\\u00a78\\u00a7oAleksandr Laptev\\nSomshubra Majumdar\\nBoris Ginsburg\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03098\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/interspeech.2022-10854\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 26 Jun 2022 07:00:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022, 5 pages, 2 figures, 7 tables\\u00a7r"}']}
{title:'Kanda et al. (§72022§r)', author: 'Naoyuki Kanda; Xiong Xiao; Yashesh Gaur; Xiaofei Wang; Zhong Meng; Zhuo Chen; Takuya Yoshioka', display:{Lore:['[{"text": "arXiv:2110.03151", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTranscribe-to-Diarize: Neural Speaker Diarization for Unlimited Number of Speakers using End-to-End Speaker-Attributed ASR\\u00a7r\\n\\n\\u00a78\\u00a7oNaoyuki Kanda\\nXiong Xiao\\nYashesh Gaur\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03151\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 22 Jan 2022 01:28:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP 2022; System labels (SC and VBx) in Table 1 have been fixed\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Seong-Hu Kim; Hyeonuk Nam; Yong-Hwa Park', display:{Lore:['[{"text": "arXiv:2110.03213", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTemporal Dynamic Convolutional Neural Network for Text-Independent Speaker Verification and Phonemetic Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oSeong-Hu Kim\\nHyeonuk Nam\\nYong-Hwa Park\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03213\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 8 Feb 2022 10:54:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022\\u00a7r"}']}
{title:'Nam et al. (§72022§r)', author: 'Hyeonuk Nam; Seong-Hu Kim; Yong-Hwa Park', display:{Lore:['[{"text": "arXiv:2110.03282", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFilterAugment: An Acoustic Environmental Data Augmentation Method\\u00a7r\\n\\n\\u00a78\\u00a7oHyeonuk Nam\\nSeong-Hu Kim\\nYong-Hwa Park\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03282\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 7 Feb 2022 05:58:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022\\u00a7r"}']}
{title:'Janbakhshi et al. (§72022§r)', author: 'Parvaneh Janbakhshi; Ina Kodrasi', display:{Lore:['[{"text": "arXiv:2110.03283", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExperimental investigation on STFT phase representations for deep learning-based dysarthric speech detection\\u00a7r\\n\\n\\u00a78\\u00a7oParvaneh Janbakhshi\\nIna Kodrasi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03283\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 24 Jan 2022 10:10:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ICASSP 2022\\u00a7r"}']}
{title:'Prabhu et al. (§72022§r)', author: 'Navin Raj Prabhu; Guillaume Carbajal; Nale Lehmann-Willenbrock; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2110.03299", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-To-End Label Uncertainty Modeling for Speech-based Arousal Recognition Using Bayesian Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oNavin Raj Prabhu\\nGuillaume Carbajal\\nNale Lehmann-Willenbrock\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03299\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 27 Jun 2022 09:34:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oACCEPTED to INTERSPEECH 2022\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Qiujia Li; Yu Zhang; David Qiu; Yanzhang He; Liangliang Cao; Philip C. Woodland', display:{Lore:['[{"text": "arXiv:2110.03327", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Confidence Estimation on Out-of-Domain Data for End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oQiujia Li\\nYu Zhang\\nDavid Qiu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03327\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 2 Mar 2022 08:06:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted as a conference paper at ICASSP 2022\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Xiaoyu Yang; Qiujia Li; Philip C. Woodland', display:{Lore:['[{"text": "arXiv:2110.03334", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKnowledge Distillation for Neural Transducers from Large Self-Supervised Pre-trained Models\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoyu Yang\\nQiujia Li\\nPhilip C. Woodland\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03334\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 2 Mar 2022 08:12:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted as a conference paper at ICASSP 2022\\u00a7r"}']}
{title:'Lu et al. (§72022§r)', author: 'Junchen Lu; Berrak Sisman; Rui Liu; Mingyang Zhang; Haizhou Li', display:{Lore:['[{"text": "arXiv:2110.03342", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisualTTS: TTS with Accurate Lip-Speech Synchronization for Automatic Voice Over\\u00a7r\\n\\n\\u00a78\\u00a7oJunchen Lu\\nBerrak Sisman\\nRui Liu\\nMingyang Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03342\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 2 Mar 2022 13:55:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at ICASSP 2022\\u00a7r"}']}
{title:'Lin et al. (§72022§r)', author: 'Guan-Ting Lin; Chan-Jan Hsu; Da-Rong Liu; Hung-Yi Lee; Yu Tsao', display:{Lore:['[{"text": "arXiv:2110.03509", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalyzing the Robustness of Unsupervised Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGuan-Ting Lin\\nChan-Jan Hsu\\nDa-Rong Liu\\nHung-Yi Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03509\\u00a7r\\n\\nVersion:\\u00a77v5 (Tue, 26 Apr 2022 11:51:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Kabzinski et al. (§72022§r)', author: 'Tobias Kabzinski; Peter Jax', display:{Lore:['[{"text": "arXiv:2110.03630", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Faster Continuous Multi-Channel HRTF Measurements Based on Learning System Models\\u00a7r\\n\\n\\u00a78\\u00a7oTobias Kabzinski\\nPeter Jax\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03630\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746559\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 11 May 2022 16:24:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, minor changes compared to v1 after reviewers\' feedbacks, accepted atICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\\u00a7r"}']}
{title:'Bergsma et al. (§72022§r)', author: 'Boris Bergsma; Minhao Yang; Milos Cernak', display:{Lore:['[{"text": "arXiv:2110.03715", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPEAF: Learnable Power Efficient Analog Acoustic Features for Audio Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oBoris Bergsma\\nMinhao Yang\\nMilos Cernak\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03715\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 29 Mar 2022 15:18:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Lu et al. (§72022§r)', author: 'Zhiyun Lu; Yanwei Pan; Thibault Doutre; Parisa Haghani; Liangliang Cao; Rohit Prabhavalkar; Chao Zhang; Trevor Strohman', display:{Lore:['[{"text": "arXiv:2110.03841", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInput Length Matters: Improving RNN-T and MWER Training for Long-form Telephony Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhiyun Lu\\nYanwei Pan\\nThibault Doutre\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03841\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 1 Apr 2022 18:50:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Tao et al. (§72022§r)', author: 'Ruijie Tao; Kong Aik Lee; Rohan Kumar Das; Ville Hautamäki; Haizhou Li', display:{Lore:['[{"text": "arXiv:2110.03869", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervised Speaker Recognition with Loss-gated Learning\\u00a7r\\n\\n\\u00a78\\u00a7oRuijie Tao\\nKong Aik Lee\\nRohan Kumar Das\\nVille Hautam\\u00e4ki\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03869\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 14 Jul 2022 13:32:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Tan et al. (§72022§r)', author: 'Daxin Tan; Guangyan Zhang; Tan Lee', display:{Lore:['[{"text": "arXiv:2110.03887", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnvironment Aware Text-to-Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oDaxin Tan\\nGuangyan Zhang\\nTan Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03887\\u00a7r\\n\\nVersion:\\u00a77v4 (Sat, 6 Aug 2022 18:55:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Aroudi et al. (§72022§r)', author: 'Ali Aroudi; Stefan Uhlich; Marc Ferras Font', display:{Lore:['[{"text": "arXiv:2110.04047", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTRUNet: Transformer-Recurrent-U Network for Multi-channel Reverberant Sound Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oAli Aroudi\\nStefan Uhlich\\nMarc Ferras Font\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04047\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 22 Aug 2022 06:53:03 GMT)\\u00a7r"}']}
{title:'Higuchi et al. (§72022§r)', author: 'Yosuke Higuchi; Keita Karube; Tetsuji Ogawa; Tetsunori Kobayashi', display:{Lore:['[{"text": "arXiv:2110.04109", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierarchical Conditional End-to-End ASR with CTC and Multi-Granular Subword Units\\u00a7r\\n\\n\\u00a78\\u00a7oYosuke Higuchi\\nKeita Karube\\nTetsuji Ogawa\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04109\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 8 Feb 2022 05:31:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP2022\\u00a7r"}']}
{title:'Fu et al. (§72022§r)', author: 'Li Fu; Xiaoxiao Li; Runyu Wang; Lu Fan; Zhengchen Zhang; Meng Chen; Youzheng Wu; Xiaodong He', display:{Lore:['[{"text": "arXiv:2110.04187", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSCaLa: Supervised Contrastive Learning for End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oLi Fu\\nXiaoxiao Li\\nRunyu Wang\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04187\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 20 Jun 2022 11:48:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2022\\u00a7r"}']}
{title:'Reddy et al. (§72022§r)', author: 'Chandan K. A. Reddy; Vishak Gopa; Harishchandra Dubey; Sergiy Matusevych; Ross Cutler; Robert Aichner', display:{Lore:['[{"text": "arXiv:2110.04331", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusicNet: Compact Convolutional Neural Network for Real-time Background Music Detection\\u00a7r\\n\\n\\u00a78\\u00a7oChandan K. A. Reddy\\nVishak Gopa\\nHarishchandra Dubey\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04331\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 15 Apr 2022 07:10:14 GMT)\\u00a7r"}']}
{title:'Jin et al. (§72022§r)', author: 'Wenyu Jin; Tim Schoof; Henning Schepker', display:{Lore:['[{"text": "arXiv:2110.04385", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIndividualized Hear-through For Acoustic Transparency Using PCA-Based Sound Pressure Estimation At The Eardrum\\u00a7r\\n\\n\\u00a78\\u00a7oWenyu Jin\\nTim Schoof\\nHenning Schepker\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04385\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 17 Feb 2022 19:22:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, accepted to ICASSP 2022\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Mu Yang; Shaojin Ding; Tianlong Chen; Tong Wang; Zhangyang Wang', display:{Lore:['[{"text": "arXiv:2110.04482", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Lifelong Learning of Multilingual Text-To-Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oMu Yang\\nShaojin Ding\\nTianlong Chen\\nTong Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04482\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 19 May 2022 02:43:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022. Camera-ready\\u00a7r"}']}
{title:'Zhu et al. (§72022§r)', author: 'Han Zhu; Li Wang; Jindong Wang; Gaofeng Cheng; Pengyuan Zhang; Yonghong Yan', display:{Lore:['[{"text": "arXiv:2110.04484", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWav2vec-S: Semi-Supervised Pre-Training for Low-Resource ASR\\u00a7r\\n\\n\\u00a78\\u00a7oHan Zhu\\nLi Wang\\nJindong Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04484\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 17 Jun 2022 08:07:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Fares et al. (§72022§r)', author: 'Mireille Fares; Catherine Pelachaud; Nicolas Obin', display:{Lore:['[{"text": "arXiv:2110.04527", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransformer Network for Semantically-Aware and Speech-Driven Upper-Face Generation\\u00a7r\\n\\n\\u00a78\\u00a7oMireille Fares\\nCatherine Pelachaud\\nNicolas Obin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04527\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 21 May 2022 10:38:33 GMT)\\u00a7r"}']}
{title:'Horiguchi et al. (§72022§r)', author: 'Shota Horiguchi; Yuki Takashima; Paola Garcia; Shinji Watanabe; Yohei Kawaguchi', display:{Lore:['[{"text": "arXiv:2110.04694", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Channel End-to-End Neural Diarization with Distributed Microphones\\u00a7r\\n\\n\\u00a78\\u00a7oShota Horiguchi\\nYuki Takashima\\nPaola Garcia\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04694\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 28 Mar 2022 10:49:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Xin Wang; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2110.04775", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEstimating the confidence of speech spoofing countermeasure\\u00a7r\\n\\n\\u00a78\\u00a7oXin Wang\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04775\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 1 Feb 2022 05:39:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWork inprogress. Comments are welcome. Accepted by ICASSP2022. Code is available https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts. Not all the comments from anonymous reviewers can be addressed within 4 "}','{"text": "pages, apologize for that\\u00a7r"}']}
{title:'Yao et al. (§72022§r)', author: 'Zengwei Yao; Wenjie Pei; Fanglin Chen; Guangming Lu; David Zhang', display:{Lore:['[{"text": "arXiv:2110.04791", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStepwise-Refining Speech Separation Network via Fine-Grained Encoding in High-order Latent Domain\\u00a7r\\n\\n\\u00a78\\u00a7oZengwei Yao\\nWenjie Pei\\nFanglin Chen\\nGuangming Lu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04791\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2022.3140556\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 31 Jan 2022 13:04:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Rui Wang; Junyi Ao; Long Zhou; Shujie Liu; Zhihua Wei; Tom Ko; Qing Li; Yu Zhang', display:{Lore:['[{"text": "arXiv:2110.05036", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-View Self-Attention Based Transformer for Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oRui Wang\\nJunyi Ao\\nLong Zhou\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05036\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 27 Jan 2022 07:10:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper to appear at ICASSP 2022\\u00a7r"}']}
{title:'Hu et al. (§72022§r)', author: 'Yuchen Hu; Nana Hou; Chen Chen; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2110.05267", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInteractive Feature Fusion for End-to-End Noise-Robust Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYuchen Hu\\nNana Hou\\nChen Chen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05267\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 8 Apr 2022 03:34:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 7 figures, Accepted by ICASSP 2022\\u00a7r"}']}
{title:'Schröter et al. (§72022§r)', author: 'Hendrik Schröter; Alberto N. Escalante-B.; Tobias Rosenkranz; Andreas Maier', display:{Lore:['[{"text": "arXiv:2110.05588", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeepFilterNet: A Low Complexity Speech Enhancement Framework for Full-Band Audio based on Deep Filtering\\u00a7r\\n\\n\\u00a78\\u00a7oHendrik Schr\\u00f6ter\\nAlberto N. Escalante-B.\\nTobias Rosenkranz\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05588\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 1 Feb 2022 06:43:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022\\u00a7r"}']}
{title:'Siriwardena et al. (§72022§r)', author: 'Yashish M. Siriwardena; Guilhem Marion; Shihab Shamma', display:{Lore:['[{"text": "arXiv:2110.05695", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Mirrornet : Learning Audio Synthesizer Controls Inspired by Sensorimotor Interaction\\u00a7r\\n\\n\\u00a78\\u00a7oYashish M. Siriwardena\\nGuilhem Marion\\nShihab Shamma\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05695\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747358\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2022 - 2022 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 18 Feb 2022 06:22:57 GMT)\\u00a7r"}']}
{title:'Zuluaga-Gomez et al. (§72022§r)', author: 'Juan Zuluaga-Gomez; Seyyed Saeed Sarfjoo; Amrutha Prasad; Iuliia Nigmatulina; Petr Motlicek; Karel Ondrej; Oliver Ohneiser; Hartmut Helmke', display:{Lore:['[{"text": "arXiv:2110.05781", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBERTraffic: BERT-based Joint Speaker Role and Speaker Change Detection for Air Traffic Control Communications\\u00a7r\\n\\n\\u00a78\\u00a7oJuan Zuluaga-Gomez\\nSeyyed Saeed Sarfjoo\\nAmrutha Prasad\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05781\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 14 Oct 2022 20:40:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in the 2022 IEEE Spoken Language Technology Workshop (SLT) (SLT 2022)\\u00a7r"}']}
{title:'Sawata et al. (§72022§r)', author: 'Ryosuke Sawata; Yosuke Kashiwagi; Shusuke Takahashi', display:{Lore:['[{"text": "arXiv:2110.05968", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Character Error Rate Is Not Equal to Having Clean Speech: Speech Enhancement for ASR Systems with Black-box Acoustic Models\\u00a7r\\n\\n\\u00a78\\u00a7oRyosuke Sawata\\nYosuke Kashiwagi\\nShusuke Takahashi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.05968\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 22 Feb 2022 10:31:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Kitić et al. (§72022§r)', author: 'Srđan Kitić; Jérôme Daniel', display:{Lore:['[{"text": "arXiv:2110.06304", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeneralized Time Domain Velocity Vector\\u00a7r\\n\\n\\u00a78\\u00a7oSr\\u0111an Kiti\\u0107\\nJ\\u00e9r\\u00f4me Daniel\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06304\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 19 May 2022 10:23:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Li-Wei Chen; Alexander Rudnicky', display:{Lore:['[{"text": "arXiv:2110.06306", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFine-grained style control in Transformer-based Text-to-speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oLi-Wei Chen\\nAlexander Rudnicky\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06306\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 16 Mar 2022 20:46:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ICASSP 2022\\u00a7r"}']}
{title:'Saijo et al. (§72022§r)', author: 'Kohei Saijo; Robin Scheibler', display:{Lore:['[{"text": "arXiv:2110.06545", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIndependence-based Joint Dereverberation and Separation with Neural Source Model\\u00a7r\\n\\n\\u00a78\\u00a7oKohei Saijo\\nRobin Scheibler\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06545\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 1 Apr 2022 05:42:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH2022\\u00a7r"}']}
{title:'Choi et al. (§72022§r)', author: 'Soonbeom Choi; Juhan Nam', display:{Lore:['[{"text": "arXiv:2110.06546", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Melody-Unsupervision Model for Singing Voice Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oSoonbeom Choi\\nJuhan Nam\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06546\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Apr 2022 12:48:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2022\\u00a7r"}']}
{title:'Mei et al. (§72022§r)', author: 'Xinhao Mei; Xubo Liu; Jianyuan Sun; Mark D. Plumbley; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2110.06691", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiverse Audio Captioning via Adversarial Training\\u00a7r\\n\\n\\u00a78\\u00a7oXinhao Mei\\nXubo Liu\\nJianyuan Sun\\nMark D. Plumbley\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06691\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 29 Mar 2022 11:43:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, accepted by ICASSP 2022\\u00a7r"}']}
{title:'Yu et al. (§72022§r)', author: 'Yechan Yu; Dongkeon Park; Hong Kook Kim', display:{Lore:['[{"text": "arXiv:2110.07116", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAuxiliary Loss of Transformer with Residual Connection for End-to-End Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oYechan Yu\\nDongkeon Park\\nHong Kook Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.07116\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 26 Sep 2022 07:14:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022, equal contribution from first two authors\\u00a7r"}']}
{title:'Shimada et al. (§72022§r)', author: 'Kazuki Shimada; Yuichiro Koyama; Shusuke Takahashi; Naoya Takahashi; Emiru Tsunoo; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2110.07124", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-ACCDOA: Localizing and Detecting Overlapping Sounds from the Same Class with Auxiliary Duplicating Permutation Invariant Training\\u00a7r\\n\\n\\u00a78\\u00a7oKazuki Shimada\\nYuichiro Koyama\\nShusuke Takahashi\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.07124\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 28 Mar 2022 01:07:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted for publication in IEEE ICASSP 2022\\u00a7r"}']}
{title:'Zhan et al. (§72022§r)', author: 'Haoyue Zhan; Xinyuan Yu; Haitong Zhang; Yang Zhang; Yue Lin', display:{Lore:['[{"text": "arXiv:2110.07192", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Timbre Disentanglement in Non-Autoregressive Cross-Lingual Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oHaoyue Zhan\\nXinyuan Yu\\nHaitong Zhang\\nYang Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.07192\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 31 Aug 2022 02:19:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Ao et al. (§72022§r)', author: 'Junyi Ao; Rui Wang; Long Zhou; Chengyi Wang; Shuo Ren; Yu Wu; Shujie Liu; Tom Ko; Qing Li; Yu Zhang; Zhihua Wei; Yao Qian; Jinyu Li; Furu Wei', display:{Lore:['[{"text": "arXiv:2110.07205", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing\\u00a7r\\n\\n\\u00a78\\u00a7oJunyi Ao\\nRui Wang\\nLong Zhou\\n+ 10 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.07205\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 24 May 2022 08:18:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ACL 2022 main conference\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'Rongjie Huang; Chenye Cui; Feiyang Chen; Yi Ren; Jinglin Liu; Zhou Zhao; Baoxing Huai; Zhefeng Wang', display:{Lore:['[{"text": "arXiv:2110.07468", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSingGAN: Generative Adversarial Network For High-Fidelity Singing Voice Generation\\u00a7r\\n\\n\\u00a78\\u00a7oRongjie Huang\\nChenye Cui\\nFeiyang Chen\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.07468\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 5 Aug 2022 06:09:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ACM Multimedia 2022\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'Chien-yu Huang; Kai-Wei Chang; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2110.07537", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lToward Degradation-Robust Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oChien-yu Huang\\nKai-Wei Chang\\nHung-yi Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.07537\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 29 Apr 2022 15:14:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in the proceedings of ICASSP 2022, equal contribution from first two authors\\u00a7r"}']}
{title:'Meng et al. (§72022§r)', author: 'Yen Meng; Yi-Hui Chou; Andy T. Liu; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2110.07957", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDon\'t speak too fast: The impact of data bias on self-supervised speech models\\u00a7r\\n\\n\\u00a78\\u00a7oYen Meng\\nYi-Hui Chou\\nAndy T. Liu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.07957\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 26 Apr 2022 11:32:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Hu et al. (§72022§r)', author: 'Chenxu Hu; Qiao Tian; Tingle Li; Yuping Wang; Yuxuan Wang; Hang Zhao', display:{Lore:['[{"text": "arXiv:2110.08243", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Dubber: Dubbing for Videos According to Scripts\\u00a7r\\n\\n\\u00a78\\u00a7oChenxu Hu\\nQiao Tian\\nTingle Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.08243\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 15 Mar 2022 14:37:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by NeurIPS 2021; Project page at https://tsinghua-mars-lab.github.io/NeuralDubber/\\u00a7r"}']}
{title:'Hu et al. (§72022§r)', author: 'Hu Hu; Sabato Marco Siniscalchi; Chao-Han Huck Yang; Chin-Hui Lee', display:{Lore:['[{"text": "arXiv:2110.08598", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Variational Bayesian Approach to Learning Latent Variables for Acoustic Knowledge Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oHu Hu\\nSabato Marco Siniscalchi\\nChao-Han Huck Yang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.08598\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 20 Feb 2022 18:32:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022. Code is available at https://github.com/MihawkHu/ASC_Knowledge_Transfer\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Yongmao Zhang; Jian Cong; Heyang Xue; Lei Xie; Pengcheng Zhu; Mengxiao Bi', display:{Lore:['[{"text": "arXiv:2110.08813", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVISinger: Variational Inference with Adversarial Learning for End-to-End Singing Voice Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYongmao Zhang\\nJian Cong\\nHeyang Xue\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.08813\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 24 Feb 2022 14:12:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, ICASSP 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Ju-Chiang Wang; Jordan B. L. Smith; Wei-Tsung Lu; Xuchen Song', display:{Lore:['[{"text": "arXiv:2110.09000", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSupervised Metric Learning for Music Structure Features\\u00a7r\\n\\n\\u00a78\\u00a7oJu-Chiang Wang\\nJordan B. L. Smith\\nWei-Tsung Lu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09000\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 29 Apr 2022 21:21:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper was accepted and presented at ISMIR 2021\\u00a7r"}']}
{title:'Thienpondt et al. (§72022§r)', author: 'Jenthe Thienpondt; Brecht Desplanques; Kris Demuynck', display:{Lore:['[{"text": "arXiv:2110.09150", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTackling the Score Shift in Cross-Lingual Speaker Verification by Exploiting Language Information\\u00a7r\\n\\n\\u00a78\\u00a7oJenthe Thienpondt\\nBrecht Desplanques\\nKris Demuynck\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09150\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746210\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 19 Jun 2022 13:00:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oproceedings of ICASSP 2022\\u00a7r"}']}
{title:'Chan et al. (§72022§r)', author: 'David M. Chan; Shalini Ghosh; Debmalya Chakrabarty; Björn Hoffmeister', display:{Lore:['[{"text": "arXiv:2110.09890", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Modal Pre-Training for Automated Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDavid M. Chan\\nShalini Ghosh\\nDebmalya Chakrabarty\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09890\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 15 Sep 2022 18:04:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at ICASSP 2022\\u00a7r"}']}
{title:'Ting et al. (§72022§r)', author: 'Wen-Yuan Ting; Syu-Siang Wang; Hsin-Li Chang; Borching Su; Yu Tsao', display:{Lore:['[{"text": "arXiv:2110.09924", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Enhancement Based on Cyclegan with Noise-informed Training\\u00a7r\\n\\n\\u00a78\\u00a7oWen-Yuan Ting\\nSyu-Siang Wang\\nHsin-Li Chang\\nBorching Su\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09924\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nISCSLP 2022\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 6 Dec 2022 14:39:52 GMT)\\u00a7r"}']}
{title:'Petermann et al. (§72022§r)', author: 'Darius Petermann; Gordon Wichern; Zhong-Qiu Wang; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2110.09958", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Cocktail Fork Problem: Three-Stem Audio Separation for Real-World Soundtracks\\u00a7r\\n\\n\\u00a78\\u00a7oDarius Petermann\\nGordon Wichern\\nZhong-Qiu Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09958\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 24 Mar 2022 01:11:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP2022. For resources and examples, see https://cocktail-fork.github.io\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Zhe Liu; Ke Li; Shreyan Bakshi; Fuchun Peng', display:{Lore:['[{"text": "arXiv:2110.10026", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrivate Language Model Adaptation for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhe Liu\\nKe Li\\nShreyan Bakshi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.10026\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 15 Jun 2022 04:00:47 GMT)\\u00a7r"}']}
{title:'Morrison et al. (§72022§r)', author: 'Max Morrison; Rithesh Kumar; Kundan Kumar; Prem Seetharaman; Aaron Courville; Yoshua Bengio', display:{Lore:['[{"text": "arXiv:2110.10139", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChunked Autoregressive GAN for Conditional Waveform Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oMax Morrison\\nRithesh Kumar\\nKundan Kumar\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.10139\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 3 Mar 2022 23:05:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished as a conference paper at ICLR 2022\\u00a7r"}']}
{title:'Du et al. (§72022§r)', author: 'Zongyang Du; Berrak Sisman; Kun Zhou; Haizhou Li', display:{Lore:['[{"text": "arXiv:2110.10326", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDisentanglement of Emotional Style and Speaker Identity for Expressive Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oZongyang Du\\nBerrak Sisman\\nKun Zhou\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.10326\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 21 Jul 2022 05:10:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Shao et al. (§72022§r)', author: 'Nian Shao; Erfan Loweimi; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2110.11144", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRCT: Random Consistency Training for Semi-supervised Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oNian Shao\\nErfan Loweimi\\nXiaofei Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.11144\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 27 Mar 2022 08:34:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint for interspeech 2022\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Yao-Yuan Yang; Moto Hira; Zhaoheng Ni; Anjali Chourdia; Artyom Astafurov; Caroline Chen; Ching-Feng Yeh; Christian Puhrsch; David Pollack; Dmitriy Genzel; Donny Greenberg; Edward Z. Yang; Jason Lian; Jay Mahadeokar; Jeff Hwang; Ji Chen; Peter Goldsborough; Prabhat Roy; Sean Narenthiran; Shinji Watanabe; Soumith Chintala; Vincent Quenneville-Bélair; Yangyang Shi', display:{Lore:['[{"text": "arXiv:2110.15018", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTorchAudio: Building Blocks for Audio and Speech Processing\\u00a7r\\n\\n\\u00a78\\u00a7oYao-Yuan Yang\\nMoto Hira\\nZhaoheng Ni\\n+ 19 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.15018\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 16 Feb 2022 17:48:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'von Neumann et al. (§72022§r)', author: 'Thilo von Neumann; Keisuke Kinoshita; Christoph Boeddeker; Marc Delcroix; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2110.15581", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSA-SDR: A novel loss function for separation of meeting style data\\u00a7r\\n\\n\\u00a78\\u00a7oThilo von Neumann\\nKeisuke Kinoshita\\nChristoph Boeddeker\\nMarc Delcroix\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.15581\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 21 Apr 2022 06:40:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at ICASSP 2022\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Yuanchao Li; Peter Bell; Catherine Lai', display:{Lore:['[{"text": "arXiv:2110.15684", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFusing ASR Outputs in Joint Training for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYuanchao Li\\nPeter Bell\\nCatherine Lai\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.15684\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 17 Mar 2022 20:36:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP 2022\\u00a7r"}']}
{title:'Kocour et al. (§72022§r)', author: 'Martin Kocour; Kateřina Žmolíková; Lucas Ondel; Ján Švec; Marc Delcroix; Tsubasa Ochiai; Lukáš Burget; Jan Černocký', display:{Lore:['[{"text": "arXiv:2111.00009", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRevisiting joint decoding based multi-talker speech recognition with DNN acoustic model\\u00a7r\\n\\n\\u00a78\\u00a7oMartin Kocour\\nKate\\u0159ina \\u017dmol\\u00edkov\\u00e1\\nLucas Ondel\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.00009\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 15 Apr 2022 13:17:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Koizumi et al. (§72022§r)', author: 'Yuma Koizumi; Shigeki Karita; Arun Narayanan; Sankaran Panchapagesan; Michiel Bacchiani', display:{Lore:['[{"text": "arXiv:2111.00764", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSNRi Target Training for Joint Speech Enhancement and Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYuma Koizumi\\nShigeki Karita\\nArun Narayanan\\nSankaran Panchapagesan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.00764\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 28 Mar 2022 05:28:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022 (v1 has been rejected from ICASSP 2022)\\u00a7r"}']}
{title:'Li (§72022§r)', author: 'Jinyu Li', display:{Lore:['[{"text": "arXiv:2111.01690", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRecent Advances in End-to-End Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJinyu Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.01690\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 2 Feb 2022 23:38:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at APSIPA Transactions on Signal and Information Processing\\u00a7r"}']}
{title:'Zezario et al. (§72022§r)', author: 'Ryandhimas E. Zezario; Szu-Wei Fu; Fei Chen; Chiou-Shann Fuh; Hsin-Min Wang; Yu Tsao', display:{Lore:['[{"text": "arXiv:2111.02363", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning-based Non-Intrusive Multi-Objective Speech Assessment Model with Cross-Domain Features\\u00a7r\\n\\n\\u00a78\\u00a7oRyandhimas E. Zezario\\nSzu-Wei Fu\\nFei Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.02363\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 23 Jun 2022 08:59:36 GMT)\\u00a7r"}']}
{title:'van Niekerk et al. (§72022§r)', author: 'Benjamin van Niekerk; Marc-André Carbonneau; Julian Zaïdi; Mathew Baas; Hugo Seuté; Herman Kamper', display:{Lore:['[{"text": "arXiv:2111.02392", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparison of Discrete and Soft Speech Units for Improved Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oBenjamin van Niekerk\\nMarc-Andr\\u00e9 Carbonneau\\nJulian Za\\u00efdi\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.02392\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746484\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 8 Jun 2022 15:06:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 2 tables. Accepted at ICASSP 2022\\u00a7r"}']}
{title:'Baas et al. (§72022§r)', author: 'Matthew Baas; Herman Kamper', display:{Lore:['[{"text": "arXiv:2111.02674", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Conversion Can Improve ASR in Very Low-Resource Settings\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew Baas\\nHerman Kamper\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.02674\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 21 Jun 2022 11:09:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 page, 4 tables, 2 figures. Accepted at Interspeech 2022\\u00a7r"}']}
{title:'Malek et al. (§72022§r)', author: 'Jiri Malek; Jakub Jansky; Zbynek Koldovsky; Tomas Kounovsky; Jaroslav Cmejla; Jindrich Zdansky', display:{Lore:['[{"text": "arXiv:2111.03482", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTarget Speech Extraction: Independent Vector Extraction Guided by Supervised Speaker Identification\\u00a7r\\n\\n\\u00a78\\u00a7oJiri Malek\\nJakub Jansky\\nZbynek Koldovsky\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.03482\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2022.3190739\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol. 30, pp. 2295-2309, 2022\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 8 Jul 2022 12:32:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oModified version of the article accepted for publication in IEEE/ACM Transactionson Audio Speech and Language Processing journal. Original results unchanged, additional experiments presented, refined discussion and "}','{"text": "conclusions\\u00a7r"}']}
{title:'Défossez (§72022§r)', author: 'Alexandre Défossez', display:{Lore:['[{"text": "arXiv:2111.03600", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHybrid Spectrogram and Waveform Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oAlexandre D\\u00e9fossez\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.03600\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 30 Aug 2022 16:07:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oISMIR 2021 MDX Workshop, 11 pages, 2 figures\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Qinghua Liu; Yating Huang; Yunzhe Hao; Jiaming Xu; Bo Xu', display:{Lore:['[{"text": "arXiv:2111.04063", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLiMuSE: Lightweight Multi-modal Speaker Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oQinghua Liu\\nYating Huang\\nYunzhe Hao\\nJiaming Xu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.04063\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 11 Oct 2022 04:55:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEESLT 2022\\u00a7r"}']}
{title:'Tak et al. (§72022§r)', author: 'Hemlata Tak; Madhu Kamble; Jose Patino; Massimiliano Todisco; Nicholas Evans', display:{Lore:['[{"text": "arXiv:2111.04433", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRawBoost: A Raw Data Boosting and Augmentation Method applied to Automatic Speaker Verification Anti-Spoofing\\u00a7r\\n\\n\\u00a78\\u00a7oHemlata Tak\\nMadhu Kamble\\nJose Patino\\nMassimiliano Todisco\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.04433\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 22 Feb 2022 08:12:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEICASSP 2022\\u00a7r"}']}
{title:'Cornell et al. (§72022§r)', author: 'Samuele Cornell; Manuel Pariente; François Grondin; Stefano Squartini', display:{Lore:['[{"text": "arXiv:2111.04614", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Filterbanks for End-to-End Acoustic Beamforming\\u00a7r\\n\\n\\u00a78\\u00a7oSamuele Cornell\\nManuel Pariente\\nFran\\u00e7ois Grondin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.04614\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 19 Feb 2022 21:12:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at ICASSP 2022\\u00a7r"}']}
{title:'Encke et al. (§72022§r)', author: 'Jörg Encke; Mathias Dietz', display:{Lore:['[{"text": "arXiv:2111.04637", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA hemispheric two-channel code accounts for binaural unmasking in humans\\u00a7r\\n\\n\\u00a78\\u00a7oJ\\u00f6rg Encke\\nMathias Dietz\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.04637\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1038/s42003-022-04098-x\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nCommun Biol 5, 1122 (2022)\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 7 Oct 2022 06:27:41 GMT)\\u00a7r"}']}
{title:'Kothapally et al. (§72022§r)', author: 'Vinay Kothapally; Yong Xu; Meng Yu; Shi-Xiong Zhang; Dong Yu', display:{Lore:['[{"text": "arXiv:2111.04904", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Neural AEC and Beamforming with Double-Talk Detection\\u00a7r\\n\\n\\u00a78\\u00a7oVinay Kothapally\\nYong Xu\\nMeng Yu\\nShi-Xiong Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.04904\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Jun 2022 20:03:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in Interspeech 2022\\u00a7r"}']}
{title:'Fu et al. (§72022§r)', author: 'Yihui Fu; Yun Liu; Jingdong Li; Dawei Luo; Shubo Lv; Yukai Jv; Lei Xie', display:{Lore:['[{"text": "arXiv:2111.06015", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUformer: A Unet based dilated complex     real dual-path conformer network for simultaneous speech enhancement and dereverberation\\u00a7r\\n\\n\\u00a78\\u00a7oYihui Fu\\nYun Liu\\nJingdong Li\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.06015\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 5 May 2022 02:58:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Cord-Landwehr et al. (§72022§r)', author: 'Tobias Cord-Landwehr; Christoph Boeddeker; Thilo von Neumann; Catalin Zorila; Rama Doddipatla; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2111.07578", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMonaural source separation: From anechoic to reverberant environments\\u00a7r\\n\\n\\u00a78\\u00a7oTobias Cord-Landwehr\\nChristoph Boeddeker\\nThilo von Neumann\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.07578\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 10 May 2022 07:04:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IWAENC 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Xin Wang; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2111.07725", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating self-supervised front ends for speech spoofing countermeasures\\u00a7r\\n\\n\\u00a78\\u00a7oXin Wang\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.07725\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 4 Feb 2022 13:25:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oV3: added sub-band analysis, submitted to ISCA Odyssey2022; V2: added min tDCF results on 2019 and 2021 LA. EERs on LA 2021 were slightly updated to fix one glitch in the score file. EERs and min tDCFs on 2021 LA and DF "}','{"text": "can be computed using the latest official code https://github.com/asvspoof-challenge/2021.Work in progress. Feedback is welcome!\\u00a7r"}']}
{title:'Nguyen et al. (§72022§r)', author: 'Thi Ngoc Tho Nguyen; Douglas L. Jones; Karn N. Watcharasupat; Huy Phan; Woon-Seng Gan', display:{Lore:['[{"text": "arXiv:2111.08192", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSALSA-Lite: A Fast and Effective Feature for Polyphonic Sound Event Localization and Detection with Microphone Arrays\\u00a7r\\n\\n\\u00a78\\u00a7oThi Ngoc Tho Nguyen\\nDouglas L. Jones\\nKarn N. Watcharasupat\\nHuy Phan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.08192\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746132\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 2022 IEEE International Conference on\\n  Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 716-720\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 4 May 2022 09:58:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2110.00275\\u00a7r"}']}
{title:'Trinh et al. (§72022§r)', author: 'Viet Anh Trinh; Sebastian Braun', display:{Lore:['[{"text": "arXiv:2111.08678", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Speech Enhancement with speech recognition embedding and disentanglement losses\\u00a7r\\n\\n\\u00a78\\u00a7oViet Anh Trinh\\nSebastian Braun\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.08678\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 19 Feb 2022 19:14:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proceeding of ICASSP 2022, May 2022\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Sunwoo Kim; Minje Kim', display:{Lore:['[{"text": "arXiv:2111.09372", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBLOOM-Net: Blockwise Optimization for Masking Networks Toward Scalable and Efficient Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oSunwoo Kim\\nMinje Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.09372\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 10 Feb 2022 16:01:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, ICASSP 2022\\u00a7r"}']}
{title:'Nakatani et al. (§72022§r)', author: 'Tomohiro Nakatani; Rintaro Ikeshita; Keisuke Kinoshita; Hiroshi Sawada; Naoyuki Kamo; Shoko Araki', display:{Lore:['[{"text": "arXiv:2111.10574", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSwitching Independent Vector Analysis and Its Extension to Blind and Spatially Guided Convolutional Beamforming Algorithms\\u00a7r\\n\\n\\u00a78\\u00a7oTomohiro Nakatani\\nRintaro Ikeshita\\nKeisuke Kinoshita\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.10574\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 24 Feb 2022 14:13:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEE/ACM Trans. Audio, Speech, and Language Processing on 27 July 2021, accepted on 22 Feb. 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Zhichao Wang; Qicong Xie; Tao Li; Hongqiang Du; Lei Xie; Pengcheng Zhu; Mengxiao Bi', display:{Lore:['[{"text": "arXiv:2111.12277", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOne-shot Voice Conversion For Style Transfer Based On Speaker Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oZhichao Wang\\nQicong Xie\\nTao Li\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.12277\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 21 Feb 2022 09:36:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Jeong et al. (§72022§r)', author: 'Yeong-Seok Jeong; Jinsung Kim; Woosung Choi; Jaehwa Chung; Soonyoung Jung', display:{Lore:['[{"text": "arXiv:2111.12516", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLightSAFT: Lightweight Latent Source Aware Frequency Transform for Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oYeong-Seok Jeong\\nJinsung Kim\\nWoosung Choi\\nJaehwa Chung\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.12516\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 26 Jan 2022 09:22:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oMDX Workshop @ ISMIR 2021, 6 pages, 1 figure\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Yucong Zhang; Qinjian Lin; Weiqing Wang; Lin Yang; Xuyang Wang; Junjie Wang; Ming Li', display:{Lore:['[{"text": "arXiv:2111.13803", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Latency Online Speaker Diarization with Graph-Based Label Generation\\u00a7r\\n\\n\\u00a78\\u00a7oYucong Zhang\\nQinjian Lin\\nWeiqing Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.13803\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 24 Jun 2022 06:21:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by Odyssey 2022\\u00a7r"}']}
{title:'Amri et al. (§72022§r)', author: 'W. Zai El Amri; O. Tautz; H. Ritter; A. Melnik', display:{Lore:['[{"text": "arXiv:2111.14200", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransfer Learning with Jukebox for Music Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oW. Zai El Amri\\nO. Tautz\\nH. Ritter\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.14200\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-031-08337-2_35\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPart of the IFIP Advances in Information and Communication\\n  Technology book series (IFIPAICT,volume 647) 2022\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 21 Sep 2022 07:52:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oConference paper (AIAI 2022), 4 pages, 2 figures, 2 tables\\u00a7r"}']}
{title:'Srinivasan et al. (§72022§r)', author: 'Sundararajan Srinivasan; Zhaocheng Huang; Katrin Kirchhoff', display:{Lore:['[{"text": "arXiv:2112.00158", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRepresentation learning through cross-modal conditional teacher-student training for speech emotion recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSundararajan Srinivasan\\nZhaocheng Huang\\nKatrin Kirchhoff\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.00158\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 27 Jan 2022 05:06:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at IEEE ICASSP 2022\\u00a7r"}']}
{title:'Luo (§72022§r)', author: 'Yi Luo', display:{Lore:['[{"text": "arXiv:2112.03533", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Time-domain Real-valued Generalized Wiener Filter for Multi-channel Neural Separation Systems\\u00a7r\\n\\n\\u00a78\\u00a7oYi Luo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.03533\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 7 Sep 2022 16:56:15 GMT)\\u00a7r"}']}
{title:'Sang et al. (§72022§r)', author: 'Mufan Sang; Haoqi Li; Fang Liu; Andrew O. Arnold; Li Wan', display:{Lore:['[{"text": "arXiv:2112.04459", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Speaker Verification with Simple Siamese Network and Self-Supervised Regularization\\u00a7r\\n\\n\\u00a78\\u00a7oMufan Sang\\nHaoqi Li\\nFang Liu\\nAndrew O. Arnold\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.04459\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 1 Feb 2022 20:30:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022\\u00a7r"}']}
{title:'Barber et al. (§72022§r)', author: 'Jarred Barber; Yifeng Fan; Tao Zhang', display:{Lore:['[{"text": "arXiv:2112.04914", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end Alexa Device Arbitration\\u00a7r\\n\\n\\u00a78\\u00a7oJarred Barber\\nYifeng Fan\\nTao Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.04914\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 16 Feb 2022 21:54:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP 2022\\u00a7r"}']}
{title:'Tolooshams et al. (§72022§r)', author: 'Bahareh Tolooshams; Kazuhito Koishida', display:{Lore:['[{"text": "arXiv:2112.04939", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Training Framework for Stereo-Aware Speech Enhancement using Deep Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oBahareh Tolooshams\\nKazuhito Koishida\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.04939\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 31 Jan 2022 19:19:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the IEEE 47th InternationalConference on Acoustics, Speech, and Signal Processing (ICASSP)\\u00a7r"}']}
{title:'Hsu et al. (§72022§r)', author: 'Yicheng Hsu; Yonghan Lee; Mingsian R. Bai', display:{Lore:['[{"text": "arXiv:2112.05686", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning-based personal speech enhancement for teleconferencing by exploiting spatial-spectral features\\u00a7r\\n\\n\\u00a78\\u00a7oYicheng Hsu\\nYonghan Lee\\nMingsian R. Bai\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.05686\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.974685910.1109/ICASSP43922.2022.9746859\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 29 Apr 2022 12:52:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by ICASSP 2022\\u00a7r"}']}
{title:'Paturi et al. (§72022§r)', author: 'Rohit Paturi; Sundararajan Srinivasan; Katrin Kirchhoff; Daniel Garcia-Romero', display:{Lore:['[{"text": "arXiv:2112.05863", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDirected Speech Separation for Automatic Speech Recognition of Long Form Conversational Speech\\u00a7r\\n\\n\\u00a78\\u00a7oRohit Paturi\\nSundararajan Srinivasan\\nKatrin Kirchhoff\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.05863\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 6 Sep 2022 05:01:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at Interspeech 2022\\u00a7r"}']}
{title:'Trinh et al. (§72022§r)', author: 'Viet Anh Trinh; Hassan Salami Kavaki; Michael I Mandel', display:{Lore:['[{"text": "arXiv:2112.07156", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImportantAug: a data augmentation agent for speech\\u00a7r\\n\\n\\u00a78\\u00a7oViet Anh Trinh\\nHassan Salami Kavaki\\nMichael I Mandel\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.07156\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 19 Feb 2022 19:39:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proceeding of ICASSP 2022, May 2022\\u00a7r"}']}
{title:'Ye et al. (§72022§r)', author: 'Zelin Ye; Min Chen', display:{Lore:['[{"text": "arXiv:2112.07627", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisualizing Ensemble Predictions of Music Mood\\u00a7r\\n\\n\\u00a78\\u00a7oZelin Ye\\nMin Chen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.07627\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TVCG.2022.3209379\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Transactions on Visualization and Computer Graphics, 29(1),\\n  2023\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 4 Sep 2022 22:51:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 7 figures, Final accepted version for VIS 2022\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Ju-ho Kim; Hye-jin Shim; Jungwoo Heo; Ha-Jin Yu', display:{Lore:['[{"text": "arXiv:2112.07935", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRawNeXt: Speaker verification system for variable-duration utterances with deep layer aggregation and extended dynamic scaling policies\\u00a7r\\n\\n\\u00a78\\u00a7oJu-ho Kim\\nHye-jin Shim\\nJungwoo Heo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.07935\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Jun 2022 06:35:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 4 tables, accepted to 2022 ICASSP as a conference paper\\u00a7r"}']}
{title:'Koepke et al. (§72022§r)', author: 'A. Sophia Koepke; Andreea-Maria Oncescu; João F. Henriques; Zeynep Akata; Samuel Albanie', display:{Lore:['[{"text": "arXiv:2112.09418", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Retrieval with Natural Language Queries: A Benchmark Study\\u00a7r\\n\\n\\u00a78\\u00a7oA. Sophia Koepke\\nAndreea-Maria Oncescu\\nJo\\u00e3o F. Henriques\\nZeynep Akata\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.09418\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TMM.2022.3149712\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Transactions on Multimedia 2022\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 27 Jan 2022 21:00:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Transactions on Multimedia. arXiv admin note: substantial text overlap with arXiv:2105.02192\\u00a7r"}']}
{title:'Sklyar et al. (§72022§r)', author: 'Ilya Sklyar; Anna Piunova; Xianrui Zheng; Yulan Liu', display:{Lore:['[{"text": "arXiv:2112.10200", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-turn RNN-T for streaming recognition of multi-party speech\\u00a7r\\n\\n\\u00a78\\u00a7oIlya Sklyar\\nAnna Piunova\\nXianrui Zheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.10200\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 10 Feb 2022 13:38:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Vielzeuf et al. (§72022§r)', author: 'Valentin Vielzeuf; Grigory Antipov', display:{Lore:['[{"text": "arXiv:2112.12572", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAre E2E ASR models ready for an industrial usage?\\u00a7r\\n\\n\\u00a78\\u00a7oValentin Vielzeuf\\nGrigory Antipov\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.12572\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 21 Oct 2022 09:08:44 GMT)\\u00a7r"}']}
{title:'Podusenko et al. (§72022§r)', author: 'Albert Podusenko; Bart van Erp; Magnus Koudahl; Bert de Vries', display:{Lore:['[{"text": "arXiv:2112.13366", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAIDA: An Active Inference-based Design Agent for Audio Processing Algorithms\\u00a7r\\n\\n\\u00a78\\u00a7oAlbert Podusenko\\nBart van Erp\\nMagnus Koudahl\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.13366\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3389/frsip.2022.842477\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 10 Jan 2022 12:28:26 GMT)\\u00a7r"}']}
{title:'Han et al. (§72022§r)', author: 'Jiangyu Han; Yanhua Long; Lukas Burget; Jan Cernocky', display:{Lore:['[{"text": "arXiv:2112.13520", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDPCCN: Densely-Connected Pyramid Complex Convolutional Network for Robust Speech Separation And Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oJiangyu Han\\nYanhua Long\\nLukas Burget\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.13520\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 29 Jan 2022 09:42:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by ICASSP 2022\\u00a7r"}']}
{title:'Gan et al. (§72022§r)', author: 'Wendong Gan; Bolong Wen; Ying Yan; Haitao Chen; Zhichao Wang; Hongqiang Du; Lei Xie; Kaixuan Guo; Hai Li', display:{Lore:['[{"text": "arXiv:2201.00269", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIQDUBBING: Prosody modeling based on discrete self-supervised speech representation for expressive voice conversion\\u00a7r\\n\\n\\u00a78\\u00a7oWendong Gan\\nBolong Wen\\nYing Yan\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.00269\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 2 Jan 2022 01:22:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Jia et al. (§72022§r)', author: 'Xupeng Jia; Dongmei Li', display:{Lore:['[{"text": "arXiv:2201.00480", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTFCN: Temporal-Frequential Convolutional Network for Single-Channel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oXupeng Jia\\nDongmei Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.00480\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 3 Jan 2022 05:17:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Mack et al. (§72022§r)', author: 'Wolfgang Mack; Julian Wechsler; Emanuël A. P. Habets', display:{Lore:['[{"text": "arXiv:2201.00503", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSignal-Aware Direction-of-Arrival Estimation Using Attention Mechanisms\\u00a7r\\n\\n\\u00a78\\u00a7oWolfgang Mack\\nJulian Wechsler\\nEmanu\\u00ebl A. P. Habets\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.00503\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.csl.2022.101363\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 3 Jan 2022 07:30:00 GMT)\\u00a7r"}']}
{title:'Gomez-Alanis et al. (§72022§r)', author: 'Alejandro Gomez-Alanis; Jose A. Gonzalez-Lopez; Antonio M. Peinado', display:{Lore:['[{"text": "arXiv:2201.01226", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Transformation of Spoofing Attacks for Voice Biometrics\\u00a7r\\n\\n\\u00a78\\u00a7oAlejandro Gomez-Alanis\\nJose A. Gonzalez-Lopez\\nAntonio M. Peinado\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.01226\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 4 Jan 2022 16:14:03 GMT)\\u00a7r"}']}
{title:'Lehmann et al. (§72022§r)', author: 'Pedro Izquierdo Lehmann; Rodrigo F. Cadiz; Carlos A. Sing Long', display:{Lore:['[{"text": "arXiv:2201.01461", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Maximizing a Perceptual Sweet Spot\\u00a7r\\n\\n\\u00a78\\u00a7oPedro Izquierdo Lehmann\\nRodrigo F. Cadiz\\nCarlos A. Sing Long\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.01461\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 4 May 2022 02:13:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o24 pages, 3 figures. Modified the perceptualmodel to account for binaural effects. Updated the methods and experiments accordingly\\u00a7r"}']}
{title:'Gowda et al. (§72022§r)', author: 'Dhananjaya Gowda; Bajibabu Bollepalli; Sudarsana Reddy Kadiri; Paavo Alku', display:{Lore:['[{"text": "arXiv:2201.01525", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFormant Tracking Using Quasi-Closed Phase Forward-Backward Linear Prediction Analysis and Deep Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oDhananjaya Gowda\\nBajibabu Bollepalli\\nSudarsana Reddy Kadiri\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.01525\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ACCESS.2021.3126280\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPublished in IEEE ACCESS. Vol. 9, 2021, pp. 151631-151640\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 5 Jan 2022 10:27:07 GMT)\\u00a7r"}']}
{title:'Haritaoglu et al. (§72022§r)', author: 'Esin Darici Haritaoglu; Nicholas Rasmussen; Daniel C. H. Tan; Jennifer Ranjani J.; Jaclyn Xiao; Gunvant Chaudhari; Akanksha Rajput; Praveen Govindan; Christian Canham; Wei Chen; Minami Yamaura; Laura Gomezjurado; Aaron Broukhim; Amil Khanzada; Mert Pilanci', display:{Lore:['[{"text": "arXiv:2201.01669", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing Deep Learning with Large Aggregated Datasets for COVID-19 Classification from Cough\\u00a7r\\n\\n\\u00a78\\u00a7oEsin Darici Haritaoglu\\nNicholas Rasmussen\\nDaniel C. H. Tan\\n+ 11 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.01669\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 30 Mar 2022 02:32:35 GMT)\\u00a7r"}']}
{title:'Shi et al. (§72022§r)', author: 'Bowen Shi; Wei-Ning Hsu; Kushal Lakhotia; Abdelrahman Mohamed', display:{Lore:['[{"text": "arXiv:2201.02184", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oBowen Shi\\nWei-Ning Hsu\\nKushal Lakhotia\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.02184\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 13 Mar 2022 01:52:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICLR 2022\\u00a7r"}']}
{title:'Dawalatabad et al. (§72022§r)', author: 'Nauman Dawalatabad; Tushar Vatsal; Ashutosh Gupta; Sungsoo Kim; Shatrughan Singh; Dhananjaya Gowda; Chanwoo Kim', display:{Lore:['[{"text": "arXiv:2201.02741", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTwo-Pass End-to-End ASR Model Compression\\u00a7r\\n\\n\\u00a78\\u00a7oNauman Dawalatabad\\nTushar Vatsal\\nAshutosh Gupta\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.02741\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 8 Jan 2022 02:19:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE ASRU 2021\\u00a7r"}']}
{title:'Grooby et al. (§72022§r)', author: 'Ethan Grooby; Chiranjibi Sitaula; Davood Fattahi; Reza Sameni; Kenneth Tan; Lindsay Zhou; Arrabella King; Ashwin Ramanathan; Atul Malhotra; Guy A. Dumont; Faezeh Marzbanrad', display:{Lore:['[{"text": "arXiv:2201.03211", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoisy Neonatal Chest Sound Separation for High-Quality Heart and Lung Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oEthan Grooby\\nChiranjibi Sitaula\\nDavood Fattahi\\n+ 7 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.03211\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JBHI.2022.3215995\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Journal of Biomedical and Health Informatics, 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 Jan 2022 08:38:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 4 figures, 3 tables. Paper submittedand under review for possible publication in IEEE\\u00a7r"}']}
{title:'Du et al. (§72022§r)', author: 'Jing Du; Shiliang Pu; Qinbo Dong; Chao Jin; Xin Qi; Dian Gu; Ru Wu; Hongwei Zhou', display:{Lore:['[{"text": "arXiv:2201.03313", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-Modal ASR Post-Processing System for Error Correction and Utterance Rejection\\u00a7r\\n\\n\\u00a78\\u00a7oJing Du\\nShiliang Pu\\nQinbo Dong\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.03313\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 Jan 2022 12:29:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmit to ICASSP2022, 5 pages,3 figures\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Xin Wang; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2201.03321", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Practical Guide to Logical Access Voice Presentation Attack Detection\\u00a7r\\n\\n\\u00a78\\u00a7oXin Wang\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.03321\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 Jan 2022 12:42:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work will appear as one chapter for a new book called Frontiers in FakeMedia Generation and Detection, edited by Mahdi Khosravy, Isao Echizen, Noboru Babaguchi. The code for this chapter is available in https://gith"}','{"text": "ub.com/nii-yamagishilab/project-NN-Pytorch-scripts\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Shoutong Wang; Jinglin Liu; Yi Ren; Zhen Wang; Changliang Xu; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2201.03864", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMR-SVS: Singing Voice Synthesis with Multi-Reference Encoder\\u00a7r\\n\\n\\u00a78\\u00a7oShoutong Wang\\nJinglin Liu\\nYi Ren\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.03864\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Jan 2022 09:58:39 GMT)\\u00a7r"}']}
{title:'Sato et al. (§72022§r)', author: 'Hiroshi Sato; Tsubasa Ochiai; Marc Delcroix; Keisuke Kinoshita; Naoyuki Kamo; Takafumi Moriya', display:{Lore:['[{"text": "arXiv:2201.03881", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning to Enhance or Not: Neural Network-Based Switching of Enhanced and Observed Signals for Overlapping Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHiroshi Sato\\nTsubasa Ochiai\\nMarc Delcroix\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.03881\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746347\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIn 2022 IEEE International Conference on Acoustics, Speech and\\n  Signal Processing (ICASSP), pp. 6287-6291\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Jan 2022 10:58:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures\\u00a7r"}']}
{title:'Hu et al. (§72022§r)', author: 'Shoukang Hu; Xurong Xie; Mingyu Cui; Jiajun Deng; Shansong Liu; Jianwei Yu; Mengzhe Geng; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2201.03943", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Architecture Search For LF-MMI Trained Time Delay Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oShoukang Hu\\nXurong Xie\\nMingyu Cui\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.03943\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 29 Mar 2022 02:47:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE/ACM Transactionson Audio, Speech and Language Processing (TASLP). arXiv admin note: text overlap with arXiv:2007.08818\\u00a7r"}']}
{title:'Erdoğan et al. (§72022§r)', author: 'Yunus Emre Erdoğan; Ali Narin', display:{Lore:['[{"text": "arXiv:2201.04872", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparison of Classification Algorithms for COVID19 Detection using Cough Acoustic Signals\\u00a7r\\n\\n\\u00a78\\u00a7oYunus Emre Erdo\\u011fan\\nAli Narin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.04872\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 13 Jan 2022 10:25:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages,3 figures,conference\\u00a7r"}']}
{title:'Boyer et al. (§72022§r)', author: 'Florian Boyer; Yusuke Shinohara; Takaaki Ishii; Hirofumi Inaguma; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2201.05420", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Study of Transducer based End-to-End ASR with ESPnet: Architecture, Auxiliary Loss and Decoding Strategies\\u00a7r\\n\\n\\u00a78\\u00a7oFlorian Boyer\\nYusuke Shinohara\\nTakaaki Ishii\\nHirofumi Inaguma\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.05420\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 Jan 2022 12:44:00 GMT)\\u00a7r"}']}
{title:'Mussakhojayeva et al. (§72022§r)', author: 'Saida Mussakhojayeva; Yerbolat Khassanov; Huseyin Atakan Varol', display:{Lore:['[{"text": "arXiv:2201.05771", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKazakhTTS2: Extending the Open-Source Kazakh TTS Corpus With More Data, Speakers, and Topics\\u00a7r\\n\\n\\u00a78\\u00a7oSaida Mussakhojayeva\\nYerbolat Khassanov\\nHuseyin Atakan Varol\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.05771\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Apr 2022 08:27:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 2 figures, 5 tables, accepted to LREC 2022\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Shansong Liu; Mengzhe Geng; Shoukang Hu; Xurong Xie; Mingyu Cui; Jianwei Yu; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2201.05845", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRecent Progress in the CUHK Dysarthric Speech Recognition System\\u00a7r\\n\\n\\u00a78\\u00a7oShansong Liu\\nMengzhe Geng\\nShoukang Hu\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.05845\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2021.3091805\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 26 Feb 2022 05:00:31 GMT)\\u00a7r"}']}
{title:'Klumpp et al. (§72022§r)', author: 'Philipp Klumpp; Tomás Arias-Vergara; Paula Andrea Pérez-Toro; Elmar Nöth; Juan Rafael Orozco-Arroyave', display:{Lore:['[{"text": "arXiv:2201.05912", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCommon Phone: A Multilingual Dataset for Robust Acoustic Modelling\\u00a7r\\n\\n\\u00a78\\u00a7oPhilipp Klumpp\\nTom\\u00e1s Arias-Vergara\\nPaula Andrea P\\u00e9rez-Toro\\nElmar N\\u00f6th\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.05912\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 31 Jan 2022 08:49:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPre-print submitted to LREC 2022 Link to Common Phone: https://zenodo.org/record/5846137\\u00a7r"}']}
{title:'Erdoğan et al. (§72022§r)', author: 'Yunus Emre Erdoğan; Ali Narin', display:{Lore:['[{"text": "arXiv:2201.06078", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparison of COVID-19 Prediction Performances of Normalization Methods on Cough Acoustics Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oYunus Emre Erdo\\u011fan\\nAli Narin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.06078\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 16 Jan 2022 16:16:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages,2 figures,1 table,International Conference of Applied Sciences and Mathematics(ICASEM 2021). arXiv admin note: text overlap with arXiv:2201.04872\\u00a7r"}']}
{title:'Iwamoto et al. (§72022§r)', author: 'Kazuma Iwamoto; Tsubasa Ochiai; Marc Delcroix; Rintaro Ikeshita; Hiroshi Sato; Shoko Araki; Shigeru Katagiri', display:{Lore:['[{"text": "arXiv:2201.06685", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHow Bad Are Artifacts?: Analyzing the Impact of Speech Enhancement Errors on ASR\\u00a7r\\n\\n\\u00a78\\u00a7oKazuma Iwamoto\\nTsubasa Ochiai\\nMarc Delcroix\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.06685\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 30 Mar 2022 11:18:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, submitted to Interspeech 2022\\u00a7r"}']}
{title:'Gref et al. (§72022§r)', author: 'Michael Gref; Nike Matthiesen; Christoph Schmidt; Sven Behnke; Joachim Köhler', display:{Lore:['[{"text": "arXiv:2201.06841", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHuman and Automatic Speech Recognition Performance on German Oral History Interviews\\u00a7r\\n\\n\\u00a78\\u00a7oMichael Gref\\nNike Matthiesen\\nChristoph Schmidt\\nSven Behnke\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.06841\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 Jan 2022 09:35:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to LREC 2022\\u00a7r"}']}
{title:'Gref et al. (§72022§r)', author: 'Michael Gref; Nike Matthiesen; Sreenivasa Hikkal Venugopala; Shalaka Satheesh; Aswinkumar Vijayananth; Duc Bach Ha; Sven Behnke; Joachim Köhler', display:{Lore:['[{"text": "arXiv:2201.06868", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Study on the Ambiguity in Human Annotation of German Oral History Interviews for Perceived Emotion Recognition and Sentiment Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oMichael Gref\\nNike Matthiesen\\nSreenivasa Hikkal Venugopala\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.06868\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 Jan 2022 10:53:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to LREC 2022\\u00a7r"}']}
{title:'Zhu et al. (§72022§r)', author: 'Qiu-Shi Zhu; Jie Zhang; Zi-Qiang Zhang; Ming-Hui Wu; Xin Fang; Li-Rong Dai', display:{Lore:['[{"text": "arXiv:2201.08930", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Noise-Robust Self-supervised Pre-training Model Based Speech Representation Learning for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oQiu-Shi Zhu\\nJie Zhang\\nZi-Qiang Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.08930\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747379\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2022 - 2022 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP), 2022, pp. 3174-3178\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 22 Jan 2022 00:22:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Xing-Yu Chen; Qiu-Shi Zhu; Jie Zhang; Li-Rong Dai', display:{Lore:['[{"text": "arXiv:2201.08934", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSupervised and Self-supervised Pretraining Based COVID-19 Detection Using Acoustic Breathing/Cough/Speech Signals\\u00a7r\\n\\n\\u00a78\\u00a7oXing-Yu Chen\\nQiu-Shi Zhu\\nJie Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.08934\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746205\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2022 - 2022 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP), 2022, pp. 561-565\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 22 Jan 2022 01:22:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Xie et al. (§72022§r)', author: 'Xurong Xie; Rukiye Ruzi; Xunying Liu; Lan Wang', display:{Lore:['[{"text": "arXiv:2201.09422", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVariational Auto-Encoder Based Variability Encoding for Dysarthric Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXurong Xie\\nRukiye Ruzi\\nXunying Liu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.09422\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-173\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jan 2022 02:35:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in Interspeech 2021, 4808-4812\\u00a7r"}']}
{title:'Hida et al. (§72022§r)', author: 'Rem Hida; Masaki Hamada; Chie Kamada; Emiru Tsunoo; Toshiyuki Sekiya; Toshiyuki Kumakura', display:{Lore:['[{"text": "arXiv:2201.09427", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPolyphone disambiguation and accent prediction using pre-trained language models in Japanese TTS front-end\\u00a7r\\n\\n\\u00a78\\u00a7oRem Hida\\nMasaki Hamada\\nChie Kamada\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.09427\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jan 2022 02:49:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures. Accepted to ICASSP2022\\u00a7r"}']}
{title:'Xie et al. (§72022§r)', author: 'Xurong Xie; Xiang Sui; Xunying Liu; Lan Wang', display:{Lore:['[{"text": "arXiv:2201.09432", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigation of Deep Neural Network Acoustic Modelling Approaches for Low Resource Accented Mandarin Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXurong Xie\\nXiang Sui\\nXunying Liu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.09432\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJOURNAL OF INTEGRATION TECHNOLOGY, Vol. 4, No. 6, Nov. 2015\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jan 2022 03:18:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in JOURNAL OF INTEGRATION TECHNOLOGY CNKI:SUN:JCJI.0.2015-06-003\\u00a7r"}']}
{title:'Pal et al. (§72022§r)', author: 'Monisankha Pal; Aditya Raikar; Ashish Panda; Sunil Kumar Kopparapu', display:{Lore:['[{"text": "arXiv:2201.09470", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynthetic speech detection using meta-learning with prototypical loss\\u00a7r\\n\\n\\u00a78\\u00a7oMonisankha Pal\\nAditya Raikar\\nAshish Panda\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.09470\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jan 2022 06:01:06 GMT)\\u00a7r"}']}
{title:'Madhavaraj et al. (§72022§r)', author: 'A. Madhavaraj; Ramakrishnan Angarai Ganesan', display:{Lore:['[{"text": "arXiv:2201.09494", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData and knowledge-driven approaches for multilingual training to improve the performance of speech recognition systems of Indian languages\\u00a7r\\n\\n\\u00a78\\u00a7oA. Madhavaraj\\nRamakrishnan Angarai Ganesan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.09494\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jan 2022 07:17:17 GMT)\\u00a7r"}']}
{title:'Yoshioka et al. (§72022§r)', author: 'Takuya Yoshioka; Xiaofei Wang; Dongmei Wang', display:{Lore:['[{"text": "arXiv:2201.09586", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPickNet: Real-Time Channel Selection for Ad Hoc Microphone Arrays\\u00a7r\\n\\n\\u00a78\\u00a7oTakuya Yoshioka\\nXiaofei Wang\\nDongmei Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.09586\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jan 2022 10:52:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figure, 2 tables, accepted for presentation at ICASSP 2022\\u00a7r"}']}
{title:'Xiang et al. (§72022§r)', author: 'Yang Xiang; Jesper Lisby Højvang; Morten Højfeldt Rasmussen; Mads Græsbøll Christensen', display:{Lore:['[{"text": "arXiv:2201.09875", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Bayesian Permutation training deep representation learning method for speech enhancement with variational autoencoder\\u00a7r\\n\\n\\u00a78\\u00a7oYang Xiang\\nJesper Lisby H\\u00f8jvang\\nMorten H\\u00f8jfeldt Rasmussen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.09875\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jan 2022 18:51:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Hussain et al. (§72022§r)', author: 'Tassadaq Hussain; Wei-Chien Wang; Mandar Gogate; Kia Dashtipour; Yu Tsao; Xugang Lu; Adeel Ahsan; Amir Hussain', display:{Lore:['[{"text": "arXiv:2201.09913", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Novel Temporal Attentive-Pooling based Convolutional Recurrent Architecture for Acoustic Signal Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oTassadaq Hussain\\nWei-Chien Wang\\nMandar Gogate\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.09913\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jan 2022 19:13:44 GMT)\\u00a7r"}']}
{title:'Lu et al. (§72022§r)', author: 'Liang Lu; Jinyu Li; Yifan Gong', display:{Lore:['[{"text": "arXiv:2201.09979", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEndpoint Detection for Streaming End-to-End Multi-talker ASR\\u00a7r\\n\\n\\u00a78\\u00a7oLiang Lu\\nJinyu Li\\nYifan Gong\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.09979\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jan 2022 22:17:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted to ICASSP 2022\\u00a7r"}']}
{title:'Deng et al. (§72022§r)', author: 'Keqi Deng; Zehui Yang; Shinji Watanabe; Yosuke Higuchi; Gaofeng Cheng; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2201.10103", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving non-autoregressive end-to-end speech recognition with pre-trained acoustic and language models\\u00a7r\\n\\n\\u00a78\\u00a7oKeqi Deng\\nZehui Yang\\nShinji Watanabe\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.10103\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 26 Jan 2022 06:36:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2022\\u00a7r"}']}
{title:'Grooby et al. (§72022§r)', author: 'Ethan Grooby; Chiranjibi Sitaula; Kenneth Tan; Lindsay Zhou; Arrabella King; Ashwin Ramanathan; Atul Malhotra; Guy A. Dumont; Faezeh Marzbanrad', display:{Lore:['[{"text": "arXiv:2201.10105", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrediction of Neonatal Respiratory Distress in Term Babies at Birth from Digital Stethoscope Recorded Chest Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oEthan Grooby\\nChiranjibi Sitaula\\nKenneth Tan\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.10105\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/EMBC48229.2022.9871449\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2022 44th Annual International Conference of the IEEE Engineering\\n  in Medicine & Biology Society (EMBC)\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Jan 2022 05:46:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 2 figures, 1 table. Paper submitted for potential publication as conference paper at 44th Annual International Conference of the IEEEEngineering in Medicine and Biology Society, 2022\\u00a7r"}']}
{title:'Tsunoo et al. (§72022§r)', author: 'Emiru Tsunoo; Chaitanya Narisetty; Michael Hentschel; Yosuke Kashiwagi; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2201.10190", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRun-and-back stitch search: novel block synchronous decoding for streaming encoder-decoder ASR\\u00a7r\\n\\n\\u00a78\\u00a7oEmiru Tsunoo\\nChaitanya Narisetty\\nMichael Hentschel\\nYosuke Kashiwagi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.10190\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Jan 2022 09:08:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP2022\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'Wenyong Huang; Zhenhe Zhang; Yu Ting Yeung; Xin Jiang; Qun Liu', display:{Lore:['[{"text": "arXiv:2201.10207", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSPIRAL: Self-supervised Perturbation-Invariant Representation Learning for Speech Pre-Training\\u00a7r\\n\\n\\u00a78\\u00a7oWenyong Huang\\nZhenhe Zhang\\nYu Ting Yeung\\nXin Jiang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.10207\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 6 Mar 2022 14:25:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICLR 2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Chao Zhang; Bo Li; Zhiyun Lu; Tara N. Sainath; Shuo-yiin Chang', display:{Lore:['[{"text": "arXiv:2201.10240", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving the fusion of acoustic and text representations in RNN-T\\u00a7r\\n\\n\\u00a78\\u00a7oChao Zhang\\nBo Li\\nZhiyun Lu\\nTara N. Sainath\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.10240\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Jan 2022 11:20:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper to appear at ICASSP 2022\\u00a7r"}']}
{title:'Gorodetskii et al. (§72022§r)', author: 'Artem Gorodetskii; Ivan Ozhiganov', display:{Lore:['[{"text": "arXiv:2201.10375", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero-Shot Long-Form Voice Cloning with Dynamic Convolution Attention\\u00a7r\\n\\n\\u00a78\\u00a7oArtem Gorodetskii\\nIvan Ozhiganov\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.10375\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 26 Jan 2022 12:30:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAdded article structure\\u00a7r"}']}
{title:'Cai et al. (§72022§r)', author: 'Zexin Cai; Ming Li', display:{Lore:['[{"text": "arXiv:2201.10687", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvertible Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oZexin Cai\\nMing Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.10687\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Jan 2022 00:25:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5-page conference paper\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Chenda Li; Lei Yang; Weiqin Wang; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2201.10800", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSkiM: Skipping Memory LSTM for Low-Latency Real-Time Continuous Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oChenda Li\\nLei Yang\\nWeiqin Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.10800\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 10 Feb 2022 07:44:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Xu Zhang; Lianwu Chen; Xiguang Zheng; Xinlei Ren; Chen Zhang; Liang Guo; Bing Yu', display:{Lore:['[{"text": "arXiv:2201.10809", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA two-step backward compatible fullband speech enhancement system\\u00a7r\\n\\n\\u00a78\\u00a7oXu Zhang\\nLianwu Chen\\nXiguang Zheng\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.10809\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 28 Jan 2022 02:15:58 GMT)\\u00a7r"}']}
{title:'Gaona et al. (§72022§r)', author: 'Alvaro Joaquín Gaona; Pedro David Arini', display:{Lore:['[{"text": "arXiv:2201.11320", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Recurrent Learning for Heart Sounds Segmentation based on Instantaneous Frequency Features\\u00a7r\\n\\n\\u00a78\\u00a7oAlvaro Joaqu\\u00edn Gaona\\nPedro David Arini\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.11320\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.37537/rev.elektron.4.2.101.2020\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nElektron: ciencia y tecnolog\\\\\'ia en la electr\\\\\'onica de hoy 4.2\\n  (2020): 52-57\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Jan 2022 04:40:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 figures, 6 pages, journal\\u00a7r"}']}
{title:'Soleymanpour et al. (§72022§r)', author: 'Mohammad Soleymanpour; Michael T. Johnson; Rahim Soleymanpour; Jeffrey Berry', display:{Lore:['[{"text": "arXiv:2201.11571", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynthesizing Dysarthric Speech Using Multi-talker TTS for Dysarthric Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Soleymanpour\\nMichael T. Johnson\\nRahim Soleymanpour\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.11571\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Jan 2022 15:22:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted ICASSP2022\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Yufei Liu; Rao Ma; Haihua Xu; Yi He; Zejun Ma; Weibin Zhang', display:{Lore:['[{"text": "arXiv:2201.11627", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInternal Language Model Estimation Through Explicit Context Vector Learning for Attention-based Encoder-decoder ASR\\u00a7r\\n\\n\\u00a78\\u00a7oYufei Liu\\nRao Ma\\nHaihua Xu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.11627\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 2 Nov 2022 09:37:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Songxiang Liu; Dan Su; Dong Yu', display:{Lore:['[{"text": "arXiv:2201.11972", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffGAN-TTS: High-Fidelity and Efficient Text-to-Speech with Denoising Diffusion GANs\\u00a7r\\n\\n\\u00a78\\u00a7oSongxiang Liu\\nDan Su\\nDong Yu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.11972\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Jan 2022 07:41:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint. 16 pages\\u00a7r"}']}
{title:'Gupta et al. (§72022§r)', author: 'Kishan Gupta; Srikanth Korse; Bernd Edler; Guillaume Fuchs', display:{Lore:['[{"text": "arXiv:2201.12039", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA DNN Based Post-Filter to Enhance the Quality of Coded Speech in MDCT Domain\\u00a7r\\n\\n\\u00a78\\u00a7oKishan Gupta\\nSrikanth Korse\\nBernd Edler\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.12039\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Jan 2022 11:08:02 GMT)\\u00a7r"}']}
{title:'Phan et al. (§72022§r)', author: 'Huy Phan; Thi Ngoc Tho Nguyen; Philipp Koch; Alfred Mertins', display:{Lore:['[{"text": "arXiv:2201.12557", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPolyphonic audio event detection: multi-label or multi-class multi-task classification problem?\\u00a7r\\n\\n\\u00a78\\u00a7oHuy Phan\\nThi Ngoc Tho Nguyen\\nPhilipp Koch\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.12557\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 29 Jan 2022 10:52:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper has been accepted to IEEE ICASSP 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Tianrui Wang; Weibin Zhu; Yingying Gao; Junlan Feng; Shilei Zhang', display:{Lore:['[{"text": "arXiv:2201.12755", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHGCN: Harmonic gated compensation network for speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oTianrui Wang\\nWeibin Zhu\\nYingying Gao\\nJunlan Feng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.12755\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 16 Mar 2022 07:28:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Korse et al. (§72022§r)', author: 'Srikanth Korse; Nicola Pia; Kishan Gupta; Guillaume Fuchs', display:{Lore:['[{"text": "arXiv:2201.13093", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPostGAN: A GAN-Based Post-Processor to Enhance the Quality of Coded Speech\\u00a7r\\n\\n\\u00a78\\u00a7oSrikanth Korse\\nNicola Pia\\nKishan Gupta\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.13093\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Jan 2022 10:03:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022\\u00a7r"}']}
{title:'Ebbers et al. (§72022§r)', author: 'Janek Ebbers; Romain Serizel; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2201.13148", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThreshold Independent Evaluation of Sound Event Detection Scores\\u00a7r\\n\\n\\u00a78\\u00a7oJanek Ebbers\\nRomain Serizel\\nReinhold Haeb-Umbach\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.13148\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Jan 2022 11:47:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted for ICASSP 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Zhenyu Wang; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:2201.13246", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImpact of Naturalistic Field Acoustic Environments on Forensic Text-independent Speaker Verification System\\u00a7r\\n\\n\\u00a78\\u00a7oZhenyu Wang\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.13246\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Jan 2022 03:03:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIAFPA-2021-International Association for Forensic Phonetics andAcoustics\\u00a7r"}']}
{title:'Mihajlik et al. (§72022§r)', author: 'P. Mihajlik; A. Balog; T. E. Gráczi; A. Kohári; B. Tarján; K. Mády', display:{Lore:['[{"text": "arXiv:2202.00601", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBEA-Base: A Benchmark for ASR of Spontaneous Hungarian\\u00a7r\\n\\n\\u00a78\\u00a7oP. Mihajlik\\nA. Balog\\nT. E. Gr\\u00e1czi\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.00601\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Feb 2022 17:45:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to LREC 2022\\u00a7r"}']}
{title:'Kanda et al. (§72022§r)', author: 'Naoyuki Kanda; Jian Wu; Yu Wu; Xiong Xiao; Zhong Meng; Xiaofei Wang; Yashesh Gaur; Zhuo Chen; Jinyu Li; Takuya Yoshioka', display:{Lore:['[{"text": "arXiv:2202.00842", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreaming Multi-Talker ASR with Token-Level Serialized Output Training\\u00a7r\\n\\n\\u00a78\\u00a7oNaoyuki Kanda\\nJian Wu\\nYu Wu\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.00842\\u00a7r\\n\\nVersion:\\u00a77v5 (Thu, 14 Jul 2022 20:40:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 1 figure, 7 tables, v2: minor fixes, v3: Appendix D has beenadded, v4: citation to [27] has been added, v5: citations to [28][29][30] have been added with minor fixes, short version accepted for presentation at"}','{"text": "Interspeech 2022\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Ke Chen; Shuai Yu; Cheng-i Wang; Wei Li; Taylor Berg-Kirkpatrick; Shlomo Dubnov', display:{Lore:['[{"text": "arXiv:2202.00951", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTONet: Tone-Octave Network for Singing Melody Extraction from Polyphonic Music\\u00a7r\\n\\n\\u00a78\\u00a7oKe Chen\\nShuai Yu\\nCheng-i Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.00951\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Feb 2022 10:55:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint Versionfor ICASSP 2022, Singapore\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Rongjin Li; Weibin Zhang; Dongpeng Chen', display:{Lore:['[{"text": "arXiv:2202.01092", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe CORAL++ Algorithm for Unsupervised Domain Adaptation of Speaker Recogntion\\u00a7r\\n\\n\\u00a78\\u00a7oRongjin Li\\nWeibin Zhang\\nDongpeng Chen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.01092\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Feb 2022 15:41:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figures. This paper has been accepted to IEEE ICASSP 2022\\u00a7r"}']}
{title:'Xu et al. (§72022§r)', author: 'Liyan Xu; Yile Gu; Jari Kolehmainen; Haidar Khan; Ankur Gandhe; Ariya Rastrow; Andreas Stolcke; Ivan Bulyko', display:{Lore:['[{"text": "arXiv:2202.01094", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRescoreBERT: Discriminative Speech Recognition Rescoring with BERT\\u00a7r\\n\\n\\u00a78\\u00a7oLiyan Xu\\nYile Gu\\nJari Kolehmainen\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.01094\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747118\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. IEEE ICASSP, May 2022, pp. 6617-6121\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 18 Feb 2022 15:38:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022\\u00a7r"}']}
{title:'Narisetty et al. (§72022§r)', author: 'Chaitanya Narisetty; Emiru Tsunoo; Xuankai Chang; Yosuke Kashiwagi; Michael Hentschel; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2202.01405", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Speech Recognition and Audio Captioning\\u00a7r\\n\\n\\u00a78\\u00a7oChaitanya Narisetty\\nEmiru Tsunoo\\nXuankai Chang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.01405\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 3 Feb 2022 04:42:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures. Accepted for ICASSP 2022\\u00a7r"}']}
{title:'Ronchini et al. (§72022§r)', author: 'Francesca Ronchini; Romain Serizel', display:{Lore:['[{"text": "arXiv:2202.01487", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA benchmark of state-of-the-art sound event detection systems evaluated on synthetic soundscapes\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesca Ronchini\\nRomain Serizel\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.01487\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747577\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2022 - 2022 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 8 Feb 2022 15:43:26 GMT)\\u00a7r"}']}
{title:'Cheng et al. (§72022§r)', author: 'Linjuan Cheng; Chengshi Zheng; Andong Li; Yuquan Wu; Renhua Peng; Xiaodong Li', display:{Lore:['[{"text": "arXiv:2202.01630", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA deep complex multi-frame filtering network for stereophonic acoustic echo cancellation\\u00a7r\\n\\n\\u00a78\\u00a7oLinjuan Cheng\\nChengshi Zheng\\nAndong Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.01630\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 6 May 2022 02:23:13 GMT)\\u00a7r"}']}
{title:'Imort et al. (§72022§r)', author: 'Johannes Imort; Giorgio Fabbro; Marco A. Martínez Ramírez; Stefan Uhlich; Yuichiro Koyama; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2202.01664", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDistortion Audio Effects: Learning How to Recover the Clean Signal\\u00a7r\\n\\n\\u00a78\\u00a7oJohannes Imort\\nGiorgio Fabbro\\nMarco A. Mart\\u00ednez Ram\\u00edrez\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.01664\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 13 Sep 2022 11:23:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAudio examples available at https://joimort.github.io/distortionremoval/\\u00a7r"}']}
{title:'Zheng et al. (§72022§r)', author: 'Naijun Zheng; Na Li; Xixin Wu; Lingwei Meng; Jiawen Kang; Haibin Wu; Chao Weng; Dan Su; Helen Meng', display:{Lore:['[{"text": "arXiv:2202.01986", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe CUHK-TENCENT speaker diarization system for the ICASSP 2022 multi-channel multi-party meeting transcription challenge\\u00a7r\\n\\n\\u00a78\\u00a7oNaijun Zheng\\nNa Li\\nXixin Wu\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.01986\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Feb 2022 05:43:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Weiqing Wang; Xiaoyi Qin; Ming Li', display:{Lore:['[{"text": "arXiv:2202.02687", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-Channel Attention-Based Target Speaker Voice Activity Detection: Experimental Results for M2MeT Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oWeiqing Wang\\nXiaoyi Qin\\nMing Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.02687\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 6 Feb 2022 02:44:07 GMT)\\u00a7r"}']}
{title:'Pu et al. (§72022§r)', author: 'Jie Pu; Yixiong Meng; Oguz Elibol', display:{Lore:['[{"text": "arXiv:2202.03125", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBuilding Synthetic Speaker Profiles in Text-to-Speech Systems\\u00a7r\\n\\n\\u00a78\\u00a7oJie Pu\\nYixiong Meng\\nOguz Elibol\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.03125\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Feb 2022 13:06:18 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Shu Wang; Yuhuang Hu; Shih-Chii Liu', display:{Lore:['[{"text": "arXiv:2202.03204", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lT-NGA: Temporal Network Grafting Algorithm for Learning to Process Spiking Audio Sensor Events\\u00a7r\\n\\n\\u00a78\\u00a7oShu Wang\\nYuhuang Hu\\nShih-Chii Liu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.03204\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Feb 2022 14:14:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures; accepted at IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Singapore, 2022\\u00a7r"}']}
{title:'Han et al. (§72022§r)', author: 'Tianxiao Han; Qianqian Yang; Zhiguo Shi; Shibo He; Zhaoyang Zhang', display:{Lore:['[{"text": "arXiv:2202.03211", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemantic-aware Speech to Text Transmission with Redundancy Removal\\u00a7r\\n\\n\\u00a78\\u00a7oTianxiao Han\\nQianqian Yang\\nZhiguo Shi\\nShibo He\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.03211\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Feb 2022 14:16:26 GMT)\\u00a7r"}']}
{title:'Peng et al. (§72022§r)', author: 'Puyuan Peng; David Harwath', display:{Lore:['[{"text": "arXiv:2202.03543", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oPuyuan Peng\\nDavid Harwath\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.03543\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 2 Mar 2022 18:02:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSAS workshop at AAAI2022, code and model weights available at https://github.com/jasonppy/FaST-VGS-Family\\u00a7r"}']}
{title:'Sachidananda et al. (§72022§r)', author: 'Vin Sachidananda; Shao-Yen Tseng; Erik Marchi; Sachin Kajarekar; Panayiotis Georgiou', display:{Lore:['[{"text": "arXiv:2202.03587", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCALM: Contrastive Aligned Audio-Language Multirate and Multimodal Representations\\u00a7r\\n\\n\\u00a78\\u00a7oVin Sachidananda\\nShao-Yen Tseng\\nErik Marchi\\nSachin Kajarekar\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.03587\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Feb 2022 01:20:37 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Zehua Chen; Xu Tan; Ke Wang; Shifeng Pan; Danilo Mandic; Lei He; Sheng Zhao', display:{Lore:['[{"text": "arXiv:2202.03751", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInferGrad: Improving Diffusion Models for Vocoder by Considering Inference in Training\\u00a7r\\n\\n\\u00a78\\u00a7oZehua Chen\\nXu Tan\\nKe Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.03751\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Feb 2022 09:40:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 Pages, 2 figures. Accepted to ICASSP 2022\\u00a7r"}']}
{title:'Ikuma et al. (§72022§r)', author: 'Takeshi Ikuma; Andrew J. McWhorter; Lacey Adkins; Melda Kunduk', display:{Lore:['[{"text": "arXiv:2202.04150", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTime-varying harmonic models for voice signal analysis\\u00a7r\\n\\n\\u00a78\\u00a7oTakeshi Ikuma\\nAndrew J. McWhorter\\nLacey Adkins\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.04150\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Feb 2022 21:19:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 12 figures, submitted to JASA\\u00a7r"}']}
{title:'Hussain et al. (§72022§r)', author: 'Tassadaq Hussain; Muhammad Diyan; Mandar Gogate; Kia Dashtipour; Ahsan Adeel; Yu Tsao; Amir Hussain', display:{Lore:['[{"text": "arXiv:2202.04172", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Speech Intelligibility Enhancement Model based on Canonical Correlation and Deep Learning for Hearing-Assistive Technologies\\u00a7r\\n\\n\\u00a78\\u00a7oTassadaq Hussain\\nMuhammad Diyan\\nMandar Gogate\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.04172\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 15 Feb 2022 12:33:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWe would like to withdraw this article because we have accidentally uploaded the revised version of the same article from another account. The updated version is titled \\"A Novel Speech Intelligibility Enhancement Model "}','{"text": "based on Canonical Correlation and Deep Learning\\" (arXiv:2202.05756)\\u00a7r"}']}
{title:'Arikawa et al. (§72022§r)', author: 'Kazuyuki Arikawa; Shoichi Koyama; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2202.04807", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatial active noise control based on individual kernel interpolation of primary and secondary sound fields\\u00a7r\\n\\n\\u00a78\\u00a7oKazuyuki Arikawa\\nShoichi Koyama\\nHiroshi Saruwatari\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.04807\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Feb 2022 02:54:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2022\\u00a7r"}']}
{title:'He et al. (§72022§r)', author: 'Maokui He; Xiang Lv; Weilin Zhou; JingJing Yin; Xiaoqi Zhang; Yuxuan Wang; Shutong Niu; Yuhang Cao; Heng Lu; Jun Du; Chin-Hui Lee', display:{Lore:['[{"text": "arXiv:2202.04855", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe USTC-Ximalaya system for the ICASSP 2022 multi-channel multi-party meeting transcription (M2MeT) challenge\\u00a7r\\n\\n\\u00a78\\u00a7oMaokui He\\nXiang Lv\\nWeilin Zhou\\n+ 7 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.04855\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Feb 2022 06:06:48 GMT)\\u00a7r"}']}
{title:'Ribeiro et al. (§72022§r)', author: 'Manuel Sam Ribeiro; Julian Roth; Giulia Comini; Goeric Huybrechts; Adam Gabrys; Jaime Lorenzo-Trueba', display:{Lore:['[{"text": "arXiv:2202.05083", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-speaker style transfer for text-to-speech using data augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oManuel Sam Ribeiro\\nJulian Roth\\nGiulia Comini\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.05083\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Feb 2022 15:10:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 4 tables. ICASSP 2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'You Zhang; Ge Zhu; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2202.05253", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Probabilistic Fusion Framework for Spoofing Aware Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oYou Zhang\\nGe Zhu\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.05253\\u00a7r\\n\\nVersion:\\u00a77v4 (Sun, 24 Apr 2022 19:36:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 5 figures, to be appear in Odyssey 2022\\u00a7r"}']}
{title:'Lu et al. (§72022§r)', author: 'Yen-Ju Lu; Zhong-Qiu Wang; Shinji Watanabe; Alexander Richard; Cheng Yu; Yu Tsao', display:{Lore:['[{"text": "arXiv:2202.05256", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConditional Diffusion Probabilistic Model for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oYen-Ju Lu\\nZhong-Qiu Wang\\nShinji Watanabe\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.05256\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Feb 2022 18:58:01 GMT)\\u00a7r"}']}
{title:'Shonibare et al. (§72022§r)', author: 'Olabanji Shonibare; Xiaosu Tong; Venkatesh Ravichandran', display:{Lore:['[{"text": "arXiv:2202.05396", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing ASR for Stuttered Speech with Limited Data Using Detect and Pass\\u00a7r\\n\\n\\u00a78\\u00a7oOlabanji Shonibare\\nXiaosu Tong\\nVenkatesh Ravichandran\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.05396\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Feb 2022 19:55:23 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Jie Wang; Yuji Liu; Binling Wang; Yiming Zhi; Song Li1; Shipeng Xia; Jiayang Zhang; Lin Li1; Qingyang Hong; Feng Tong', display:{Lore:['[{"text": "arXiv:2202.05744", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe xmuspeech system for multi-channel multi-party meeting transcription challenge\\u00a7r\\n\\n\\u00a78\\u00a7oJie Wang\\nYuji Liu\\nBinling Wang\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.05744\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Feb 2022 16:32:52 GMT)\\u00a7r"}']}
{title:'Ravi et al. (§72022§r)', author: 'Vijay Ravi; Jinhan Wang; Jonathan Flint; Abeer Alwan', display:{Lore:['[{"text": "arXiv:2202.05912", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrAUG: A Frame Rate Based Data Augmentation Method for Depression Detection from Speech Signals\\u00a7r\\n\\n\\u00a78\\u00a7oVijay Ravi\\nJinhan Wang\\nJonathan Flint\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.05912\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Feb 2022 21:37:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022. copyright 2022 IEEE. Personal use of this material is permitted\\u00a7r"}']}
{title:'Seneviratne et al. (§72022§r)', author: 'Nadee Seneviratne; Carol Espy-Wilson', display:{Lore:['[{"text": "arXiv:2202.06238", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultimodal Depression Classification Using Articulatory Coordination Features And Hierarchical Attention Based Text Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oNadee Seneviratne\\nCarol Espy-Wilson\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.06238\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 13 Feb 2022 07:37:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022. arXiv admin note: text overlapwith arXiv:2104.04195\\u00a7r"}']}
{title:'Lajszczak et al. (§72022§r)', author: 'Mateusz Lajszczak; Animesh Prasad; Arent van Korlaar; Bajibabu Bollepalli; Antonio Bonafonte; Arnaud Joly; Marco Nicolis; Alexis Moinet; Thomas Drugman; Trevor Wood; Elena Sokolova', display:{Lore:['[{"text": "arXiv:2202.06409", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDistribution augmentation for low-resource expressive text-to-speech\\u00a7r\\n\\n\\u00a78\\u00a7oMateusz Lajszczak\\nAnimesh Prasad\\nArent van Korlaar\\n+ 7 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.06409\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 19 Feb 2022 11:24:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2022: camera-ready\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Kuan-Chen Wang; Kai-Chun Liu; Hsin-Min Wang; Yu Tsao', display:{Lore:['[{"text": "arXiv:2202.06507", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEMGSE: Acoustic/EMG Fusion for Multimodal Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oKuan-Chen Wang\\nKai-Chun Liu\\nHsin-Min Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.06507\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Feb 2022 06:39:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, and 3 tables\\u00a7r"}']}
{title:'Kinoshita et al. (§72022§r)', author: 'Keisuke Kinoshita; Marc Delcroix; Tomoharu Iwata', display:{Lore:['[{"text": "arXiv:2202.06524", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTight integration of neural- and clustering-based diarization through deep unfolding of infinite Gaussian mixture model\\u00a7r\\n\\n\\u00a78\\u00a7oKeisuke Kinoshita\\nMarc Delcroix\\nTomoharu Iwata\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.06524\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Feb 2022 07:45:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEICASSP-2022, 5 pages, 2 figures\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Haibin Wu; Heng-Cheng Kuo; Naijun Zheng; Kuo-Hsuan Hung; Hung-Yi Lee; Yu Tsao; Hsin-Min Wang; Helen Meng', display:{Lore:['[{"text": "arXiv:2202.06684", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPartially Fake Audio Detection by Self-attention-based Fake Span Discovery\\u00a7r\\n\\n\\u00a78\\u00a7oHaibin Wu\\nHeng-Cheng Kuo\\nNaijun Zheng\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.06684\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 15 Feb 2022 09:07:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Zheng et al. (§72022§r)', author: 'Chengshi Zheng; Wenzhe Liu; Andong Li; Yuxuan Ke; Xiaodong Li', display:{Lore:['[{"text": "arXiv:2202.06764", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-latency Monaural Speech Enhancement with Deep Filter-bank Equalizer\\u00a7r\\n\\n\\u00a78\\u00a7oChengshi Zheng\\nWenzhe Liu\\nAndong Li\\nYuxuan Ke\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.06764\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0011396\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Feb 2022 14:34:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o35 pages, 8 figures\\u00a7r"}']}
{title:'Baki et al. (§72022§r)', author: 'Pınar Baki; Heysem Kaya; Elvan Çiftçi; Hüseyin Güleç; Albert Ali Salah', display:{Lore:['[{"text": "arXiv:2202.06766", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Analysis for Automatic Mania Assessment in Bipolar Disorder\\u00a7r\\n\\n\\u00a78\\u00a7oP\\u0131nar Baki\\nHeysem Kaya\\nElvan \\u00c7ift\\u00e7i\\nH\\u00fcseyin G\\u00fcle\\u00e7\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.06766\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 5 Feb 2022 14:30:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oConference, 5 pages, in Turkish language\\u00a7r"}']}
{title:'Guo et al. (§72022§r)', author: 'Yiwei Guo; Chenpeng Du; Kai Yu', display:{Lore:['[{"text": "arXiv:2202.07200", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised word-level prosody tagging for controllable speech synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYiwei Guo\\nChenpeng Du\\nKai Yu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.07200\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 16 Feb 2022 05:43:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 6 figures, accepted to ICASSP2022\\u00a7r"}']}
{title:'Petermann et al. (§72022§r)', author: 'Darius Petermann; Minje Kim', display:{Lore:['[{"text": "arXiv:2202.07523", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpaIn-Net: Spatially-Informed Stereophonic Music Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oDarius Petermann\\nMinje Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.07523\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Feb 2022 15:49:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo Appear in Proc. ICASSP2022\\u00a7r"}']}
{title:'Lea et al. (§72022§r)', author: 'Colin Lea; Zifang Huang; Dhruv Jain; Lauren Tooley; Zeinab Liaghat; Shrinath Thelapurath; Leah Findlater; Jeffrey P. Bigham', display:{Lore:['[{"text": "arXiv:2202.07750", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNonverbal Sound Detection for Disordered Speech\\u00a7r\\n\\n\\u00a78\\u00a7oColin Lea\\nZifang Huang\\nDhruv Jain\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.07750\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Feb 2022 22:02:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2022\\u00a7r"}']}
{title:'Ren et al. (§72022§r)', author: 'Yi Ren; Ming Lei; Zhiying Huang; Shiliang Zhang; Qian Chen; Zhijie Yan; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2202.07816", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProsoSpeech: Enhancing Prosody With Quantized Vector Pre-training in Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oYi Ren\\nMing Lei\\nZhiying Huang\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.07816\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Feb 2022 01:42:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Tanaka et al. (§72022§r)', author: 'Tomoro Tanaka; Kohei Yatabe; Masahiro Yasuda; Yasuhiro Oikawa', display:{Lore:['[{"text": "arXiv:2202.08028", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAPPLADE: Adjustable Plug-and-play Audio Declipper Combining DNN with Sparse Optimization\\u00a7r\\n\\n\\u00a78\\u00a7oTomoro Tanaka\\nKohei Yatabe\\nMasahiro Yasuda\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.08028\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Feb 2022 12:44:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 7 figures, accepted to IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP), 2022\\u00a7r"}']}
{title:'Gabryś et al. (§72022§r)', author: 'Adam Gabryś; Goeric Huybrechts; Manuel Sam Ribeiro; Chung-Ming Chien; Julian Roth; Giulia Comini; Roberto Barra-Chicote; Bartek Perz; Jaime Lorenzo-Trueba', display:{Lore:['[{"text": "arXiv:2202.08164", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Filter: Few-shot text-to-speech speaker adaptation using voice conversion as a post-processing module\\u00a7r\\n\\n\\u00a78\\u00a7oAdam Gabry\\u015b\\nGoeric Huybrechts\\nManuel Sam Ribeiro\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.08164\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Feb 2022 16:12:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2022\\u00a7r"}']}
{title:'Shen et al. (§72022§r)', author: 'Ying Shen; Huiyu Yang; Lin Lin', display:{Lore:['[{"text": "arXiv:2202.08210", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Depression Detection: An Emotional Audio-Textual Corpus and a GRU/BiLSTM-based Model\\u00a7r\\n\\n\\u00a78\\u00a7oYing Shen\\nHuiyu Yang\\nLin Lin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.08210\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Feb 2022 03:29:39 GMT)\\u00a7r"}']}
{title:'Sakuma et al. (§72022§r)', author: 'Jin Sakuma; Tatsuya Komatsu; Robin Scheibler', display:{Lore:['[{"text": "arXiv:2202.08456", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMLP-ASR: Sequence-length agnostic all-MLP architectures for speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJin Sakuma\\nTatsuya Komatsu\\nRobin Scheibler\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.08456\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Feb 2022 06:06:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 4 figures\\u00a7r"}']}
{title:'Nagatomo et al. (§72022§r)', author: 'Kento Nagatomo; Masahiro Yasuda; Kohei Yatabe; Shoichiro Saito; Yasuhiro Oikawa', display:{Lore:['[{"text": "arXiv:2202.08458", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWearable SELD dataset: Dataset for sound event localization and detection using wearable devices around head\\u00a7r\\n\\n\\u00a78\\u00a7oKento Nagatomo\\nMasahiro Yasuda\\nKohei Yatabe\\nShoichiro Saito\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.08458\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Feb 2022 06:08:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 6 figures, accepted to IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP), 2022\\u00a7r"}']}
{title:'Komatsu et al. (§72022§r)', author: 'Tatsuya Komatsu; Shinji Watanabe; Koichi Miyazaki; Tomoki Hayashi', display:{Lore:['[{"text": "arXiv:2202.08470", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic Event Detection with Classifier Chains\\u00a7r\\n\\n\\u00a78\\u00a7oTatsuya Komatsu\\nShinji Watanabe\\nKoichi Miyazaki\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.08470\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-2218\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Feb 2022 06:46:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5pages, presented at Interspeech2021\\u00a7r"}']}
{title:'Komatsu (§72022§r)', author: 'Tatsuya Komatsu', display:{Lore:['[{"text": "arXiv:2202.08474", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-Autoregressive ASR with Self-Conditioned Folded Encoders\\u00a7r\\n\\n\\u00a78\\u00a7oTatsuya Komatsu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.08474\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Feb 2022 06:53:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted at ICASSP2022\\u00a7r"}']}
{title:'Koo et al. (§72022§r)', author: 'Junghyun Koo; Seungryeol Paik; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2202.08520", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end Music Remastering System Using Self-supervised and Adversarial Training\\u00a7r\\n\\n\\u00a78\\u00a7oJunghyun Koo\\nSeungryeol Paik\\nKyogu Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.08520\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Feb 2022 08:50:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2022\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Chao-Han Huck Yang; Zeeshan Ahmed; Yile Gu; Joseph Szurley; Roger Ren; Linda Liu; Andreas Stolcke; Ivan Bulyko', display:{Lore:['[{"text": "arXiv:2202.08532", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMitigating Closed-model Adversarial Examples with Bayesian Neural Modeling for Enhanced End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oChao-Han Huck Yang\\nZeeshan Ahmed\\nYile Gu\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.08532\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Feb 2022 09:17:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022\\u00a7r"}']}
{title:'Moliner et al. (§72022§r)', author: 'Eloi Moliner; Vesa Välimäki', display:{Lore:['[{"text": "arXiv:2202.08702", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Two-Stage U-Net for High-Fidelity Denoising of Historical Recordings\\u00a7r\\n\\n\\u00a78\\u00a7oEloi Moliner\\nVesa V\\u00e4lim\\u00e4ki\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.08702\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 19 Feb 2022 12:51:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2022\\u00a7r"}']}
{title:'Han et al. (§72022§r)', author: 'Cong Han; E. Merve Kaya; Kyle Hoefer; Malcolm Slaney; Simon Carlile', display:{Lore:['[{"text": "arXiv:2202.08793", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Channel Speech Denoising for Machine Ears\\u00a7r\\n\\n\\u00a78\\u00a7oCong Han\\nE. Merve Kaya\\nKyle Hoefer\\nMalcolm Slaney\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.08793\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Feb 2022 17:59:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022\\u00a7r"}']}
{title:'Kuznetsova et al. (§72022§r)', author: 'Anastasia Kuznetsova; Anurag Kumar; Jennifer Drexler Fox; Francis Tyers', display:{Lore:['[{"text": "arXiv:2202.08883", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCurriculum optimization for low-resource speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAnastasia Kuznetsova\\nAnurag Kumar\\nJennifer Drexler Fox\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.08883\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Feb 2022 19:47:50 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Disong Wang; Shan Yang; Dan Su; Xunying Liu; Dong Yu; Helen Meng', display:{Lore:['[{"text": "arXiv:2202.09081", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVCVTS: Multi-speaker Video-to-Speech synthesis via cross-modal knowledge transfer from voice conversion\\u00a7r\\n\\n\\u00a78\\u00a7oDisong Wang\\nShan Yang\\nDan Su\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.09081\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Feb 2022 08:58:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022. Demo pageis available at https://wendison.github.io/VCVTS-demo/\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Disong Wang; Songxiang Liu; Xixin Wu; Hui Lu; Lifa Sun; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2202.09082", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Identity Preservation in Dysarthric Speech Reconstruction by Adversarial Speaker Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oDisong Wang\\nSongxiang Liu\\nXixin Wu\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.09082\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Feb 2022 08:59:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022. Demo pageis available at https://wendison.github.io/ASA-DSR-demo/\\u00a7r"}']}
{title:'Yasuda et al. (§72022§r)', author: 'Masahiro Yasuda; Yasunori Ohishi; Shoichiro Saito', display:{Lore:['[{"text": "arXiv:2202.09121", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEcho-aware Adaptation of Sound Event Localization and Detection in Unknown Environments\\u00a7r\\n\\n\\u00a78\\u00a7oMasahiro Yasuda\\nYasunori Ohishi\\nShoichiro Saito\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.09121\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Feb 2022 10:54:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, to appear in IEEE ICASSP 2022\\u00a7r"}']}
{title:'Yasuda et al. (§72022§r)', author: 'Masahiro Yasuda; Yasunori Ohishi; Shoichiro Saito; Noboru Harada', display:{Lore:['[{"text": "arXiv:2202.09124", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-view and Multi-modal Event Detection Utilizing Transformer-based Multi-sensor fusion\\u00a7r\\n\\n\\u00a78\\u00a7oMasahiro Yasuda\\nYasunori Ohishi\\nShoichiro Saito\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.09124\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Feb 2022 11:10:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, to appear in IEEE ICASSP 2022\\u00a7r"}']}
{title:'Johnson et al. (§72022§r)', author: 'Alexander Johnson; Ruchao Fan; Robin Morris; Abeer Alwan', display:{Lore:['[{"text": "arXiv:2202.09529", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLPC Augment: An LPC-Based ASR Data Augmentation Algorithm for Low and Zero-Resource Children\'s Dialects\\u00a7r\\n\\n\\u00a78\\u00a7oAlexander Johnson\\nRuchao Fan\\nRobin Morris\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.09529\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2022\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 22 Feb 2022 03:30:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures\\u00a7r"}']}
{title:'Johnson et al. (§72022§r)', author: 'Alexander Johnson; Alejandra Martin; Marlen Quintero; Alison Bailey; Abeer Alwan', display:{Lore:['[{"text": "arXiv:2202.09531", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCan Social Robots Effectively Elicit Curiosity in STEM Topics from K-1 Students During Oral Assessments?\\u00a7r\\n\\n\\u00a78\\u00a7oAlexander Johnson\\nAlejandra Martin\\nMarlen Quintero\\nAlison Bailey\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.09531\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Educon 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 19 Feb 2022 05:45:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 2 figures\\u00a7r"}']}
{title:'Ge et al. (§72022§r)', author: 'Meng Ge; Chenglin Xu; Longbiao Wang; Eng Siong Chng; Jianwu Dang; Haizhou Li', display:{Lore:['[{"text": "arXiv:2202.09995", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lL-SpEx: Localized Target Speaker Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oMeng Ge\\nChenglin Xu\\nLongbiao Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.09995\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Feb 2022 05:17:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ICASSP 2022\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Jingdong Li; Yuanyuan Zhu; Dawei Luo; Yun Liu; Guohui Cui; Zhaoxia Li', display:{Lore:['[{"text": "arXiv:2202.10017", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe PCG-AIID System for L3DAS22 Challenge: MIMO and MISO convolutional recurrent Network for Multi Channel Speech Enhancement and Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJingdong Li\\nYuanyuan Zhu\\nDawei Luo\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.10017\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Feb 2022 07:06:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at ICASSP 2022 (Accepted)\\u00a7r"}']}
{title:'Zhao et al. (§72022§r)', author: 'Hang Zhao; Chen Zhang; Belei Zhu; Zejun Ma; Kejun Zhang', display:{Lore:['[{"text": "arXiv:2202.10139", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lS3T: Self-Supervised Pre-training with Swin Transformer for Music Classification\\u00a7r\\n\\n\\u00a78\\u00a7oHang Zhao\\nChen Zhang\\nBelei Zhu\\nZejun Ma\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.10139\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Feb 2022 11:36:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2022\\u00a7r"}']}
{title:'Geng et al. (§72022§r)', author: 'Mengzhe Geng; Xurong Xie; Zi Ye; Tianzi Wang; Guinan Li; Shujie Hu; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2202.10290", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Adaptation Using Spectro-Temporal Deep Features for Dysarthric and Elderly Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMengzhe Geng\\nXurong Xie\\nZi Ye\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.10290\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 17 Mar 2022 04:31:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn submission to IEEE/ACM Transactions on Audio Speech and Language Processing\\u00a7r"}']}
{title:'Guizzo et al. (§72022§r)', author: 'Eric Guizzo; Christian Marinoni; Marco Pennese; Xinlei Ren; Xiguang Zheng; Chen Zhang; Bruno Masiero; Aurelio Uncini; Danilo Comminiello', display:{Lore:['[{"text": "arXiv:2202.10372", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lL3DAS22 Challenge: Learning 3D Audio Sources in a Real Office Environment\\u00a7r\\n\\n\\u00a78\\u00a7oEric Guizzo\\nChristian Marinoni\\nMarco Pennese\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.10372\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746872\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2022 IEEE International Conference on Acoustics, Speech and Signal\\n  Processing (ICASSP), 2022, pp. 9186-9190\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Feb 2022 17:05:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to 2022 IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP 2022). arXiv admin note: substantial text overlap with arXiv:2104.05499\\u00a7r"}']}
{title:'Esparza (§72022§r)', author: 'Mario Esparza', display:{Lore:['[{"text": "arXiv:2202.10536", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpanish and English Phoneme Recognition by Training on Simulated Classroom Audio Recordings of Collaborative Learning Environments\\u00a7r\\n\\n\\u00a78\\u00a7oMario Esparza\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.10536\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Feb 2022 21:25:41 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Jinhan Wang; Xiaosu Tong; Jinxi Guo; Di He; Roland Maas', display:{Lore:['[{"text": "arXiv:2202.10593", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVADOI:Voice-Activity-Detection Overlapping Inference For End-to-end Long-form Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJinhan Wang\\nXiaosu Tong\\nJinxi Guo\\nDi He\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.10593\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Feb 2022 00:13:50 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Xin Zhang; Minho Jin; Roger Cheng; Ruirui Li; Eunjung Han; Andreas Stolcke', display:{Lore:['[{"text": "arXiv:2202.10672", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContrastive-mixup learning for improved speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oXin Zhang\\nMinho Jin\\nRoger Cheng\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.10672\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746411\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. IEEE ICASSP, May 2022, pp. 7652-7656\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Feb 2022 05:09:22 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Syu-Siang Wang; Chi-Te Wang; Chih-Chung Lai; Yu Tsao; Shih-Hau Fang', display:{Lore:['[{"text": "arXiv:2202.10777", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContinuous Speech for Improved Learning Pathological Voice Disorders\\u00a7r\\n\\n\\u00a78\\u00a7oSyu-Siang Wang\\nChi-Te Wang\\nChih-Chung Lai\\nYu Tsao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.10777\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Feb 2022 09:58:31 GMT)\\u00a7r"}']}
{title:'Beck et al. (§72022§r)', author: 'Gustavo Teodoro Döhler Beck; Ulme Wennberg; Zofia Malisz; Gustav Eje Henter', display:{Lore:['[{"text": "arXiv:2202.10973", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWavebender GAN: An architecture for phonetically meaningful speech manipulation\\u00a7r\\n\\n\\u00a78\\u00a7oGustavo Teodoro D\\u00f6hler Beck\\nUlme Wennberg\\nZofia Malisz\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.10973\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747442\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Feb 2022 15:26:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures; to appear at ICASSP 2022\\u00a7r"}']}
{title:'Valin et al. (§72022§r)', author: 'Jean-Marc Valin; Umut Isik; Paris Smaragdis; Arvindh Krishnaswamy', display:{Lore:['[{"text": "arXiv:2202.11169", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Speech Synthesis on a Shoestring: Improving the Efficiency of LPCNet\\u00a7r\\n\\n\\u00a78\\u00a7oJean-Marc Valin\\nUmut Isik\\nParis Smaragdis\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.11169\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Feb 2022 20:42:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP 2022, 5 pages\\u00a7r"}']}
{title:'Das et al. (§72022§r)', author: 'Orchisama Das; Jonathan S. Abel', display:{Lore:['[{"text": "arXiv:2202.11192", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModal Estimation on a Warped Frequency Axis for Linear System Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oOrchisama Das\\nJonathan S. Abel\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.11192\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Feb 2022 21:59:42 GMT)\\u00a7r"}']}
{title:'Zhao et al. (§72022§r)', author: 'Chendong Zhao; Jianzong Wang; Xiaoyang Qu; Haoqian Wang; Jing Xiao', display:{Lore:['[{"text": "arXiv:2202.11194", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lr-G2P: Evaluating and Enhancing Robustness of Grapheme to Phoneme Conversion by Controlled noise introducing and Contextual information incorporation\\u00a7r\\n\\n\\u00a78\\u00a7oChendong Zhao\\nJianzong Wang\\nXiaoyang Qu\\nHaoqian Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.11194\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Feb 2022 13:29:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, accepted to ICASSP 2022\\u00a7r"}']}
{title:'Subramani et al. (§72022§r)', author: 'Krishna Subramani; Jean-Marc Valin; Umut Isik; Paris Smaragdis; Arvindh Krishnaswamy', display:{Lore:['[{"text": "arXiv:2202.11301", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end LPCNet: A Neural Vocoder With Fully-Differentiable LPC Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oKrishna Subramani\\nJean-Marc Valin\\nUmut Isik\\nParis Smaragdis\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.11301\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 29 Mar 2022 18:37:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Shen et al. (§72022§r)', author: 'Hua Shen; Yuguang Yang; Guoli Sun; Ryan Langman; Eunjung Han; Jasha Droppo; Andreas Stolcke', display:{Lore:['[{"text": "arXiv:2202.11323", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving fairness in speaker verification via Group-adapted Fusion Network\\u00a7r\\n\\n\\u00a78\\u00a7oHua Shen\\nYuguang Yang\\nGuoli Sun\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.11323\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747384\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. IEEE ICASSP, May 2022, pp. 7077-7081\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 23 Feb 2022 06:20:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proc. IEEE ICASSP 2022\\u00a7r"}']}
{title:'Götz et al. (§72022§r)', author: 'Philipp Götz; Cagdas Tuna; Andreas Walther; Emanuël A. P. Habets', display:{Lore:['[{"text": "arXiv:2202.11790", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBlind Reverberation Time Estimation in Dynamic Acoustic Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oPhilipp G\\u00f6tz\\nCagdas Tuna\\nAndreas Walther\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.11790\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746457\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 23 Feb 2022 21:18:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted for publication in ICASSP 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Quan Wang; Yang Yu; Jason Pelecanos; Yiling Huang; Ignacio Lopez Moreno', display:{Lore:['[{"text": "arXiv:2202.12163", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttentive Temporal Pooling for Conformer-based Streaming Language Identification in Long-form Speech\\u00a7r\\n\\n\\u00a78\\u00a7oQuan Wang\\nYang Yu\\nJason Pelecanos\\nYiling Huang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.12163\\u00a7r\\n\\nVersion:\\u00a77v4 (Sun, 1 May 2022 15:52:48 GMT)\\u00a7r"}']}
{title:'Rikhye et al. (§72022§r)', author: 'Rajeev Rikhye; Quan Wang; Qiao Liang; Yanzhang He; Ian McGraw', display:{Lore:['[{"text": "arXiv:2202.12169", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lClosing the Gap between Single-User and Multi-User VoiceFilter-Lite\\u00a7r\\n\\n\\u00a78\\u00a7oRajeev Rikhye\\nQuan Wang\\nQiao Liang\\nYanzhang He\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.12169\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 26 Apr 2022 20:55:58 GMT)\\u00a7r"}']}
{title:'Tak et al. (§72022§r)', author: 'Hemlata Tak; Massimiliano Todisco; Xin Wang; Jee-weon Jung; Junichi Yamagishi; Nicholas Evans', display:{Lore:['[{"text": "arXiv:2202.12233", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic speaker verification spoofing and deepfake detection using wav2vec 2.0 and data augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oHemlata Tak\\nMassimiliano Todisco\\nXin Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.12233\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 28 Feb 2022 11:50:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Speaker Odyssey Workshop 2022\\u00a7r"}']}
{title:'Lu et al. (§72022§r)', author: 'Yen-Ju Lu; Samuele Cornell; Xuankai Chang; Wangyou Zhang; Chenda Li; Zhaoheng Ni; Zhong-Qiu Wang; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2202.12298", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Low-distortion Multi-channel Speech Enhancement: The ESPNet-SE Submission to The L3DAS22 Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oYen-Ju Lu\\nSamuele Cornell\\nXuankai Chang\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.12298\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Feb 2022 18:58:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oto be published in IEEE ICASSP 2022\\u00a7r"}']}
{title:'Zhu et al. (§72022§r)', author: 'Yunzheng Zhu; Ruchao Fan; Abeer Alwan', display:{Lore:['[{"text": "arXiv:2202.12326", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Better Meta-Initialization with Task Augmentation for Kindergarten-aged Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYunzheng Zhu\\nRuchao Fan\\nAbeer Alwan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.12326\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Feb 2022 19:20:37 GMT)\\u00a7r"}']}
{title:'C et al. (§72022§r)', author: 'Kishan K C; Zhenning Tan; Long Chen; Minho Jin; Eunjung Han; Andreas Stolcke; Chul Lee', display:{Lore:['[{"text": "arXiv:2202.12349", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lopenFEAT: Improving Speaker Identification by Open-set Few-shot Embedding Adaptation with Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oKishan K C\\nZhenning Tan\\nLong Chen\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.12349\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747613\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. IEEE ICASSP, May 2022, pp. 7062-7066\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Feb 2022 20:23:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proc. IEEE ICASSP 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Tianrui Wang; Weibin Zhu; Yingying Gao; Yanan Chen; Junlan Feng; Shilei Zhang', display:{Lore:['[{"text": "arXiv:2202.12643", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHarmonic gated compensation network plus for ICASSP 2022 DNS CHALLENGE\\u00a7r\\n\\n\\u00a78\\u00a7oTianrui Wang\\nWeibin Zhu\\nYingying Gao\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.12643\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Feb 2022 12:07:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Faundez-Zanuy et al. (§72022§r)', author: 'Marcos Faundez-Zanuy; Enric Monte-Moreno', display:{Lore:['[{"text": "arXiv:2202.12705", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lState-of-the-art in speaker recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos Faundez-Zanuy\\nEnric Monte-Moreno\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.12705\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/MAES.2005.1432568\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Aerospace and Electronic Systems Magazine, vol. 20, no. 5,\\n  pp. 7-12, May 2005\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 23 Feb 2022 11:49:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages. arXiv admin note:text overlap with arXiv:2202.11459\\u00a7r"}']}
{title:'Havtorn et al. (§72022§r)', author: 'Jakob D. Havtorn; Lasse Borgholt; Søren Hauberg; Jes Frellsen; Lars Maaløe', display:{Lore:['[{"text": "arXiv:2202.12707", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBenchmarking Generative Latent Variable Models for Speech\\u00a7r\\n\\n\\u00a78\\u00a7oJakob D. Havtorn\\nLasse Borgholt\\nS\\u00f8ren Hauberg\\nJes Frellsen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.12707\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 5 Apr 2022 11:23:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the 2022 ICLR workshop on Deep Generative Modelsfor Highly Structured Data (https://deep-gen-struct.github.io)\\u00a7r"}']}
{title:'García et al. (§72022§r)', author: 'Mario Alejandro García; Ana Lorena Rosset', display:{Lore:['[{"text": "arXiv:2202.12957", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Neural Network for Automatic Assessment of Dysphonia\\u00a7r\\n\\n\\u00a78\\u00a7oMario Alejandro Garc\\u00eda\\nAna Lorena Rosset\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.12957\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Feb 2022 20:13:32 GMT)\\u00a7r"}']}
{title:'Ren et al. (§72022§r)', author: 'Yi Ren; Xu Tan; Tao Qin; Zhou Zhao; Tie-Yan Liu', display:{Lore:['[{"text": "arXiv:2202.13066", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRevisiting Over-Smoothness in Text to Speech\\u00a7r\\n\\n\\u00a78\\u00a7oYi Ren\\nXu Tan\\nTao Qin\\nZhou Zhao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.13066\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 26 Feb 2022 05:22:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ACL 2022\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Jinglin Liu; Chengxi Li; Yi Ren; Zhiying Zhu; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2202.13277", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning the Beauty in Songs: Neural Singing Voice Beautifier\\u00a7r\\n\\n\\u00a78\\u00a7oJinglin Liu\\nChengxi Li\\nYi Ren\\nZhiying Zhu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.13277\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 2 Mar 2022 11:22:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ACL 2022 Main conference; Code: https://github.com/MoonInTheRiver/NeuralSVB\\u00a7r"}']}
{title:'Dubey et al. (§72022§r)', author: 'Harishchandra Dubey; Vishak Gopal; Ross Cutler; Ashkan Aazami; Sergiy Matusevych; Sebastian Braun; Sefik Emre Eskimez; Manthan Thakker; Takuya Yoshioka; Hannes Gamper; Robert Aichner', display:{Lore:['[{"text": "arXiv:2202.13288", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lICASSP 2022 Deep Noise Suppression Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oHarishchandra Dubey\\nVishak Gopal\\nRoss Cutler\\n+ 7 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.13288\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 27 Feb 2022 04:28:57 GMT)\\u00a7r"}']}
{title:'Cutler et al. (§72022§r)', author: 'Ross Cutler; Ando Saabas; Tanel Parnamaa; Marju Purin; Hannes Gamper; Sebastian Braun; Karsten Sørensen; Robert Aichner', display:{Lore:['[{"text": "arXiv:2202.13290", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lICASSP 2022 Acoustic Echo Cancellation Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oRoss Cutler\\nAndo Saabas\\nTanel Parnamaa\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.13290\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 27 Feb 2022 04:32:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2009.04972\\u00a7r"}']}
{title:'Ge et al. (§72022§r)', author: 'Wanying Ge; Massimiliano Todisco; Nicholas Evans', display:{Lore:['[{"text": "arXiv:2202.13693", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExplainable deepfake and spoofing detection: an attack analysis using SHapley Additive exPlanations\\u00a7r\\n\\n\\u00a78\\u00a7oWanying Ge\\nMassimiliano Todisco\\nNicholas Evans\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.13693\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 4 May 2022 09:14:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Speaker Odyssey Workshop 2022\\u00a7r"}']}
{title:'Kuzmin et al. (§72022§r)', author: 'Nikita Kuzmin; Igor Fedorov; Alexey Sholokhov', display:{Lore:['[{"text": "arXiv:2202.13826", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMagnitude-aware Probabilistic Speaker Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oNikita Kuzmin\\nIgor Fedorov\\nAlexey Sholokhov\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.13826\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Odyssey.2022-1\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 23 Oct 2022 14:05:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Odyssey 2022: The Speaker and Language Recognition Workshop, camera-ready version\\u00a7r"}']}
{title:'Shor et al. (§72022§r)', author: 'Joel Shor; Subhashini Venugopalan', display:{Lore:['[{"text": "arXiv:2203.00236", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTRILLsson: Distilled Universal Paralinguistic Speech Representations\\u00a7r\\n\\n\\u00a78\\u00a7oJoel Shor\\nSubhashini Venugopalan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.00236\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-118\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2022, 356-360\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 20 Mar 2022 21:13:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Martín-Doñas et al. (§72022§r)', author: 'Juan M. Martín-Doñas; Aitor Álvarez', display:{Lore:['[{"text": "arXiv:2203.01573", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Vicomtech Audio Deepfake Detection System based on Wav2Vec2 for the 2022 ADD Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oJuan M. Mart\\u00edn-Do\\u00f1as\\nAitor \\u00c1lvarez\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.01573\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 3 Mar 2022 08:49:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Haubner et al. (§72022§r)', author: 'Thomas Haubner; Walter Kellermann', display:{Lore:['[{"text": "arXiv:2203.01793", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning-Based Joint Control of Acoustic Echo Cancellation, Beamforming and Postfiltering\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Haubner\\nWalter Kellermann\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.01793\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 10 Aug 2022 06:49:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for European Signal Processing Conference (EUSIPCO) 2022, Belgrade, Serbia\\u00a7r"}']}
{title:'Faundez-Zanuy et al. (§72022§r)', author: 'Marcos Faundez-Zanuy; Oscar Oliva-Suarez', display:{Lore:['[{"text": "arXiv:2203.01818", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lADPCM with nonlinear prediction\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos Faundez-Zanuy\\nOscar Oliva-Suarez\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.01818\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n9th European Signal Processing Conference (EUSIPCO 1998), 1998,\\n  pp. 1-4\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Feb 2022 00:17:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages\\u00a7r"}']}
{title:'Faundez-Zanuy et al. (§72022§r)', author: 'Marcos Faundez-Zanuy; Francesc Vallverdu-Bayes', display:{Lore:['[{"text": "arXiv:2203.01819", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech segmentation using multilevel hybrid filters\\u00a7r\\n\\n\\u00a78\\u00a7oMarcos Faundez-Zanuy\\nFrancesc Vallverdu-Bayes\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.01819\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n1996 8th European Signal Processing Conference (EUSIPCO 1996),\\n  1996, pp. 1-4\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Feb 2022 00:03:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages\\u00a7r"}']}
{title:'Fu et al. (§72022§r)', author: 'Kaiqi Fu; Shaojun Gao; Kai Wang; Wei Li; Xiaohai Tian; Zejun Ma', display:{Lore:['[{"text": "arXiv:2203.01826", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Non-native Word-level Pronunciation Scoring with Phone-level Mixup Data Augmentation and Multi-source Information\\u00a7r\\n\\n\\u00a78\\u00a7oKaiqi Fu\\nShaojun Gao\\nKai Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.01826\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Mar 2022 05:29:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures. This paper is submitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Borgholt et al. (§72022§r)', author: 'Lasse Borgholt; Jakob Drachmann Havtorn; Joakim Edin; Lars Maaløe; Christian Igel', display:{Lore:['[{"text": "arXiv:2203.01829", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Brief Overview of Unsupervised Neural Speech Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oLasse Borgholt\\nJakob Drachmann Havtorn\\nJoakim Edin\\nLars Maal\\u00f8e\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.01829\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Mar 2022 11:15:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe 2nd Workshop on Self-supervised Learning for Audio and Speech Processing (SAS) at AAAI\\u00a7r"}']}
{title:'Satue-Villar et al. (§72022§r)', author: 'Antonio Satue-Villar; Marcos Faundez-Zanuy', display:{Lore:['[{"text": "arXiv:2203.01992", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the relevance of language in speaker recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAntonio Satue-Villar\\nMarcos Faundez-Zanuy\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.01992\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n6th European Conference on Speech Communication and Technology\\n  (EUROSPEECH 99) Budapest, Hungary, September 5-9, 1999\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Mar 2022 10:31:19 GMT)\\u00a7r"}']}
{title:'Park et al. (§72022§r)', author: 'Hyun Joon Park; Byung Ha Kang; Wooseok Shin; Jin Sob Kim; Sung Won Han', display:{Lore:['[{"text": "arXiv:2203.02181", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMANNER: Multi-view Attention Network for Noise Erasure\\u00a7r\\n\\n\\u00a78\\u00a7oHyun Joon Park\\nByung Ha Kang\\nWooseok Shin\\nJin Sob Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.02181\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Mar 2022 08:27:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP 2022\\u00a7r"}']}
{title:'Liang et al. (§72022§r)', author: 'Yunhao Liang; Yanhua Long; Yijie Li; Jiaen Liang', display:{Lore:['[{"text": "arXiv:2203.02191", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelective Pseudo-labeling and Class-wise Discriminative Fusion for Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oYunhao Liang\\nYanhua Long\\nYijie Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.02191\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Mar 2022 08:54:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis article was submitted toInterspeech 2022\\u00a7r"}']}
{title:'Ge et al. (§72022§r)', author: 'Xiaofeng Ge; Jiangyu Han; Yanhua Long; Haixin Guan', display:{Lore:['[{"text": "arXiv:2203.02263", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPercepNet+: A Phase and SNR Aware PercepNet for Real-Time Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oXiaofeng Ge\\nJiangyu Han\\nYanhua Long\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.02263\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Mar 2022 12:23:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis article was submitted toInterspeech 2022\\u00a7r"}']}
{title:'Fang et al. (§72022§r)', author: 'Huajian Fang; Tal Peer; Stefan Wermter; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2203.02288", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntegrating Statistical Uncertainty into Neural Network-Based Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oHuajian Fang\\nTal Peer\\nStefan Wermter\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.02288\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747642\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2022 - 2022 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Mar 2022 12:55:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or "}','{"text": "promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Xubo Liu; Xinhao Mei; Qiushi Huang; Jianyuan Sun; Jinzheng Zhao; Haohe Liu; Mark D. Plumbley; Volkan Kılıç; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2203.02838", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Pre-trained BERT for Audio Captioning\\u00a7r\\n\\n\\u00a78\\u00a7oXubo Liu\\nXinhao Mei\\nQiushi Huang\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.02838\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 27 Mar 2022 22:52:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to the 30th European Signal Processing Conference (EUSIPCO), 5 pages, 2 figures\\u00a7r"}']}
{title:'Lee et al. (§72022§r)', author: 'Gyeong-Tae Lee; Sang-Min Choi; Byeong-Yun Ko; Yong-Hwa Park', display:{Lore:['[{"text": "arXiv:2203.03166", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHRTF measurement for accurate sound localization cues\\u00a7r\\n\\n\\u00a78\\u00a7oGyeong-Tae Lee\\nSang-Min Choi\\nByeong-Yun Ko\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.03166\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Apr 2022 03:31:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o39 pages, 27 figures, and 1table\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Hexin Liu; Leibny Paola Garcia Perera; Andy W. H. Khong; Justin Dauwels; Suzy J. Styles; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:2203.03218", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhance Language Identification using Dual-mode Model with Knowledge Distillation\\u00a7r\\n\\n\\u00a78\\u00a7oHexin Liu\\nLeibny Paola Garcia Perera\\nAndy W. H. Khong\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.03218\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Mar 2022 09:12:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Odyssey 2022\\u00a7r"}']}
{title:'Berghi et al. (§72022§r)', author: 'Davide Berghi; Adrian Hilton; Philip J. B. Jackson', display:{Lore:['[{"text": "arXiv:2203.03291", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisually Supervised Speaker Detection and Localization via Microphone Array\\u00a7r\\n\\n\\u00a78\\u00a7oDavide Berghi\\nAdrian Hilton\\nPhilip J. B. Jackson\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.03291\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE 23rd International Workshop on Multimedia Signal Processing\\n  (MMSP), 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Mar 2022 11:12:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oErratum: Due to abug in the evaluation script, the correct average distance (aD) metric is here reportedin yellow. The analysis remains unchanged from the original paper as the trend between the old and new measures are"}','{"text": "perfectly monotonic. The bug was caused by an incorrect normalization factor\\u00a7r"}']}
{title:'Sun et al. (§72022§r)', author: 'Jianyuan Sun; Xubo Liu; Xinhao Mei; Jinzheng Zhao; Mark D. Plumbley; Volkan Kılıç; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2203.03436", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Neural Decision Forest for Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oJianyuan Sun\\nXubo Liu\\nXinhao Mei\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.03436\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Mar 2022 14:39:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to the 30th European Signal Processing Conference (EUSIPCO), 5 pages, 2 figures\\u00a7r"}']}
{title:'Slizovskaia et al. (§72022§r)', author: 'Olga Slizovskaia; Gordon Wichern; Zhong-Qiu Wang; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2203.04197", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLocate This, Not That: Class-Conditioned Sound Event DOA Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oOlga Slizovskaia\\nGordon Wichern\\nZhong-Qiu Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.04197\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Mar 2022 16:49:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at ICASSP 2022\\u00a7r"}']}
{title:'Lotfidereshgi et al. (§72022§r)', author: 'Reza Lotfidereshgi; Philippe Gournay', display:{Lore:['[{"text": "arXiv:2203.04415", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPractical cognitive speech compression\\u00a7r\\n\\n\\u00a78\\u00a7oReza Lotfidereshgi\\nPhilippe Gournay\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.04415\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Mar 2022 21:44:38 GMT)\\u00a7r"}']}
{title:'Parikh et al. (§72022§r)', author: 'Rahil Parikh; Ilya Kavalerov; Carol Espy-Wilson; Shihab Shamma', display:{Lore:['[{"text": "arXiv:2203.04420", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHarmonicity Plays a Critical Role in DNN Based Versus in Biologically-Inspired Monaural Speech Segregation Systems\\u00a7r\\n\\n\\u00a78\\u00a7oRahil Parikh\\nIlya Kavalerov\\nCarol Espy-Wilson\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.04420\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Mar 2022 21:51:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, IEEE International Conference on Acoustics, Speech, Signal Processing (ICASSP), 2022\\u00a7r"}']}
{title:'Lu et al. (§72022§r)', author: 'Yizhou Lu; Mingkun Huang; Xinghua Qu; Pengfei Wei; Zejun Ma', display:{Lore:['[{"text": "arXiv:2203.04583", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLanguage Adaptive Cross-lingual Speech Representation Learning with Sparse Sharing Sub-networks\\u00a7r\\n\\n\\u00a78\\u00a7oYizhou Lu\\nMingkun Huang\\nXinghua Qu\\nPengfei Wei\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.04583\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 9 Mar 2022 09:01:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP 2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Yike Zhang; Xiaobing Feng; Yi Liu; Songjun Cao; Long Ma', display:{Lore:['[{"text": "arXiv:2203.04767", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA practical framework for multi-domain speech recognition and an instance sampling method to neural language modeling\\u00a7r\\n\\n\\u00a78\\u00a7oYike Zhang\\nXiaobing Feng\\nYi Liu\\nSongjun Cao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.04767\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 9 Mar 2022 14:33:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 1 figure\\u00a7r"}']}
{title:'Ramesh et al. (§72022§r)', author: 'Krithika Ramesh; Ashiqur R. KhudaBukhsh; Sumeet Kumar', display:{Lore:['[{"text": "arXiv:2203.04837", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CY\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l\'Beach\' to \'Bitch\': Inadvertent Unsafe Transcription of Kids\' Content on YouTube\\u00a7r\\n\\n\\u00a78\\u00a7oKrithika Ramesh\\nAshiqur R. KhudaBukhsh\\nSumeet Kumar\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.04837\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Feb 2022 19:19:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper got accepted at AAAI 2022, AI for Social Impact track\\u00a7r"}']}
{title:'Daniel et al. (§72022§r)', author: 'Jérôme Daniel; Srđan Kitić', display:{Lore:['[{"text": "arXiv:2203.05265", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEcho-enabled Direction-of-Arrival and range estimation of a mobile source in Ambisonic domain\\u00a7r\\n\\n\\u00a78\\u00a7oJ\\u00e9r\\u00f4me Daniel\\nSr\\u0111an Kiti\\u0107\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.05265\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Mar 2022 09:53:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted\\u00a7r"}']}
{title:'Bayerl et al. (§72022§r)', author: 'Sebastian P. Bayerl; Alexander Wolff von Gudenberg; Florian Hönig; Elmar Nöth; Korbinian Riedhammer', display:{Lore:['[{"text": "arXiv:2203.05383", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKSoF: The Kassel State of Fluency Dataset \\u2013 A Therapy Centered Dataset of Stuttering\\u00a7r\\n\\n\\u00a78\\u00a7oSebastian P. Bayerl\\nAlexander Wolff von Gudenberg\\nFlorian H\\u00f6nig\\nElmar N\\u00f6th\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.05383\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 16 Jun 2022 11:29:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at LREC 2022 Conference on Language Resources and Evaluation\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Mohan Li; Shucong Zhang; Catalin Zorila; Rama Doddipatla', display:{Lore:['[{"text": "arXiv:2203.05736", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransformer-based Streaming ASR with Cumulative Attention\\u00a7r\\n\\n\\u00a78\\u00a7oMohan Li\\nShucong Zhang\\nCatalin Zorila\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.05736\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Mar 2022 03:22:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, accepted at ICASSP 2022\\u00a7r"}']}
{title:'Parikh et al. (§72022§r)', author: 'Rahil Parikh; Nadee Seneviratne; Ganesh Sivaraman; Shihab Shamma; Carol Espy-Wilson', display:{Lore:['[{"text": "arXiv:2203.05780", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic To Articulatory Speech Inversion Using Multi-Resolution Spectro-Temporal Representations Of Speech Signals\\u00a7r\\n\\n\\u00a78\\u00a7oRahil Parikh\\nNadee Seneviratne\\nGanesh Sivaraman\\nShihab Shamma\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.05780\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 25 Jun 2022 07:26:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ISCAInterspeech 2022\\u00a7r"}']}
{title:'Singh et al. (§72022§r)', author: 'Vishwanath Pratap Singh; Hardik Sailor; Supratik Bhattacharya; Abhishek Pandey', display:{Lore:['[{"text": "arXiv:2203.06600", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpectral Modification Based Data Augmentation For Improving End-to-End ASR For Children\'s Speech\\u00a7r\\n\\n\\u00a78\\u00a7oVishwanath Pratap Singh\\nHardik Sailor\\nSupratik Bhattacharya\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.06600\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 13 Mar 2022 08:46:31 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Zehua Zhang; Lu Zhang; Xuyi Zhuang; Yukun Qian; Heng Li; Mingjiang Wang', display:{Lore:['[{"text": "arXiv:2203.07684", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFB-MSTCN: A Full-Band Single-Channel Speech Enhancement Method Based on Multi-Scale Temporal Convolutional Network\\u00a7r\\n\\n\\u00a78\\u00a7oZehua Zhang\\nLu Zhang\\nXuyi Zhuang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.07684\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Mar 2022 06:55:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022, Deep Noise Suppression Challenge\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'Zili Huang; Shinji Watanabe; Shu-wen Yang; Paola Garcia; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:2203.07960", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating self-supervised learning for speech enhancement and separation\\u00a7r\\n\\n\\u00a78\\u00a7oZili Huang\\nShinji Watanabe\\nShu-wen Yang\\nPaola Garcia\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.07960\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Mar 2022 14:43:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP 2022\\u00a7r"}']}
{title:'Merritt et al. (§72022§r)', author: 'Thomas Merritt; Abdelhamid Ezzerg; Piotr Biliński; Magdalena Proszewska; Kamil Pokora; Roberto Barra-Chicote; Daniel Korzekwa', display:{Lore:['[{"text": "arXiv:2203.08009", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lText-free non-parallel many-to-many voice conversion using normalising flows\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Merritt\\nAbdelhamid Ezzerg\\nPiotr Bili\\u0144ski\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.08009\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Mar 2022 15:45:54 GMT)\\u00a7r"}']}
{title:'Jung et al. (§72022§r)', author: 'Jee-weon Jung; You Jin Kim; Hee-Soo Heo; Bong-Jin Lee; Youngki Kwon; Joon Son Chung', display:{Lore:['[{"text": "arXiv:2203.08488", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPushing the limits of raw waveform speaker recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJee-weon Jung\\nYou Jin Kim\\nHee-Soo Heo\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.08488\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 29 Mar 2022 01:34:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to INTERSPEECH 2022 as a conference paper. 5 pages, 2 figures, 5 tables\\u00a7r"}']}
{title:'Feng et al. (§72022§r)', author: 'Tiantian Feng; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2203.08810", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-FedSER: Semi-supervised Learning for Speech Emotion Recognition On Federated Learning using Multiview Pseudo-Labeling\\u00a7r\\n\\n\\u00a78\\u00a7oTiantian Feng\\nShrikanth Narayanan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.08810\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-141\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Mar 2022 21:50:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper was submitted to Insterspeech 2022 forreview\\u00a7r"}']}
{title:'Peri et al. (§72022§r)', author: 'Raghuveer Peri; Krishna Somandepalli; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2203.09122", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTo train or not to train adversarially: A study of bias mitigation strategies for speaker recognition\\u00a7r\\n\\n\\u00a78\\u00a7oRaghuveer Peri\\nKrishna Somandepalli\\nShrikanth Narayanan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.09122\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Mar 2022 06:56:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint submitted to Computer Speech and Language (Elsevier)\\u00a7r"}']}
{title:'Hung et al. (§72022§r)', author: 'Yun-Ning Hung; Alexander Lerch', display:{Lore:['[{"text": "arXiv:2203.09132", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFeature-informed Latent Space Regularization for Music Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oYun-Ning Hung\\nAlexander Lerch\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.09132\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Jun 2022 16:44:50 GMT)\\u00a7r"}']}
{title:'Champion et al. (§72022§r)', author: 'Pierre Champion; Denis Jouvet; Anthony Larcher', display:{Lore:['[{"text": "arXiv:2203.09518", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrivacy-Preserving Speech Representation Learning using Vector Quantization\\u00a7r\\n\\n\\u00a78\\u00a7oPierre Champion\\nDenis Jouvet\\nAnthony Larcher\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.09518\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Mar 2022 14:01:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oJourn\\u00e9es d\'\\u00c9tudes sur la Parole - JEP2022, Jun 2022, \\u00cele de Noirmoutier, France\\u00a7r"}']}
{title:'Bai et al. (§72022§r)', author: 'He Bai; Renjie Zheng; Junkun Chen; Xintong Li; Mingbo Ma; Liang Huang', display:{Lore:['[{"text": "arXiv:2203.09690", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA^3T: Alignment-Aware Acoustic and Text Pretraining for Speech Synthesis and Editing\\u00a7r\\n\\n\\u00a78\\u00a7oHe Bai\\nRenjie Zheng\\nJunkun Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.09690\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 18 Jun 2022 04:52:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICML 2022, 12 pages, 10 figures\\u00a7r"}']}
{title:'Yokota (§72022§r)', author: 'Tatsuya Yokota', display:{Lore:['[{"text": "arXiv:2203.09746", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSoft Smoothness for Audio Inpainting Using a Latent Matrix Model in Delay-embedded Space\\u00a7r\\n\\n\\u00a78\\u00a7oTatsuya Yokota\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.09746\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Mar 2022 04:58:18 GMT)\\u00a7r"}']}
{title:'Mathad et al. (§72022§r)', author: 'Vikram C. Mathad; Julie M. Liss; Kathy Chapman; Nancy Scherer; Visar Berisha', display:{Lore:['[{"text": "arXiv:2203.10054", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConsonant-Vowel Transition Models Based on Deep Learning for Objective Evaluation of Articulation\\u00a7r\\n\\n\\u00a78\\u00a7oVikram C. Mathad\\nJulie M. Liss\\nKathy Chapman\\nNancy Scherer\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.10054\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Mar 2022 16:58:26 GMT)\\u00a7r"}']}
{title:'Hu et al. (§72022§r)', author: 'Shujie Hu; Shansong Liu; Xurong Xie; Mengzhe Geng; Tianzi Wang; Shoukang Hu; Mingyu Cui; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2203.10274", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting Cross Domain Acoustic-to-articulatory Inverted Features For Disordered Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oShujie Hu\\nShansong Liu\\nXurong Xie\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.10274\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 19 Mar 2022 08:47:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by ICASSP 2022\\u00a7r"}']}
{title:'Silnova et al. (§72022§r)', author: 'Anna Silnova; Themos Stafylakis; Ladislav Mosner; Oldrich Plchot; Johan Rohdin; Pavel Matejka; Lukas Burget; Ondrej Glembek; Niko Brummer', display:{Lore:['[{"text": "arXiv:2203.10300", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalyzing speaker verification embedding extractors and back-ends under language and channel mismatch\\u00a7r\\n\\n\\u00a78\\u00a7oAnna Silnova\\nThemos Stafylakis\\nLadislav Mosner\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.10300\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 19 Mar 2022 11:21:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Odyssey 2022, under review\\u00a7r"}']}
{title:'Raitio et al. (§72022§r)', author: 'Tuomo Raitio; Petko Petkov; Jiangchuan Li; Muhammed Shifas; Andrea Davis; Yannis Stylianou', display:{Lore:['[{"text": "arXiv:2203.10637", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVocal effort modeling in neural TTS for improving the intelligibility of synthetic speech in noise\\u00a7r\\n\\n\\u00a78\\u00a7oTuomo Raitio\\nPetko Petkov\\nJiangchuan Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.10637\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 29 Mar 2022 02:45:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures. Submitted to Interspeech 2022, revision includes more data in results and improved text\\u00a7r"}']}
{title:'Heo et al. (§72022§r)', author: 'Dongseok Heo; Cheul Young Park; Jaemin Cheun; Myung Jin Ko', display:{Lore:['[{"text": "arXiv:2203.10827", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSeparating Content from Speaker Identity in Speech for the Assessment of Cognitive Impairments\\u00a7r\\n\\n\\u00a78\\u00a7oDongseok Heo\\nCheul Young Park\\nJaemin Cheun\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.10827\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Mar 2022 09:29:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, submitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Haoyu Li; Yun Liu; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2203.11500", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Noise Reduction and Listening Enhancement for Full-End Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oHaoyu Li\\nYun Liu\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.11500\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Mar 2022 07:21:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Haici Yang; Sanna Wager; Spencer Russell; Mike Luo; Minje Kim; Wontak Kim', display:{Lore:['[{"text": "arXiv:2203.12053", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUpmixing via style transfer: a variational autoencoder for disentangling spatial images and musical content\\u00a7r\\n\\n\\u00a78\\u00a7oHaici Yang\\nSanna Wager\\nSpencer Russell\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.12053\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Mar 2022 21:25:18 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Hexin Liu; Leibny Paola Garcia Perera; Andy W. H. Khong; Suzy J. Styles; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:2203.12366", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPHO-LID: A Unified Model Incorporating Acoustic-Phonetic and Phonotactic Information for Language Identification\\u00a7r\\n\\n\\u00a78\\u00a7oHexin Liu\\nLeibny Paola Garcia Perera\\nAndy W. H. Khong\\nSuzy J. Styles\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.12366\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 31 Mar 2022 05:59:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022, updated to the submitted version\\u00a7r"}']}
{title:'Tomashenko et al. (§72022§r)', author: 'Natalia Tomashenko; Xin Wang; Xiaoxiao Miao; Hubert Nourtel; Pierre Champion; Massimiliano Todisco; Emmanuel Vincent; Nicholas Evans; Junichi Yamagishi; Jean-François Bonastre', display:{Lore:['[{"text": "arXiv:2203.12468", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CR\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe VoicePrivacy 2022 Challenge Evaluation Plan\\u00a7r\\n\\n\\u00a78\\u00a7oNatalia Tomashenko\\nXin Wang\\nXiaoxiao Miao\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.12468\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 28 Sep 2022 11:00:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7othe file is unchanged; minor correction in metadata\\u00a7r"}']}
{title:'Biadsy et al. (§72022§r)', author: 'Fadi Biadsy; Youzheng Chen; Xia Zhang; Oleg Rybakov; Andrew Rosenberg; Pedro J. Moreno', display:{Lore:['[{"text": "arXiv:2203.12559", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Scalable Model Specialization Framework for Training and Inference using Submodels and its Application to Speech Model Personalization\\u00a7r\\n\\n\\u00a78\\u00a7oFadi Biadsy\\nYouzheng Chen\\nXia Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.12559\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10613\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 13 Sep 2022 20:34:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH\\u00a7r"}']}
{title:'Nathwani et al. (§72022§r)', author: 'Karan Nathwani; Bhavya Dixit; Sunil Kumar Kopparapu', display:{Lore:['[{"text": "arXiv:2203.13259", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComputing Optimal Location of Microphone for Improved Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oKaran Nathwani\\nBhavya Dixit\\nSunil Kumar Kopparapu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.13259\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Mar 2022 14:27:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Kum et al. (§72022§r)', author: 'Sangeun Kum; Jongpil Lee; Keunhyoung Luke Kim; Taehyoung Kim; Juhan Nam', display:{Lore:['[{"text": "arXiv:2203.13422", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPseudo-Label Transfer from Frame-Level to Note-Level in a Teacher-Student Framework for Singing Transcription from Polyphonic Music\\u00a7r\\n\\n\\u00a78\\u00a7oSangeun Kum\\nJongpil Lee\\nKeunhyoung Luke Kim\\nTaehyoung Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.13422\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 30 Mar 2022 01:08:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at the 45th International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2022)\\u00a7r"}']}
{title:'Lam et al. (§72022§r)', author: 'Max W. Y. Lam; Jun Wang; Dan Su; Dong Yu', display:{Lore:['[{"text": "arXiv:2203.13508", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBDDM: Bilateral Denoising Diffusion Models for Fast and High-Quality Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oMax W. Y. Lam\\nJun Wang\\nDan Su\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.13508\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInternational Conference on Learning Representations 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Mar 2022 08:53:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ICLR 2022. arXiv admin note: text overlap with arXiv:2108.11514\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Xue Yang; Changchun Bao', display:{Lore:['[{"text": "arXiv:2203.13574", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmbedding Recurrent Layers with Dual-Path Strategy in a Variant of Convolutional Network for Speaker-Independent Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oXue Yang\\nChangchun Bao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.13574\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 16 Jun 2022 09:32:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Sharma et al. (§72022§r)', author: 'Dushyant Sharma; Rong Gong; James Fosburgh; Stanislav Yu. Kruchinin; Patrick A. Naylor; Ljubomir Milanovic', display:{Lore:['[{"text": "arXiv:2203.13919", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatial Processing Front-End For Distant ASR Exploiting Self-Attention Channel Combinator\\u00a7r\\n\\n\\u00a78\\u00a7oDushyant Sharma\\nRong Gong\\nJames Fosburgh\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.13919\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Mar 2022 21:43:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oto be presented at ICASSP 2022\\u00a7r"}']}
{title:'Zhou et al. (§72022§r)', author: 'Yao Zhou; Changchun Bao', display:{Lore:['[{"text": "arXiv:2203.14010", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Neural Vocoder Based Packet Loss Concealment Algorithm\\u00a7r\\n\\n\\u00a78\\u00a7oYao Zhou\\nChangchun Bao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14010\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 26 Mar 2022 07:15:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 8 figures, already submitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Saijo et al. (§72022§r)', author: 'Kohei Saijo; Tetsuji Ogawa', display:{Lore:['[{"text": "arXiv:2203.14080", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRemix-cycle-consistent Learning on Adversarially Learned Separator for Accurate and Stable Unsupervised Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oKohei Saijo\\nTetsuji Ogawa\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14080\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 26 Mar 2022 13:49:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2022\\u00a7r"}']}
{title:'Chan et al. (§72022§r)', author: 'Chak Ho Chan; Kaizhi Qian; Yang Zhang; Mark Hasegawa-Johnson', display:{Lore:['[{"text": "arXiv:2203.14156", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeechSplit 2.0: Unsupervised speech disentanglement for voice conversion Without tuning autoencoder Bottlenecks\\u00a7r\\n\\n\\u00a78\\u00a7oChak Ho Chan\\nKaizhi Qian\\nYang Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14156\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 26 Mar 2022 21:01:26 GMT)\\u00a7r"}']}
{title:'Tran et al. (§72022§r)', author: 'Minh Tran; Mohammad Soleymani', display:{Lore:['[{"text": "arXiv:2203.14171", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Speech Representation Anonymization Framework via Selective Noise Perturbation\\u00a7r\\n\\n\\u00a78\\u00a7oMinh Tran\\nMohammad Soleymani\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14171\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 28 Oct 2022 00:55:08 GMT)\\u00a7r"}']}
{title:'Lin et al. (§72022§r)', author: 'Guan-Ting Lin; Shang-Wen Li; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2203.14222", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lListen, Adapt, Better WER: Source-free Single-utterance Test-time Adaptation for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGuan-Ting Lin\\nShang-Wen Li\\nHung-yi Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14222\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 21 Jun 2022 16:06:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Park et al. (§72022§r)', author: 'Sangjun Park; Kihyun Choo; Joohyung Lee; Anton V. Porov; Konstantin Osipov; June Sig Sung', display:{Lore:['[{"text": "arXiv:2203.14416", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBunched LPCNet2: Efficient Neural Vocoders Covering Devices from Cloud to Edge\\u00a7r\\n\\n\\u00a78\\u00a7oSangjun Park\\nKihyun Choo\\nJoohyung Lee\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14416\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 30 Jun 2022 07:20:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2022\\u00a7r"}']}
{title:'Zhou et al. (§72022§r)', author: 'Jing Zhou; Changchun Bao', display:{Lore:['[{"text": "arXiv:2203.14494", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-source wideband doa estimation method by frequency focusing and error weighting\\u00a7r\\n\\n\\u00a78\\u00a7oJing Zhou\\nChangchun Bao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14494\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Mar 2022 04:44:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, this paper has been submitted to interspeech 2022, confirmation number: 445\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Xin Wang; Junich Yamagishi', display:{Lore:['[{"text": "arXiv:2203.14553", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating Active-learning-based Training Data Selection for Speech Spoofing Countermeasure\\u00a7r\\n\\n\\u00a78\\u00a7oXin Wang\\nJunich Yamagishi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14553\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 7 Oct 2022 12:45:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proc. SLT 2022, modified based ona paper rejected by Interspeech 2022\\u00a7r"}']}
{title:'Tan et al. (§72022§r)', author: 'Fengqi Tan; Changchun Bao', display:{Lore:['[{"text": "arXiv:2203.14561", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Effective Dereverberation Algorithm by Fusing MVDR and MCLP\\u00a7r\\n\\n\\u00a78\\u00a7oFengqi Tan\\nChangchun Bao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14561\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Mar 2022 08:05:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, this paper have been submitted to INTERSPEECH 2022, Confirmation number:546\\u00a7r"}']}
{title:'Raina et al. (§72022§r)', author: 'Akshay Raina; Vipul Arora', display:{Lore:['[{"text": "arXiv:2203.14639", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSyncNet: Using Causal Convolutions and Correlating Objective for Time Delay Estimation in Audio Signals\\u00a7r\\n\\n\\u00a78\\u00a7oAkshay Raina\\nVipul Arora\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14639\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Apr 2022 11:52:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022 conference\\u00a7r"}']}
{title:'Das et al. (§72022§r)', author: 'Shuvayanti Das; Jennifer Williams; Catherine Lai', display:{Lore:['[{"text": "arXiv:2203.14640", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalysis of Voice Conversion and Code-Switching Synthesis Using VQ-VAE\\u00a7r\\n\\n\\u00a78\\u00a7oShuvayanti Das\\nJennifer Williams\\nCatherine Lai\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14640\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Mar 2022 10:53:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Jung et al. (§72022§r)', author: 'Jee-weon Jung; Hemlata Tak; Hye-jin Shim; Hee-Soo Heo; Bong-Jin Lee; Soo-Whan Chung; Ha-Jin Yu; Nicholas Evans; Tomi Kinnunen', display:{Lore:['[{"text": "arXiv:2203.14732", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSASV 2022: The First Spoofing-Aware Speaker Verification Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oJee-weon Jung\\nHemlata Tak\\nHye-jin Shim\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14732\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Mar 2022 13:19:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 2 tables, submitted to Interspeech 2022 as a conference paper\\u00a7r"}']}
{title:'Das et al. (§72022§r)', author: 'Sneha Das; Nicole Nadine Lønfeldt; Anne Katrine Pagsberg; Line H. Clemmensen', display:{Lore:['[{"text": "arXiv:2203.14865", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Transferable Speech Emotion Representation: On loss functions for cross-lingual latent representations\\u00a7r\\n\\n\\u00a78\\u00a7oSneha Das\\nNicole Nadine L\\u00f8nfeldt\\nAnne Katrine Pagsberg\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14865\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Mar 2022 16:14:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint of paper accepted to be presented at the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022. Source code at https://bit.ly/34CgkSZ. arXiv admin note: text overlap with "}','{"text": "arXiv:2105.02055\\u00a7r"}']}
{title:'Das et al. (§72022§r)', author: 'Sneha Das; Nicklas Leander Lund; Nicole Nadine Lønfeldt; Anne Katrine Pagsberg; Line H. Clemmensen', display:{Lore:['[{"text": "arXiv:2203.14867", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContinuous Metric Learning For Transferable Speech Emotion Recognition and Embedding Across Low-resource Languages\\u00a7r\\n\\n\\u00a78\\u00a7oSneha Das\\nNicklas Leander Lund\\nNicole Nadine L\\u00f8nfeldt\\nAnne Katrine Pagsberg\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14867\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Mar 2022 16:14:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint of paper accepted to be presented at the Northern Lights Deep Learning Conference (NLDL), 2022. The labelsare available at: https://bit.ly/3rg6VsA\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Haohe Liu; Woosung Choi; Xubo Liu; Qiuqiang Kong; Qiao Tian; DeLiang Wang', display:{Lore:['[{"text": "arXiv:2203.14941", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Vocoder is All You Need for Speech Super-resolution\\u00a7r\\n\\n\\u00a78\\u00a7oHaohe Liu\\nWoosung Choi\\nXubo Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14941\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-11017\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Mar 2022 17:51:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Yuan et al. (§72022§r)', author: 'Siyuan Yuan; Zhepei Wang; Umut Isik; Ritwik Giri; Jean-Marc Valin; Michael M. Goodwin; Arvindh Krishnaswamy', display:{Lore:['[{"text": "arXiv:2203.15092", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved singing voice separation with chromagram-based pitch-aware remixing\\u00a7r\\n\\n\\u00a78\\u00a7oSiyuan Yuan\\nZhepei Wang\\nUmut Isik\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15092\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Mar 2022 20:55:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at ICASSP 2022, 5 pages, 1 figure\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Xubo Liu; Haohe Liu; Qiuqiang Kong; Xinhao Mei; Jinzheng Zhao; Qiushi Huang; Mark D. Plumbley; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2203.15147", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSeparate What You Describe: Language-Queried Audio Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oXubo Liu\\nHaohe Liu\\nQiuqiang Kong\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15147\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Mar 2022 23:47:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022, 5 pages, 3 figures\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Jialu Li; Mark Hasegawa-Johnson; Nancy L. McElwain', display:{Lore:['[{"text": "arXiv:2203.15183", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisualizations of Complex Sequences of Family-Infant Vocalizations Using Bag-of-Audio-Words Approach Based on Wav2vec 2.0 Features\\u00a7r\\n\\n\\u00a78\\u00a7oJialu Li\\nMark Hasegawa-Johnson\\nNancy L. McElwain\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15183\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Mar 2022 01:46:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Seong-Hu Kim; Hyeonuk Nam; Yong-Hwa Park', display:{Lore:['[{"text": "arXiv:2203.15277", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDecomposed Temporal Dynamic CNN: Efficient Time-Adaptive Network for Text-Independent Speaker Verification Explained with Speaker Activation Map\\u00a7r\\n\\n\\u00a78\\u00a7oSeong-Hu Kim\\nHyeonuk Nam\\nYong-Hwa Park\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15277\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 27 Oct 2022 11:58:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Mehlman et al. (§72022§r)', author: 'Nicholas Mehlman; Anirudh Sreeram; Raghuveer Peri; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2203.15283", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMel Frequency Spectral Domain Defenses against Adversarial Attacks on Speech Recognition Systems\\u00a7r\\n\\n\\u00a78\\u00a7oNicholas Mehlman\\nAnirudh Sreeram\\nRaghuveer Peri\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15283\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Mar 2022 06:58:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper is 5 pages long and was submitted to Interspeech 2022\\u00a7r"}']}
{title:'Nam et al. (§72022§r)', author: 'Hyeonuk Nam; Seong-Hu Kim; Byeong-Yun Ko; Yong-Hwa Park', display:{Lore:['[{"text": "arXiv:2203.15296", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrequency Dynamic Convolution: Frequency-Adaptive Pattern Recognition for Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oHyeonuk Nam\\nSeong-Hu Kim\\nByeong-Yun Ko\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15296\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 4 Jul 2022 02:37:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Ng et al. (§72022§r)', author: 'Si-Ioi Ng; Cymie Wing-Yee Ng; Jiarui Wang; Tan Lee', display:{Lore:['[{"text": "arXiv:2203.15405", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Detection of Speech Sound Disorder in Child Speech Using Posterior-based Speaker Representations\\u00a7r\\n\\n\\u00a78\\u00a7oSi-Ioi Ng\\nCymie Wing-Yee Ng\\nJiarui Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15405\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 29 Jun 2022 08:21:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Stafylakis et al. (§72022§r)', author: 'Themos Stafylakis; Ladislav Mošner; Oldřich Plchot; Johan Rohdin; Anna Silnova; Lukáš Burget; Jan "Honza\'\' Černocký', display:{Lore:['[{"text": "arXiv:2203.15436", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTraining Speaker Embedding Extractors Using Multi-Speaker Audio with Unknown Speaker Boundaries\\u00a7r\\n\\n\\u00a78\\u00a7oThemos Stafylakis\\nLadislav Mo\\u0161ner\\nOld\\u0159ich Plchot\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15436\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 Aug 2022 09:22:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2022\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Minchan Kim; Myeonghun Jeong; Byoung Jin Choi; Sunghwan Ahn; Joun Yeop Lee; Nam Soo Kim', display:{Lore:['[{"text": "arXiv:2203.15447", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransfer Learning Framework for Low-Resource Text-to-Speech using a Large-Scale Unlabeled Speech Corpus\\u00a7r\\n\\n\\u00a78\\u00a7oMinchan Kim\\nMyeonghun Jeong\\nByoung Jin Choi\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15447\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-225\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 6 Oct 2022 07:51:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech2022\\u00a7r"}']}
{title:'Dobashi et al. (§72022§r)', author: 'Akihiro Dobashi; Chee Siang Leow; Hiromitsu Nishizaki', display:{Lore:['[{"text": "arXiv:2203.15473", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrequency-Directional Attention Model for Multilingual Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAkihiro Dobashi\\nChee Siang Leow\\nHiromitsu Nishizaki\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15473\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Mar 2022 12:21:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to INTERSPEECH2022\\u00a7r"}']}
{title:'Mei et al. (§72022§r)', author: 'Xinhao Mei; Xubo Liu; Jianyuan Sun; Mark D. Plumbley; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2203.15537", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Metric Learning for Audio-Text Cross-Modal Retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oXinhao Mei\\nXubo Liu\\nJianyuan Sun\\nMark D. Plumbley\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15537\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 30 Jun 2022 10:53:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted to InterSpeech2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Rui Wang; Qibing Bai; Junyi Ao; Long Zhou; Zhixiang Xiong; Zhihua Wei; Yu Zhang; Tom Ko; Haizhou Li', display:{Lore:['[{"text": "arXiv:2203.15610", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT\\u00a7r\\n\\n\\u00a78\\u00a7oRui Wang\\nQibing Bai\\nJunyi Ao\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15610\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 18 Jun 2022 10:07:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, accepted to Insterspeech 2022\\u00a7r"}']}
{title:'Muckenhirn et al. (§72022§r)', author: 'Hannah Muckenhirn; Aleksandr Safin; Hakan Erdogan; Felix de Chaumont Quitry; Marco Tagliasacchi; Scott Wisdom; John R. Hershey', display:{Lore:['[{"text": "arXiv:2203.15652", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCycleGAN-Based Unpaired Speech Dereverberation\\u00a7r\\n\\n\\u00a78\\u00a7oHannah Muckenhirn\\nAleksandr Safin\\nHakan Erdogan\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15652\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Mar 2022 15:10:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Singh et al. (§72022§r)', author: 'Arshdeep Singh; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:2203.15751", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CC\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Passive Similarity based CNN Filter Pruning for Efficient Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oArshdeep Singh\\nMark D. Plumbley\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15751\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Mar 2022 17:00:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022 conference\\u00a7r"}']}
{title:'Ni et al. (§72022§r)', author: 'Junrui Ni; Liming Wang; Heting Gao; Kaizhi Qian; Yang Zhang; Shiyu Chang; Mark Hasegawa-Johnson', display:{Lore:['[{"text": "arXiv:2203.15796", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Text-to-Speech Synthesis by Unsupervised Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJunrui Ni\\nLiming Wang\\nHeting Gao\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15796\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 15 Aug 2022 20:39:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2022\\u00a7r"}']}
{title:'Gao et al. (§72022§r)', author: 'Heting Gao; Junrui Ni; Kaizhi Qian; Yang Zhang; Shiyu Chang; Mark Hasegawa-Johnson', display:{Lore:['[{"text": "arXiv:2203.15863", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWAVPROMPT: Towards Few-Shot Spoken Language Understanding with Frozen Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oHeting Gao\\nJunrui Ni\\nKaizhi Qian\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15863\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Apr 2022 03:32:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Mu Yang; Kevin Hirschi; Stephen D. Looney; Okim Kang; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:2203.15937", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness and Intelligibility Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oMu Yang\\nKevin Hirschi\\nStephen D. Looney\\nOkim Kang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15937\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 12 Jul 2022 02:42:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Park et al. (§72022§r)', author: 'Tae Jin Park; Nithin Rao Koluguri; Jagadeesh Balam; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2203.15974", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-scale Speaker Diarization with Dynamic Scale Weighting\\u00a7r\\n\\n\\u00a78\\u00a7oTae Jin Park\\nNithin Rao Koluguri\\nJagadeesh Balam\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15974\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Mar 2022 01:26:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Garg et al. (§72022§r)', author: 'Vineet Garg; Ognjen Rudovic; Pranay Dighe; Ahmed H. Abdelaziz; Erik Marchi; Saurabh Adya; Chandra Dhir; Ahmed Tewfik', display:{Lore:['[{"text": "arXiv:2203.15975", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDevice-Directed Speech Detection: Regularization via Distillation for Weakly-Supervised Models\\u00a7r\\n\\n\\u00a78\\u00a7oVineet Garg\\nOgnjen Rudovic\\nPranay Dighe\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15975\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Mar 2022 01:27:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Jung et al. (§72022§r)', author: 'Myunghun Jung; Hoirin Kim', display:{Lore:['[{"text": "arXiv:2203.16080", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAsymmetric Proxy Loss for Multi-View Acoustic Word Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oMyunghun Jung\\nHoirin Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16080\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Jun 2022 11:51:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Sulun et al. (§72022§r)', author: 'Serkan Sulun; Matthew E. P. Davies; Paula Viana', display:{Lore:['[{"text": "arXiv:2203.16165", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSymbolic music generation conditioned on continuous-valued emotions\\u00a7r\\n\\n\\u00a78\\u00a7oSerkan Sulun\\nMatthew E. P. Davies\\nPaula Viana\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16165\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ACCESS.2022.3169744\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nvolume:10, year:2022, pages:44617-44626\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 4 May 2022 12:04:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in IEEEAccess\\u00a7r"}']}
{title:'de Seyssel et al. (§72022§r)', author: 'Maureen de Seyssel; Marvin Lavechin; Yossi Adi; Emmanuel Dupoux; Guillaume Wisniewski', display:{Lore:['[{"text": "arXiv:2203.16193", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProbing phoneme, language and speaker information in unsupervised speech representations\\u00a7r\\n\\n\\u00a78\\u00a7oMaureen de Seyssel\\nMarvin Lavechin\\nYossi Adi\\nEmmanuel Dupoux\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16193\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-373\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Mar 2022 10:27:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022, 5 pages, 2 figures\\u00a7r"}']}
{title:'Peer et al. (§72022§r)', author: 'Tal Peer; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2203.16222", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhase-Aware Deep Speech Enhancement: It\'s All About The Frame Length\\u00a7r\\n\\n\\u00a78\\u00a7oTal Peer\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16222\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0014875\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJASA Express Letters 2, 104802 (2022)\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 4 Oct 2022 14:59:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe following article has been accepted by JASA Express Letters. Afterit is published, it will be found at http://asa.scitation.org/journal/jel\\u00a7r"}']}
{title:'Kataria et al. (§72022§r)', author: 'Saurabh Kataria; Jesús Villalba; Laureano Moro-Velázquez; Najim Dehak', display:{Lore:['[{"text": "arXiv:2203.16614", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint domain adaptation and speech bandwidth extension using time-domain GANs for speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oSaurabh Kataria\\nJes\\u00fas Villalba\\nLaureano Moro-Vel\\u00e1zquez\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16614\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Mar 2022 18:51:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Kanda et al. (§72022§r)', author: 'Naoyuki Kanda; Jian Wu; Yu Wu; Xiong Xiao; Zhong Meng; Xiaofei Wang; Yashesh Gaur; Zhuo Chen; Jinyu Li; Takuya Yoshioka', display:{Lore:['[{"text": "arXiv:2203.16685", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreaming Speaker-Attributed ASR with Token-Level Speaker Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oNaoyuki Kanda\\nJian Wu\\nYu Wu\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16685\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Jul 2022 20:38:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for presentation at Interspeech 2022\\u00a7r"}']}
{title:'Baade et al. (§72022§r)', author: 'Alan Baade; Puyuan Peng; David Harwath', display:{Lore:['[{"text": "arXiv:2203.16691", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMAE-AST: Masked Autoencoding Audio Spectrogram Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oAlan Baade\\nPuyuan Peng\\nDavid Harwath\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16691\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Mar 2022 22:06:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH. 5 pages, 2 figures, 5 tables\\u00a7r"}']}
{title:'Lian et al. (§72022§r)', author: 'Jiachen Lian; Chunlei Zhang; Dong Yu', display:{Lore:['[{"text": "arXiv:2203.16705", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Disentangled Variational Speech Representation Learning for Zero-shot Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oJiachen Lian\\nChunlei Zhang\\nDong Yu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16705\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Mar 2022 23:03:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to 2022 ICASSP\\u00a7r"}']}
{title:'Koizumi et al. (§72022§r)', author: 'Yuma Koizumi; Heiga Zen; Kohei Yatabe; Nanxin Chen; Michiel Bacchiani', display:{Lore:['[{"text": "arXiv:2203.16749", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpecGrad: Diffusion Probabilistic Model based Neural Vocoder with Adaptive Noise Spectral Shaping\\u00a7r\\n\\n\\u00a78\\u00a7oYuma Koizumi\\nHeiga Zen\\nKohei Yatabe\\nNanxin Chen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16749\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 4 Aug 2022 22:29:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'An et al. (§72022§r)', author: 'Keyu An; Ji Xiao; Zhijian Ou', display:{Lore:['[{"text": "arXiv:2203.16757", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting Single-Channel Speech for Multi-Channel End-to-End Speech Recognition: A Comparative Study\\u00a7r\\n\\n\\u00a78\\u00a7oKeyu An\\nJi Xiao\\nZhijian Ou\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16757\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 8 Oct 2022 09:57:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ISCSLP 2022. arXiv admin note: substantial text overlap with arXiv:2107.02670\\u00a7r"}']}
{title:'An et al. (§72022§r)', author: 'Keyu An; Huahuan Zheng; Zhijian Ou; Hongyu Xiang; Ke Ding; Guanglu Wan', display:{Lore:['[{"text": "arXiv:2203.16758", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCUSIDE: Chunking, Simulating Future Context and Decoding for Streaming ASR\\u00a7r\\n\\n\\u00a78\\u00a7oKeyu An\\nHuahuan Zheng\\nZhijian Ou\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16758\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 2 Aug 2022 12:23:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted into INTERSPEECH 2022\\u00a7r"}']}
{title:'Chang et al. (§72022§r)', author: 'Kai-Wei Chang; Wei-Cheng Tseng; Shang-Wen Li; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2203.16773", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeechPrompt: An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oKai-Wei Chang\\nWei-Cheng Tseng\\nShang-Wen Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16773\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 10 Jul 2022 19:30:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to be published in the Proceedings of Interspeech 2022\\u00a7r"}']}
{title:'Zheng et al. (§72022§r)', author: 'Huahuan Zheng; Keyu An; Zhijian Ou; Chen Huang; Ke Ding; Guanglu Wan', display:{Lore:['[{"text": "arXiv:2203.16776", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Empirical Study of Language Model Integration for Transducer based Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHuahuan Zheng\\nKeyu An\\nZhijian Ou\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16776\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 3 Aug 2022 08:10:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted into INTERSPEECH 2022\\u00a7r"}']}
{title:'Zuluaga-Gomez et al. (§72022§r)', author: 'Juan Zuluaga-Gomez; Amrutha Prasad; Iuliia Nigmatulina; Saeed Sarfjoo; Petr Motlicek; Matthias Kleinert; Hartmut Helmke; Oliver Ohneiser; Qingran Zhan', display:{Lore:['[{"text": "arXiv:2203.16822", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHow Does Pre-trained Wav2Vec 2.0 Perform on Domain Shifted ASR? An Extensive Benchmark on Air Traffic Control Communications\\u00a7r\\n\\n\\u00a78\\u00a7oJuan Zuluaga-Gomez\\nAmrutha Prasad\\nIuliia Nigmatulina\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16822\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 17 Oct 2022 13:21:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in the 2022 IEEE Spoken Language Technology Workshop (SLT) (SLT 2022)\\u00a7r"}']}
{title:'Pan et al. (§72022§r)', author: 'Zexu Pan; Xinyuan Qian; Haizhou Li', display:{Lore:['[{"text": "arXiv:2203.16840", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Extraction with Co-Speech Gestures Cue\\u00a7r\\n\\n\\u00a78\\u00a7oZexu Pan\\nXinyuan Qian\\nHaizhou Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16840\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2022.3175130\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 10 May 2022 05:36:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE Signal Processing Letters\\u00a7r"}']}
{title:'Pan et al. (§72022§r)', author: 'Zexu Pan; Meng Ge; Haizhou Li', display:{Lore:['[{"text": "arXiv:2203.16843", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Hybrid Continuity Loss to Reduce Over-Suppression for Time-domain Target Speaker Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oZexu Pan\\nMeng Ge\\nHaizhou Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16843\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 20 Jun 2022 07:59:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech2022\\u00a7r"}']}
{title:'Lim et al. (§72022§r)', author: 'Dan Lim; Sunghee Jung; Eesung Kim', display:{Lore:['[{"text": "arXiv:2203.16852", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech\\u00a7r\\n\\n\\u00a78\\u00a7oDan Lim\\nSunghee Jung\\nEesung Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16852\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 1 Jul 2022 18:20:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Lee et al. (§72022§r)', author: 'Jaesong Lee; Lukas Lee; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2203.16868", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMemory-Efficient Training of RNN-Transducer with Sampled Softmax\\u00a7r\\n\\n\\u00a78\\u00a7oJaesong Lee\\nLukas Lee\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16868\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Mar 2022 07:51:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Diaz-Guerra et al. (§72022§r)', author: 'David Diaz-Guerra; Antonio Miguel; Jose R. Beltran', display:{Lore:['[{"text": "arXiv:2203.16940", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDirection of Arrival Estimation of Sound Sources Using Icosahedral CNNs\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Diaz-Guerra\\nAntonio Miguel\\nJose R. Beltran\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16940\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2022.3224282\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol. 31, pp. 313-321, 2023\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 6 Dec 2022 15:33:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe code to reproduce this work can be found in our GitHub repository: https://github.com/DavidDiazGuerra/icoDOA\\u00a7r"}']}
{title:'Kukk et al. (§72022§r)', author: 'Kunnar Kukk; Tanel Alumäe', display:{Lore:['[{"text": "arXiv:2203.16972", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Language Identification of Accented Speech\\u00a7r\\n\\n\\u00a78\\u00a7oKunnar Kukk\\nTanel Alum\\u00e4e\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16972\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 1 Jul 2022 10:16:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Guo et al. (§72022§r)', author: 'Shuai Guo; Jiatong Shi; Tao Qian; Shinji Watanabe; Qin Jin', display:{Lore:['[{"text": "arXiv:2203.17001", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSingAug: Data Augmentation for Singing Voice Synthesis with Cycle-consistent Training Strategy\\u00a7r\\n\\n\\u00a78\\u00a7oShuai Guo\\nJiatong Shi\\nTao Qian\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.17001\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Jul 2022 08:06:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2022\\u00a7r"}']}
{title:'Welker et al. (§72022§r)', author: 'Simon Welker; Julius Richter; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2203.17004", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Enhancement with Score-Based Generative Models in the Complex STFT Domain\\u00a7r\\n\\n\\u00a78\\u00a7oSimon Welker\\nJulius Richter\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.17004\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 7 Jul 2022 16:12:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Chernyak et al. (§72022§r)', author: 'Bronya R. Chernyak; Talia Ben Simon; Yael Segal; Jeremy Steffman; Eleanor Chodroff; Jennifer S. Cole; Joseph Keshet', display:{Lore:['[{"text": "arXiv:2203.17019", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeepFry: Identifying Vocal Fry Using Deep Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oBronya R. Chernyak\\nTalia Ben Simon\\nYael Segal\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.17019\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 26 Jun 2022 12:16:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Lu et al. (§72022§r)', author: 'Xugang Lu; Peng Shen; Yu Tsao; Hisashi Kawai', display:{Lore:['[{"text": "arXiv:2203.17036", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPartial Coupling of Optimal Transport for Spoken Language Identification\\u00a7r\\n\\n\\u00a78\\u00a7oXugang Lu\\nPeng Shen\\nYu Tsao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.17036\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Mar 2022 14:00:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work was submitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Maiti et al. (§72022§r)', author: 'Soumi Maiti; Yushi Ueda; Shinji Watanabe; Chunlei Zhang; Meng Yu; Shi-Xiong Zhang; Yong Xu', display:{Lore:['[{"text": "arXiv:2203.17068", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEEND-SS: Joint End-to-End Neural Speaker Diarization and Speech Separation for Flexible Number of Speakers\\u00a7r\\n\\n\\u00a78\\u00a7oSoumi Maiti\\nYushi Ueda\\nShinji Watanabe\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.17068\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 15 Dec 2022 19:35:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in SLT 2022\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Mingjie Chen; Yanghao Zhou; Heyan Huang; Thomas Hain', display:{Lore:['[{"text": "arXiv:2203.17172", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Non-Autoregressive GAN Voice Conversion using VQWav2vec Features and Dynamic Convolution\\u00a7r\\n\\n\\u00a78\\u00a7oMingjie Chen\\nYanghao Zhou\\nHeyan Huang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.17172\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Mar 2022 16:46:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to interspeech 2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Guangyan Zhang; Kaitao Song; Xu Tan; Daxin Tan; Yuzi Yan; Yanqing Liu; Gang Wang; Wei Zhou; Tao Qin; Tan Lee; Sheng Zhao', display:{Lore:['[{"text": "arXiv:2203.17190", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech\\u00a7r\\n\\n\\u00a78\\u00a7oGuangyan Zhang\\nKaitao Song\\nXu Tan\\n+ 7 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.17190\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 19 Jul 2022 19:19:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by interspeech 2022\\u00a7r"}']}
{title:'Chaubey et al. (§72022§r)', author: 'Ashutosh Chaubey; Sparsh Sinha; Susmita Ghose', display:{Lore:['[{"text": "arXiv:2203.17218", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Relation Networks for End-to-End Speaker Verification and Identification\\u00a7r\\n\\n\\u00a78\\u00a7oAshutosh Chaubey\\nSparsh Sinha\\nSusmita Ghose\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.17218\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 21 Jul 2022 18:01:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Fan-Lin Wang; Po-chun Hsu; Da-rong Liu; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2204.00170", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUniversal Adaptor: Converting Mel-Spectrograms Between Different Configurations for Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oFan-Lin Wang\\nPo-chun Hsu\\nDa-rong Liu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00170\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 29 Oct 2022 13:25:23 GMT)\\u00a7r"}']}
{title:'Saijo et al. (§72022§r)', author: 'Kohei Saijo; Robin Scheibler', display:{Lore:['[{"text": "arXiv:2204.00210", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatial Loss for Unsupervised Multi-channel Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oKohei Saijo\\nRobin Scheibler\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00210\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Apr 2022 05:13:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH2022\\u00a7r"}']}
{title:'Scheibler et al. (§72022§r)', author: 'Robin Scheibler; Wangyou Zhang; Xuankai Chang; Shinji Watanabe; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2204.00218", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Multi-speaker ASR with Independent Vector Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oRobin Scheibler\\nWangyou Zhang\\nXuankai Chang\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00218\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Apr 2022 05:45:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH2022. 5 pages, 2 figures, 3 tables\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Tianrui Wang; Weibin Zhu; Yingying Gao; Junlan Feng; Shilei Zhang', display:{Lore:['[{"text": "arXiv:2204.00226", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultiple Confidence Gates For Joint Training Of SE And ASR\\u00a7r\\n\\n\\u00a78\\u00a7oTianrui Wang\\nWeibin Zhu\\nYingying Gao\\nJunlan Feng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00226\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Apr 2022 06:19:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Yihan Wu; Xu Tan; Bohan Li; Lei He; Sheng Zhao; Ruihua Song; Tao Qin; Tie-Yan Liu', display:{Lore:['[{"text": "arXiv:2204.00436", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaSpeech 4: Adaptive Text to Speech in Zero-Shot Scenarios\\u00a7r\\n\\n\\u00a78\\u00a7oYihan Wu\\nXu Tan\\nBohan Li\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00436\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Apr 2022 13:47:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 tables, 2 figure. Submitted to Interspeech 2022\\u00a7r"}']}
{title:'Lian et al. (§72022§r)', author: 'Jiachen Lian; Alan W Black; Louis Goldstein; Gopala Krishna Anumanchipalli', display:{Lore:['[{"text": "arXiv:2204.00465", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Neural Convolutive Matrix Factorization for Articulatory Representation Decomposition\\u00a7r\\n\\n\\u00a78\\u00a7oJiachen Lian\\nAlan W Black\\nLouis Goldstein\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00465\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 21 Jun 2022 00:14:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to 2022 Interspeech. Code is publicly available at https://github.com/Berkeley-Speech-Group/ema_gesture\\u00a7r"}']}
{title:'Singh (§72022§r)', author: 'Arshdeep Singh', display:{Lore:['[{"text": "arXiv:2204.00555", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l1-D CNN based Acoustic Scene Classification via Reducing Layer-wise Dimensionality\\u00a7r\\n\\n\\u00a78\\u00a7oArshdeep Singh\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00555\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Mar 2022 02:00:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oNo comments\\u00a7r"}']}
{title:'Flemotomos et al. (§72022§r)', author: 'Nikolaos Flemotomos; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2204.00657", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultimodal Clustering with Role Induced Constraints for Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oNikolaos Flemotomos\\nShrikanth Narayanan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00657\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 11 Jul 2022 14:23:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at Interspeech 2022\\u00a7r"}']}
{title:'Du et al. (§72022§r)', author: 'Chenpeng Du; Yiwei Guo; Xie Chen; Kai Yu', display:{Lore:['[{"text": "arXiv:2204.00768", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVQTTS: High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature\\u00a7r\\n\\n\\u00a78\\u00a7oChenpeng Du\\nYiwei Guo\\nXie Chen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00768\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 30 Jun 2022 07:05:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Thakker et al. (§72022§r)', author: 'Manthan Thakker; Sefik Emre Eskimez; Takuya Yoshioka; Huaming Wang', display:{Lore:['[{"text": "arXiv:2204.00771", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast Real-time Personalized Speech Enhancement: End-to-End Enhancement Network (E3Net) and Knowledge Distillation\\u00a7r\\n\\n\\u00a78\\u00a7oManthan Thakker\\nSefik Emre Eskimez\\nTakuya Yoshioka\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00771\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Apr 2022 05:47:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech conference 2022 https://interspeech2022.org/\\u00a7r"}']}
{title:'Landini et al. (§72022§r)', author: 'Federico Landini; Alicia Lozano-Diez; Mireia Diez; Lukáš Burget', display:{Lore:['[{"text": "arXiv:2204.00890", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrom Simulated Mixtures to Simulated Conversations as Training Data for End-to-End Neural Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oFederico Landini\\nAlicia Lozano-Diez\\nMireia Diez\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00890\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 25 Jun 2022 13:42:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2022\\u00a7r"}']}
{title:'Johnson et al. (§72022§r)', author: 'Alexander Johnson; Kevin Everson; Vijay Ravi; Anissa Gladney; Mari Ostendorf; Abeer Alwan', display:{Lore:['[{"text": "arXiv:2204.00967", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Dialect Density Estimation for African American English\\u00a7r\\n\\n\\u00a78\\u00a7oAlexander Johnson\\nKevin Everson\\nVijay Ravi\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00967\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInterspeech 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 3 Apr 2022 01:34:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures\\u00a7r"}']}
{title:'Mun et al. (§72022§r)', author: 'Sung Hwan Mun; Jee-weon Jung; Min Hyun Han; Nam Soo Kim', display:{Lore:['[{"text": "arXiv:2204.01005", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrequency and Multi-Scale Selective Kernel Attention for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oSung Hwan Mun\\nJee-weon Jung\\nMin Hyun Han\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.01005\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 12 Oct 2022 04:39:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE SLT 2022. 7 pages, 4 figures, 1 table. Code is available at https://github.com/msh9184/ska-tdnn.git\\u00a7r"}']}
{title:'Lee et al. (§72022§r)', author: 'Jihwan Lee; Joun Yeop Lee; Heejin Choi; Seongkyu Mun; Sangjun Park; Jae-Sung Bae; Chanwoo Kim', display:{Lore:['[{"text": "arXiv:2204.01271", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInto-TTS : Intonation Template Based Prosody Control System\\u00a7r\\n\\n\\u00a78\\u00a7oJihwan Lee\\nJoun Yeop Lee\\nHeejin Choi\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.01271\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 6 Nov 2022 10:08:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Westhausen et al. (§72022§r)', author: 'Nils L. Westhausen; Bernd T. Meyer', display:{Lore:['[{"text": "arXiv:2204.01300", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7ltPLCnet: Real-time Deep Packet Loss Concealment in the Time Domain Using a Short Temporal Context\\u00a7r\\n\\n\\u00a78\\u00a7oNils L. Westhausen\\nBernd T. Meyer\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.01300\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Apr 2022 08:14:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Hajal et al. (§72022§r)', author: 'Karl El Hajal; Milos Cernak; Pablo Mainar', display:{Lore:['[{"text": "arXiv:2204.01345", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMOSRA: Joint Mean Opinion Score and Room Acoustics Speech Quality Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oKarl El Hajal\\nMilos Cernak\\nPablo Mainar\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.01345\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 29 Jun 2022 12:16:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Zhao et al. (§72022§r)', author: 'Zifeng Zhao; Dongchao Yang; Rongzhi Gu; Haoran Zhang; Yuexian Zou', display:{Lore:['[{"text": "arXiv:2204.01355", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTarget Confusion in End-to-end Speaker Extraction: Analysis and Approaches\\u00a7r\\n\\n\\u00a78\\u00a7oZifeng Zhao\\nDongchao Yang\\nRongzhi Gu\\nHaoran Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.01355\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Apr 2022 09:57:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 table, 5 figures. Submitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Eom et al. (§72022§r)', author: 'Youngsik Eom; Yeonghyeon Lee; Ji Sub Um; Hoirin Kim', display:{Lore:['[{"text": "arXiv:2204.01387", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnti-Spoofing Using Transfer Learning with Variational Information Bottleneck\\u00a7r\\n\\n\\u00a78\\u00a7oYoungsik Eom\\nYeonghyeon Lee\\nJi Sub Um\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.01387\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10200\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 14 Dec 2022 08:45:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Martín-Doñas et al. (§72022§r)', author: 'Juan M. Martín-Doñas; Iván G. Torre; Aitor Álvarez; Joaquin Arellano', display:{Lore:['[{"text": "arXiv:2204.01399", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Vicomtech Spoofing-Aware Biometric System for the SASV Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oJuan M. Mart\\u00edn-Do\\u00f1as\\nIv\\u00e1n G. Torre\\nAitor \\u00c1lvarez\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.01399\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Apr 2022 11:32:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Sheikh et al. (§72022§r)', author: 'Shakeel Ahmad Sheikh; Md Sahidullah; Fabrice Hirsch; Slim Ouni', display:{Lore:['[{"text": "arXiv:2204.01735", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Stuttering Detection via Multi-task and Adversarial Learning\\u00a7r\\n\\n\\u00a78\\u00a7oShakeel Ahmad Sheikh\\nMd Sahidullah\\nFabrice Hirsch\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.01735\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Apr 2022 15:44:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder Review in European Signal Processing Conference 2022\\u00a7r"}']}
{title:'Grassucci et al. (§72022§r)', author: 'Eleonora Grassucci; Gioia Mancini; Christian Brignone; Aurelio Uncini; Danilo Comminiello', display:{Lore:['[{"text": "arXiv:2204.01851", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDual Quaternion Ambisonics Array for Six-Degree-of-Freedom Acoustic Representation\\u00a7r\\n\\n\\u00a78\\u00a7oEleonora Grassucci\\nGioia Mancini\\nChristian Brignone\\nAurelio Uncini\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.01851\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 14 Dec 2022 21:23:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper accepted for publication in Elsevier Pattern Recognition Letters\\u00a7r"}']}
{title:'Lu et al. (§72022§r)', author: 'Zhiyun Lu; Yongqiang Wang; Yu Zhang; Wei Han; Zhehuai Chen; Parisa Haghani', display:{Lore:['[{"text": "arXiv:2204.01981", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Data Selection via Discrete Speech Representation for ASR\\u00a7r\\n\\n\\u00a78\\u00a7oZhiyun Lu\\nYongqiang Wang\\nYu Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.01981\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Apr 2022 04:35:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Becerra et al. (§72022§r)', author: 'Helard Becerra; Alessandro Ragano; Andrew Hines', display:{Lore:['[{"text": "arXiv:2204.02135", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring the influence of fine-tuning data on wav2vec 2.0 model for blind speech quality prediction\\u00a7r\\n\\n\\u00a78\\u00a7oHelard Becerra\\nAlessandro Ragano\\nAndrew Hines\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02135\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Apr 2022 11:57:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Xie et al. (§72022§r)', author: 'Yuying Xie; Thomas Arildsen; Zheng-Hua Tan', display:{Lore:['[{"text": "arXiv:2204.02166", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDisentangled Speech Representation Learning Based on Factorized Hierarchical Variational Autoencoder with Self-Supervised Objective\\u00a7r\\n\\n\\u00a78\\u00a7oYuying Xie\\nThomas Arildsen\\nZheng-Hua Tan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02166\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/MLSP52302.2021.9596320\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Apr 2022 12:47:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in: 2021 IEEE 31st International Workshopon Machine Learning for Signal Processing (MLSP)\\u00a7r"}']}
{title:'Sharon et al. (§72022§r)', author: 'Rini Sharon; Heet Shah; Debdoot Mukherjee; Vikram Gupta', display:{Lore:['[{"text": "arXiv:2204.02263", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultilingual and Multimodal Abuse Detection\\u00a7r\\n\\n\\u00a78\\u00a7oRini Sharon\\nHeet Shah\\nDebdoot Mukherjee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02263\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 3 Apr 2022 13:28:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Hutiri et al. (§72022§r)', author: 'Wiebke Toussaint Hutiri; Lauriane Gorce; Aaron Yi Ding', display:{Lore:['[{"text": "arXiv:2204.02281", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CY\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDesign Guidelines for Inclusive Speaker Verification Evaluation Datasets\\u00a7r\\n\\n\\u00a78\\u00a7oWiebke Toussaint Hutiri\\nLauriane Gorce\\nAaron Yi Ding\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02281\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 13 Sep 2022 13:05:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022 (submitted version)\\u00a7r"}']}
{title:'Morrone et al. (§72022§r)', author: 'Giovanni Morrone; Samuele Cornell; Desh Raj; Luca Serafini; Enrico Zovato; Alessio Brutti; Stefano Squartini', display:{Lore:['[{"text": "arXiv:2204.02306", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Latency Speech Separation Guided Diarization for Telephone Conversations\\u00a7r\\n\\n\\u00a78\\u00a7oGiovanni Morrone\\nSamuele Cornell\\nDesh Raj\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02306\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 27 Oct 2022 10:53:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for Presentation at IEEE Spoken Language Technology Workshop (SLT) 2022\\u00a7r"}']}
{title:'Das et al. (§72022§r)', author: 'Nilaksh Das; Duen Horng Chau', display:{Lore:['[{"text": "arXiv:2204.02381", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHear No Evil: Towards Adversarial Robustness of Automatic Speech Recognition via Multi-Task Learning\\u00a7r\\n\\n\\u00a78\\u00a7oNilaksh Das\\nDuen Horng Chau\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02381\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Apr 2022 17:40:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Insterspeech 2022\\u00a7r"}']}
{title:'Lee et al. (§72022§r)', author: 'Jin Woo Lee; Sungho Lee; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2204.02637", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGlobal HRTF Interpolation via Learned Affine Transformation of Hyper-conditioned Features\\u00a7r\\n\\n\\u00a78\\u00a7oJin Woo Lee\\nSungho Lee\\nKyogu Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02637\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 3 Nov 2022 05:32:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Lee et al. (§72022§r)', author: 'Jin Woo Lee; Eungbeom Kim; Junghyun Koo; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2204.02639", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRepresentation Selective Self-distillation and wav2vec 2.0 Feature Exploration for Spoof-aware Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oJin Woo Lee\\nEungbeom Kim\\nJunghyun Koo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02639\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-11460\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 2 Jul 2022 13:20:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to be published in the Proceedings of Interspeech 2022\\u00a7r"}']}
{title:'Lemercier et al. (§72022§r)', author: 'Jean-Marie Lemercier; Joachim Thiemann; Raphael Koning; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2204.02694", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCustomizable End-to-end Optimization of Online Neural Network-supported Dereverberation for Hearing Devices\\u00a7r\\n\\n\\u00a78\\u00a7oJean-Marie Lemercier\\nJoachim Thiemann\\nRaphael Koning\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02694\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746235\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2022 - IEEE International Conference on Acoustics, Speech\\n  and Signal Processing\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Apr 2022 09:43:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or "}','{"text": "promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Lemercier et al. (§72022§r)', author: 'Jean-Marie Lemercier; Joachim Thiemann; Raphael Koning; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2204.02741", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Network-augmented Kalman Filtering for Robust Online Speech Dereverberation in Noisy Reverberant Environments\\u00a7r\\n\\n\\u00a78\\u00a7oJean-Marie Lemercier\\nJoachim Thiemann\\nRaphael Koning\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02741\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 23 Jun 2022 12:49:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Cuccovillo et al. (§72022§r)', author: 'L. Cuccovillo; A. Giganti; P. Bestagini; P. Aichroth; S. Tubaro', display:{Lore:['[{"text": "arXiv:2204.02841", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpectral Denoising for Microphone Classification\\u00a7r\\n\\n\\u00a78\\u00a7oL. Cuccovillo\\nA. Giganti\\nP. Bestagini\\nP. Aichroth\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02841\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3512732.3533586\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nin ACM International Workshop on Multimedia AI against\\n  Disinformation (MAD), Newark, NJ, USA, 2022, pp. 10-17\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 1 Jul 2022 16:40:12 GMT)\\u00a7r"}']}
{title:'Rao (§72022§r)', author: 'Preeti Rao', display:{Lore:['[{"text": "arXiv:2204.03166", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusical Information Extraction from the Singing Voice\\u00a7r\\n\\n\\u00a78\\u00a7oPreeti Rao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03166\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Apr 2022 02:34:57 GMT)\\u00a7r"}']}
{title:'Tseng et al. (§72022§r)', author: 'Wei-Cheng Tseng; Wei-Tsung Kao; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2204.03219", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDDOS: A MOS Prediction Framework utilizing Domain Adaptive Pre-training and Distribution of Opinion Scores\\u00a7r\\n\\n\\u00a78\\u00a7oWei-Cheng Tseng\\nWei-Tsung Kao\\nHung-yi Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03219\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 15 Aug 2022 17:35:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022. Code will be available in the future\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Xiaofei Wang; Dongmei Wang; Naoyuki Kanda; Sefik Emre Eskimez; Takuya Yoshioka', display:{Lore:['[{"text": "arXiv:2204.03232", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Real Conversational Data for Multi-Channel Continuous Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oXiaofei Wang\\nDongmei Wang\\nNaoyuki Kanda\\nSefik Emre Eskimez\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03232\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Apr 2022 05:45:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Yutian Wang; Yuankun Xie; Kun Zhao; Hui Wang; Qin Zhang', display:{Lore:['[{"text": "arXiv:2204.03238", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Quantized Prosody Representation for Controllable Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYutian Wang\\nYuankun Xie\\nKun Zhao\\nHui Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03238\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Apr 2022 06:09:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by IEEE International Conference on Multimedia and Expo 2022 (ICME2022)\\u00a7r"}']}
{title:'Zezario et al. (§72022§r)', author: 'Ryandhimas E. Zezario; Fei Chen; Chiou-Shann Fuh; Hsin-Min Wang; Yu Tsao', display:{Lore:['[{"text": "arXiv:2204.03305", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMBI-Net: A Non-Intrusive Multi-Branched Speech Intelligibility Prediction Model for Hearing Aids\\u00a7r\\n\\n\\u00a78\\u00a7oRyandhimas E. Zezario\\nFei Chen\\nChiou-Shann Fuh\\nHsin-Min Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03305\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 31 Aug 2022 03:17:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Gao et al. (§72022§r)', author: 'Xiaoxue Gao; Chitralekha Gupta; Haizhou Li', display:{Lore:['[{"text": "arXiv:2204.03306", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic-robust Automatic Lyrics Transcription of Polyphonic Music\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoxue Gao\\nChitralekha Gupta\\nHaizhou Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03306\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 22 Apr 2022 12:06:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 2 figures, accepted by 2022 Sound and Music Computing\\u00a7r"}']}
{title:'Zezario et al. (§72022§r)', author: 'Ryandhimas E. Zezario; Szu-wei Fu; Fei Chen; Chiou-Shann Fuh; Hsin-Min Wang; Yu Tsao', display:{Lore:['[{"text": "arXiv:2204.03310", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMTI-Net: A Multi-Target Speech Intelligibility Prediction Model\\u00a7r\\n\\n\\u00a78\\u00a7oRyandhimas E. Zezario\\nSzu-wei Fu\\nFei Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03310\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 31 Aug 2022 03:22:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Hung et al. (§72022§r)', author: 'Kuo-Hsuan Hung; Szu-wei Fu; Huan-Hsin Tseng; Hsin-Tien Chiang; Yu Tsao; Chii-Wann Lin', display:{Lore:['[{"text": "arXiv:2204.03339", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBoosting Self-Supervised Embeddings for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oKuo-Hsuan Hung\\nSzu-wei Fu\\nHuan-Hsin Tseng\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03339\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 5 Jul 2022 12:30:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to INTERSPEECH-2022\\u00a7r"}']}
{title:'Ben-Simon et al. (§72022§r)', author: 'Talia Ben-Simon; Felix Kreuk; Faten Awwad; Jacob T. Cohen; Joseph Keshet', display:{Lore:['[{"text": "arXiv:2204.03379", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCorrecting Mispronunciations in Speech using Spectrogram Inpainting\\u00a7r\\n\\n\\u00a78\\u00a7oTalia Ben-Simon\\nFelix Kreuk\\nFaten Awwad\\nJacob T. Cohen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03379\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 30 Jun 2022 11:37:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at Interspeech 2022\\u00a7r"}']}
{title:'Bayerl et al. (§72022§r)', author: 'Sebastian P. Bayerl; Dominik Wagner; Elmar Nöth; Korbinian Riedhammer', display:{Lore:['[{"text": "arXiv:2204.03417", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetecting Dysfluencies in Stuttering Therapy Using wav2vec 2.0\\u00a7r\\n\\n\\u00a78\\u00a7oSebastian P. Bayerl\\nDominik Wagner\\nElmar N\\u00f6th\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03417\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-347\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 16 Jun 2022 09:41:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2022\\u00a7r"}']}
{title:'Ding et al. (§72022§r)', author: "Shaojin Ding; Rajeev Rikhye; Qiao Liang; Yanzhang He; Quan Wang; Arun Narayanan; Tom O'Malley; Ian McGraw", display:{Lore:['[{"text": "arXiv:2204.03793", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPersonal VAD 2.0: Optimizing Personal Voice Activity Detection for On-Device Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oShaojin Ding\\nRajeev Rikhye\\nQiao Liang\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03793\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 25 Jun 2022 02:12:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2022\\u00a7r"}']}
{title:'Joshi et al. (§72022§r)', author: 'Sonal Joshi; Saurabh Kataria; Jesus Villalba; Najim Dehak', display:{Lore:['[{"text": "arXiv:2204.03848", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdvEst: Adversarial Perturbation Estimation to Classify and Detect Adversarial Attacks against Speaker Identification\\u00a7r\\n\\n\\u00a78\\u00a7oSonal Joshi\\nSaurabh Kataria\\nJesus Villalba\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03848\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Apr 2022 05:23:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to InterSpeech 2022\\u00a7r"}']}
{title:'Joshi et al. (§72022§r)', author: 'Sonal Joshi; Saurabh Kataria; Yiwen Shao; Piotr Zelasko; Jesus Villalba; Sanjeev Khudanpur; Najim Dehak', display:{Lore:['[{"text": "arXiv:2204.03851", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDefense against Adversarial Attacks on Hybrid Speech Recognition using Joint Adversarial Fine-tuning with Denoiser\\u00a7r\\n\\n\\u00a78\\u00a7oSonal Joshi\\nSaurabh Kataria\\nYiwen Shao\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03851\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Apr 2022 05:31:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Eesung Kim; Jae-Jin Jeon; Hyeji Seo; Hoon Kim', display:{Lore:['[{"text": "arXiv:2204.03863", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Pronunciation Assessment using Self-Supervised Speech Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oEesung Kim\\nJae-Jin Jeon\\nHyeji Seo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03863\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Apr 2022 06:13:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Delcroix et al. (§72022§r)', author: 'Marc Delcroix; Jorge Bennasar Vázquez; Tsubasa Ochiai; Keisuke Kinoshita; Yasunori Ohishi; Shoko Araki', display:{Lore:['[{"text": "arXiv:2204.03895", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSoundBeam: Target sound extraction conditioned on sound-class labels and enrollment clues for increased performance and continuous learning\\u00a7r\\n\\n\\u00a78\\u00a7oMarc Delcroix\\nJorge Bennasar V\\u00e1zquez\\nTsubasa Ochiai\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03895\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 2 Nov 2022 05:12:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEE/ACM Trans. Audio, Speech, and Language Processing on Feb. 10th, 2022, and accepted on Oct. 20, 2022\\u00a7r"}']}
{title:'Ou et al. (§72022§r)', author: 'Longshen Ou; Ziyi Guo; Emmanouil Benetos; Jiqing Han; Ye Wang', display:{Lore:['[{"text": "arXiv:2204.03898", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Transformer\'s potential on automatic piano transcription\\u00a7r\\n\\n\\u00a78\\u00a7oLongshen Ou\\nZiyi Guo\\nEmmanouil Benetos\\nJiqing Han\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03898\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Apr 2022 07:52:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Qiongqiong Wang; Kong Aik Lee; Tianchi Liu', display:{Lore:['[{"text": "arXiv:2204.03965", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScoring of Large-Margin Embeddings for Speaker Verification: Cosine or PLDA?\\u00a7r\\n\\n\\u00a78\\u00a7oQiongqiong Wang\\nKong Aik Lee\\nTianchi Liu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03965\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 11 Apr 2022 02:02:49 GMT)\\u00a7r"}']}
{title:'Bae et al. (§72022§r)', author: 'Jae-Sung Bae; Jinhyeok Yang; Tae-Jun Bak; Young-Sun Joo', display:{Lore:['[{"text": "arXiv:2204.04004", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierarchical and Multi-Scale Variational Autoencoder for Diverse and Natural Non-Autoregressive Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oJae-Sung Bae\\nJinhyeok Yang\\nTae-Jun Bak\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.04004\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 15 Aug 2022 11:32:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Bous et al. (§72022§r)', author: 'Frederik Bous; Axel Roebel', display:{Lore:['[{"text": "arXiv:2204.04006", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalysis and transformations of voice level in singing voice\\u00a7r\\n\\n\\u00a78\\u00a7oFrederik Bous\\nAxel Roebel\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.04006\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10095740\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 22 Nov 2022 14:41:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Weise et al. (§72022§r)', author: 'Tobias Weise; Philipp Klumpp; Kubilay Can Demir; Andreas Maier; Elmar Noeth; Bjoern Heismann; Maria Schuster; Seung Hee Yang', display:{Lore:['[{"text": "arXiv:2204.04016", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDisentangled Latent Speech Representation for Automatic Pathological Intelligibility Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oTobias Weise\\nPhilipp Klumpp\\nKubilay Can Demir\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.04016\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Jun 2022 14:21:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted and Accepted at INTERSPEECH2022\\u00a7r"}']}
{title:'Jonscher et al. (§72022§r)', author: 'Markus Jonscher; Jürgen Seiler; André Kaup', display:{Lore:['[{"text": "arXiv:2204.04068", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeclipping of Speech Signals Using Frequency Selective Extrapolation\\u00a7r\\n\\n\\u00a78\\u00a7oMarkus Jonscher\\nJ\\u00fcrgen Seiler\\nAndr\\u00e9 Kaup\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.04068\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Apr 2022 16:00:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 5 figures, 2 tables, Speech Communication 11. ITG Symposium\\u00a7r"}']}
{title:'Kakoulidis et al. (§72022§r)', author: 'Panos Kakoulidis; Nikolaos Ellinas; Georgios Vamvoukakis; Konstantinos Markopoulos; June Sig Sung; Gunu Jho; Pirros Tsiakoulis; Aimilios Chalamandaris', display:{Lore:['[{"text": "arXiv:2204.04127", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKaraoker: Alignment-free singing voice synthesis with speech training data\\u00a7r\\n\\n\\u00a78\\u00a7oPanos Kakoulidis\\nNikolaos Ellinas\\nGeorgios Vamvoukakis\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.04127\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10446\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 31 Aug 2022 08:44:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Zaiem et al. (§72022§r)', author: 'Salah Zaiem; Titouan Parcollet; Slim Essid', display:{Lore:['[{"text": "arXiv:2204.04170", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Data Augmentation Selection and Parametrization in Contrastive Self-Supervised Speech Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oSalah Zaiem\\nTitouan Parcollet\\nSlim Essid\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.04170\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Apr 2022 16:30:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Tu et al. (§72022§r)', author: 'Zehai Tu; Jack Deadman; Ning Ma; Jon Barker', display:{Lore:['[{"text": "arXiv:2204.04284", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAuditory-Based Data Augmentation for End-to-End Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZehai Tu\\nJack Deadman\\nNing Ma\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.04284\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Apr 2022 20:34:42 GMT)\\u00a7r"}']}
{title:'Tu et al. (§72022§r)', author: 'Zehai Tu; Ning Ma; Jon Barker', display:{Lore:['[{"text": "arXiv:2204.04287", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting Hidden Representations from a DNN-based Speech Recogniser for Speech Intelligibility Prediction in Hearing-impaired Listeners\\u00a7r\\n\\n\\u00a78\\u00a7oZehai Tu\\nNing Ma\\nJon Barker\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.04287\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Jul 2022 13:59:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH2022\\u00a7r"}']}
{title:'Tu et al. (§72022§r)', author: 'Zehai Tu; Ning Ma; Jon Barker', display:{Lore:['[{"text": "arXiv:2204.04288", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Uncertainty Measures of Automatic Speech Recognition for Non-intrusive Speech Intelligibility Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oZehai Tu\\nNing Ma\\nJon Barker\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.04288\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Jul 2022 14:03:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH2022\\u00a7r"}']}
{title:'Lee et al. (§72022§r)', author: 'Shih-Kuang Lee; Yu Tsao; Hsin-Min Wang', display:{Lore:['[{"text": "arXiv:2204.04333", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Study of Using Cepstrogram for Countermeasure Against Replay Attacks\\u00a7r\\n\\n\\u00a78\\u00a7oShih-Kuang Lee\\nYu Tsao\\nHsin-Min Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.04333\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 26 Oct 2022 13:08:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to SLT 2022\\u00a7r"}']}
{title:'Oshiro (§72022§r)', author: 'Scott Oshiro', display:{Lore:['[{"text": "arXiv:2204.04370", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.ET\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a75quant-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQuiKo: A Quantum Beat Generation Application\\u00a7r\\n\\n\\u00a78\\u00a7oScott Oshiro\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.04370\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 26 Apr 2022 19:33:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPre-publication draft, to appear in the book \\"Quantum Computer Music\\", E. R. Miranda (Ed.)\\u00a7r"}']}
{title:'Delcroix et al. (§72022§r)', author: 'Marc Delcroix; Keisuke Kinoshita; Tsubasa Ochiai; Katerina Zmolikova; Hiroshi Sato; Tomohiro Nakatani', display:{Lore:['[{"text": "arXiv:2204.04811", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lListen only to me! How well can target speech extraction handle false alarms?\\u00a7r\\n\\n\\u00a78\\u00a7oMarc Delcroix\\nKeisuke Kinoshita\\nTsubasa Ochiai\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.04811\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Jul 2022 05:47:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Tan et al. (§72022§r)', author: 'Daxin Tan; Liqun Deng; Nianzu Zheng; Yu Ting Yeung; Xin Jiang; Xiao Chen; Tan Lee', display:{Lore:['[{"text": "arXiv:2204.05460", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCorrectSpeech: A Fully Automated System for Speech Correction and Accent Reduction\\u00a7r\\n\\n\\u00a78\\u00a7oDaxin Tan\\nLiqun Deng\\nNianzu Zheng\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.05460\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 14 Oct 2022 02:31:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ISCSLP 2022\\u00a7r"}']}
{title:'Schuller (§72022§r)', author: 'Gerald Schuller', display:{Lore:['[{"text": "arXiv:2204.05609", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow Latency Time Domain Multichannel Speech and Music Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oGerald Schuller\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.05609\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n55th Asilomar Conference on Signals, Systems, and Computers, ACSSC\\n  2021, Pacific Grove, CA, USA, October 31 - November 3, 2021. IEEE 2021, ISBN\\n  978-1-6654-5828-3\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Apr 2022 08:17:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper was published at the Asilomar Conference on Signals, Systems, and Computers in November 2021. A software repository for the paper is: https://github.com/TUIlmenauAMS/LowDelayMultichannelSourceSeparation_Random"}','{"text": "-Directions_Demo\\u00a7r"}']}
{title:'Kilgour et al. (§72022§r)', author: 'Kevin Kilgour; Beat Gfeller; Qingqing Huang; Aren Jansen; Scott Wisdom; Marco Tagliasacchi', display:{Lore:['[{"text": "arXiv:2204.05738", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lText-Driven Separation of Arbitrary Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oKevin Kilgour\\nBeat Gfeller\\nQingqing Huang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.05738\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Apr 2022 12:26:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Bae et al. (§72022§r)', author: 'Hanbin Bae; Young-Sun Joo', display:{Lore:['[{"text": "arXiv:2204.05753", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancement of Pitch Controllability using Timbre-Preserving Pitch Augmentation in FastPitch\\u00a7r\\n\\n\\u00a78\\u00a7oHanbin Bae\\nYoung-Sun Joo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.05753\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Apr 2022 12:48:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Haohe Liu; Xubo Liu; Qiuqiang Kong; Qiao Tian; Yan Zhao; DeLiang Wang; Chuanzeng Huang; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2204.05841", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoiceFixer: A Unified Framework for High-Fidelity Speech Restoration\\u00a7r\\n\\n\\u00a78\\u00a7oHaohe Liu\\nXubo Liu\\nQiuqiang Kong\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.05841\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-11026\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2022\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 17 Apr 2022 10:08:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Ding et al. (§72022§r)', author: 'Shaojin Ding; Weiran Wang; Ding Zhao; Tara N. Sainath; Yanzhang He; Robert David; Rami Botros; Xin Wang; Rina Panigrahy; Qiao Liang; Dongseong Hwang; Ian McGraw; Rohit Prabhavalkar; Trevor Strohman', display:{Lore:['[{"text": "arXiv:2204.06164", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Unified Cascaded Encoder ASR Model for Dynamic Model Sizes\\u00a7r\\n\\n\\u00a78\\u00a7oShaojin Ding\\nWeiran Wang\\nDing Zhao\\n+ 10 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.06164\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 24 Jun 2022 22:33:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2022\\u00a7r"}']}
{title:'Hard et al. (§72022§r)', author: 'Andrew Hard; Kurt Partridge; Neng Chen; Sean Augenstein; Aishanee Shah; Hyun Jin Park; Alex Park; Sara Ng; Jessica Nguyen; Ignacio Lopez Moreno; Rajiv Mathews; Françoise Beaufays', display:{Lore:['[{"text": "arXiv:2204.06322", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProduction federated keyword spotting via distillation, filtering, and joint federated-centralized training\\u00a7r\\n\\n\\u00a78\\u00a7oAndrew Hard\\nKurt Partridge\\nNeng Chen\\n+ 8 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.06322\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 29 Jun 2022 21:03:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Moliner et al. (§72022§r)', author: 'Eloi Moliner; Vesa Välimäki', display:{Lore:['[{"text": "arXiv:2204.06478", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBEHM-GAN: Bandwidth Extension of Historical Music using Generative Adversarial Networks\\u00a7r\\n\\n\\u00a78\\u00a7oEloi Moliner\\nVesa V\\u00e4lim\\u00e4ki\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.06478\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 28 Jun 2022 08:20:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at IEEETransactions on Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Scharf et al. (§72022§r)', author: 'Maximilian Karl Scharf; Sabine Hochmuth; Lena L. N. Wong; Birger Kollmeier; Anna Warzybok', display:{Lore:['[{"text": "arXiv:2204.06907", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLombard Effect for Bilingual Speakers in Cantonese and English: importance of spectro-temporal features\\u00a7r\\n\\n\\u00a78\\u00a7oMaximilian Karl Scharf\\nSabine Hochmuth\\nLena L. N. Wong\\nBirger Kollmeier\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.06907\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Apr 2022 12:06:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH2022\\u00a7r"}']}
{title:'Ozturk et al. (§72022§r)', author: 'Muhammed Zahid Ozturk; Chenshu Wu; Beibei Wang; Min Wu; K. J. Ray Liu', display:{Lore:['[{"text": "arXiv:2204.07092", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRadioSES: mmWave-Based Audioradio Speech Enhancement and Separation System\\u00a7r\\n\\n\\u00a78\\u00a7oMuhammed Zahid Ozturk\\nChenshu Wu\\nBeibei Wang\\nMin Wu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.07092\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Apr 2022 16:33:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProject webpage: https://zahidozt.github.io/RadioSES/\\u00a7r"}']}
{title:'Nishida et al. (§72022§r)', author: 'Tomoya Nishida; Kota Dohi; Takashi Endo; Masaaki Yamamoto; Yohei Kawaguchi', display:{Lore:['[{"text": "arXiv:2204.07353", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnomalous Sound Detection Based on Machine Activity Detection\\u00a7r\\n\\n\\u00a78\\u00a7oTomoya Nishida\\nKota Dohi\\nTakashi Endo\\nMasaaki Yamamoto\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.07353\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Apr 2022 07:23:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 1 table\\u00a7r"}']}
{title:'Zhao et al. (§72022§r)', author: 'Zifeng Zhao; Rongzhi Gu; Dongchao Yang; Jinchuan Tian; Yuexian Zou', display:{Lore:['[{"text": "arXiv:2204.07375", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker-Aware Mixture of Mixtures Training for Weakly Supervised Speaker Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oZifeng Zhao\\nRongzhi Gu\\nDongchao Yang\\nJinchuan Tian\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.07375\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Apr 2022 08:22:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 tables, 4 figures. Submitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Niizumi et al. (§72022§r)', author: 'Daisuke Niizumi; Daiki Takeuchi; Yasunori Ohishi; Noboru Harada; Kunio Kashino', display:{Lore:['[{"text": "arXiv:2204.07402", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBYOL for Audio: Exploring Pre-trained General-purpose Audio Representations\\u00a7r\\n\\n\\u00a78\\u00a7oDaisuke Niizumi\\nDaiki Takeuchi\\nYasunori Ohishi\\nNoboru Harada\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.07402\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2022.3221007\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Trans. Audio, Speech, Language Process. 31 (2023) 137-151\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 16 Jun 2022 05:25:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 6 figures, and 15tables. Under the review process\\u00a7r"}']}
{title:'Cwitkowitz et al. (§72022§r)', author: 'Frank Cwitkowitz; Jonathan Driedger; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2204.08094", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Data-Driven Methodology for Considering Feasibility and Pairwise Likelihood in Deep Learning Based Guitar Tablature Transcription Systems\\u00a7r\\n\\n\\u00a78\\u00a7oFrank Cwitkowitz\\nJonathan Driedger\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.08094\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 17 Apr 2022 22:10:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSound and Music Computing Conference (SMC) 2022\\u00a7r"}']}
{title:'Wen et al. (§72022§r)', author: 'Cheng Wen; Tingwei Guo; Xingjun Tan; Rui Yan; Shuran Zhou; Chuandong Xie; Wei Zou; Xiangang Li', display:{Lore:['[{"text": "arXiv:2204.08692", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTime Domain Adversarial Voice Conversion for ADD 2022\\u00a7r\\n\\n\\u00a78\\u00a7oCheng Wen\\nTingwei Guo\\nXingjun Tan\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.08692\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Apr 2022 03:20:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022\\u00a7r"}']}
{title:'Yan et al. (§72022§r)', author: 'Rui Yan; Cheng Wen; Shuran Zhou; Tingwei Guo; Wei Zou; Xiangang Li', display:{Lore:['[{"text": "arXiv:2204.08720", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Deep Fake Detection System with Neural Stitching for ADD 2022\\u00a7r\\n\\n\\u00a78\\u00a7oRui Yan\\nCheng Wen\\nShuran Zhou\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.08720\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Apr 2022 03:18:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022\\u00a7r"}']}
{title:'Zhou et al. (§72022§r)', author: 'Rui Zhou; Wenye Zhu; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2204.08765", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Dereverberation with A Reverberation Time Shortening Target\\u00a7r\\n\\n\\u00a78\\u00a7oRui Zhou\\nWenye Zhu\\nXiaofei Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.08765\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 21 Nov 2022 02:28:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Moritz et al. (§72022§r)', author: 'Niko Moritz; Frank Seide; Duc Le; Jay Mahadeokar; Christian Fuegen', display:{Lore:['[{"text": "arXiv:2204.08858", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Investigation of Monotonic Transducers for Large-Scale Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oNiko Moritz\\nFrank Seide\\nDuc Le\\nJay Mahadeokar\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.08858\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 21 Oct 2022 19:00:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to SLT 2022\\u00a7r"}']}
{title:'Zhu et al. (§72022§r)', author: 'Ge Zhu; Jordan Darefsky; Fei Jiang; Anton Selitskiy; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2204.09079", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Source Separation with Generative Flow\\u00a7r\\n\\n\\u00a78\\u00a7oGe Zhu\\nJordan Darefsky\\nFei Jiang\\nAnton Selitskiy\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.09079\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2022.3219355\\u00a7r\\n\\nVersion:\\u00a77v4 (Sun, 16 Oct 2022 20:33:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Signal Processing Letters\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'Rongjie Huang; Max W. Y. Lam; Jun Wang; Dan Su; Dong Yu; Yi Ren; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2204.09934", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oRongjie Huang\\nMax W. Y. Lam\\nJun Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.09934\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Apr 2022 07:49:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IJCAI 2022\\u00a7r"}']}
{title:'Terashima et al. (§72022§r)', author: 'Ryo Terashima; Ryuichi Yamamoto; Eunwoo Song; Yuma Shirahata; Hyun-Wook Yoon; Jae-Min Kim; Kentaro Tachibana', display:{Lore:['[{"text": "arXiv:2204.10020", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-Speaker Emotion Transfer for Low-Resource Text-to-Speech Using Non-Parallel Voice Conversion with Pitch-Shift Data Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oRyo Terashima\\nRyuichi Yamamoto\\nEunwoo Song\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.10020\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 5 Jul 2022 12:20:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Jiudong Yang; Peiying Wang; Yi Zhu; Mingchao Feng; Meng Chen; Xiaodong He', display:{Lore:['[{"text": "arXiv:2204.10172", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGated Multimodal Fusion with Contrastive Learning for Turn-taking Prediction in Human-robot Dialogue\\u00a7r\\n\\n\\u00a78\\u00a7oJiudong Yang\\nPeiying Wang\\nYi Zhu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.10172\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Apr 2022 05:18:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Sadjadi et al. (§72022§r)', author: 'Seyed Omid Sadjadi; Craig Greenberg; Elliot Singer; Lisa Mason; Douglas Reynolds', display:{Lore:['[{"text": "arXiv:2204.10228", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe NIST CTS Speaker Recognition Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oSeyed Omid Sadjadi\\nCraig Greenberg\\nElliot Singer\\nLisa Mason\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.10228\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Apr 2022 16:06:27 GMT)\\u00a7r"}']}
{title:'Sadjadi et al. (§72022§r)', author: 'Seyed Omid Sadjadi; Craig Greenberg; Elliot Singer; Lisa Mason; Douglas Reynolds', display:{Lore:['[{"text": "arXiv:2204.10242", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe 2021 NIST Speaker Recognition Evaluation\\u00a7r\\n\\n\\u00a78\\u00a7oSeyed Omid Sadjadi\\nCraig Greenberg\\nElliot Singer\\nLisa Mason\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.10242\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Apr 2022 16:18:52 GMT)\\u00a7r"}']}
{title:'Gyires-Tóth et al. (§72022§r)', author: 'Bálint Gyires-Tóth; Csaba Zainkó', display:{Lore:['[{"text": "arXiv:2204.11030", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Self-Supervised Learning-based MOS Prediction Networks\\u00a7r\\n\\n\\u00a78\\u00a7oB\\u00e1lint Gyires-T\\u00f3th\\nCsaba Zaink\\u00f3\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.11030\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 23 Apr 2022 09:19:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Han et al. (§72022§r)', author: 'Jiangyu Han; Yanhua Long', display:{Lore:['[{"text": "arXiv:2204.11032", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHeterogeneous Separation Consistency Training for Adaptation of Unsupervised Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oJiangyu Han\\nYanhua Long\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.11032\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 6 Aug 2022 08:43:26 GMT)\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Yanxiong Li; Wucheng Wang; Hao Chen; Wenchang Cao; Wei Li; Qianhua He', display:{Lore:['[{"text": "arXiv:2204.11180", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFew-Shot Speaker Identification Using Depthwise Separable Convolutional Network with Channel Attention\\u00a7r\\n\\n\\u00a78\\u00a7oYanxiong Li\\nWucheng Wang\\nHao Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.11180\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 24 Apr 2022 03:31:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Odyssey 2022 (The Speaker and Language Recognition Workshop 2022, Beijing, China)\\u00a7r"}']}
{title:'Yamashita et al. (§72022§r)', author: 'Natsuo Yamashita; Shota Horiguchi; Takeshi Homma', display:{Lore:['[{"text": "arXiv:2204.11232", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving the Naturalness of Simulated Conversations for End-to-End Neural Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oNatsuo Yamashita\\nShota Horiguchi\\nTakeshi Homma\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.11232\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 24 Apr 2022 09:55:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Speaker Odyssey 2022\\u00a7r"}']}
{title:'Kumar et al. (§72022§r)', author: 'Shashi Kumar; Shakti P. Rath; Abhishek Pandey', display:{Lore:['[{"text": "arXiv:2204.11286", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved far-field speech recognition using Joint Variational Autoencoder\\u00a7r\\n\\n\\u00a78\\u00a7oShashi Kumar\\nShakti P. Rath\\nAbhishek Pandey\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.11286\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 24 Apr 2022 14:14:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 3 tables\\u00a7r"}']}
{title:'Tong et al. (§72022§r)', author: 'Fuchuan Tong; Siqi Zheng; Min Zhang; Yafeng Chen; Hongbin Suo; Qingyang Hong; Lin Li', display:{Lore:['[{"text": "arXiv:2204.11501", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGraph Convolutional Network Based Semi-Supervised Learning on Multi-Speaker Meeting Data\\u00a7r\\n\\n\\u00a78\\u00a7oFuchuan Tong\\nSiqi Zheng\\nMin Zhang\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.11501\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Apr 2022 08:30:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Xian Li; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2204.12076", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lATST: Audio Representation Learning with Teacher-Student Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oXian Li\\nXiaofei Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.12076\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10126\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 23 Sep 2022 06:06:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH2022(Accepted)\\u00a7r"}']}
{title:'Narayanan et al. (§72022§r)', author: 'Arun Narayanan; James Walker; Sankaran Panchapagesan; Nathan Howard; Yuma Koizumi', display:{Lore:['[{"text": "arXiv:2204.12092", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMask scalar prediction for improving robust automatic speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oArun Narayanan\\nJames Walker\\nSankaran Panchapagesan\\nNathan Howard\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.12092\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Apr 2022 06:10:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Niizumi et al. (§72022§r)', author: 'Daisuke Niizumi; Daiki Takeuchi; Yasunori Ohishi; Noboru Harada; Kunio Kashino', display:{Lore:['[{"text": "arXiv:2204.12260", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMasked Spectrogram Modeling using Masked Autoencoders for Learning General-purpose Audio Representation\\u00a7r\\n\\n\\u00a78\\u00a7oDaisuke Niizumi\\nDaiki Takeuchi\\nYasunori Ohishi\\nNoboru Harada\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.12260\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nHEAR: Holistic Evaluation of Audio Representations (NeurIPS 2021\\n  Competition) PMLR 166 (2022) 1-24\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Apr 2022 12:32:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o22 pages, 8 figures. Underthe review process\\u00a7r"}']}
{title:'Pagliarini et al. (§72022§r)', author: 'Silvia Pagliarini; Sara Schneider; Christopher T. Kello; Anne S. Warlaumont', display:{Lore:['[{"text": "arXiv:2204.12279", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-dimensional representation of infant and adult vocalization acoustics\\u00a7r\\n\\n\\u00a78\\u00a7oSilvia Pagliarini\\nSara Schneider\\nChristopher T. Kello\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.12279\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Apr 2022 17:58:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review at Interspeech 2022\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Gene-Ping Yang; Hao Tang', display:{Lore:['[{"text": "arXiv:2204.12308", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSupervised Attention in Sequence-to-Sequence Models for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGene-Ping Yang\\nHao Tang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.12308\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Apr 2022 15:38:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2022\\u00a7r"}']}
{title:'Estevez et al. (§72022§r)', author: 'Mariel Estevez; Luciana Ferrer', display:{Lore:['[{"text": "arXiv:2204.12649", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStudy on the Fairness of Speaker Verification Systems on Underrepresented Accents in English\\u00a7r\\n\\n\\u00a78\\u00a7oMariel Estevez\\nLuciana Ferrer\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.12649\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Apr 2022 01:25:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, submitted to INTERSPEECH\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Sanyuan Chen; Yu Wu; Zhuo Chen; Jian Wu; Takuya Yoshioka; Shujie Liu; Jinyu Li; Xiangzhan Yu', display:{Lore:['[{"text": "arXiv:2204.12777", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUltra Fast Speech Separation Model with Teacher Student Learning\\u00a7r\\n\\n\\u00a78\\u00a7oSanyuan Chen\\nYu Wu\\nZhuo Chen\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.12777\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-142\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Apr 2022 09:02:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by interspeech 2021\\u00a7r"}']}
{title:'Watcharasupat et al. (§72022§r)', author: 'Karn N. Watcharasupat; Kenneth Ooi; Bhan Lam; Trevor Wong; Zhen-Ting Ong; Woon-Seng Gan', display:{Lore:['[{"text": "arXiv:2204.13883", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutonomous In-Situ Soundscape Augmentation via Joint Selection of Masker and Gain\\u00a7r\\n\\n\\u00a78\\u00a7oKarn N. Watcharasupat\\nKenneth Ooi\\nBhan Lam\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.13883\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2022.3194419\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Signal Processing Letters, Vol. 29, pp. 1749 - 1753, 2022\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 23 Jul 2022 13:45:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEESignal Processing Letters. (c) 2022 IEEE\\u00a7r"}']}
{title:'Wong et al. (§72022§r)', author: 'Trevor Wong; Karn N. Watcharasupat; Bhan Lam; Kenneth Ooi; Zhen-Ting Ong; Furi Andi Karnapi; Woon-Seng Gan', display:{Lore:['[{"text": "arXiv:2204.13890", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7acs.SY\\u00a7r, \\u00a7eeess.SY\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeployment of an IoT System for Adaptive In-Situ Soundscape Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oTrevor Wong\\nKarn N. Watcharasupat\\nBhan Lam\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.13890\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3397/IN_2022_0290\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nINTER-NOISE and NOISE-CON Congress and Conference Proceedings,\\n  Feb. 2022, vol. 265, no. 5, pp. 2013-2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Apr 2022 05:34:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be presented at the 51st International Congressand Exposition on Noise Control Engineering\\u00a7r"}']}
{title:'Sholokhov et al. (§72022§r)', author: 'Alexey Sholokhov; Xuechen Liu; Md Sahidullah; Tomi Kinnunen', display:{Lore:['[{"text": "arXiv:2205.00288", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBaselines and Protocols for Household Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAlexey Sholokhov\\nXuechen Liu\\nMd Sahidullah\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.00288\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 5 May 2022 16:52:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Odyssey 2022\\u00a7r"}']}
{title:'Gburrek et al. (§72022§r)', author: 'Tobias Gburrek; Christoph Boeddeker; Thilo von Neumann; Tobias Cord-Landwehr; Joerg Schmalenstroeer; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2205.00944", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Meeting Transcription System for an Ad-Hoc Acoustic Sensor Network\\u00a7r\\n\\n\\u00a78\\u00a7oTobias Gburrek\\nChristoph Boeddeker\\nThilo von Neumann\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.00944\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 2 May 2022 14:43:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Xu et al. (§72022§r)', author: 'Xinmeng Xu; Rongzhi Gu; Yuexian Zou', display:{Lore:['[{"text": "arXiv:2205.01280", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Dual-Microphone Speech Enhancement by Learning Cross-Channel Features with Multi-Head Attention\\u00a7r\\n\\n\\u00a78\\u00a7oXinmeng Xu\\nRongzhi Gu\\nYuexian Zou\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.01280\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746359\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 May 2022 02:58:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Donghyeon Kim; Gwantae Kim; Bokyeung Lee; Jeong-gi Kwak; David K. Han; Hanseok Ko', display:{Lore:['[{"text": "arXiv:2205.01304", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient dynamic filter for robust and low computational feature extraction\\u00a7r\\n\\n\\u00a78\\u00a7oDonghyeon Kim\\nGwantae Kim\\nBokyeung Lee\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.01304\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 20 Oct 2022 18:29:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccept to SLT2022\\u00a7r"}']}
{title:'Kang et al. (§72022§r)', author: 'Woo Hyun Kang; Jahangir Alam; Abderrahim Fathan', display:{Lore:['[{"text": "arXiv:2205.01528", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttentive activation function for improving end-to-end spoofing countermeasure systems\\u00a7r\\n\\n\\u00a78\\u00a7oWoo Hyun Kang\\nJahangir Alam\\nAbderrahim Fathan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.01528\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 May 2022 14:30:00 GMT)\\u00a7r"}']}
{title:'Baird et al. (§72022§r)', author: 'Alice Baird; Panagiotis Tzirakis; Gauthier Gidel; Marco Jiralerspong; Eilif B. Muller; Kory Mathewson; Björn Schuller; Erik Cambria; Dacher Keltner; Alan Cowen', display:{Lore:['[{"text": "arXiv:2205.01780", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe ICML 2022 Expressive Vocalizations Workshop and Competition: Recognizing, Generating, and Personalizing Vocal Bursts\\u00a7r\\n\\n\\u00a78\\u00a7oAlice Baird\\nPanagiotis Tzirakis\\nGauthier Gidel\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.01780\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 12 Jul 2022 16:32:32 GMT)\\u00a7r"}']}
{title:'Wilczek et al. (§72022§r)', author: 'Jan Wilczek; Alec Wright; Vesa Välimäki; Emanuël Habets', display:{Lore:['[{"text": "arXiv:2205.01897", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVirtual Analog Modeling of Distortion Circuits Using Neural Ordinary Differential Equations\\u00a7r\\n\\n\\u00a78\\u00a7oJan Wilczek\\nAlec Wright\\nVesa V\\u00e4lim\\u00e4ki\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.01897\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 1 Jul 2022 06:47:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 10 figures, accepted for DAFx 2022 conference, for associated audio examples, see https://thewolfsound.com/publications/dafx2022/\\u00a7r"}']}
{title:'Xu et al. (§72022§r)', author: 'Ziyi Xu; Maximilian Strake; Tim Fingscheidt', display:{Lore:['[{"text": "arXiv:2205.02085", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDoes a PESQNet (Loss) Require a Clean Reference Input? The Original PESQ Does, But ACR Listening Tests Don\'t\\u00a7r\\n\\n\\u00a78\\u00a7oZiyi Xu\\nMaximilian Strake\\nTim Fingscheidt\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.02085\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 13 May 2022 11:29:30 GMT)\\u00a7r"}']}
{title:'Ribeiro et al. (§72022§r)', author: 'Juliano G. C. Ribeiro; Shoichi Koyama; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2205.02750", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRegion-to-region kernel interpolation of acoustic transfer function with directional weighting\\u00a7r\\n\\n\\u00a78\\u00a7oJuliano G. C. Ribeiro\\nShoichi Koyama\\nHiroshi Saruwatari\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.02750\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746842\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2022 - 2022 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP), 2022, pp. 576-580\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 5 May 2022 16:30:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\\u00a7r"}']}
{title:'Panchapagesan et al. (§72022§r)', author: 'Sankaran Panchapagesan; Arun Narayanan; Turaj Zakizadeh Shabestary; Shuai Shao; Nathan Howard; Alex Park; James Walker; Alexander Gruenstein', display:{Lore:['[{"text": "arXiv:2205.03481", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Conformer-based Waveform-domain Neural Acoustic Echo Canceller Optimized for ASR Accuracy\\u00a7r\\n\\n\\u00a78\\u00a7oSankaran Panchapagesan\\nArun Narayanan\\nTuraj Zakizadeh Shabestary\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.03481\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 May 2022 21:35:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Ochiai et al. (§72022§r)', author: 'Tsubasa Ochiai; Marc Delcroix; Tomohiro Nakatani; Shoko Araki', display:{Lore:['[{"text": "arXiv:2205.03568", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMask-based Neural Beamforming for Moving Speakers with Self-Attention-based Tracking\\u00a7r\\n\\n\\u00a78\\u00a7oTsubasa Ochiai\\nMarc Delcroix\\nTomohiro Nakatani\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.03568\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 7 May 2022 06:23:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 7 figures, Submitted to IEEE/ACM Trans. Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Tsai et al. (§72022§r)', author: 'Yuefeng Tsai; Yicheng Hsu; Mingsian Bai', display:{Lore:['[{"text": "arXiv:2205.03594", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic echo suppression using a learning-based multi-frame minimum variance distortionless response filter\\u00a7r\\n\\n\\u00a78\\u00a7oYuefeng Tsai\\nYicheng Hsu\\nMingsian Bai\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.03594\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 7 May 2022 08:35:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to International Workshopon Acoustic Signal Enhancement (IWAENC) 2022\\u00a7r"}']}
{title:'Oh et al. (§72022§r)', author: 'Sangshin Oh; Seyun Um; Hong-Goo Kang', display:{Lore:['[{"text": "arXiv:2205.04104", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReCAB-VAE: Gumbel-Softmax Variational Inference Based on Analytic Divergence\\u00a7r\\n\\n\\u00a78\\u00a7oSangshin Oh\\nSeyun Um\\nHong-Goo Kang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.04104\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 9 May 2022 08:11:46 GMT)\\u00a7r"}']}
{title:'Seidel et al. (§72022§r)', author: 'Ernst Seidel; Rasmus Kongsgaard Olsson; Karim Haddad; Zhengyang Li; Pejman Mowlaee; Tim Fingscheidt', display:{Lore:['[{"text": "arXiv:2205.04276", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBandwidth-Scalable Fully Mask-Based Deep FCRN Acoustic Echo Cancellation and Postfiltering\\u00a7r\\n\\n\\u00a78\\u00a7oErnst Seidel\\nRasmus Kongsgaard Olsson\\nKarim Haddad\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.04276\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 7 Nov 2022 12:42:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, accepted for IWAENC 2022\\u00a7r"}']}
{title:'Tan et al. (§72022§r)', author: 'Xu Tan; Jiawei Chen; Haohe Liu; Jian Cong; Chen Zhang; Yanqing Liu; Xi Wang; Yichong Leng; Yuanhao Yi; Lei He; Frank Soong; Tao Qin; Sheng Zhao; Tie-Yan Liu', display:{Lore:['[{"text": "arXiv:2205.04421", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level Quality\\u00a7r\\n\\n\\u00a78\\u00a7oXu Tan\\nJiawei Chen\\nHaohe Liu\\n+ 10 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.04421\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 10 May 2022 15:25:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o19 pages, 3 figures, 8 tables\\u00a7r"}']}
{title:'Zorila et al. (§72022§r)', author: 'Catalin Zorila; Rama Doddipatla', display:{Lore:['[{"text": "arXiv:2205.04433", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Reinforcement Using Target Source Extraction for Robust Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oCatalin Zorila\\nRama Doddipatla\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.04433\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747584\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 9 May 2022 17:34:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP 2022\\u00a7r"}']}
{title:'Lam et al. (§72022§r)', author: 'Bhan Lam; Kenneth Ooi; Karn N. Watcharasupat; Zhen-Ting Ong; Yun-Ting Lau; Trevor Wong; Woon-Seng Gan', display:{Lore:['[{"text": "arXiv:2205.04728", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPreliminary assessment of a cost-effective headphone calibration procedure for soundscape evaluations\\u00a7r\\n\\n\\u00a78\\u00a7oBhan Lam\\nKenneth Ooi\\nKarn N. Watcharasupat\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.04728\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 May 2022 08:01:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to the 28th International Congress on Sound andVibration\\u00a7r"}']}
{title:'Sklyar et al. (§72022§r)', author: 'Ilya Sklyar; Anna Piunova; Christian Osendorfer', display:{Lore:['[{"text": "arXiv:2205.05199", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSeparator-Transducer-Segmenter: Streaming Recognition and Segmentation of Multi-party Speech\\u00a7r\\n\\n\\u00a78\\u00a7oIlya Sklyar\\nAnna Piunova\\nChristian Osendorfer\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.05199\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 May 2022 22:40:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to InterSpeech 2022\\u00a7r"}']}
{title:'Braga et al. (§72022§r)', author: 'Otavio Braga; Olivier Siohan', display:{Lore:['[{"text": "arXiv:2205.05206", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBest of Both Worlds: Multi-task Audio-Visual Automatic Speech Recognition and Active Speaker Detection\\u00a7r\\n\\n\\u00a78\\u00a7oOtavio Braga\\nOlivier Siohan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.05206\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 May 2022 23:03:19 GMT)\\u00a7r"}']}
{title:'Lian et al. (§72022§r)', author: 'Jiachen Lian; Chunlei Zhang; Gopala Krishna Anumanchipalli; Dong Yu', display:{Lore:['[{"text": "arXiv:2205.05227", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Improved Zero-shot Voice Conversion with Conditional DSVAE\\u00a7r\\n\\n\\u00a78\\u00a7oJiachen Lian\\nChunlei Zhang\\nGopala Krishna Anumanchipalli\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.05227\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 20 Jun 2022 23:30:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to 2022 Interspeech. Demo link is here https://jlian2.github.io/Improved-Voice-Conversion-with-Conditional-DSVAE/\\u00a7r"}']}
{title:'Schröter et al. (§72022§r)', author: 'Hendrik Schröter; Alberto N. Escalante-B.; Tobias Rosenkranz; Andreas Maier', display:{Lore:['[{"text": "arXiv:2205.05474", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeepFilterNet2: Towards Real-Time Speech Enhancement on Embedded Devices for Full-Band Audio\\u00a7r\\n\\n\\u00a78\\u00a7oHendrik Schr\\u00f6ter\\nAlberto N. Escalante-B.\\nTobias Rosenkranz\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.05474\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 11 May 2022 13:19:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IWAENC 2022\\u00a7r"}']}
{title:'Peer et al. (§72022§r)', author: 'Tal Peer; Simon Welker; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2205.05496", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBeyond Griffin-Lim: Improved Iterative Phase Retrieval for Speech\\u00a7r\\n\\n\\u00a78\\u00a7oTal Peer\\nSimon Welker\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.05496\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/IWAENC53105.2022.9914686\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 11 May 2022 13:38:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IWAENC 2022\\u00a7r"}']}
{title:'Xiang et al. (§72022§r)', author: 'Yang Xiang; Jesper Lisby Højvang; Morten Højfeldt Rasmussen; Mads Græsbøll Christensen', display:{Lore:['[{"text": "arXiv:2205.05581", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA deep representation learning speech enhancement method using \\u03b2-VAE\\u00a7r\\n\\n\\u00a78\\u00a7oYang Xiang\\nJesper Lisby H\\u00f8jvang\\nMorten H\\u00f8jfeldt Rasmussen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.05581\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 11 May 2022 15:49:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Eurosipco\\u00a7r"}']}
{title:'Braga et al. (§72022§r)', author: 'Otavio Braga; Takaki Makino; Olivier Siohan; Hank Liao', display:{Lore:['[{"text": "arXiv:2205.05586", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Multi-Person Audio/Visual Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oOtavio Braga\\nTakaki Makino\\nOlivier Siohan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.05586\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 11 May 2022 15:57:47 GMT)\\u00a7r"}']}
{title:'Braga et al. (§72022§r)', author: 'Otavio Braga; Olivier Siohan', display:{Lore:['[{"text": "arXiv:2205.05684", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Closer Look at Audio-Visual Multi-Person Speech Recognition and Active Speaker Selection\\u00a7r\\n\\n\\u00a78\\u00a7oOtavio Braga\\nOlivier Siohan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.05684\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 11 May 2022 15:55:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2205.05586\\u00a7r"}']}
{title:'Valin et al. (§72022§r)', author: 'Jean-Marc Valin; Ahmed Mustafa; Christopher Montgomery; Timothy B. Terriberry; Michael Klingbeil; Paris Smaragdis; Arvindh Krishnaswamy', display:{Lore:['[{"text": "arXiv:2205.05785", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-Time Packet Loss Concealment With Mixed Generative and Predictive Model\\u00a7r\\n\\n\\u00a78\\u00a7oJean-Marc Valin\\nAhmed Mustafa\\nChristopher Montgomery\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.05785\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 11 May 2022 21:55:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Mei et al. (§72022§r)', author: 'Xinhao Mei; Xubo Liu; Mark D. Plumbley; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2205.05949", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomated Audio Captioning: An Overview of Recent Progress and New Challenges\\u00a7r\\n\\n\\u00a78\\u00a7oXinhao Mei\\nXubo Liu\\nMark D. Plumbley\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.05949\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1186/s13636-022-00259-2\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 27 Sep 2022 01:24:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by EURASIP Journal on Audio Speech and Music Processing\\u00a7r"}']}
{title:'Ohlenbusch et al. (§72022§r)', author: 'Mattes Ohlenbusch; Christian Rollwage; Simon Doclo', display:{Lore:['[{"text": "arXiv:2205.06157", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTraining Strategies for Own Voice Reconstruction in Hearing Protection Devices using an In-ear Microphone\\u00a7r\\n\\n\\u00a78\\u00a7oMattes Ohlenbusch\\nChristian Rollwage\\nSimon Doclo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.06157\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/IWAENC53105.2022.9914801\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 17 Nov 2022 11:06:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IWAENC 2022\\u00a7r"}']}
{title:'Jin et al. (§72022§r)', author: 'Zengrui Jin; Mengzhe Geng; Jiajun Deng; Tianzi Wang; Shujie Hu; Guinan Li; Xunying Liu', display:{Lore:['[{"text": "arXiv:2205.06445", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPersonalized Adversarial Data Augmentation for Dysarthric and Elderly Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZengrui Jin\\nMengzhe Geng\\nJiajun Deng\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.06445\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3323888\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 24 Jun 2022 02:44:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2202.10290\\u00a7r"}']}
{title:'Haubner et al. (§72022§r)', author: 'Thomas Haubner; Zbyněk Koldovský; Walter Kellermann', display:{Lore:['[{"text": "arXiv:2205.06473", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Acoustic Echo Cancellation and Blind Source Extraction based on Independent Vector Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Haubner\\nZbyn\\u011bk Koldovsk\\u00fd\\nWalter Kellermann\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.06473\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 10 Aug 2022 06:58:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for International Workshopon Acoustic Signal Enhancement (IWAENC 2022)\\u00a7r"}']}
{title:'Braun et al. (§72022§r)', author: 'Sebastian Braun; Maria Luis Valero', display:{Lore:['[{"text": "arXiv:2205.06931", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTask splitting for DNN-based acoustic echo and noise removal\\u00a7r\\n\\n\\u00a78\\u00a7oSebastian Braun\\nMaria Luis Valero\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.06931\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 13 Jul 2022 16:38:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oto appear in IEEE IWAENC 2022\\u00a7r"}']}
{title:'Alumäe et al. (§72022§r)', author: 'Tanel Alumäe; Kunnar Kukk', display:{Lore:['[{"text": "arXiv:2205.07083", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPretraining Approaches for Spoken Language Recognition: TalTech Submission to the OLR 2021 Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oTanel Alum\\u00e4e\\nKunnar Kukk\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.07083\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 14 May 2022 15:17:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Speaker Odyssey 2022\\u00a7r"}']}
{title:'Kalda et al. (§72022§r)', author: 'Joonas Kalda; Tanel Alumäe', display:{Lore:['[{"text": "arXiv:2205.07086", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCollar-aware Training for Streaming Speaker Change Detection in Broadcast Speech\\u00a7r\\n\\n\\u00a78\\u00a7oJoonas Kalda\\nTanel Alum\\u00e4e\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.07086\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 14 May 2022 15:35:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Speaker Odyssey 2022\\u00a7r"}']}
{title:'Shi et al. (§72022§r)', author: 'Bowen Shi; Abdelrahman Mohamed; Wei-Ning Hsu', display:{Lore:['[{"text": "arXiv:2205.07180", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Lip-Based Audio-Visual Speaker Embeddings with AV-HuBERT\\u00a7r\\n\\n\\u00a78\\u00a7oBowen Shi\\nAbdelrahman Mohamed\\nWei-Ning Hsu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.07180\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Jul 2022 23:02:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2022\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'Rongjie Huang; Yi Ren; Jinglin Liu; Chenye Cui; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2205.07211", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oRongjie Huang\\nYi Ren\\nJinglin Liu\\nChenye Cui\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.07211\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 12 Oct 2022 13:59:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to NeurIPS 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Zhepei Wang; Cem Subakan; Xilin Jiang; Junkai Wu; Efthymios Tzinis; Mirco Ravanelli; Paris Smaragdis', display:{Lore:['[{"text": "arXiv:2205.07390", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Representations for New Sound Classes With Continual Self-Supervised Learning\\u00a7r\\n\\n\\u00a78\\u00a7oZhepei Wang\\nCem Subakan\\nXilin Jiang\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.07390\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2022.3229643\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 13 Dec 2022 17:12:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEESignal Processing Letters\\u00a7r"}']}
{title:'Aksënova et al. (§72022§r)', author: 'Alëna Aksënova; Zhehuai Chen; Chung-Cheng Chiu; Daan van Esch; Pavel Golik; Wei Han; Levi King; Bhuvana Ramabhadran; Andrew Rosenberg; Suzan Schwartz; Gary Wang', display:{Lore:['[{"text": "arXiv:2205.08014", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAccented Speech Recognition: Benchmarking, Pre-training, and Diverse Data\\u00a7r\\n\\n\\u00a78\\u00a7oAl\\u00ebna Aks\\u00ebnova\\nZhehuai Chen\\nChung-Cheng Chiu\\n+ 7 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.08014\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 16 May 2022 23:01:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 tables\\u00a7r"}']}
{title:'Niizumi et al. (§72022§r)', author: 'Daisuke Niizumi; Daiki Takeuchi; Yasunori Ohishi; Noboru Harada; Kunio Kashino', display:{Lore:['[{"text": "arXiv:2205.08138", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComposing General Audio Representation by Fusing Multilayer Features of a Pre-trained Model\\u00a7r\\n\\n\\u00a78\\u00a7oDaisuke Niizumi\\nDaiki Takeuchi\\nYasunori Ohishi\\nNoboru Harada\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.08138\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 17 May 2022 07:10:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures and 4 tables. Accepted by EUSIPCO 2022\\u00a7r"}']}
{title:'Caroselli et al. (§72022§r)', author: 'Joe Caroselli; Arun Narayanan; Yiteng Huang', display:{Lore:['[{"text": "arXiv:2205.08555", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreaming Noise Context Aware Enhancement For Automatic Speech Recognition in Multi-Talker Environments\\u00a7r\\n\\n\\u00a78\\u00a7oJoe Caroselli\\nArun Narayanan\\nYiteng Huang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.08555\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 17 May 2022 18:00:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IWAENC 2022\\u00a7r"}']}
{title:'Xu et al. (§72022§r)', author: 'Xinmeng Xu; Jianjun Hao', display:{Lore:['[{"text": "arXiv:2205.08681", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lU-Former: Improving Monaural Speech Enhancement with Multi-head Self and Cross Attention\\u00a7r\\n\\n\\u00a78\\u00a7oXinmeng Xu\\nJianjun Hao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.08681\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 12 Oct 2022 09:50:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICPR 2022\\u00a7r"}']}
{title:'Brümann et al. (§72022§r)', author: 'Klaus Brümann; Simon Doclo', display:{Lore:['[{"text": "arXiv:2205.08960", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l3D Single Source Localization Based on Euclidean Distance Matrices\\u00a7r\\n\\n\\u00a78\\u00a7oKlaus Br\\u00fcmann\\nSimon Doclo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.08960\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 18 May 2022 14:33:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages (last page references), 3 figures,1 table, submitted to \\"International Workshop on Acoustic Signal Enhancement (IWAENC), Bamberg, 2022\\"\\u00a7r"}']}
{title:'Tammen et al. (§72022§r)', author: 'Marvin Tammen; Simon Doclo', display:{Lore:['[{"text": "arXiv:2205.08983", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Multi-Frame MVDR Filtering for Binaural Noise Reduction\\u00a7r\\n\\n\\u00a78\\u00a7oMarvin Tammen\\nSimon Doclo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.08983\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/IWAENC53105.2022.9914742\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 14 Nov 2022 09:49:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at IWAENC 2022\\u00a7r"}']}
{title:'Fejgin et al. (§72022§r)', author: 'Daniel Fejgin; Simon Doclo', display:{Lore:['[{"text": "arXiv:2205.08985", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCoherence-Based Frequency Subset Selection For Binaural RTF-Vector-Based Direction of Arrival Estimation for Multiple Speakers\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Fejgin\\nSimon Doclo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.08985\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/IWAENC53105.2022.9914768\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 18 May 2022 15:17:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\\u00a7r"}']}
{title:'Tammen et al. (§72022§r)', author: 'Marvin Tammen; Xilin Li; Simon Doclo; Lalin Theverapperuma', display:{Lore:['[{"text": "arXiv:2205.09017", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDictionary-Based Fusion of Contact and Acoustic Microphones for Wind Noise Reduction\\u00a7r\\n\\n\\u00a78\\u00a7oMarvin Tammen\\nXilin Li\\nSimon Doclo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.09017\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/IWAENC53105.2022.9914710\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 14 Nov 2022 09:55:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at IWAENC 22\\u00a7r"}']}
{title:'Sofronievski et al. (§72022§r)', author: 'Bojan Sofronievski; Elena Velovska; Martin Velichkovski; Violeta Argirova; Tea Veljkovikj; Risto Chavdarov; Stefan Janev; Kristijan Lazarev; Toni Bachvarovski; Zoran Ivanovski; Dimitar Tashkovski; Branislav Gerazov', display:{Lore:['[{"text": "arXiv:2205.09198", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMacedonian Speech Synthesis for Assistive Technology Applications\\u00a7r\\n\\n\\u00a78\\u00a7oBojan Sofronievski\\nElena Velovska\\nMartin Velichkovski\\n+ 8 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.09198\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.23919/EUSIPCO55093.2022.9909778\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 18 Jun 2022 17:14:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, EUSIPCO conference 2022\\u00a7r"}']}
{title:'Götz et al. (§72022§r)', author: 'Georg Götz; Ricardo Falcón Pérez; Sebastian J. Schlecht; Ville Pulkki', display:{Lore:['[{"text": "arXiv:2205.09644", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural network for multi-exponential sound energy decay analysis\\u00a7r\\n\\n\\u00a78\\u00a7oGeorg G\\u00f6tz\\nRicardo Falc\\u00f3n P\\u00e9rez\\nSebastian J. Schlecht\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.09644\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0013416\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJ. Acoust. Soc. Am., Vol. 152, No. 2, pp. 942-953, 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 19 May 2022 16:03:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe following article has been submitted to the Journal of the Acoustical Society of America (JASA). After it is published, it will be found at http://asa.scitation.org/journal/jas\\u00a7r"}']}
{title:'Nijhawan et al. (§72022§r)', author: 'Siddharth S. Nijhawan; Homayoon Beigi', display:{Lore:['[{"text": "arXiv:2205.09709", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBi-LSTM Scoring Based Similarity Measurement with Agglomerative Hierarchical Clustering (AHC) for Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oSiddharth S. Nijhawan\\nHomayoon Beigi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.09709\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.13140/RG.2.2.13977.29288\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 19 May 2022 17:20:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 3 figures, 2 tables, 1 algorithm, Technical Report: Recognition Technologies, Inc\\u00a7r"}']}
{title:'Ekstedt et al. (§72022§r)', author: 'Erik Ekstedt; Gabriel Skantze', display:{Lore:['[{"text": "arXiv:2205.09812", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Activity Projection: Self-supervised Learning of Turn-taking Events\\u00a7r\\n\\n\\u00a78\\u00a7oErik Ekstedt\\nGabriel Skantze\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.09812\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 19 May 2022 19:16:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022, 5 pages, 4 figures\\u00a7r"}']}
{title:'Chan et al. (§72022§r)', author: 'David M. Chan; Shalini Ghosh', display:{Lore:['[{"text": "arXiv:2205.09872", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContent-Context Factorized Representations for Automated Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDavid M. Chan\\nShalini Ghosh\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.09872\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 15 Sep 2022 17:02:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at Interspeech 2022 (On-Site Oral Presentation)\\u00a7r"}']}
{title:'Záviška et al. (§72022§r)', author: 'Pavel Záviška; Pavel Rajmic', display:{Lore:['[{"text": "arXiv:2205.10215", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Declipping with (Weighted) Analysis Social Sparsity\\u00a7r\\n\\n\\u00a78\\u00a7oPavel Z\\u00e1vi\\u0161ka\\nPavel Rajmic\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.10215\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TSP55681.2022.9851269\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2022 45th International Conference on Telecommunications and\\n  Signal Processing (TSP)\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Jun 2022 08:57:48 GMT)\\u00a7r"}']}
{title:'Yu et al. (§72022§r)', author: 'Meng Yu; Yong Xu; Chunlei Zhang; Shi-Xiong Zhang; Dong Yu', display:{Lore:['[{"text": "arXiv:2205.10401", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeuralEcho: A Self-Attentive Recurrent Neural Network For Unified Acoustic Echo Suppression And Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oMeng Yu\\nYong Xu\\nChunlei Zhang\\nShi-Xiong Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.10401\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 20 May 2022 18:36:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Hui Zhang; Tian Yuan; Junkun Chen; Xintong Li; Renjie Zheng; Yuxin Huang; Xiaojie Chen; Enlei Gong; Zeyu Chen; Xiaoguang Hu; Dianhai Yu; Yanjun Ma; Liang Huang', display:{Lore:['[{"text": "arXiv:2205.12007", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPaddleSpeech: An Easy-to-Use All-in-One Speech Toolkit\\u00a7r\\n\\n\\u00a78\\u00a7oHui Zhang\\nTian Yuan\\nJunkun Chen\\n+ 9 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.12007\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 20 May 2022 10:14:53 GMT)\\u00a7r"}']}
{title:'Hoedt et al. (§72022§r)', author: 'Katharina Hoedt; Arthur Flexer; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2205.12032", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDefending a Music Recommender Against Hubness-Based Adversarial Attacks\\u00a7r\\n\\n\\u00a78\\u00a7oKatharina Hoedt\\nArthur Flexer\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.12032\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5281/zenodo.6573391\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 24 May 2022 12:31:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, to be published in Proceedings of the 19th Sound and Music Computing Conference 2022 (SMC-22)\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Wei Liu; Jingyu Li; Tan Lee', display:{Lore:['[{"text": "arXiv:2205.12477", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Investigation on Applying Acoustic Feature Conversion to ASR of Adult and Child Speech\\u00a7r\\n\\n\\u00a78\\u00a7oWei Liu\\nJingyu Li\\nTan Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.12477\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 25 May 2022 04:00:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, submitted to InterSpeech2022\\u00a7r"}']}
{title:'Han et al. (§72022§r)', author: 'Tianxiao Han; Qianqian Yang; Zhiguo Shi; Shibo He; Zhaoyang Zhang', display:{Lore:['[{"text": "arXiv:2205.12727", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemantic-preserved Communication System for Highly Efficient Speech Transmission\\u00a7r\\n\\n\\u00a78\\u00a7oTianxiao Han\\nQianqian Yang\\nZhiguo Shi\\nShibo He\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.12727\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 25 May 2022 12:38:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2202.03211\\u00a7r"}']}
{title:'Zhu et al. (§72022§r)', author: 'Qiu-Shi Zhu; Jie Zhang; Zi-Qiang Zhang; Li-Rong Dai', display:{Lore:['[{"text": "arXiv:2205.13293", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Training of Speech Enhancement and Self-supervised Model for Noise-robust ASR\\u00a7r\\n\\n\\u00a78\\u00a7oQiu-Shi Zhu\\nJie Zhang\\nZi-Qiang Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.13293\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 26 May 2022 12:13:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to IEEE/ACM TASLP. arXiv admin note: text overlapwith arXiv:2201.08930\\u00a7r"}']}
{title:'Arango-Sánchez et al. (§72022§r)', author: 'Jose A. Arango-Sánchez; Julián D. Arias-Londoño', display:{Lore:['[{"text": "arXiv:2205.13657", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn enhanced Conv-TasNet model for speech separation using a speaker distance-based loss function\\u00a7r\\n\\n\\u00a78\\u00a7oJose A. Arango-S\\u00e1nchez\\nJuli\\u00e1n D. Arias-Londo\\u00f1o\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.13657\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 17 Jun 2022 15:13:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ohttps://github.com/DW-Speech-Separation/train-test-ConvTasNet\\u00a7r"}']}
{title:'Siriwardena et al. (§72022§r)', author: 'Yashish M. Siriwardena; Ganesh Sivaraman; Carol Espy-Wilson', display:{Lore:['[{"text": "arXiv:2205.13755", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic-to-articulatory Speech Inversion with Multi-task Learning\\u00a7r\\n\\n\\u00a78\\u00a7oYashish M. Siriwardena\\nGanesh Sivaraman\\nCarol Espy-Wilson\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.13755\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-11164\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 27 May 2022 03:52:25 GMT)\\u00a7r"}']}
{title:'Sinha et al. (§72022§r)', author: 'Ragini Sinha; Marvin Tammen; Christian Rollwage; Simon Doclo', display:{Lore:['[{"text": "arXiv:2205.13851", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker-conditioning Single-channel Target Speaker Extraction using Conformer-based Architectures\\u00a7r\\n\\n\\u00a78\\u00a7oRagini Sinha\\nMarvin Tammen\\nChristian Rollwage\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.13851\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 27 May 2022 09:27:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to IWAENC 2022\\u00a7r"}']}
{title:'Tong et al. (§72022§r)', author: 'Fuchuan Tong; Siqi Zheng; Haodong Zhou; Xingjia Xie; Qingyang Hong; Lin Li', display:{Lore:['[{"text": "arXiv:2205.14294", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Representation Decomposition for Rate-Invariant Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oFuchuan Tong\\nSiqi Zheng\\nHaodong Zhou\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.14294\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 28 May 2022 01:27:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Odyssey 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Ju-Chiang Wang; Yun-Ning Hung; Jordan B. L. Smith', display:{Lore:['[{"text": "arXiv:2205.14700", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTo catch a chorus, verse, intro, or anything else: Analyzing a song with structural functions\\u00a7r\\n\\n\\u00a78\\u00a7oJu-Chiang Wang\\nYun-Ning Hung\\nJordan B. L. Smith\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.14700\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 29 May 2022 16:01:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis manuscript is accepted by ICASSP 2022\\u00a7r"}']}
{title:'Leng et al. (§72022§r)', author: 'Yichong Leng; Zehua Chen; Junliang Guo; Haohe Liu; Jiawei Chen; Xu Tan; Danilo Mandic; Lei He; Xiang-Yang Li; Tao Qin; Sheng Zhao; Tie-Yan Liu', display:{Lore:['[{"text": "arXiv:2205.14807", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBinauralGrad: A Two-Stage Conditional Diffusion Probabilistic Model for Binaural Audio Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYichong Leng\\nZehua Chen\\nJunliang Guo\\n+ 8 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.14807\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 29 Nov 2022 09:02:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oNeurIPS 2022 camera version\\u00a7r"}']}
{title:'Morrone et al. (§72022§r)', author: 'Giovanni Morrone; Samuele Cornell; Enrico Zovato; Alessio Brutti; Stefano Squartini', display:{Lore:['[{"text": "arXiv:2205.15700", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConversational Speech Separation: an Evaluation Study for Streaming Applications\\u00a7r\\n\\n\\u00a78\\u00a7oGiovanni Morrone\\nSamuele Cornell\\nEnrico Zovato\\nAlessio Brutti\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.15700\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 31 May 2022 11:35:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAudio Engineering Society Convention 152, May 2022, The Hague, Netherlands\\u00a7r"}']}
{title:'Shastri et al. (§72022§r)', author: 'Parth Shastri; Chirag Patil; Poorval Wanere; Dr. Shrinivas Mahajan; Dr. Abhishek Bhatt; Dr. Hardik Sailor', display:{Lore:['[{"text": "arXiv:2205.15747", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial synthesis based data-augmentation for code-switched spoken language identification\\u00a7r\\n\\n\\u00a78\\u00a7oParth Shastri\\nChirag Patil\\nPoorval Wanere\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.15747\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 1 Jun 2022 18:17:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 8 figures, updated\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Sehoon Kim; Amir Gholami; Albert Shaw; Nicholas Lee; Karttikeya Mangalam; Jitendra Malik; Michael W. Mahoney; Kurt Keutzer', display:{Lore:['[{"text": "arXiv:2206.00888", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSqueezeformer: An Efficient Transformer for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSehoon Kim\\nAmir Gholami\\nAlbert Shaw\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.00888\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 15 Oct 2022 22:27:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oNeurIPS 2022\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Chang Liu; Zhen-Hua Ling; Ling-Hui Chen', display:{Lore:['[{"text": "arXiv:2206.00951", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPronunciation Dictionary-Free Multilingual Speech Synthesis by Combining Unsupervised and Supervised Phonetic Representations\\u00a7r\\n\\n\\u00a78\\u00a7oChang Liu\\nZhen-Hua Ling\\nLing-Hui Chen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.00951\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 Jun 2022 09:28:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Shanshan Wang; Archontis Politis; Annamaria Mesaros; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2206.00970", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervised Learning of Audio Representations from Audio-Visual Data using Spatial Alignment\\u00a7r\\n\\n\\u00a78\\u00a7oShanshan Wang\\nArchontis Politis\\nAnnamaria Mesaros\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.00970\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JSTSP.2022.3180592\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 Jun 2022 10:14:33 GMT)\\u00a7r"}']}
{title:'Politis et al. (§72022§r)', author: 'Archontis Politis; Kazuki Shimada; Parthasaarathy Sudarsanam; Sharath Adavanne; Daniel Krause; Yuichiro Koyama; Naoya Takahashi; Shusuke Takahashi; Yuki Mitsufuji; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2206.01948", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSTARSS22: A dataset of spatial recordings of real scenes with spatiotemporal annotations of sound events\\u00a7r\\n\\n\\u00a78\\u00a7oArchontis Politis\\nKazuki Shimada\\nParthasaarathy Sudarsanam\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.01948\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 2 Sep 2022 09:48:10 GMT)\\u00a7r"}']}
{title:'Paulus et al. (§72022§r)', author: 'Jouni Paulus; Matteo Torcoli', display:{Lore:['[{"text": "arXiv:2206.02124", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSampling Frequency Independent Dialogue Separation\\u00a7r\\n\\n\\u00a78\\u00a7oJouni Paulus\\nMatteo Torcoli\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.02124\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 5 Jun 2022 08:40:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted into EUSIPCO 2022\\u00a7r"}']}
{title:'Paulus et al. (§72022§r)', author: 'Jouni Paulus; Matteo Torcoli', display:{Lore:['[{"text": "arXiv:2206.02125", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeometrically-Motivated Primary-Ambient Decomposition With Center-Channel Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oJouni Paulus\\nMatteo Torcoli\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.02125\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 5 Jun 2022 08:45:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted into EUSIPCO 2022\\u00a7r"}']}
{title:'Horiguchi et al. (§72022§r)', author: 'Shota Horiguchi; Shinji Watanabe; Paola Garcia; Yuki Takashima; Yohei Kawaguchi', display:{Lore:['[{"text": "arXiv:2206.02432", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOnline Neural Diarization of Unlimited Numbers of Speakers Using Global and Local Attractors\\u00a7r\\n\\n\\u00a78\\u00a7oShota Horiguchi\\nShinji Watanabe\\nPaola Garcia\\nYuki Takashima\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.02432\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2022.3233237\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol. 31, pp. 706-720, 2023\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 22 Dec 2022 11:18:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEE/ACM TASLP\\u00a7r"}']}
{title:'Lian et al. (§72022§r)', author: 'Jiachen Lian; Chunlei Zhang; Gopala Krishna Anumanchipalli; Dong Yu', display:{Lore:['[{"text": "arXiv:2206.02512", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUTTS: Unsupervised TTS with Conditional Disentangled Sequential Variational Auto-encoder\\u00a7r\\n\\n\\u00a78\\u00a7oJiachen Lian\\nChunlei Zhang\\nGopala Krishna Anumanchipalli\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.02512\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 11 Oct 2022 20:37:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder Review\\u00a7r"}']}
{title:'Mehmood et al. (§72022§r)', author: 'Haaris Mehmood; Agnieszka Dobrowolska; Karthikeyan Saravanan; Mete Ozay', display:{Lore:['[{"text": "arXiv:2206.02797", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.DC\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFedNST: Federated Noisy Student Training for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHaaris Mehmood\\nAgnieszka Dobrowolska\\nKarthikeyan Saravanan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.02797\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 12 Jul 2022 20:03:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2022\\u00a7r"}']}
{title:'Oneata et al. (§72022§r)', author: 'Dan Oneata; Beata Lorincz; Adriana Stan; Horia Cucu', display:{Lore:['[{"text": "arXiv:2206.03206", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFlexLip: A Controllable Text-to-Lip System\\u00a7r\\n\\n\\u00a78\\u00a7oDan Oneata\\nBeata Lorincz\\nAdriana Stan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.03206\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3390/s22114104\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nSensors. 2022; 22(11):4104\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Jun 2022 11:51:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages, 4 tables, 4 figures\\u00a7r"}']}
{title:'Bayerl et al. (§72022§r)', author: 'Sebastian P. Bayerl; Dominik Wagner; Elmar Nöth; Tobias Bocklet; Korbinian Riedhammer', display:{Lore:['[{"text": "arXiv:2206.03400", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Influence of Dataset Partitioning on Dysfluency Detection Systems\\u00a7r\\n\\n\\u00a78\\u00a7oSebastian P. Bayerl\\nDominik Wagner\\nElmar N\\u00f6th\\nTobias Bocklet\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.03400\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-031-16270-1_35\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Jun 2022 15:50:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the 25th International Conference on Text, Speech and Dialogue (TSD 2022)\\u00a7r"}']}
{title:'Martín-Morató et al. (§72022§r)', author: 'Irene Martín-Morató; Francesco Paissan; Alberto Ancilotto; Toni Heittola; Annamaria Mesaros; Elisabetta Farella; Alessio Brutti; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2206.03835", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-complexity acoustic scene classification in DCASE 2022 Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oIrene Mart\\u00edn-Morat\\u00f3\\nFrancesco Paissan\\nAlberto Ancilotto\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.03835\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 13 Jul 2022 13:10:54 GMT)\\u00a7r"}']}
{title:'Riemens et al. (§72022§r)', author: 'Ellen Riemens; Pablo Martínez-Nuevo; Jorge Martinez; Martin Møller; Richard C. Hendriks', display:{Lore:['[{"text": "arXiv:2206.03885", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Integration of Acoustics and LiDAR: a Multi-Modal Approach to Acoustic Reflector Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oEllen Riemens\\nPablo Mart\\u00ednez-Nuevo\\nJorge Martinez\\nMartin M\\u00f8ller\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.03885\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 8 Jun 2022 13:36:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 9 figures, to be published in EUSIPCO 2022\\u00a7r"}']}
{title:'Baby et al. (§72022§r)', author: 'Arun Baby; Saranya Vinnaitherthan; Akhil Kerhalkar; Pranav Jawale; Sharath Adavanne; Nagaraj Adiga', display:{Lore:['[{"text": "arXiv:2206.04305", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContext-based out-of-vocabulary word recovery for ASR systems in Indian languages\\u00a7r\\n\\n\\u00a78\\u00a7oArun Baby\\nSaranya Vinnaitherthan\\nAkhil Kerhalkar\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.04305\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Jun 2022 06:51:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages\\u00a7r"}']}
{title:'Hung et al. (§72022§r)', author: 'Yun-Ning Hung; Alexander Lerch', display:{Lore:['[{"text": "arXiv:2206.04850", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFeature-informed Embedding Space Regularization For Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oYun-Ning Hung\\nAlexander Lerch\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.04850\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 10 Jun 2022 02:51:59 GMT)\\u00a7r"}']}
{title:'Mittal et al. (§72022§r)', author: 'Deepak Mittal; Amir H. Poorjam; Debottam Dutta; Debarpan Bhattacharya; Zemin Yu; Sriram Ganapathy; Maneesh Singh', display:{Lore:['[{"text": "arXiv:2206.05462", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSvadhyaya system for the Second Diagnosing COVID-19 using Acoustics Challenge 2021\\u00a7r\\n\\n\\u00a78\\u00a7oDeepak Mittal\\nAmir H. Poorjam\\nDebottam Dutta\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.05462\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 11 Jun 2022 08:26:11 GMT)\\u00a7r"}']}
{title:'Kowalk et al. (§72022§r)', author: 'Ulrik Kowalk; Simon Doclo; Joerg Bitzer', display:{Lore:['[{"text": "arXiv:2206.05606", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSignal-informed DNN-based DOA Estimation combining an External Microphone and GCC-PHAT Features\\u00a7r\\n\\n\\u00a78\\u00a7oUlrik Kowalk\\nSimon Doclo\\nJoerg Bitzer\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.05606\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 11 Jun 2022 20:14:37 GMT)\\u00a7r"}']}
{title:'Xie et al. (§72022§r)', author: 'Huang Xie; Samuel Lipping; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2206.06108", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLanguage-based Audio Retrieval Task in DCASE 2022 Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oHuang Xie\\nSamuel Lipping\\nTuomas Virtanen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.06108\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 30 Sep 2022 19:31:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at DCASE 2022 Workshop\\u00a7r"}']}
{title:'Herzog et al. (§72022§r)', author: 'Adrian Herzog; Srikanth Raj Chetupalli; Emanuël A. P. Habets', display:{Lore:['[{"text": "arXiv:2206.06184", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAmbiSep: Ambisonic-to-Ambisonic Reverberant Speech Separation Using Transformer Networks\\u00a7r\\n\\n\\u00a78\\u00a7oAdrian Herzog\\nSrikanth Raj Chetupalli\\nEmanu\\u00ebl A. P. Habets\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.06184\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Jun 2022 14:18:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint submitted to IWAENC 2022 (https://iwaenc2022.org)\\u00a7r"}']}
{title:'Faria et al. (§72022§r)', author: 'Arlo Faria; Adam Janin; Korbinian Riedhammer; Sidhi Adkoli', display:{Lore:['[{"text": "arXiv:2206.06192", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lToward Zero Oracle Word Error Rate on the Switchboard Benchmark\\u00a7r\\n\\n\\u00a78\\u00a7oArlo Faria\\nAdam Janin\\nKorbinian Riedhammer\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.06192\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Jun 2022 14:44:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Braun et al. (§72022§r)', author: 'Franziska Braun; Markus Förstel; Bastian Oppermann; Andreas Erzigkeit; Thomas Hillemacher; Hartmut Lehfeld; Korbinian Riedhammer', display:{Lore:['[{"text": "arXiv:2206.06208", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomated Evaluation of Standardized Dementia Screening Tests\\u00a7r\\n\\n\\u00a78\\u00a7oFranziska Braun\\nMarkus F\\u00f6rstel\\nBastian Oppermann\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.06208\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Jun 2022 14:41:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022. arXiv admin note: text overlap with arXiv:2206.05018\\u00a7r"}']}
{title:'Moliner et al. (§72022§r)', author: 'Eloi Moliner; Vesa Välimäki', display:{Lore:['[{"text": "arXiv:2206.06259", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRealistic Gramophone Noise Synthesis using a Diffusion Model\\u00a7r\\n\\n\\u00a78\\u00a7oEloi Moliner\\nVesa V\\u00e4lim\\u00e4ki\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.06259\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 30 Jun 2022 08:14:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at DAFx 20in22\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Yongtao Wu; Grigorios G Chrysos; Volkan Cevher', display:{Lore:['[{"text": "arXiv:2206.06811", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Audio Synthesis with Complex-valued Polynomial Networks\\u00a7r\\n\\n\\u00a78\\u00a7oYongtao Wu\\nGrigorios G Chrysos\\nVolkan Cevher\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.06811\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 21 Jun 2022 13:30:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted as oralpresentation in Workshop on Machine Learning for Audio Synthesis at ICML 2022\\u00a7r"}']}
{title:'Jose et al. (§72022§r)', author: 'Christin Jose; Joseph Wang; Grant P. Strimel; Mohammad Omar Khursheed; Yuriy Mishchenko; Brian Kulis', display:{Lore:['[{"text": "arXiv:2206.07261", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLatency Control for Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oChristin Jose\\nJoseph Wang\\nGrant P. Strimel\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.07261\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10608\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Jun 2022 02:45:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH\\u00a7r"}']}
{title:'Tsunoo et al. (§72022§r)', author: 'Emiru Tsunoo; Yosuke Kashiwagi; Chaitanya Narisetty; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2206.07430", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lResidual Language Model for End-to-end Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oEmiru Tsunoo\\nYosuke Kashiwagi\\nChaitanya Narisetty\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.07430\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Jun 2022 10:04:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for Interspeech2022\\u00a7r"}']}
{title:'Stan (§72022§r)', author: 'Adriana Stan', display:{Lore:['[{"text": "arXiv:2206.07448", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe ZevoMOS entry to VoiceMOS Challenge 2022\\u00a7r\\n\\n\\u00a78\\u00a7oAdriana Stan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.07448\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Jun 2022 10:53:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2022 - VoiceMOS Challenge; 5 pages, 2 figures, 2 tables\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Jingyu Li; Wei Liu; Tan Lee', display:{Lore:['[{"text": "arXiv:2206.07548", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEDITnet: A Lightweight Network for Unsupervised Domain Adaptation in Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oJingyu Li\\nWei Liu\\nTan Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.07548\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Jun 2022 14:10:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech2022\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Jingyu Li; Yusheng Tian; Tan Lee', display:{Lore:['[{"text": "arXiv:2206.07563", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearnable Frequency Filters for Speech Feature Extraction in Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oJingyu Li\\nYusheng Tian\\nTan Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.07563\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Jun 2022 14:28:43 GMT)\\u00a7r"}']}
{title:'Xie et al. (§72022§r)', author: 'Qicong Xie; Shan Yang; Yi Lei; Lei Xie; Dan Su', display:{Lore:['[{"text": "arXiv:2206.07569", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Voice Conversion with Information Perturbation\\u00a7r\\n\\n\\u00a78\\u00a7oQicong Xie\\nShan Yang\\nYi Lei\\nLei Xie\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.07569\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Jun 2022 14:38:31 GMT)\\u00a7r"}']}
{title:'Chak et al. (§72022§r)', author: 'Wai Ho Chak; Naoki Saito; David Weber', display:{Lore:['[{"text": "arXiv:2206.07857", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Scattering Transform Network with Generalized Morse Wavelets and Its Application to Music Genre Classification\\u00a7r\\n\\n\\u00a78\\u00a7oWai Ho Chak\\nNaoki Saito\\nDavid Weber\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.07857\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Jun 2022 00:30:09 GMT)\\u00a7r"}']}
{title:'Valin et al. (§72022§r)', author: 'Jean-Marc Valin; Ritwik Giri; Shrikant Venkataramani; Umut Isik; Arvindh Krishnaswamy', display:{Lore:['[{"text": "arXiv:2206.07917", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTo Dereverb Or Not to Dereverb? Perceptual Studies On Real-Time Dereverberation Targets\\u00a7r\\n\\n\\u00a78\\u00a7oJean-Marc Valin\\nRitwik Giri\\nShrikant Venkataramani\\nUmut Isik\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.07917\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Jun 2022 04:43:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Fan et al. (§72022§r)', author: 'Ruchao Fan; Abeer Alwan', display:{Lore:['[{"text": "arXiv:2206.07931", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDRAFT: A Novel Framework to Reduce Domain Shifting in Self-supervised Learning and Its Application to Children\'s ASR\\u00a7r\\n\\n\\u00a78\\u00a7oRuchao Fan\\nAbeer Alwan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.07931\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Jun 2022 05:28:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Gao et al. (§72022§r)', author: 'Yingying Gao; Junlan Feng; Tianrui Wang; Chao Deng; Shilei Zhang', display:{Lore:['[{"text": "arXiv:2206.08031", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA CTC Triggered Siamese Network with Spatial-Temporal Dropout for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYingying Gao\\nJunlan Feng\\nTianrui Wang\\nChao Deng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.08031\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 22 Jun 2022 06:50:43 GMT)\\u00a7r"}']}
{title:'Baumann et al. (§72022§r)', author: 'Ilja Baumann; Dominik Wagner; Sebastian Bayerl; Tobias Bocklet', display:{Lore:['[{"text": "arXiv:2206.08058", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNonwords Pronunciation Classification in Language Development Tests for Preschool Children\\u00a7r\\n\\n\\u00a78\\u00a7oIlja Baumann\\nDominik Wagner\\nSebastian Bayerl\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.08058\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 17 Jun 2022 07:08:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2022\\u00a7r"}']}
{title:'Sato et al. (§72022§r)', author: 'Hiroshi Sato; Tsubasa Ochiai; Marc Delcroix; Keisuke Kinoshita; Takafumi Moriya; Naoki Makishima; Mana Ihori; Tomohiro Tanaka; Ryo Masumura', display:{Lore:['[{"text": "arXiv:2206.08174", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStrategies to Improve Robustness of Target Speech Extraction to Enrollment Variations\\u00a7r\\n\\n\\u00a78\\u00a7oHiroshi Sato\\nTsubasa Ochiai\\nMarc Delcroix\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.08174\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Jun 2022 13:41:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 3 tables Submitted to Interspeech 2022\\u00a7r"}']}
{title:'Han et al. (§72022§r)', author: 'Seungu Han; Junhyeok Lee', display:{Lore:['[{"text": "arXiv:2206.08545", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling Rates\\u00a7r\\n\\n\\u00a78\\u00a7oSeungu Han\\nJunhyeok Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.08545\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-45\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 1 Jul 2022 00:35:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Lee et al. (§72022§r)', author: 'Chi-Chang Lee; Cheng-Hung Hu; Yu-Chen Lin; Chu-Song Chen; Hsin-Min Wang; Yu Tsao', display:{Lore:['[{"text": "arXiv:2206.09058", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNASTAR: Noise Adaptive Speech Enhancement with Target-Conditional Resampling\\u00a7r\\n\\n\\u00a78\\u00a7oChi-Chang Lee\\nCheng-Hung Hu\\nYu-Chen Lin\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.09058\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 18 Jun 2022 00:15:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Zhepei Wang; Ritwik Giri; Shrikant Venkataramani; Umut Isik; Jean-Marc Valin; Paris Smaragdis; Mike Goodwin; Arvindh Krishnaswamy', display:{Lore:['[{"text": "arXiv:2206.09072", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-supervised Time Domain Target Speaker Extraction with Attention\\u00a7r\\n\\n\\u00a78\\u00a7oZhepei Wang\\nRitwik Giri\\nShrikant Venkataramani\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.09072\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 18 Jun 2022 00:49:35 GMT)\\u00a7r"}']}
{title:'Zhu et al. (§72022§r)', author: 'Han Zhu; Jindong Wang; Gaofeng Cheng; Pengyuan Zhang; Yonghong Yan', display:{Lore:['[{"text": "arXiv:2206.09102", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.DC\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDecoupled Federated Learning for ASR with Non-IID Data\\u00a7r\\n\\n\\u00a78\\u00a7oHan Zhu\\nJindong Wang\\nGaofeng Cheng\\nPengyuan Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.09102\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 18 Jun 2022 03:44:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Cai et al. (§72022§r)', author: 'Danwei Cai; Zexin Cai; Ming Li', display:{Lore:['[{"text": "arXiv:2206.09103", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIdentifying Source Speakers for Voice Conversion based Spoofing Attacks on Speaker Verification Systems\\u00a7r\\n\\n\\u00a78\\u00a7oDanwei Cai\\nZexin Cai\\nMing Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.09103\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 31 Oct 2022 18:50:41 GMT)\\u00a7r"}']}
{title:'Thienpondt et al. (§72022§r)', author: 'Jenthe Thienpondt; Kris Demuynck', display:{Lore:['[{"text": "arXiv:2206.09396", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransfer Learning for Robust Low-Resource Children\'s Speech ASR with Transformers and Source-Filter Warping\\u00a7r\\n\\n\\u00a78\\u00a7oJenthe Thienpondt\\nKris Demuynck\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.09396\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 19 Jun 2022 12:57:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oproceedings of INTERSPEECH 2022\\u00a7r"}']}
{title:'Hutiri et al. (§72022§r)', author: 'W. T. Hutiri; A. Y. Ding', display:{Lore:['[{"text": "arXiv:2206.09523", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CY\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Trustworthy Edge Intelligence: Insights from Voice-Activated Services\\u00a7r\\n\\n\\u00a78\\u00a7oW. T. Hutiri\\nA. Y. Ding\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.09523\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Jun 2022 00:56:21 GMT)\\u00a7r"}']}
{title:'Ravi et al. (§72022§r)', author: 'Vijay Ravi; Jinhan Wang; Jonathan Flint; Abeer Alwan', display:{Lore:['[{"text": "arXiv:2206.09530", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Step Towards Preserving Speakers\' Identity While Detecting Depression Via Speaker Disentanglement\\u00a7r\\n\\n\\u00a78\\u00a7oVijay Ravi\\nJinhan Wang\\nJonathan Flint\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.09530\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10798\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 29 Jun 2022 05:00:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Parikh et al. (§72022§r)', author: 'Rahil Parikh; Gaspar Rochette; Carol Espy-Wilson; Shihab Shamma', display:{Lore:['[{"text": "arXiv:2206.09556", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Empirical Analysis on the Vulnerabilities of End-to-End Speech Segregation Models\\u00a7r\\n\\n\\u00a78\\u00a7oRahil Parikh\\nGaspar Rochette\\nCarol Espy-Wilson\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.09556\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Jun 2022 03:46:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2022\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Yuan Chen; Yicheng Hsu; Mingsian R. Bai', display:{Lore:['[{"text": "arXiv:2206.09728", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-channel end-to-end neural network for speech enhancement, source localization, and voice activity detection\\u00a7r\\n\\n\\u00a78\\u00a7oYuan Chen\\nYicheng Hsu\\nMingsian R. Bai\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.09728\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Jun 2022 11:53:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICA2022\\u00a7r"}']}
{title:'Tal et al. (§72022§r)', author: 'Or Tal; Moshe Mandel; Felix Kreuk; Yossi Adi', display:{Lore:['[{"text": "arXiv:2206.11000", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Systematic Comparison of Phonetic Aware Techniques for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oOr Tal\\nMoshe Mandel\\nFelix Kreuk\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.11000\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 Jun 2022 12:00:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished @ Interspeech 2022\\u00a7r"}']}
{title:'Triantafyllopoulos et al. (§72022§r)', author: 'Andreas Triantafyllopoulos; Anastasia Semertzidou; Meishu Song; Florian B. Pokorny; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2206.11045", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCOVYT: Introducing the Coronavirus YouTube and TikTok speech dataset featuring the same speakers with and without infection\\u00a7r\\n\\n\\u00a78\\u00a7oAndreas Triantafyllopoulos\\nAnastasia Semertzidou\\nMeishu Song\\nFlorian B. Pokorny\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.11045\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Jun 2022 16:26:51 GMT)\\u00a7r"}']}
{title:'Weninger et al. (§72022§r)', author: 'Felix Weninger; Marco Gaudesi; Md Akmal Haidar; Nicola Ferri; Jesús Andrés-Ferrer; Puming Zhan', display:{Lore:['[{"text": "arXiv:2206.11157", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConformer with dual-mode chunked attention for joint online and offline ASR\\u00a7r\\n\\n\\u00a78\\u00a7oFelix Weninger\\nMarco Gaudesi\\nMd Akmal Haidar\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.11157\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 Jun 2022 15:04:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in INTERSPEECH 2022\\u00a7r"}']}
{title:'Tesch et al. (§72022§r)', author: 'Kristina Tesch; Nils-Hendrik Mohrmann; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2206.11181", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Role of Spatial, Spectral, and Temporal Processing for DNN-based Non-linear Multi-channel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oKristina Tesch\\nNils-Hendrik Mohrmann\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.11181\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 Jun 2022 15:42:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2022\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Tae-Woo Kim; Min-Su Kang; Gyeong-Hoon Lee', display:{Lore:['[{"text": "arXiv:2206.11558", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Multi-Task Learning for Disentangling Timbre and Pitch in Singing Voice Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oTae-Woo Kim\\nMin-Su Kang\\nGyeong-Hoon Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.11558\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Jun 2022 09:11:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Cui et al. (§72022§r)', author: 'Mingyu Cui; Jiajun Deng; Shoukang Hu; Xurong Xie; Tianzi Wang; Shujie Hu; Mengzhe Geng; Boyang Xue; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2206.11596", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTwo-pass Decoding and Cross-adaptation Based System Combination of End-to-end Conformer and Hybrid TDNN ASR Systems\\u00a7r\\n\\n\\u00a78\\u00a7oMingyu Cui\\nJiajun Deng\\nShoukang Hu\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.11596\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-696\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Jun 2022 10:17:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIt\' s accepted to ISCA 2022\\u00a7r"}']}
{title:'Nam et al. (§72022§r)', author: 'Hyeonuk Nam; Seong-Hu Kim; Deokki Min; Byeong-Yun Ko; Seung-Deok Choi; Yong-Hwa Park', display:{Lore:['[{"text": "arXiv:2206.11645", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrequency Dependent Sound Event Detection for DCASE 2022 Challenge Task 4\\u00a7r\\n\\n\\u00a78\\u00a7oHyeonuk Nam\\nSeong-Hu Kim\\nDeokki Min\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.11645\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Jun 2022 12:06:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTechnical Reprotsubmitted for DCASE2022 Challenge Task4\\u00a7r"}']}
{title:'de Oliveira et al. (§72022§r)', author: 'Danilo de Oliveira; Tal Peer; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2206.11703", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Transformer-based Speech Enhancement Using Long Frames and STFT Magnitudes\\u00a7r\\n\\n\\u00a78\\u00a7oDanilo de Oliveira\\nTal Peer\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.11703\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10781\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Jun 2022 13:50:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2022\\u00a7r"}']}
{title:'van der Merwe et al. (§72022§r)', author: 'Werner van der Merwe; Herman Kamper; Johan du Preez', display:{Lore:['[{"text": "arXiv:2206.11706", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Temporal Extension of Latent Dirichlet Allocation for Unsupervised Acoustic Unit Discovery\\u00a7r\\n\\n\\u00a78\\u00a7oWerner van der Merwe\\nHerman Kamper\\nJohan du Preez\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.11706\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 29 Jun 2022 07:47:53 GMT)\\u00a7r"}']}
{title:'Teixeira et al. (§72022§r)', author: 'Francisco Teixeira; Alberto Abad; Bhiksha Raj; Isabel Trancoso', display:{Lore:['[{"text": "arXiv:2206.11750", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards End-to-End Private Automatic Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oFrancisco Teixeira\\nAlberto Abad\\nBhiksha Raj\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.11750\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10672\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2022, 2798-2802\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Jun 2022 14:49:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at Interspeech 2022\\u00a7r"}']}
{title:'Mitsui et al. (§72022§r)', author: 'Kentaro Mitsui; Tianyu Zhao; Kei Sawada; Yukiya Hono; Yoshihiko Nankaku; Keiichi Tokuda', display:{Lore:['[{"text": "arXiv:2206.12040", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Text-to-Speech Based on Latent Representation of Speaking Styles Using Spontaneous Dialogue\\u00a7r\\n\\n\\u00a78\\u00a7oKentaro Mitsui\\nTianyu Zhao\\nKei Sawada\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12040\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Jun 2022 02:32:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted for INTERSPEECH 2022. Audio samples: https://rinnakk.github.io/research/publications/DialogueTTS/\\u00a7r"}']}
{title:'Deng et al. (§72022§r)', author: 'Jiajun Deng; Xurong Xie; Tianzi Wang; Mingyu Cui; Boyang Xue; Zengrui Jin; Mengzhe Geng; Guinan Li; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2206.12045", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConfidence Score Based Conformer Speaker Adaptation for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJiajun Deng\\nXurong Xie\\nTianzi Wang\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12045\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Jun 2022 02:48:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIt\'s accepted to INTERSPEECH 2022. arXiv admin note: text overlap with arXiv:2206.11596\\u00a7r"}']}
{title:'Ko et al. (§72022§r)', author: 'Byeong-Yun Ko; Hyeonuk Nam; Seong-Hu Kim; Deokki Min; Seung-Deok Choi; Yong-Hwa Park', display:{Lore:['[{"text": "arXiv:2206.12059", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData Augmentation and Squeeze-and-Excitation Network on Multiple Dimension for Sound Event Localization and Detection in Real Scenes\\u00a7r\\n\\n\\u00a78\\u00a7oByeong-Yun Ko\\nHyeonuk Nam\\nSeong-Hu Kim\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12059\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Jun 2022 03:31:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTechnical Reportsubmitted for DCASE2022 Challenge Task3\\u00a7r"}']}
{title:'Cho et al. (§72022§r)', author: 'Hyunjae Cho; Wonbin Jung; Junhyeok Lee; Sang Hoon Woo', display:{Lore:['[{"text": "arXiv:2206.12132", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oHyunjae Cho\\nWonbin Jung\\nJunhyeok Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12132\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-46\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Jun 2022 07:53:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Fu et al. (§72022§r)', author: 'Yanjie Fu; Meng Ge; Haoran Yin; Xinyuan Qian; Longbiao Wang; Gaoyan Zhang; Jianwu Dang', display:{Lore:['[{"text": "arXiv:2206.12273", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIterative Sound Source Localization for Unknown Number of Sources\\u00a7r\\n\\n\\u00a78\\u00a7oYanjie Fu\\nMeng Ge\\nHaoran Yin\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12273\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Jun 2022 13:19:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Szwajcowski (§72022§r)', author: 'Adam Szwajcowski', display:{Lore:['[{"text": "arXiv:2206.12283", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOpen-source objective-oriented framework for head-related transfer function\\u00a7r\\n\\n\\u00a78\\u00a7oAdam Szwajcowski\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12283\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Jun 2022 13:31:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oNot submitted anywhere in the current form\\u00a7r"}']}
{title:'Manocha et al. (§72022§r)', author: 'Pranay Manocha; Anurag Kumar', display:{Lore:['[{"text": "arXiv:2206.12285", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Quality Assessment through MOS using Non-Matching References\\u00a7r\\n\\n\\u00a78\\u00a7oPranay Manocha\\nAnurag Kumar\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12285\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Jun 2022 13:34:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo Appear, Interspeech 2022\\u00a7r"}']}
{title:'Manocha et al. (§72022§r)', author: 'Pranay Manocha; Anurag Kumar; Buye Xu; Anjali Menon; Israel D. Gebru; Vamsi K. Ithapu; Paul Calamia', display:{Lore:['[{"text": "arXiv:2206.12297", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSAQAM: Spatial Audio Quality Assessment Metric\\u00a7r\\n\\n\\u00a78\\u00a7oPranay Manocha\\nAnurag Kumar\\nBuye Xu\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12297\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Jun 2022 13:47:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo Appear, Interspeech 2022\\u00a7r"}']}
{title:'Bhattacharya et al. (§72022§r)', author: 'Debarpan Bhattacharya; Debottam Dutta; Neeraj Kumar Sharma; Srikanth Raj Chetupalli; Pravin Mote; Sriram Ganapathy; Chandrakiran C; Sahiti Nori; Suhail K K; Sadhana Gonuguntla; Murali Alagesan', display:{Lore:['[{"text": "arXiv:2206.12309", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalyzing the impact of SARS-CoV-2 variants on respiratory sound signals\\u00a7r\\n\\n\\u00a78\\u00a7oDebarpan Bhattacharya\\nDebottam Dutta\\nNeeraj Kumar Sharma\\n+ 7 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12309\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInterspeech, 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Jun 2022 14:10:31 GMT)\\u00a7r"}']}
{title:'Ji et al. (§72022§r)', author: 'Hang Ji; Tanvina Patel; Odette Scharenborg', display:{Lore:['[{"text": "arXiv:2206.12489", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPredicting within and across language phoneme recognition performance of self-supervised learning speech pre-trained models\\u00a7r\\n\\n\\u00a78\\u00a7oHang Ji\\nTanvina Patel\\nOdette Scharenborg\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12489\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Jun 2022 20:55:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Gao et al. (§72022§r)', author: 'Yingying Gao; Junlan Feng; Chao Deng; Shilei Zhang', display:{Lore:['[{"text": "arXiv:2206.12774", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMeta Auxiliary Learning for Low-resource Spoken Language Understanding\\u00a7r\\n\\n\\u00a78\\u00a7oYingying Gao\\nJunlan Feng\\nChao Deng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12774\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 26 Jun 2022 03:12:33 GMT)\\u00a7r"}']}
{title:'Tian et al. (§72022§r)', author: 'Yusheng Tian; Jingyu Li; Tan Lee', display:{Lore:['[{"text": "arXiv:2206.12857", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransport-Oriented Feature Aggregation for Speaker Embedding Learning\\u00a7r\\n\\n\\u00a78\\u00a7oYusheng Tian\\nJingyu Li\\nTan Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.12857\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 26 Jun 2022 12:22:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for presentation at INTERSPEECH 2022\\u00a7r"}']}
{title:'Masuyama et al. (§72022§r)', author: 'Yoshiki Masuyama; Kouei Yamaoka; Nobutaka Ono', display:{Lore:['[{"text": "arXiv:2206.13014", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Optimization of Sampling Rate Offsets Based on Entire Signal Relationship Among Distributed Microphones\\u00a7r\\n\\n\\u00a78\\u00a7oYoshiki Masuyama\\nKouei Yamaoka\\nNobutaka Ono\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13014\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Jun 2022 01:57:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures,accepted by Interspeech2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Jinhan Wang; Vijay Ravi; Jonathan Flint; Abeer Alwan', display:{Lore:['[{"text": "arXiv:2206.13016", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Instance Discriminative Learning for Depression Detection from Speech Signals\\u00a7r\\n\\n\\u00a78\\u00a7oJinhan Wang\\nVijay Ravi\\nJonathan Flint\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13016\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Jun 2022 02:05:48 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Ju-ho Kim; Jungwoo Heo; Hye-jin Shim; Ha-Jin Yu', display:{Lore:['[{"text": "arXiv:2206.13044", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExtended U-Net for Speaker Verification in Noisy Environments\\u00a7r\\n\\n\\u00a78\\u00a7oJu-ho Kim\\nJungwoo Heo\\nHye-jin Shim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13044\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Jun 2022 05:01:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 4 tables, accepted to 2022 Interspeech as a conference paper\\u00a7r"}']}
{title:'Arora (§72022§r)', author: 'Rohit Arora', display:{Lore:['[{"text": "arXiv:2206.13066", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetection of Doctored Speech: Towards an End-to-End Parametric Learn-able Filter Approach\\u00a7r\\n\\n\\u00a78\\u00a7oRohit Arora\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13066\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Jun 2022 06:28:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:1904.05441 by other authors\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'Jinmiao Huang; Waseem Gharbieh; Qianhui Wan; Han Suk Shim; Chul Lee', display:{Lore:['[{"text": "arXiv:2206.13231", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQbyE-MLPMixer: Query-by-Example Open-Vocabulary Keyword Spotting using MLPMixer\\u00a7r\\n\\n\\u00a78\\u00a7oJinmiao Huang\\nWaseem Gharbieh\\nQianhui Wan\\nHan Suk Shim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13231\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Jun 2022 18:18:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Tianzi Wang; Jiajun Deng; Mengzhe Geng; Zi Ye; Shoukang Hu; Yi Wang; Mingyu Cui; Zengrui Jin; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2206.13232", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConformer Based Elderly Speech Recognition System for Alzheimer\'s Disease Detection\\u00a7r\\n\\n\\u00a78\\u00a7oTianzi Wang\\nJiajun Deng\\nMengzhe Geng\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13232\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Jun 2022 12:50:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, accepted by INTERSPEECH 2022\\u00a7r"}']}
{title:'Kuang et al. (§72022§r)', author: 'Fangjun Kuang; Liyong Guo; Wei Kang; Long Lin; Mingshuang Luo; Zengwei Yao; Daniel Povey', display:{Lore:['[{"text": "arXiv:2206.13236", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPruned RNN-T for fast, memory-efficient ASR training\\u00a7r\\n\\n\\u00a78\\u00a7oFangjun Kuang\\nLiyong Guo\\nWei Kang\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13236\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Jun 2022 12:18:03 GMT)\\u00a7r"}']}
{title:'Joshi et al. (§72022§r)', author: 'Raviraj Joshi; Anupam Singh', display:{Lore:['[{"text": "arXiv:2206.13240", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Simple Baseline for Domain Adaptation in End to End ASR Systems Using Synthetic Data\\u00a7r\\n\\n\\u00a78\\u00a7oRaviraj Joshi\\nAnupam Singh\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13240\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.18653/v1/2022.ecnlp-1.28\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 Jun 2022 12:07:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ECNLP @ACL 2022\\u00a7r"}']}
{title:'Dutta et al. (§72022§r)', author: 'Debottam Dutta; Debarpan Bhattacharya; Sriram Ganapathy; Amir H. Poorjam; Deepak Mittal; Maneesh Singh', display:{Lore:['[{"text": "arXiv:2206.13365", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInterpretable Acoustic Representation Learning on Breathing and Speech Signals for COVID-19 Detection\\u00a7r\\n\\n\\u00a78\\u00a7oDebottam Dutta\\nDebarpan Bhattacharya\\nSriram Ganapathy\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13365\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Jun 2022 15:20:51 GMT)\\u00a7r"}']}
{title:'Manocha et al. (§72022§r)', author: 'Pranay Manocha; Zeyu Jin; Adam Finkelstein', display:{Lore:['[{"text": "arXiv:2206.13411", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Similarity is Unreliable as a Proxy for Audio Quality\\u00a7r\\n\\n\\u00a78\\u00a7oPranay Manocha\\nZeyu Jin\\nAdam Finkelstein\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13411\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Jun 2022 16:02:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo Appear, Interspeech 2022\\u00a7r"}']}
{title:'Sarkar et al. (§72022§r)', author: 'Eklavya Sarkar; RaviShankar Prasad; Mathew Magimai. -Doss', display:{Lore:['[{"text": "arXiv:2206.13420", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Voice Activity Detection by Modeling Source and System Information using Zero Frequency Filtering\\u00a7r\\n\\n\\u00a78\\u00a7oEklavya Sarkar\\nRaviShankar Prasad\\nMathew Magimai. -Doss\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13420\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10535\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Jun 2022 16:15:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2022\\u00a7r"}']}
{title:'Karlapati et al. (§72022§r)', author: 'Sri Karlapati; Penny Karanasou; Mateusz Lajszczak; Ammar Abbas; Alexis Moinet; Peter Makarov; Ray Li; Arent van Korlaar; Simon Slangen; Thomas Drugman', display:{Lore:['[{"text": "arXiv:2206.13443", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCopyCat2: A Single Model for Multi-Speaker TTS and Many-to-Many Fine-Grained Prosody Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oSri Karlapati\\nPenny Karanasou\\nMateusz Lajszczak\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13443\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Jun 2022 16:48:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to be published in the Proceedings of InterSpeech 2022\\u00a7r"}']}
{title:'Afshan et al. (§72022§r)', author: 'Amber Afshan; Abeer Alwan', display:{Lore:['[{"text": "arXiv:2206.13680", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention-based conditioning methods using variable frame rate for style-robust speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oAmber Afshan\\nAbeer Alwan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13680\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Jun 2022 01:14:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Interspeech, 2022\\u00a7r"}']}
{title:'Afshan et al. (§72022§r)', author: 'Amber Afshan; Abeer Alwan', display:{Lore:['[{"text": "arXiv:2206.13684", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning from human perception to improve automatic speaker verification in style-mismatched conditions\\u00a7r\\n\\n\\u00a78\\u00a7oAmber Afshan\\nAbeer Alwan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13684\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Jun 2022 01:24:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Interspeech, 2022\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Yifan Chen; Yifan Guo; Qingxuan Li; Gaofeng Cheng; Pengyuan Zhang; Yonghong Yan', display:{Lore:['[{"text": "arXiv:2206.13760", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInterrelate Training and Searching: A Unified Online Clustering Framework for Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oYifan Chen\\nYifan Guo\\nQingxuan Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13760\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Jun 2022 05:10:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Xu Li; Shansong Liu; Ying Shan', display:{Lore:['[{"text": "arXiv:2206.13762", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Hierarchical Speaker Representation Framework for One-shot Singing Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oXu Li\\nShansong Liu\\nYing Shan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13762\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Jul 2022 13:46:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022; Made some motificationsin Fig.1 so that the system architecture will be more clear\\u00a7r"}']}
{title:'Heo et al. (§72022§r)', author: 'Jungwoo Heo; Ju-ho Kim; Hyun-seo Shin', display:{Lore:['[{"text": "arXiv:2206.13807", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTwo Methods for Spoofing-Aware Speaker Verification: Multi-Layer Perceptron Score Fusion Model and Integrated Embedding Projector\\u00a7r\\n\\n\\u00a78\\u00a7oJungwoo Heo\\nJu-ho Kim\\nHyun-seo Shin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13807\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-602\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Jun 2022 07:42:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, 5 tables, accepted to 2022 Interspeech as a conference paper\\u00a7r"}']}
{title:'Aloradi et al. (§72022§r)', author: 'Ahmad Aloradi; Wolfgang Mack; Mohamed Elminshawi; Emanuël A. P. Habets', display:{Lore:['[{"text": "arXiv:2206.13808", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Verification in Multi-Speaker Environments Using Temporal Feature Fusion\\u00a7r\\n\\n\\u00a78\\u00a7oAhmad Aloradi\\nWolfgang Mack\\nMohamed Elminshawi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13808\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Jun 2022 07:45:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be presented at EUSIPCO 2022\\u00a7r"}']}
{title:'Yin et al. (§72022§r)', author: 'Dacheng Yin; Chuanxin Tang; Yanqing Liu; Xiaoqiang Wang; Zhiyuan Zhao; Yucheng Zhao; Zhiwei Xiong; Sheng Zhao; Chong Luo', display:{Lore:['[{"text": "arXiv:2206.13865", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRetrieverTTS: Modeling Decomposed Factors for Text-Based Speech Insertion\\u00a7r\\n\\n\\u00a78\\u00a7oDacheng Yin\\nChuanxin Tang\\nYanqing Liu\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13865\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Jun 2022 10:02:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, 3 tables. Accepted by Interspeech 2022\\u00a7r"}']}
{title:'Abbas et al. (§72022§r)', author: 'Ammar Abbas; Thomas Merritt; Alexis Moinet; Sri Karlapati; Ewa Muszynska; Simon Slangen; Elia Gatti; Thomas Drugman', display:{Lore:['[{"text": "arXiv:2206.14165", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExpressive, Variable, and Controllable Duration Modelling in TTS\\u00a7r\\n\\n\\u00a78\\u00a7oAmmar Abbas\\nThomas Merritt\\nAlexis Moinet\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.14165\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Jun 2022 17:21:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to be published in the Proceedings of InterSpeech 2022\\u00a7r"}']}
{title:'Kroon (§72022§r)', author: 'Anja Kroon', display:{Lore:['[{"text": "arXiv:2206.14357", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparing Conventional Pitch Detection Algorithms with a Neural Network Approach\\u00a7r\\n\\n\\u00a78\\u00a7oAnja Kroon\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.14357\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 29 Jun 2022 01:53:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 11 figures\\u00a7r"}']}
{title:'Hu et al. (§72022§r)', author: 'Qinwen Hu; Zhongshu Hou; Xiaohuai Le; Jing Lu', display:{Lore:['[{"text": "arXiv:2206.14524", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA light-weight full-band speech enhancement model\\u00a7r\\n\\n\\u00a78\\u00a7oQinwen Hu\\nZhongshu Hou\\nXiaohuai Le\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.14524\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 3 Jul 2022 06:13:07 GMT)\\u00a7r"}']}
{title:'Albesano et al. (§72022§r)', author: 'Dario Albesano; Jesús Andrés-Ferrer; Nicola Ferri; Puming Zhan', display:{Lore:['[{"text": "arXiv:2206.14618", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Prediction Network Architecture in RNN-T for ASR\\u00a7r\\n\\n\\u00a78\\u00a7oDario Albesano\\nJes\\u00fas Andr\\u00e9s-Ferrer\\nNicola Ferri\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.14618\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 29 Jun 2022 13:11:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at Interspeech 2022\\u00a7r"}']}
{title:'Andrés-Ferrer et al. (§72022§r)', author: 'Jesús Andrés-Ferrer; Dario Albesano; Puming Zhan; Paul Vozila', display:{Lore:['[{"text": "arXiv:2206.14623", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContextual Density Ratio for Language Model Biasing of Sequence to Sequence ASR Systems\\u00a7r\\n\\n\\u00a78\\u00a7oJes\\u00fas Andr\\u00e9s-Ferrer\\nDario Albesano\\nPuming Zhan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.14623\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-443\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInterspeech 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 29 Jun 2022 13:12:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2021 (draft)\\u00a7r"}']}
{title:'Segal et al. (§72022§r)', author: 'Yael Segal; Kasia Hitczenko; Matthew Goldrick; Adam Buchwald; Angela Roberts; Joseph Keshet', display:{Lore:['[{"text": "arXiv:2206.14639", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDDKtor: Automatic Diadochokinetic Speech Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oYael Segal\\nKasia Hitczenko\\nMatthew Goldrick\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.14639\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 29 Jun 2022 13:34:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Makarov et al. (§72022§r)', author: 'Peter Makarov; Ammar Abbas; Mateusz Łajszczak; Arnaud Joly; Sri Karlapati; Alexis Moinet; Thomas Drugman; Penny Karanasou', display:{Lore:['[{"text": "arXiv:2206.14643", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSimple and Effective Multi-sentence TTS with Expressive and Coherent Prosody\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Makarov\\nAmmar Abbas\\nMateusz \\u0141ajszczak\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.14643\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 29 Jun 2022 13:37:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to be published in the Proceedings of InterSpeech 2022\\u00a7r"}']}
{title:'Jiang et al. (§72022§r)', author: 'Yongjun Jiang; Jian Yu; Wenwen Yang; Bihong Zhang; Yanfeng Wang', display:{Lore:['[{"text": "arXiv:2206.14747", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNextformer: A ConvNeXt Augmented Conformer For End-To-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYongjun Jiang\\nJian Yu\\nWenwen Yang\\nBihong Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.14747\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 30 Jun 2022 07:44:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure\\u00a7r"}']}
{title:'Xu et al. (§72022§r)', author: 'Xinmeng Xu; Yang Wang; Jie Jia; Binbin Chen; Jianjun Hao', display:{Lore:['[{"text": "arXiv:2206.14962", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGLD-Net: Improving Monaural Speech Enhancement by Learning Global and Local Dependency Features with GLD Block\\u00a7r\\n\\n\\u00a78\\u00a7oXinmeng Xu\\nYang Wang\\nJie Jia\\nBinbin Chen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.14962\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Jun 2022 01:16:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Xu et al. (§72022§r)', author: 'Xinmeng Xu; Yang Wang; Jie Jia; Binbin Chen; Dejun Li', display:{Lore:['[{"text": "arXiv:2206.14964", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Visual Speech Enhancement Network by Learning Audio-visual Affinity with Multi-head Attention\\u00a7r\\n\\n\\u00a78\\u00a7oXinmeng Xu\\nYang Wang\\nJie Jia\\nBinbin Chen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.14964\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Jun 2022 01:20:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022. arXiv admin note: substantial text overlap with arXiv:2101.06268\\u00a7r"}']}
{title:'Song et al. (§72022§r)', author: 'Eunwoo Song; Ryuichi Yamamoto; Ohsung Kwon; Chan-Ho Song; Min-Jae Hwang; Suhyeon Oh; Hyun-Wook Yoon; Jin-Seob Kim; Jae-Min Kim', display:{Lore:['[{"text": "arXiv:2206.14984", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTTS-by-TTS 2: Data-selective augmentation for neural speech synthesis using ranking support vector machine with variational autoencoder\\u00a7r\\n\\n\\u00a78\\u00a7oEunwoo Song\\nRyuichi Yamamoto\\nOhsung Kwon\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.14984\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Jun 2022 02:26:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the conference of INTERSPEECH 2022\\u00a7r"}']}
{title:'Jin et al. (§72022§r)', author: 'Wenyu Jin; Patrick McPherson; Chris Pike; Adib Mehrabi', display:{Lore:['[{"text": "arXiv:2206.15356", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic Room Compensation Using Local PCA-based Room Average Power Response Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oWenyu Jin\\nPatrick McPherson\\nChris Pike\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.15356\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 23 Jul 2022 17:49:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 7 figures, to appear in IWAENC 2022\\u00a7r"}']}
{title:'Shin et al. (§72022§r)', author: 'Hyeon-Kyeong Shin; Hyewon Han; Doyeon Kim; Soo-Whan Chung; Hong-Goo Kang', display:{Lore:['[{"text": "arXiv:2206.15400", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Audio-Text Agreement for Open-vocabulary Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oHyeon-Kyeong Shin\\nHyewon Han\\nDoyeon Kim\\nSoo-Whan Chung\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.15400\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 1 Jul 2022 06:42:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Zhen et al. (§72022§r)', author: 'Kai Zhen; Hieu Duy Nguyen; Raviteja Chinta; Nathan Susanj; Athanasios Mouchtaris; Tariq Afzal; Ariya Rastrow', display:{Lore:['[{"text": "arXiv:2206.15408", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSub-8-Bit Quantization Aware Training for 8-Bit Neural Network Accelerator with On-Device Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oKai Zhen\\nHieu Duy Nguyen\\nRaviteja Chinta\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.15408\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Jun 2022 16:52:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in INTERSPEECH 2022\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'Wei-Ping Huang; Po-Chun Chen; Sung-Feng Huang; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2206.15427", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFew-Shot Cross-Lingual TTS Using Transferable Phoneme Embedding\\u00a7r\\n\\n\\u00a78\\u00a7oWei-Ping Huang\\nPo-Chun Chen\\nSung-Feng Huang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.15427\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 3 Aug 2022 10:50:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Ciccarelli et al. (§72022§r)', author: 'Gregory Ciccarelli; Jarred Barber; Arun Nair; Israel Cohen; Tao Zhang', display:{Lore:['[{"text": "arXiv:2206.15432", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChallenges and Opportunities in Multi-device Speech Processing\\u00a7r\\n\\n\\u00a78\\u00a7oGregory Ciccarelli\\nJarred Barber\\nArun Nair\\nIsrael Cohen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.15432\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Jun 2022 19:18:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for INTERSPEECH 2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Yuxiang Zhang; Zhuo Li; Wenchao Wang; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2207.00150", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSASV Based on Pre-trained ASV System and Integrated Scoring Module\\u00a7r\\n\\n\\u00a78\\u00a7oYuxiang Zhang\\nZhuo Li\\nWenchao Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.00150\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Jul 2022 02:27:09 GMT)\\u00a7r"}']}
{title:'Takashima et al. (§72022§r)', author: 'Yuki Takashima; Shota Horiguchi; Shinji Watanabe; Paola García; Yohei Kawaguchi', display:{Lore:['[{"text": "arXiv:2207.00216", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUpdating Only Encoders Prevents Catastrophic Forgetting of End-to-End ASR Models\\u00a7r\\n\\n\\u00a78\\u00a7oYuki Takashima\\nShota Horiguchi\\nShinji Watanabe\\nPaola Garc\\u00eda\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.00216\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Jul 2022 05:54:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for Interspeech 2022\\u00a7r"}']}
{title:'Bollens et al. (§72022§r)', author: 'Lies Bollens; Tom Francart; Hugo Van Hamme', display:{Lore:['[{"text": "arXiv:2207.00323", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Subject-Invariant Representations from Speech-Evoked EEG Using Variational Autoencoders\\u00a7r\\n\\n\\u00a78\\u00a7oLies Bollens\\nTom Francart\\nHugo Van Hamme\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.00323\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747297\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2022 - 2022 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP), 2022, pp. 1256-1260\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Jul 2022 10:26:44 GMT)\\u00a7r"}']}
{title:'Lee et al. (§72022§r)', author: 'Yeonghyeon Lee; Kangwook Jang; Jahyun Goo; Youngmoon Jung; Hoirin Kim', display:{Lore:['[{"text": "arXiv:2207.00555", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFitHuBERT: Going Thinner and Deeper for Knowledge Distillation of Speech Self-Supervised Learning\\u00a7r\\n\\n\\u00a78\\u00a7oYeonghyeon Lee\\nKangwook Jang\\nJahyun Goo\\nYoungmoon Jung\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.00555\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Jul 2022 17:11:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Gomez (§72022§r)', author: 'Antonio Gomez', display:{Lore:['[{"text": "arXiv:2207.00660", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Diarization and Identification from Single-Channel Classroom Audio Recording Using Virtual Microphones\\u00a7r\\n\\n\\u00a78\\u00a7oAntonio Gomez\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.00660\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Jul 2022 21:03:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTotal of 170 pages, 59 figures. This work is part of the partial fulfillment of the requirements for the degree of PhD in engineering at the University of New Mexico\\u00a7r"}']}
{title:'Breiner et al. (§72022§r)', author: 'Theresa Breiner; Swaroop Ramaswamy; Ehsan Variani; Shefali Garg; Rajiv Mathews; Khe Chai Sim; Kilol Gupta; Mingqing Chen; Lara McConnaughey', display:{Lore:['[{"text": "arXiv:2207.00706", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUserLibri: A Dataset for ASR Personalization Using Only Text\\u00a7r\\n\\n\\u00a78\\u00a7oTheresa Breiner\\nSwaroop Ramaswamy\\nEhsan Variani\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.00706\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Jul 2022 01:03:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in Interspeech 2022. 9 total pages with appendix, 9 total tables,5 total figures\\u00a7r"}']}
{title:'Korzekwa et al. (§72022§r)', author: 'Daniel Korzekwa; Jaime Lorenzo-Trueba; Thomas Drugman; Bozena Kostek', display:{Lore:['[{"text": "arXiv:2207.00774", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComputer-assisted Pronunciation Training \\u2013 Speech synthesis is almost all you need\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Korzekwa\\nJaime Lorenzo-Trueba\\nThomas Drugman\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.00774\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.specom.2022.06.003\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Jul 2022 08:33:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in Speech Communication Journal\\u00a7r"}']}
{title:'Hu et al. (§72022§r)', author: 'Ying Hu; Yuwu Tang; Hao Huang; Liang He', display:{Lore:['[{"text": "arXiv:2207.00940", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Graph Isomorphism Network with Weighted Multiple Aggregators for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYing Hu\\nYuwu Tang\\nHao Huang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.00940\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-637\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInterspeech 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 3 Jul 2022 02:58:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Wei et al. (§72022§r)', author: 'Kun Wei; Yike Zhang; Sining Sun; Lei Xie; Long Ma', display:{Lore:['[{"text": "arXiv:2207.01039", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Acoustic Contextual Representation by Audio-textual Cross-modal Learning for Conversational ASR\\u00a7r\\n\\n\\u00a78\\u00a7oKun Wei\\nYike Zhang\\nSining Sun\\nLei Xie\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.01039\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 3 Jul 2022 13:32:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech2022\\u00a7r"}']}
{title:'Ferrer et al. (§72022§r)', author: 'Miguel Ferrer; María de Diego; Gema Piñero; Amin Hassani; Marc Moonen; Alberto González', display:{Lore:['[{"text": "arXiv:2207.01102", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransfer functions of FXLMS-based Multi-channel Multi-tone Active Noise Equalizers\\u00a7r\\n\\n\\u00a78\\u00a7oMiguel Ferrer\\nMar\\u00eda de Diego\\nGema Pi\\u00f1ero\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.01102\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 3 Jul 2022 18:57:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 5 figures\\u00a7r"}']}
{title:'Proszewska et al. (§72022§r)', author: 'Magdalena Proszewska; Grzegorz Beringer; Daniel Sáez-Trigueros; Thomas Merritt; Abdelhamid Ezzerg; Roberto Barra-Chicote', display:{Lore:['[{"text": "arXiv:2207.01454", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGlowVC: Mel-spectrogram space disentangling model for language-independent text-free voice conversion\\u00a7r\\n\\n\\u00a78\\u00a7oMagdalena Proszewska\\nGrzegorz Beringer\\nDaniel S\\u00e1ez-Trigueros\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.01454\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Jul 2022 14:42:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Ziyao Zhang; Alessio Falai; Ariadna Sanchez; Orazio Angelini; Kayoko Yanagisawa', display:{Lore:['[{"text": "arXiv:2207.01507", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMix and Match: An Empirical Study on Training Corpus Composition for Polyglot Text-To-Speech (TTS)\\u00a7r\\n\\n\\u00a78\\u00a7oZiyao Zhang\\nAlessio Falai\\nAriadna Sanchez\\nOrazio Angelini\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.01507\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Jul 2022 15:23:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to be published in the Proceedings of InterSpeech 2022\\u00a7r"}']}
{title:'Sanchez et al. (§72022§r)', author: 'Ariadna Sanchez; Alessio Falai; Ziyao Zhang; Orazio Angelini; Kayoko Yanagisawa', display:{Lore:['[{"text": "arXiv:2207.01547", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnify and Conquer: How Phonetic Feature Representation Affects Polyglot Text-To-Speech (TTS)\\u00a7r\\n\\n\\u00a78\\u00a7oAriadna Sanchez\\nAlessio Falai\\nZiyao Zhang\\nOrazio Angelini\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.01547\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Jul 2022 16:14:57 GMT)\\u00a7r"}']}
{title:'Cheng et al. (§72022§r)', author: 'Guoliang Cheng; Lele Liao; Kai Chen; Yuxiang Hu; Changbao Zhu; Jing Lu', display:{Lore:['[{"text": "arXiv:2207.01556", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-blind source separation using convolutive transfer function for nonlinear acoustic echo cancellation\\u00a7r\\n\\n\\u00a78\\u00a7oGuoliang Cheng\\nLele Liao\\nKai Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.01556\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0016823\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Jul 2022 16:24:37 GMT)\\u00a7r"}']}
{title:'Larsen et al. (§72022§r)', author: 'Claus Meyer Larsen; Peter Koch; Zheng-Hua Tan', display:{Lore:['[{"text": "arXiv:2207.01691", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Multi-Task Deep Learning for Noise-Robust Voice Activity Detection with Low Algorithmic Delay\\u00a7r\\n\\n\\u00a78\\u00a7oClaus Meyer Larsen\\nPeter Koch\\nZheng-Hua Tan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.01691\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Jul 2022 19:43:09 GMT)\\u00a7r"}']}
{title:'Xie et al. (§72022§r)', author: 'Jiamin Xie; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:2207.01732", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDEFORMER: Coupling Deformed Localized Patterns with Global Context for Robust End-to-end Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJiamin Xie\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.01732\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Jul 2022 01:59:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Puffay et al. (§72022§r)', author: 'Corentin Puffay; Jana Van Canneyt; Jonas Vanthornhout; Hugo Van Hamme; Tom Francart', display:{Lore:['[{"text": "arXiv:2207.01963", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRelating the fundamental frequency of speech with EEG using a dilated convolutional network\\u00a7r\\n\\n\\u00a78\\u00a7oCorentin Puffay\\nJana Van Canneyt\\nJonas Vanthornhout\\nHugo Van Hamme\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.01963\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Jul 2022 11:20:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for Interspeech 2022\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Zehan Li; Haoran Miao; Keqi Deng; Gaofeng Cheng; Sanli Tian; Ta Li; Yonghong Yan', display:{Lore:['[{"text": "arXiv:2207.02495", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Streaming End-to-End ASR on Transformer-based Causal Models with Encoder States Revision Strategies\\u00a7r\\n\\n\\u00a78\\u00a7oZehan Li\\nHaoran Miao\\nKeqi Deng\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.02495\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Jul 2022 07:59:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Nozaki et al. (§72022§r)', author: 'Jumon Nozaki; Tatsuya Kawahara; Kenkichi Ishizuka; Taiichi Hashimoto', display:{Lore:['[{"text": "arXiv:2207.03169", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end Speech-to-Punctuated-Text Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJumon Nozaki\\nTatsuya Kawahara\\nKenkichi Ishizuka\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.03169\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Jul 2022 08:58:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH2022\\u00a7r"}']}
{title:'Pia et al. (§72022§r)', author: 'Nicola Pia; Kishan Gupta; Srikanth Korse; Markus Multrus; Guillaume Fuchs', display:{Lore:['[{"text": "arXiv:2207.03282", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNESC: Robust Neural End-2-End Speech Coding with GANs\\u00a7r\\n\\n\\u00a78\\u00a7oNicola Pia\\nKishan Gupta\\nSrikanth Korse\\nMarkus Multrus\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.03282\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Jul 2022 13:23:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper accepted to Interspeech 2022 Please check our demo at: https://fhgspco.github.io/nesc/\\u00a7r"}']}
{title:'Ghosh et al. (§72022§r)', author: 'Arindam Ghosh; Mark Fuhs; Deblin Bagchi; Bahman Farahani; Monika Woszczyna', display:{Lore:['[{"text": "arXiv:2207.03331", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-resource Low-footprint Wake-word Detection using Knowledge Distillation\\u00a7r\\n\\n\\u00a78\\u00a7oArindam Ghosh\\nMark Fuhs\\nDeblin Bagchi\\nBahman Farahani\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.03331\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Jul 2022 15:45:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Mitra et al. (§72022§r)', author: 'Vikramjit Mitra; Hsiang-Yun Sherry Chien; Vasudha Kowtha; Joseph Yitan Cheng; Erdrin Azemi', display:{Lore:['[{"text": "arXiv:2207.03334", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Emotion: Investigating Model Representations, Multi-Task Learning and Knowledge Distillation\\u00a7r\\n\\n\\u00a78\\u00a7oVikramjit Mitra\\nHsiang-Yun Sherry Chien\\nVasudha Kowtha\\nJoseph Yitan Cheng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.03334\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 2 Jul 2022 17:34:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, Interspeech 2022\\u00a7r"}']}
{title:'Meyer et al. (§72022§r)', author: 'Josh Meyer; David Ifeoluwa Adelani; Edresson Casanova; Alp Öktem; Daniel Whitenack Julian Weber; Salomon Kabongo; Elizabeth Salesky; Iroro Orife; Colin Leong; Perez Ogayo; Chris Emezue; Jonathan Mukiibi; Salomey Osei; Apelete Agbolo; Victor Akinode; Bernard Opoku; Samuel Olanrewaju; Jesujoba Alabi; Shamsuddeen Muhammad', display:{Lore:['[{"text": "arXiv:2207.03546", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBibleTTS: a large, high-fidelity, multilingual, and uniquely African speech corpus\\u00a7r\\n\\n\\u00a78\\u00a7oJosh Meyer\\nDavid Ifeoluwa Adelani\\nEdresson Casanova\\n+ 15 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.03546\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Jul 2022 19:35:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Baird et al. (§72022§r)', author: 'Alice Baird; Panagiotis Tzirakis; Jeffrey A. Brooks; Christopher B. Gregory; Björn Schuller; Anton Batliner; Dacher Keltner; Alan Cowen', display:{Lore:['[{"text": "arXiv:2207.03572", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe ACII 2022 Affective Vocal Bursts Workshop     Competition: Understanding a critically understudied modality of emotional expression\\u00a7r\\n\\n\\u00a78\\u00a7oAlice Baird\\nPanagiotis Tzirakis\\nJeffrey A. Brooks\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.03572\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 27 Oct 2022 20:26:07 GMT)\\u00a7r"}']}
{title:'Kolic et al. (§72022§r)', author: 'Blas Kolic; Mateo Tonatiuh Rodriguez-Cervantes; Pablo Padilla-Longoria; Francis Knights', display:{Lore:['[{"text": "arXiv:2207.03602", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.IT\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a72math.IT\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRhythm and form in music: a complex systems approach\\u00a7r\\n\\n\\u00a78\\u00a7oBlas Kolic\\nMateo Tonatiuh Rodriguez-Cervantes\\nPablo Padilla-Longoria\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.03602\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Jul 2022 22:03:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 8 figures\\u00a7r"}']}
{title:'Zheng et al. (§72022§r)', author: 'Xianrui Zheng; Chao Zhang; Philip C. Woodland', display:{Lore:['[{"text": "arXiv:2207.03852", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTandem Multitask Training of Speaker Diarisation and Speech Recognition for Meeting Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oXianrui Zheng\\nChao Zhang\\nPhilip C. Woodland\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.03852\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Jul 2022 12:06:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Interspeech 2022\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Long Chen; Yixiong Meng; Venkatesh Ravichandran; Andreas Stolcke', display:{Lore:['[{"text": "arXiv:2207.04081", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGraph-based Multi-View Fusion and Local Adaptation: Mitigating Within-Household Confusability for Speaker Identification\\u00a7r\\n\\n\\u00a78\\u00a7oLong Chen\\nYixiong Meng\\nVenkatesh Ravichandran\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.04081\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-11053\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech, Sept. 2022, pp. 4805-4809\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Jul 2022 18:12:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Interspeech 2022. arXiv admin note: text overlap with arXiv:2106.08207\\u00a7r"}']}
{title:'Peng et al. (§72022§r)', author: 'Yizhou Peng; Yufei Liu; Jicheng Zhang; Haihua Xu; Yi He; Hao Huang; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2207.04176", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInternal Language Model Estimation based Language Model Fusion for Cross-Domain Code-Switching Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYizhou Peng\\nYufei Liu\\nJicheng Zhang\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.04176\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 9 Jul 2022 02:08:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages. Submitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Jicheng Zhang; Yizhou Peng; Haihua Xu; Yi He; Eng Siong Chng; Hao Huang', display:{Lore:['[{"text": "arXiv:2207.04177", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntermediate-layer output Regularization for Attention-based Speech Recognition with Shared Decoder\\u00a7r\\n\\n\\u00a78\\u00a7oJicheng Zhang\\nYizhou Peng\\nHaihua Xu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.04177\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 9 Jul 2022 02:21:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages. Submitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Sang et al. (§72022§r)', author: 'Mufan Sang; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:2207.04540", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Frequency Information Enhanced Channel Attention Module for Speaker Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oMufan Sang\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.04540\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 10 Jul 2022 21:19:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Parada et al. (§72022§r)', author: 'Pablo Peso Parada; Agnieszka Dobrowolska; Karthikeyan Saravanan; Mete Ozay', display:{Lore:['[{"text": "arXiv:2207.04949", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lpMCT: Patched Multi-Condition Training for Robust Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oPablo Peso Parada\\nAgnieszka Dobrowolska\\nKarthikeyan Saravanan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.04949\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Jul 2022 15:34:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2022\\u00a7r"}']}
{title:'Flechl et al. (§72022§r)', author: 'Martin Flechl; Shou-Chun Yin; Junho Park; Peter Skala', display:{Lore:['[{"text": "arXiv:2207.05469", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end speech recognition modeling from de-identified data\\u00a7r\\n\\n\\u00a78\\u00a7oMartin Flechl\\nShou-Chun Yin\\nJunho Park\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.05469\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Jul 2022 11:29:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Lepage et al. (§72022§r)', author: 'Théo Lepage; Réda Dehak', display:{Lore:['[{"text": "arXiv:2207.05506", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLabel-Efficient Self-Supervised Speaker Verification With Information Maximization and Contrastive Learning\\u00a7r\\n\\n\\u00a78\\u00a7oTh\\u00e9o Lepage\\nR\\u00e9da Dehak\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.05506\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nINTERSPEECH 2022, September 18-22, Incheon, Korea\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Jul 2022 13:01:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Encke et al. (§72022§r)', author: 'Jörg Encke; Mathias Dietz', display:{Lore:['[{"text": "arXiv:2207.05541", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStatistics of the interaural parameters for dichotic tones in diotic noise (N_0 S_\\u03c8)\\u00a7r\\n\\n\\u00a78\\u00a7oJ\\u00f6rg Encke\\nMathias Dietz\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.05541\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Jul 2022 14:01:19 GMT)\\u00a7r"}']}
{title:'Koch et al. (§72022§r)', author: 'Julia Koch; Florian Lux; Nadja Schauffler; Toni Bernhart; Felix Dieterle; Jonas Kuhn; Sandra Richter; Gabriel Viehhauser; Ngoc Thang Vu', display:{Lore:['[{"text": "arXiv:2207.05549", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPoeticTTS \\u2013 Controllable Poetry Reading for Literary Studies\\u00a7r\\n\\n\\u00a78\\u00a7oJulia Koch\\nFlorian Lux\\nNadja Schauffler\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.05549\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 18 Oct 2022 20:48:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at Interspeech 2022\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Yi-Chiao Wu; Patrick Lumban Tobing; Kazuki Yasuhara; Noriyuki Matsunaga; Yamato Ohtani; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2207.05913", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Cyclical Approach to Synthetic and Natural Speech Mismatch Refinement of Neural Post-filter for Low-cost Text-to-speech System\\u00a7r\\n\\n\\u00a78\\u00a7oYi-Chiao Wu\\nPatrick Lumban Tobing\\nKazuki Yasuhara\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.05913\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1561/116.00000020\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nAPSIPA Transactions on Signal and Information Processing, Vol 11,\\n  Issue 1, 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Jul 2022 01:40:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 7 figures, 10 tables\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Weiqing Wang; Qingjian Lin; Ming Li', display:{Lore:['[{"text": "arXiv:2207.05920", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOnline Target Speaker Voice Activity Detection for Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oWeiqing Wang\\nQingjian Lin\\nMing Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.05920\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Jul 2022 01:56:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Qin et al. (§72022§r)', author: 'Xiaoyi Qin; Na Li; Chao Weng; Dan Su; Ming Li', display:{Lore:['[{"text": "arXiv:2207.05929", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-Age Speaker Verification: Learning Age-Invariant Speaker Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoyi Qin\\nNa Li\\nChao Weng\\nDan Su\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.05929\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Jul 2022 02:28:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech2022\\u00a7r"}']}
{title:'Goswami et al. (§72022§r)', author: 'Nabarun Goswami; Tatsuya Harada', display:{Lore:['[{"text": "arXiv:2207.06011", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSATTS: Speaker Attractor Text to Speech, Learning to Speak by Learning to Separate\\u00a7r\\n\\n\\u00a78\\u00a7oNabarun Goswami\\nTatsuya Harada\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.06011\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Jul 2022 07:35:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022. Visit https://naba89.github.io/SATTS-demo/ for a demo\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'Rongjie Huang; Zhou Zhao; Huadai Liu; Jinglin Liu; Chenye Cui; Yi Ren', display:{Lore:['[{"text": "arXiv:2207.06389", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProDiff: Progressive Fast Diffusion Model For High-Quality Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oRongjie Huang\\nZhou Zhao\\nHuadai Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.06389\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Jul 2022 17:45:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ACM Multimedia 2022\\u00a7r"}']}
{title:'Du et al. (§72022§r)', author: 'Yicheng Du; Aditya Arie Nugraha; Kouhei Sekiguchi; Yoshiaki Bando; Mathieu Fontaine; Kazuyoshi Yoshii', display:{Lore:['[{"text": "arXiv:2207.07273", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDirection-Aware Joint Adaptation of Neural Speech Enhancement and Recognition in Real Multiparty Conversational Environments\\u00a7r\\n\\n\\u00a78\\u00a7oYicheng Du\\nAditya Arie Nugraha\\nKouhei Sekiguchi\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.07273\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Jul 2022 03:43:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2022\\u00a7r"}']}
{title:'Sekiguchi et al. (§72022§r)', author: 'Kouhei Sekiguchi; Aditya Arie Nugraha; Yicheng Du; Yoshiaki Bando; Mathieu Fontaine; Kazuyoshi Yoshii', display:{Lore:['[{"text": "arXiv:2207.07296", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDirection-Aware Adaptive Online Neural Speech Enhancement with an Augmented Reality Headset in Real Noisy Conversational Environments\\u00a7r\\n\\n\\u00a78\\u00a7oKouhei Sekiguchi\\nAditya Arie Nugraha\\nYicheng Du\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.07296\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Jul 2022 05:14:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE/RSJ IROS 2022\\u00a7r"}']}
{title:'Yin et al. (§72022§r)', author: 'Haoran Yin; Meng Ge; Yanjie Fu; Gaoyan Zhang; Longbiao Wang; Lei Zhang; Lin Qiu; Jianwu Dang', display:{Lore:['[{"text": "arXiv:2207.07307", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMIMO-DoAnet: Multi-channel Input and Multiple Outputs DoA Network with Unknown Number of Sound Sources\\u00a7r\\n\\n\\u00a78\\u00a7oHaoran Yin\\nMeng Ge\\nYanjie Fu\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.07307\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10493\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 17 Nov 2022 03:48:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Xingming Wang; Xiaoyi Qin; Yikang Wang; Yunfei Xu; Ming Li', display:{Lore:['[{"text": "arXiv:2207.07510", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe DKU-OPPO System for the 2022 Spoofing-Aware Speaker Verification Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oXingming Wang\\nXiaoyi Qin\\nYikang Wang\\nYunfei Xu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.07510\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Jul 2022 14:56:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech2022\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Haohe Liu; Xubo Liu; Xinhao Mei; Qiuqiang Kong; Wenwu Wang; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:2207.07773", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSegment-level Metric Learning for Few-shot Bioacoustic Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oHaohe Liu\\nXubo Liu\\nXinhao Mei\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.07773\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Jul 2022 22:41:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2nd place in the DCASE 2022Challenge Task 5. Submitted to the DCASE 2022 workshop\\u00a7r"}']}
{title:'Jin et al. (§72022§r)', author: 'Minho Jin; Chelsea J. -T. Ju; Zeya Chen; Yi-Chieh Liu; Jasha Droppo; Andreas Stolcke', display:{Lore:['[{"text": "arXiv:2207.07776", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Reweighting for Speaker Verification Fairness\\u00a7r\\n\\n\\u00a78\\u00a7oMinho Jin\\nChelsea J. -T. Ju\\nZeya Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.07776\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10948\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech, Sept. 2022, pp. 4800-4804\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Jul 2022 22:50:31 GMT)\\u00a7r"}']}
{title:'Trinh et al. (§72022§r)', author: 'Viet Anh Trinh; Pegah Ghahremani; Brian King; Jasha Droppo; Andreas Stolcke; Roland Maas', display:{Lore:['[{"text": "arXiv:2207.07850", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReducing Geographic Disparities in Automatic Speech Recognition via Elastic Weight Consolidation\\u00a7r\\n\\n\\u00a78\\u00a7oViet Anh Trinh\\nPegah Ghahremani\\nBrian King\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.07850\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-11063\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech, Sept. 2022, pp. 1298-1302\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 16 Jul 2022 06:04:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at Interspeech 2022\\u00a7r"}']}
{title:'Hsu et al. (§72022§r)', author: 'Yicheng Hsu; Yonghan Lee; Mingsian R. Bai', display:{Lore:['[{"text": "arXiv:2207.08126", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-channel target speech enhancement based on ERB-scaled spatial coherence features\\u00a7r\\n\\n\\u00a78\\u00a7oYicheng Hsu\\nYonghan Lee\\nMingsian R. Bai\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.08126\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 17 Jul 2022 09:53:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by International Congresson Acoustics (ICA) 2022. arXiv admin note: substantial text overlap with arXiv:2112.05686\\u00a7r"}']}
{title:'Ghanavi et al. (§72022§r)', author: 'Reza Ghanavi; Craig Jin', display:{Lore:['[{"text": "arXiv:2207.08314", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving spatial cues for hearables using a parameterized binaural CDR estimator\\u00a7r\\n\\n\\u00a78\\u00a7oReza Ghanavi\\nCraig Jin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.08314\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 17 Jul 2022 22:56:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICA2022. An Australian provisional patent application based on this manuscript has been filed by the University of Sydney\\u00a7r"}']}
{title:'Bu et al. (§72022§r)', author: 'Zhaoyang Bu; Hanhaodi Zhang; Xiaohu Zhu', display:{Lore:['[{"text": "arXiv:2207.09145", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGAFX: A General Audio Feature eXtractor\\u00a7r\\n\\n\\u00a78\\u00a7oZhaoyang Bu\\nHanhaodi Zhang\\nXiaohu Zhu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.09145\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Jul 2022 09:37:44 GMT)\\u00a7r"}']}
{title:'Ong et al. (§72022§r)', author: 'Zhen-Ting Ong; Bhan Lam; Kenneth Ooi; Karn N. Watcharasupat; Trevor Wong; Woon-Seng Gan', display:{Lore:['[{"text": "arXiv:2207.09221", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7cstat.AP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDo uHear? Validation of uHear App for Preliminary Screening of Hearing Ability in Soundscape Studies\\u00a7r\\n\\n\\u00a78\\u00a7oZhen-Ting Ong\\nBhan Lam\\nKenneth Ooi\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.09221\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 16 Jul 2022 08:19:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oFull paper submitted to 24thInternational Congresson Acoustics\\u00a7r"}']}
{title:'Lu et al. (§72022§r)', author: 'Yen-Ju Lu; Xuankai Chang; Chenda Li; Wangyou Zhang; Samuele Cornell; Zhaoheng Ni; Yoshiki Masuyama; Brian Yan; Robin Scheibler; Zhong-Qiu Wang; Yu Tsao; Yanmin Qian; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2207.09514", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding\\u00a7r\\n\\n\\u00a78\\u00a7oYen-Ju Lu\\nXuankai Chang\\nChenda Li\\n+ 9 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.09514\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Jul 2022 18:55:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Interspeech 2022\\u00a7r"}']}
{title:'Takeuchi et al. (§72022§r)', author: 'Daiki Takeuchi; Yasunori Ohishi; Daisuke Niizumi; Noboru Harada; Kunio Kashino', display:{Lore:['[{"text": "arXiv:2207.09732", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntroducing Auxiliary Text Query-modifier to Content-based Audio Retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oDaiki Takeuchi\\nYasunori Ohishi\\nDaisuke Niizumi\\nNoboru Harada\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.09732\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Jul 2022 08:19:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Ou et al. (§72022§r)', author: 'Longshen Ou; Xiangming Gu; Ye Wang', display:{Lore:['[{"text": "arXiv:2207.09747", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransfer Learning of wav2vec 2.0 for Automatic Lyric Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oLongshen Ou\\nXiangming Gu\\nYe Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.09747\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 16 Oct 2022 06:01:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCamera ready version of ISMIR 2022 submission\\u00a7r"}']}
{title:'Atmaja et al. (§72022§r)', author: 'Bagus Tris Atmaja; Zanjabila; Akira Sasou', display:{Lore:['[{"text": "arXiv:2207.10333", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJointly Predicting Emotion, Age, and Country Using Pre-Trained Acoustic Embedding\\u00a7r\\n\\n\\u00a78\\u00a7oBagus Tris Atmaja\\nZanjabila\\nAkira Sasou\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.10333\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInternational Conference on Affective Computing and Intelligent\\n  Interaction Workshops and Demos (ACIIW) 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Jul 2022 07:04:41 GMT)\\u00a7r"}']}
{title:'Nugraha et al. (§72022§r)', author: 'Aditya Arie Nugraha; Kouhei Sekiguchi; Mathieu Fontaine; Yoshiaki Bando; Kazuyoshi Yoshii', display:{Lore:['[{"text": "arXiv:2207.10934", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDNN-Free Low-Latency Adaptive Speech Enhancement Based on Frame-Online Beamforming Powered by Block-Online FastMNMF\\u00a7r\\n\\n\\u00a78\\u00a7oAditya Arie Nugraha\\nKouhei Sekiguchi\\nMathieu Fontaine\\nYoshiaki Bando\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.10934\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Jul 2022 08:17:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIWAENC 2022\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Dong Yang; Fei Jiang; Wei Wu; Xuefei Fang; Muyong Cao', display:{Lore:['[{"text": "arXiv:2207.11388", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Complexity Acoustic Echo Cancellation with Neural Kalman Filtering\\u00a7r\\n\\n\\u00a78\\u00a7oDong Yang\\nFei Jiang\\nWei Wu\\nXuefei Fang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.11388\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 30 Oct 2022 02:30:06 GMT)\\u00a7r"}']}
{title:'Singh et al. (§72022§r)', author: 'Arshdeep Singh; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:2207.11529", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-complexity CNNs for Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oArshdeep Singh\\nMark D. Plumbley\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.11529\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 23 Jul 2022 14:37:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to DCASE 2022 Workshop\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Chunxi Liu; Yuan Shangguan; Haichuan Yang; Yangyang Shi; Raghuraman Krishnamoorthi; Ozlem Kalinli', display:{Lore:['[{"text": "arXiv:2207.11906", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning a Dual-Mode Speech Recognition Model via Self-Pruning\\u00a7r\\n\\n\\u00a78\\u00a7oChunxi Liu\\nYuan Shangguan\\nHaichuan Yang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.11906\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 6 Oct 2022 21:30:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 1 figure. Accepted for publication at IEEE Spoken Language Technology Workshop (SLT), 2022\\u00a7r"}']}
{title:'Ohishi et al. (§72022§r)', author: 'Yasunori Ohishi; Marc Delcroix; Tsubasa Ochiai; Shoko Araki; Daiki Takeuchi; Daisuke Niizumi; Akisato Kimura; Noboru Harada; Kunio Kashino', display:{Lore:['[{"text": "arXiv:2207.11964", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConceptBeam: Concept Driven Target Speech Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oYasunori Ohishi\\nMarc Delcroix\\nTsubasa Ochiai\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.11964\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3503161.3548397\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Jul 2022 08:06:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ACM Multimedia 2022\\u00a7r"}']}
{title:'Park et al. (§72022§r)', author: 'Chanho Park; Rehan Ahmad; Thomas Hain', display:{Lore:['[{"text": "arXiv:2207.12028", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised data selection for Speech Recognition with contrastive loss ratios\\u00a7r\\n\\n\\u00a78\\u00a7oChanho Park\\nRehan Ahmad\\nThomas Hain\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.12028\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747390\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEEInt.Conf.Acoust.SpeechSignalProcess. (2022) 8587-8591\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Jul 2022 10:08:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted by ICASSP 2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Song Zhang; Ken Zheng; Xiaoxu Zhu; Baoxiang Li', display:{Lore:['[{"text": "arXiv:2207.12089", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Polyphone BERT for Polyphone Disambiguation in Mandarin Chinese\\u00a7r\\n\\n\\u00a78\\u00a7oSong Zhang\\nKen Zheng\\nXiaoxu Zhu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.12089\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Jul 2022 09:16:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for INTERSPEECH 2022\\u00a7r"}']}
{title:'Prabhu et al. (§72022§r)', author: 'Navin Raj Prabhu; Nale Lehmann-Willenbrock; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2207.12135", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLabel Uncertainty Modeling and Prediction for Speech Emotion Recognition using t-Distributions\\u00a7r\\n\\n\\u00a78\\u00a7oNavin Raj Prabhu\\nNale Lehmann-Willenbrock\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.12135\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Jul 2022 12:38:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oACCEPTED to ACII2022 -10th INTERNATIONAL CONFERENCE ON AFFECTIVE COMPUTING INTELLIGENT INTERACTION\\u00a7r"}']}
{title:'Fernandez et al. (§72022§r)', author: 'Raul Fernandez; David Haws; Guy Lorberbom; Slava Shechtman; Alexander Sorin', display:{Lore:['[{"text": "arXiv:2207.12262", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransplantation of Conversational Speaking Style with Interjections in Sequence-to-Sequence Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oRaul Fernandez\\nDavid Haws\\nGuy Lorberbom\\nSlava Shechtman\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.12262\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Jul 2022 15:32:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for presentation at Interspeech 2022\\u00a7r"}']}
{title:'V et al. (§72022§r)', author: 'Viswanatha V; Ramachandra A. C; Raghavendra Prasanna; Prem Chowdary Kakarla; Viveka Simha PJ; Nishant Mohan', display:{Lore:['[{"text": "arXiv:2207.12866", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImplementation Of Tiny Machine Learning Models On Arduino 33 BLE For Gesture And Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oViswanatha V\\nRamachandra A. C\\nRaghavendra Prasanna\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.12866\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 23 Jul 2022 10:53:26 GMT)\\u00a7r"}']}
{title:'Lee et al. (§72022§r)', author: 'Yoonhyung Lee; Seunghyun Yoon; Kyomin Jung', display:{Lore:['[{"text": "arXiv:2207.12895", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultimodal Speech Emotion Recognition using Cross Attention with Aligned Audio and Text\\u00a7r\\n\\n\\u00a78\\u00a7oYoonhyung Lee\\nSeunghyun Yoon\\nKyomin Jung\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.12895\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2020-2312\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2020, 2717-2721\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Jul 2022 13:44:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted by INTERSPEECH 2020\\u00a7r"}']}
{title:'Lam et al. (§72022§r)', author: 'Bhan Lam; Kenneth Ooi; Zhen-Ting Ong; Karn N. Watcharasupat; Trevor Wong; Woon-Seng Gan', display:{Lore:['[{"text": "arXiv:2207.12899", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAssessment of a cost-effective headphone calibration procedure for soundscape evaluations\\u00a7r\\n\\n\\u00a78\\u00a7oBhan Lam\\nKenneth Ooi\\nZhen-Ting Ong\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.12899\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nin Proc. 24th Int. Congr. Acoust., 2022, pp. 1-8\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 24 Jul 2022 05:39:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oFor 24th International Congress on Acoustics\\u00a7r"}']}
{title:'A et al. (§72022§r)', author: 'Madhavaraj A; Bharathi Pilar; Ramakrishnan A G', display:{Lore:['[{"text": "arXiv:2207.13331", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSubword Dictionary Learning and Segmentation Techniques for Automatic Speech Recognition in Tamil and Kannada\\u00a7r\\n\\n\\u00a78\\u00a7oMadhavaraj A\\nBharathi Pilar\\nRamakrishnan A G\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.13331\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Jul 2022 07:24:34 GMT)\\u00a7r"}']}
{title:'A et al. (§72022§r)', author: 'Madhavaraj A; Bharathi Pilar; Ramakrishnan A G', display:{Lore:['[{"text": "arXiv:2207.13333", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKnowledge-driven Subword Grammar Modeling for Automatic Speech Recognition in Tamil and Kannada\\u00a7r\\n\\n\\u00a78\\u00a7oMadhavaraj A\\nBharathi Pilar\\nRamakrishnan A G\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.13333\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Jul 2022 07:29:27 GMT)\\u00a7r"}']}
{title:'Kinoshita et al. (§72022§r)', author: 'Keisuke Kinoshita; Thilo von Neumann; Marc Delcroix; Christoph Boeddeker; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2207.13888", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUtterance-by-utterance overlap-aware neural diarization with Graph-PIT\\u00a7r\\n\\n\\u00a78\\u00a7oKeisuke Kinoshita\\nThilo von Neumann\\nMarc Delcroix\\nChristoph Boeddeker\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.13888\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Jul 2022 05:49:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022 (5 pages, 1 figure)\\u00a7r"}']}
{title:'Brendel et al. (§72022§r)', author: 'Andreas Brendel; Thomas Haubner; Walter Kellermann', display:{Lore:['[{"text": "arXiv:2207.13934", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Unifying View on Blind Source Separation of Convolutive Mixtures based on Independent Component Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oAndreas Brendel\\nThomas Haubner\\nWalter Kellermann\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.13934\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TSP.2023.3255552\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Jul 2022 07:57:11 GMT)\\u00a7r"}']}
{title:'Kons et al. (§72022§r)', author: 'Zvi Kons; Hagai Aronowitz; Edmilson Morais; Matheus Damasceno; Hong-Kwang Kuo; Samuel Thomas; George Saon', display:{Lore:['[{"text": "arXiv:2207.13965", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExtending RNN-T-based speech recognition systems with emotion and language classification\\u00a7r\\n\\n\\u00a78\\u00a7oZvi Kons\\nHagai Aronowitz\\nEdmilson Morais\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.13965\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Jul 2022 09:11:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in Interspeech 2022\\u00a7r"}']}
{title:'Torcoli et al. (§72022§r)', author: 'Matteo Torcoli; Thomas Robotham; Emanuël A. P. Habets', display:{Lore:['[{"text": "arXiv:2207.14240", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDialogue Enhancement and Listening Effort in Broadcast Audio: A Multimodal Evaluation\\u00a7r\\n\\n\\u00a78\\u00a7oMatteo Torcoli\\nThomas Robotham\\nEmanu\\u00ebl A. P. Habets\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.14240\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/QoMEX55416.2022.9900884\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 3 Aug 2022 06:34:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper accepted to 14th International Conference on Quality of Multimedia Experience (QoMEX), Lippstadt, Germany, 2022 - version 2 fixes some typos\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Yuxiang Wang; You Zhang; Zhiyao Duan; Mark Bocko', display:{Lore:['[{"text": "arXiv:2207.14352", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPredicting Global Head-Related Transfer Functions From Scanned Head Geometry Using Deep Learning and Compact Representations\\u00a7r\\n\\n\\u00a78\\u00a7oYuxiang Wang\\nYou Zhang\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.14352\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Jul 2022 19:13:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 14 figures\\u00a7r"}']}
{title:'Comini et al. (§72022§r)', author: 'Giulia Comini; Goeric Huybrechts; Manuel Sam Ribeiro; Adam Gabrys; Jaime Lorenzo-Trueba', display:{Lore:['[{"text": "arXiv:2207.14607", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-data? No problem: low-resource, language-agnostic conversational text-to-speech via F0-conditioned data augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oGiulia Comini\\nGoeric Huybrechts\\nManuel Sam Ribeiro\\nAdam Gabrys\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.14607\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Jul 2022 11:00:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for presentation at Interspeech 2022\\u00a7r"}']}
{title:'Guo et al. (§72022§r)', author: 'Z. Guo; C. Chen; E. S. Chng', display:{Lore:['[{"text": "arXiv:2208.00987", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDENT-DDSP: Data-efficient noisy speech generator using differentiable digital signal processors for explicit distortion modelling and noise-robust speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZ. Guo\\nC. Chen\\nE. S. Chng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.00987\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Aug 2022 16:56:07 GMT)\\u00a7r"}']}
{title:'Arushi et al. (§72022§r)', author: 'Arushi; Roberto Dillon; Ai Ni Teoh; Denise Dillon', display:{Lore:['[{"text": "arXiv:2208.01041", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Analysis for Stress Detection and Application in Virtual Reality to Improve Public Speaking in Real-time: A Review\\u00a7r\\n\\n\\u00a78\\u00a7oArushi\\nRoberto Dillon\\nAi Ni Teoh\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.01041\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Aug 2022 03:51:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o41 pages, 7 figures, 4 tables\\u00a7r"}']}
{title:'Singh et al. (§72022§r)', author: 'Arshdeep Singh; James A King; Xubo Liu; Wenwu Wang; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:2208.01555", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-complexity CNNs for Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oArshdeep Singh\\nJames A King\\nXubo Liu\\nWenwu Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.01555\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Aug 2022 16:02:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTechnical ReportDCASE 2022 TASK 1. arXiv admin note: substantial text overlap with arXiv:2207.11529\\u00a7r"}']}
{title:'Bai et al. (§72022§r)', author: 'Qibing Bai; Tom Ko; Yu Zhang', display:{Lore:['[{"text": "arXiv:2208.02189", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Study of Modeling Rising Intonation in Cantonese Neural Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oQibing Bai\\nTom Ko\\nYu Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.02189\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Aug 2022 16:21:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2022\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Yanxiong Li; Wenchang Cao; Konstantinos Drossos; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2208.02406", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDomestic Activity Clustering from Audio via Depthwise Separable Convolutional Autoencoder Network\\u00a7r\\n\\n\\u00a78\\u00a7oYanxiong Li\\nWenchang Cao\\nKonstantinos Drossos\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.02406\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Aug 2022 02:20:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 5 figures, 4 tables. Accepted by IEEE MMSP 2022\\u00a7r"}']}
{title:'Götz et al. (§72022§r)', author: 'Philipp Götz; Cagdas Tuna; Andreas Walther; Emanuël A. P. Habets', display:{Lore:['[{"text": "arXiv:2208.03023", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAID: Open-source Anechoic Interferer Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oPhilipp G\\u00f6tz\\nCagdas Tuna\\nAndreas Walther\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.03023\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Aug 2022 07:34:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at IWAENC 2022\\u00a7r"}']}
{title:'Bai et al. (§72022§r)', author: 'Jisheng Bai; Jianfeng Chen; Mou Wang; Muhammad Saad Ayub; Qingli Yan', display:{Lore:['[{"text": "arXiv:2208.03421", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSSDPT: Self-Supervised Dual-Path Transformer for Anomalous Sound Detection in Machine Condition Monitoring\\u00a7r\\n\\n\\u00a78\\u00a7oJisheng Bai\\nJianfeng Chen\\nMou Wang\\nMuhammad Saad Ayub\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.03421\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.dsp.2023.103939\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 6 Aug 2022 02:17:16 GMT)\\u00a7r"}']}
{title:'Luo et al. (§72022§r)', author: 'Yi Luo; Jianwei Yu', display:{Lore:['[{"text": "arXiv:2208.04101", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFRA-RIR: Fast Random Approximation of the Image-source Method\\u00a7r\\n\\n\\u00a78\\u00a7oYi Luo\\nJianwei Yu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.04101\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Aug 2022 12:46:30 GMT)\\u00a7r"}']}
{title:'Zhao et al. (§72022§r)', author: 'Zhiyuan Zhao; Chuanxin Tang; Chengdong Yao; Chong Luo', display:{Lore:['[{"text": "arXiv:2208.04622", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Anchor-Free Detector for Continuous Speech Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oZhiyuan Zhao\\nChuanxin Tang\\nChengdong Yao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.04622\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Aug 2022 09:37:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022\\u00a7r"}']}
{title:'Gul et al. (§72022§r)', author: 'Sania Gul; Muhammad Salman Khan; Syed Waqar Shah; Ata Ur-Rehman', display:{Lore:['[{"text": "arXiv:2208.04626", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRecycling an anechoic pre-trained speech separation deep neural network for binaural dereverberation of a single source\\u00a7r\\n\\n\\u00a78\\u00a7oSania Gul\\nMuhammad Salman Khan\\nSyed Waqar Shah\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.04626\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Aug 2022 09:43:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 4 figures\\u00a7r"}']}
{title:'Berg et al. (§72022§r)', author: "Axel Berg; Mark O'Connor; Kalle Åström; Magnus Oskarsson", display:{Lore:['[{"text": "arXiv:2208.04654", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExtending GCC-PHAT using Shift Equivariant Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oAxel Berg\\nMark O\'Connor\\nKalle \\u00c5str\\u00f6m\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.04654\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-524\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2022, 1791-1795\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Aug 2022 10:31:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH\\u00a7r"}']}
{title:'Song et al. (§72022§r)', author: 'Kaitao Song; Teng Wan; Bixia Wang; Huiqiang Jiang; Luna Qiu; Jiahang Xu; Liping Jiang; Qun Lou; Yuqing Yang; Dongsheng Li; Xudong Wang; Lili Qiu', display:{Lore:['[{"text": "arXiv:2208.05122", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Hypernasality Estimation with Automatic Speech Recognition in Cleft Palate Speech\\u00a7r\\n\\n\\u00a78\\u00a7oKaitao Song\\nTeng Wan\\nBixia Wang\\n+ 8 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.05122\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Aug 2022 03:15:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by InterSpeech 2022\\u00a7r"}']}
{title:'Gul et al. (§72022§r)', author: 'Sania Gul; Muhammad Salman Khan; Syed Waqar Shah', display:{Lore:['[{"text": "arXiv:2208.05184", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPreserving the beamforming effect for spatial cue-based pseudo-binaural dereverberation of a single source\\u00a7r\\n\\n\\u00a78\\u00a7oSania Gul\\nMuhammad Salman Khan\\nSyed Waqar Shah\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.05184\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Aug 2022 07:07:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o25 pages, 7 figures\\u00a7r"}']}
{title:'Cho et al. (§72022§r)', author: 'Jaejin Cho; Raghavendra Pappagari; Piotr Żelasko; Laureano Moro-Velazquez; Jesús Villalba; Najim Dehak', display:{Lore:['[{"text": "arXiv:2208.05413", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-Contrastive Self-Supervised Learning of Utterance-Level Speech Representations\\u00a7r\\n\\n\\u00a78\\u00a7oJaejin Cho\\nRaghavendra Pappagari\\nPiotr \\u017belasko\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.05413\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Aug 2022 16:04:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2022\\u00a7r"}']}
{title:'Cho et al. (§72022§r)', author: "Jaejin Cho; Jes'us Villalba; Laureano Moro-Velazquez; Najim Dehak", display:{Lore:['[{"text": "arXiv:2208.05445", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-Contrastive Self-supervised Learning for Utterance-Level Information Extraction from Speech\\u00a7r\\n\\n\\u00a78\\u00a7oJaejin Cho\\nJes\'us Villalba\\nLaureano Moro-Velazquez\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.05445\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JSTSP.2022.3197315\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Aug 2022 16:56:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oEARLY ACCESS of IEEE JSTSP Special Issue on Self-Supervised Learning for Speech and Audio Processing\\u00a7r"}']}
{title:'Papapanagiotou et al. (§72022§r)', author: 'Vasileios Papapanagiotou; Anastasia Liapi; Anastasios Delopoulos', display:{Lore:['[{"text": "arXiv:2208.05735", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChewing Detection from Commercial Smart-glasses\\u00a7r\\n\\n\\u00a78\\u00a7oVasileios Papapanagiotou\\nAnastasia Liapi\\nAnastasios Delopoulos\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.05735\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3552484.3555746\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 7th International Workshop on Multimedia\\n  Proceedings of the 7th International Workshop on Multimedia Assisted Dietary\\n  Management (MADiMa \'22), October 10, 2022, Lisboa, Portugal\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Aug 2022 10:01:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 4 figures, 1 table, conference\\u00a7r"}']}
{title:'Karakasidis et al. (§72022§r)', author: 'Georgios Karakasidis; Tamás Grósz; Mikko Kurimo', display:{Lore:['[{"text": "arXiv:2208.05782", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparison and Analysis of New Curriculum Criteria for End-to-End ASR\\u00a7r\\n\\n\\u00a78\\u00a7oGeorgios Karakasidis\\nTam\\u00e1s Gr\\u00f3sz\\nMikko Kurimo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.05782\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Aug 2022 06:56:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, in Proceedings Interspeech 2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Chunlei Zhang; Dong Yu', display:{Lore:['[{"text": "arXiv:2208.07446", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lC3-DINO: Joint Contrastive and Non-contrastive Self-Supervised Learning for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oChunlei Zhang\\nDong Yu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.07446\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JSTSP.2022.3198315\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Aug 2022 21:52:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEJournal of Selected Topics in Signal Processing\\u00a7r"}']}
{title:'Mun et al. (§72022§r)', author: 'Sung Hwan Mun; Min Hyun Han; Minchan Kim; Dongjune Lee; Nam Soo Kim', display:{Lore:['[{"text": "arXiv:2208.08012", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDisentangled Speaker Representation Learning via Mutual Information Minimization\\u00a7r\\n\\n\\u00a78\\u00a7oSung Hwan Mun\\nMin Hyun Han\\nMinchan Kim\\nDongjune Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.08012\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 12 Oct 2022 09:51:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by APSIPA ASC 2022. Camera-ready. 8 pages, 4 figures, and 1 table\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'SiCheng Yang; Methawee Tantrawenith; Haolin Zhuang; Zhiyong Wu; Aolan Sun; Jianzong Wang; Ning Cheng; Huaizhen Tang; Xintao Zhao; Jie Wang; Helen Meng', display:{Lore:['[{"text": "arXiv:2208.08757", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Representation Disentanglement with Adversarial Mutual Information Learning for One-shot Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oSiCheng Yang\\nMethawee Tantrawenith\\nHaolin Zhuang\\n+ 7 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.08757\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Aug 2022 10:36:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages,5 figures,INTERSPEECH 2022\\u00a7r"}']}
{title:'Abeysinghe et al. (§72022§r)', author: 'Binu Abeysinghe; Jesin James; Catherine I. Watson; Felix Marattukalam', display:{Lore:['[{"text": "arXiv:2208.09775", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisualising Model Training via Vowel Space for Text-To-Speech Systems\\u00a7r\\n\\n\\u00a78\\u00a7oBinu Abeysinghe\\nJesin James\\nCatherine I. Watson\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.09775\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 21 Aug 2022 01:33:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Gupta et al. (§72022§r)', author: 'Chitralekha Gupta; Yize Wei; Zequn Gong; Purnima Kamath; Zhuoyao Li; Lonce Wyse', display:{Lore:['[{"text": "arXiv:2208.10743", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParameter Sensitivity of Deep-Feature based Evaluation Metrics for Audio Textures\\u00a7r\\n\\n\\u00a78\\u00a7oChitralekha Gupta\\nYize Wei\\nZequn Gong\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.10743\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Aug 2022 05:40:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted for publication at ISMIR 2022\\u00a7r"}']}
{title:'Hahmann et al. (§72022§r)', author: 'Manuel Hahmann; Efren Fernandez-Grande', display:{Lore:['[{"text": "arXiv:2208.11324", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA convolutional plane wave model for sound field reconstruction\\u00a7r\\n\\n\\u00a78\\u00a7oManuel Hahmann\\nEfren Fernandez-Grande\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.11324\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0015227\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 7 Nov 2022 09:31:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oImprove description of figures and figure labels, add code reference\\u00a7r"}']}
{title:'Martínez-Ramírez et al. (§72022§r)', author: 'Marco A. Martínez-Ramírez; Wei-Hsiang Liao; Giorgio Fabbro; Stefan Uhlich; Chihiro Nagashima; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2208.11428", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic music mixing with deep learning and out-of-domain data\\u00a7r\\n\\n\\u00a78\\u00a7oMarco A. Mart\\u00ednez-Ram\\u00edrez\\nWei-Hsiang Liao\\nGiorgio Fabbro\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.11428\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 29 Aug 2022 04:56:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o23rd International Society for Music Information Retrieval Conference (ISMIR), December, 2022. Source code, demo and audio examples: https://marco-martinez-sony.github.io/FxNorm-automix/ -added acknowledgements\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'Qingqing Huang; Aren Jansen; Joonseok Lee; Ravi Ganti; Judith Yue Li; Daniel P. W. Ellis', display:{Lore:['[{"text": "arXiv:2208.12415", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMuLan: A Joint Embedding of Music Audio and Natural Language\\u00a7r\\n\\n\\u00a78\\u00a7oQingqing Huang\\nAren Jansen\\nJoonseok Lee\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.12415\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Aug 2022 03:13:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ISMIR 2022\\u00a7r"}']}
{title:'Elsayed et al. (§72022§r)', author: 'Nelly Elsayed; Zag ElSayed; Navid Asadizanjani; Murat Ozer; Ahmed Abdelgawad; Magdy Bayoumi', display:{Lore:['[{"text": "arXiv:2208.12812", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Emotion Recognition using Supervised Deep Recurrent System for Mental Health Monitoring\\u00a7r\\n\\n\\u00a78\\u00a7oNelly Elsayed\\nZag ElSayed\\nNavid Asadizanjani\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.12812\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 27 Oct 2022 02:27:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 5 figures, 3 tables, accepted in the IEEE WFIoT2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Dongmei Wang; Xiong Xiao; Naoyuki Kanda; Takuya Yoshioka; Jian Wu', display:{Lore:['[{"text": "arXiv:2208.13085", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTarget Speaker Voice Activity Detection with Transformers and Its Integration with End-to-End Neural Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oDongmei Wang\\nXiong Xiao\\nNaoyuki Kanda\\nTakuya Yoshioka\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.13085\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 26 Sep 2022 01:30:26 GMT)\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Bo Li; Tara N. Sainath; Ruoming Pang; Shuo-yiin Chang; Qiumin Xu; Trevor Strohman; Vince Chen; Qiao Liang; Heguang Liu; Yanzhang He; Parisa Haghani; Sameer Bidichandani', display:{Lore:['[{"text": "arXiv:2208.13916", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Language Agnostic Multilingual Streaming On-Device ASR System\\u00a7r\\n\\n\\u00a78\\u00a7oBo Li\\nTara N. Sainath\\nRuoming Pang\\n+ 8 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.13916\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 Aug 2022 22:34:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in Interspeech 2022\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Zizhao Chen; Hongliang Wang; Chia-Hui Yeh; Xilin Liu', display:{Lore:['[{"text": "arXiv:2208.13943", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lClassify Respiratory Abnormality in Lung Sounds Using STFT and a Fine-Tuned ResNet18 Network\\u00a7r\\n\\n\\u00a78\\u00a7oZizhao Chen\\nHongliang Wang\\nChia-Hui Yeh\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.13943\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Aug 2022 01:09:34 GMT)\\u00a7r"}']}
{title:'Heydari et al. (§72022§r)', author: 'Mojtaba Heydari; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2208.14578", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSinging Beat Tracking With Self-supervised Front-end and Linear Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oMojtaba Heydari\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.14578\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 Aug 2022 00:29:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o23rd International Society for Music Information Retrieval Conference (ISMIR 2022)\\u00a7r"}']}
{title:'Zeng et al. (§72022§r)', author: 'Chang Zeng; Lin Zhang; Meng Liu; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2209.00423", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpoofing-Aware Attention based ASV Back-end with Multiple Enrollment Utterances and a Sampling Strategy for the SASV Challenge 2022\\u00a7r\\n\\n\\u00a78\\u00a7oChang Zeng\\nLin Zhang\\nMeng Liu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.00423\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10495\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Sep 2022 13:01:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by InterSpeech2022\\u00a7r"}']}
{title:'Zeng et al. (§72022§r)', author: 'Chang Zeng; Xiaoxiao Miao; Xin Wang; Erica Cooper; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2209.00485", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Speaker Encoder and Neural Back-end Model for Fully End-to-End Automatic Speaker Verification with Multiple Enrollment Utterances\\u00a7r\\n\\n\\u00a78\\u00a7oChang Zeng\\nXiaoxiao Miao\\nXin Wang\\nErica Cooper\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.00485\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Sep 2022 14:15:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to TASLP\\u00a7r"}']}
{title:'Ge et al. (§72022§r)', author: 'Wanying Ge; Hemlata Tak; Massimiliano Todisco; Nicholas Evans', display:{Lore:['[{"text": "arXiv:2209.00506", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the potential of jointly-optimised solutions to spoofing attack detection and automatic speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oWanying Ge\\nHemlata Tak\\nMassimiliano Todisco\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.00506\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 7 Oct 2022 09:53:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IberSPEECH 2022 Conference\\u00a7r"}']}
{title:'Duke et al. (§72022§r)', author: 'Ryan Duke; Alex Doboli', display:{Lore:['[{"text": "arXiv:2209.00619", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7ldiaLogic: Non-Invasive Speaker-Focused Data Acquisition for Team Behavior Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oRyan Duke\\nAlex Doboli\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.00619\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Sep 2022 17:33:11 GMT)\\u00a7r"}']}
{title:'Goki et al. (§72022§r)', author: 'Seyedamiryousef Hosseini Goki; Mahdieh Ghazvini; Sajad Hamzenejadi', display:{Lore:['[{"text": "arXiv:2209.00733", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Wavelet Transform Based Scheme to Extract Speech Pitch and Formant Frequencies\\u00a7r\\n\\n\\u00a78\\u00a7oSeyedamiryousef Hosseini Goki\\nMahdieh Ghazvini\\nSajad Hamzenejadi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.00733\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/CFIS.2019.8692150\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2019 7th Iranian Joint Congress on Fuzzy and Intelligent Systems\\n  (CFIS)\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 7 Sep 2022 04:52:01 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Lianwu Chen; Xiguang Zheng; Chen Zhang; Liang Guo; Bing Yu', display:{Lore:['[{"text": "arXiv:2209.00805", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-scale temporal-frequency attention for music source separation\\u00a7r\\n\\n\\u00a78\\u00a7oLianwu Chen\\nXiguang Zheng\\nChen Zhang\\nLiang Guo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.00805\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Sep 2022 03:53:52 GMT)\\u00a7r"}']}
{title:'Frost et al. (§72022§r)', author: 'Geoffrey Frost; Grant Theron; Thomas Niesler', display:{Lore:['[{"text": "arXiv:2209.00934", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTB or not TB? Acoustic cough analysis for tuberculosis classification\\u00a7r\\n\\n\\u00a78\\u00a7oGeoffrey Frost\\nGrant Theron\\nThomas Niesler\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.00934\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Sep 2022 10:17:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at Interspeech 2022\\u00a7r"}']}
{title:'Nakashima et al. (§72022§r)', author: 'Taishi Nakashima; Nobutaka Ono', display:{Lore:['[{"text": "arXiv:2209.00937", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInverse-free Online Independent Vector Analysis with Flexible Iterative Source Steering\\u00a7r\\n\\n\\u00a78\\u00a7oTaishi Nakashima\\nNobutaka Ono\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.00937\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Sep 2022 10:23:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures. Submitted to APSIPA 2022\\u00a7r"}']}
{title:'Kataria et al. (§72022§r)', author: 'Saurabh Kataria; Jesús Villalba; Laureano Moro-Velázquez; Piotr Żelasko; Najim Dehak', display:{Lore:['[{"text": "arXiv:2209.01702", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTime-domain speech super-resolution with GAN based modeling for telephony speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oSaurabh Kataria\\nJes\\u00fas Villalba\\nLaureano Moro-Vel\\u00e1zquez\\nPiotr \\u017belasko\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.01702\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 4 Sep 2022 22:34:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmit to IEEE/ACM Transactions on Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Lee et al. (§72022§r)', author: 'Sunghwa Lee; Younghoon Shin', display:{Lore:['[{"text": "arXiv:2209.01762", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMovement Detection of Tongue and Related Body Parts Using IR-UWB Radar\\u00a7r\\n\\n\\u00a78\\u00a7oSunghwa Lee\\nYounghoon Shin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.01762\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICTC55196.2022.9952644\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Sep 2022 04:51:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to the 13th International Conference on ICT Convergence (ICTC)\\u00a7r"}']}
{title:'Hu et al. (§72022§r)', author: 'Jinbo Hu; Yin Cao; Ming Wu; Qiuqiang Kong; Feiran Yang; Mark D. Plumbley; Jun Yang', display:{Lore:['[{"text": "arXiv:2209.01802", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Event Localization and Detection for Real Spatial Sound Scenes: Event-Independent Network and Data Augmentation Chains\\u00a7r\\n\\n\\u00a78\\u00a7oJinbo Hu\\nYin Cao\\nMing Wu\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.01802\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 9 Sep 2022 10:06:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to DCASE 2022 Workshop. Code is available at https://github.com/Jinbo-Hu/DCASE2022-TASK3\\u00a7r"}']}
{title:'Kuhlmann et al. (§72022§r)', author: 'Michael Kuhlmann; Fritz Seebauer; Janek Ebbers; Petra Wagner; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2209.01978", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigation into Target Speaking Rate Adaptation for Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oMichael Kuhlmann\\nFritz Seebauer\\nJanek Ebbers\\nPetra Wagner\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.01978\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Sep 2022 14:20:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Moriya et al. (§72022§r)', author: 'Takafumi Moriya; Hiroshi Sato; Tsubasa Ochiai; Marc Delcroix; Takahiro Shinozaki', display:{Lore:['[{"text": "arXiv:2209.04175", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreaming Target-Speaker ASR with Neural Transducer\\u00a7r\\n\\n\\u00a78\\u00a7oTakafumi Moriya\\nHiroshi Sato\\nTsubasa Ochiai\\nMarc Delcroix\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.04175\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 19 Sep 2022 14:54:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022\\u00a7r"}']}
{title:'Kanda et al. (§72022§r)', author: 'Naoyuki Kanda; Jian Wu; Xiaofei Wang; Zhuo Chen; Jinyu Li; Takuya Yoshioka', display:{Lore:['[{"text": "arXiv:2209.04974", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVarArray Meets t-SOT: Advancing the State of the Art of Streaming Distant Conversational Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oNaoyuki Kanda\\nJian Wu\\nXiaofei Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.04974\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 4 Oct 2022 03:29:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 2 figure, 3 tables, v2: Appendix A has been added\\u00a7r"}']}
{title:'Szwajcowski (§72022§r)', author: 'Adam Szwajcowski', display:{Lore:['[{"text": "arXiv:2209.05110", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContinuous head-related transfer function representation based on hyperspherical harmonics\\u00a7r\\n\\n\\u00a78\\u00a7oAdam Szwajcowski\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.05110\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 12 Sep 2022 09:40:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Archives of Acoustics 4.06.2022\\u00a7r"}']}
{title:'Ekstedt et al. (§72022§r)', author: 'Erik Ekstedt; Gabriel Skantze', display:{Lore:['[{"text": "arXiv:2209.05161", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHow Much Does Prosody Help Turn-taking? Investigations using Voice Activity Projection Models\\u00a7r\\n\\n\\u00a78\\u00a7oErik Ekstedt\\nGabriel Skantze\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.05161\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 12 Sep 2022 11:40:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSIGDIAL 2022 Best Paper Award Winner\\u00a7r"}']}
{title:'Qin et al. (§72022§r)', author: 'Xiaoyi Qin; Ming Li; Hui Bu; Shrikanth Narayanan; Haizhou Li', display:{Lore:['[{"text": "arXiv:2209.05273", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe 2022 Far-field Speaker Verification Challenge: Exploring domain mismatch and semi-supervised learning under the far-field scenario\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoyi Qin\\nMing Li\\nHui Bu\\nShrikanth Narayanan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.05273\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 16 Sep 2022 03:08:23 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Zhe Liu; Fuchun Peng', display:{Lore:['[{"text": "arXiv:2209.05281", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModeling Dependent Structure for Utterances in ASR Evaluation\\u00a7r\\n\\n\\u00a78\\u00a7oZhe Liu\\nFuchun Peng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.05281\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 8 Oct 2022 21:44:34 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Chao Zhang; Bo Li; Tara Sainath; Trevor Strohman; Sepand Mavandadi; Shuo-yiin Chang; Parisa Haghani', display:{Lore:['[{"text": "arXiv:2209.06058", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreaming End-to-End Multilingual Speech Recognition with Joint Language Identification\\u00a7r\\n\\n\\u00a78\\u00a7oChao Zhang\\nBo Li\\nTara Sainath\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.06058\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Sep 2022 15:10:41 GMT)\\u00a7r"}']}
{title:'Korzekwa (§72022§r)', author: 'Daniel Korzekwa', display:{Lore:['[{"text": "arXiv:2209.06265", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.OT\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomated detection of pronunciation errors in non-native English speech employing deep learning\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Korzekwa\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.06265\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Sep 2022 19:09:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPhD Thesis, in English + extended summary in Polish\\u00a7r"}']}
{title:'Wu et al. (§72022§r)', author: 'Peter Wu; Shinji Watanabe; Louis Goldstein; Alan W Black; Gopala K. Anumanchipalli', display:{Lore:['[{"text": "arXiv:2209.06337", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Speech Synthesis from Articulatory Representations\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Wu\\nShinji Watanabe\\nLouis Goldstein\\nAlan W Black\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.06337\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Sep 2022 22:51:47 GMT)\\u00a7r"}']}
{title:"O'Malley et al. (§72022§r)", author: "Tom O'Malley; Arun Narayanan; Quan Wang", display:{Lore:['[{"text": "arXiv:2209.06410", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Universally-Deployable ASR Frontend for Joint Acoustic Echo Cancellation, Speech Enhancement, and Voice Separation\\u00a7r\\n\\n\\u00a78\\u00a7oTom O\'Malley\\nArun Narayanan\\nQuan Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.06410\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Sep 2022 04:34:07 GMT)\\u00a7r"}']}
{title:'Shahgir et al. (§72022§r)', author: 'H. A. Z. Sameen Shahgir; Khondker Salman Sayeed; Tanjeem Azwad Zaman', display:{Lore:['[{"text": "arXiv:2209.06581", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lApplying wav2vec2 for Speech Recognition on Bengali Common Voices Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oH. A. Z. Sameen Shahgir\\nKhondker Salman Sayeed\\nTanjeem Azwad Zaman\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.06581\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 11 Sep 2022 15:05:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Peng et al. (§72022§r)', author: 'Yukun Peng; Zhenhua Ling', display:{Lore:['[{"text": "arXiv:2209.06789", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDecoupled Pronunciation and Prosody Modeling in Meta-Learning-Based Multilingual Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYukun Peng\\nZhenhua Ling\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.06789\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Sep 2022 17:17:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2022\\u00a7r"}']}
{title:'Wang (§72022§r)', author: 'Jun Wang', display:{Lore:['[{"text": "arXiv:2209.06913", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lESSumm: Extractive Speech Summarization from Untranscribed Meeting\\u00a7r\\n\\n\\u00a78\\u00a7oJun Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.06913\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Sep 2022 20:13:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2022\\u00a7r"}']}
{title:'Showrav (§72022§r)', author: 'Tushar Talukder Showrav', display:{Lore:['[{"text": "arXiv:2209.08119", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Automatic Speech Recognition System for Bengali Language based on Wav2Vec2 and Transfer Learning\\u00a7r\\n\\n\\u00a78\\u00a7oTushar Talukder Showrav\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.08119\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 20 Sep 2022 02:22:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oBUET DL Sprint, 4 pages\\u00a7r"}']}
{title:'Bai et al. (§72022§r)', author: 'Ye Bai; Jie Li; Wenjing Han; Hao Ni; Kaituo Xu; Zhuo Zhang; Cheng Yi; Xiaorui Wang', display:{Lore:['[{"text": "arXiv:2209.08326", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParameter-Efficient Conformers via Sharing Sparsely-Gated Experts for End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYe Bai\\nJie Li\\nWenjing Han\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.08326\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 17 Sep 2022 13:22:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted in INTERSPEECH 2022\\u00a7r"}']}
{title:'Miller et al. (§72022§r)', author: 'Gabriel Figueiredo Miller; Juan Camilo Vásquez-Correa; Juan Rafael Orozco-Arroyave; Elmar Nöth', display:{Lore:['[{"text": "arXiv:2209.08379", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRepresentation Learning Strategies to Model Pathological Speech: Effect of Multiple Spectral Resolutions\\u00a7r\\n\\n\\u00a78\\u00a7oGabriel Figueiredo Miller\\nJuan Camilo V\\u00e1squez-Correa\\nJuan Rafael Orozco-Arroyave\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.08379\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 17 Sep 2022 17:46:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 3 figures\\u00a7r"}']}
{title:'Pouyani et al. (§72022§r)', author: 'Mozhde Firoozi Pouyani; Mansour Vali; Mohammad Amin Ghasemi', display:{Lore:['[{"text": "arXiv:2209.09512", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Combined Model for Noise Reduction of Lung Sound Signals Based on Empirical Mode Decomposition and Artificial Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oMozhde Firoozi Pouyani\\nMansour Vali\\nMohammad Amin Ghasemi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.09512\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Sep 2022 07:04:23 GMT)\\u00a7r"}']}
{title:'Someki et al. (§72022§r)', author: 'Masao Someki; Yosuke Higuchi; Tomoki Hayashi; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2209.09756", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lESPnet-ONNX: Bridging a Gap Between Research and Production\\u00a7r\\n\\n\\u00a78\\u00a7oMasao Someki\\nYosuke Higuchi\\nTomoki Hayashi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.09756\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 14 Nov 2022 16:37:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to APSIPA ASC 2022\\u00a7r"}']}
{title:'Xie et al. (§72022§r)', author: 'Huang Xie; Samuel Lipping; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2209.09967", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLanguage-based Audio Retrieval Task in DCASE 2022 Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oHuang Xie\\nSamuel Lipping\\nTuomas Virtanen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.09967\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 4 Oct 2022 13:50:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUpdate for arXiv:2206.06108 mistakenly submitted as a new article\\u00a7r"}']}
{title:'Si et al. (§72022§r)', author: 'Shijing Si; Jianzong Wang; Xulong Zhang; Xiaoyang Qu; Ning Cheng; Jing Xiao', display:{Lore:['[{"text": "arXiv:2209.10088", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBoosting Star-GANs for Voice Conversion with Contrastive Discriminator\\u00a7r\\n\\n\\u00a78\\u00a7oShijing Si\\nJianzong Wang\\nXulong Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.10088\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 27 Sep 2022 15:45:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 3 figures, Accepted by ICONIP 2022\\u00a7r"}']}
{title:'Suh et al. (§72022§r)', author: 'Sangwon Suh; Sunjong Park', display:{Lore:['[{"text": "arXiv:2209.10147", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe ReturnZero System for VoxCeleb Speaker Recognition Challenge 2022\\u00a7r\\n\\n\\u00a78\\u00a7oSangwon Suh\\nSunjong Park\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.10147\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Sep 2022 06:54:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 4 tables, technical report\\u00a7r"}']}
{title:'Park et al. (§72022§r)', author: 'Dongkeon Park; Yechan Yu; Kyeong Wan Park; Ji Won Kim; Hong Kook Kim', display:{Lore:['[{"text": "arXiv:2209.10357", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGIST-AiTeR System for the Diarization Task of the 2022 VoxCeleb Speaker Recognition Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oDongkeon Park\\nYechan Yu\\nKyeong Wan Park\\nJi Won Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.10357\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 6 Oct 2022 05:21:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2022 VoxSRC Track4\\u00a7r"}']}
{title:'Cho et al. (§72022§r)', author: 'Yin-Ping Cho; Yu Tsao; Hsin-Min Wang; Yi-Wen Liu', display:{Lore:['[{"text": "arXiv:2209.10446", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMandarin Singing Voice Synthesis with Denoising Diffusion Probabilistic Wasserstein GAN\\u00a7r\\n\\n\\u00a78\\u00a7oYin-Ping Cho\\nYu Tsao\\nHsin-Min Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.10446\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Sep 2022 15:47:39 GMT)\\u00a7r"}']}
{title:'Bhatia et al. (§72022§r)', author: 'Rhythm Bhatia; Tomi H. Kinnunen', display:{Lore:['[{"text": "arXiv:2209.10479", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Initial study on Birdsong Re-synthesis Using Neural Vocoders\\u00a7r\\n\\n\\u00a78\\u00a7oRhythm Bhatia\\nTomi H. Kinnunen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.10479\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Sep 2022 16:30:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in 24th International Conference on Speech and Computer (SPECOM),GURUGRAM, INDIA\\u00a7r"}']}
{title:'Tobin et al. (§72022§r)', author: 'Jimmy Tobin; Qisheng Li; Subhashini Venugopalan; Katie Seaver; Richard Cave; Katrin Tomanek', display:{Lore:['[{"text": "arXiv:2209.10591", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAssessing ASR Model Quality on Disordered Speech using BERTScore\\u00a7r\\n\\n\\u00a78\\u00a7oJimmy Tobin\\nQisheng Li\\nSubhashini Venugopalan\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.10591\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Sep 2022 18:33:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2022 Workshop on Speech for Social Good\\u00a7r"}']}
{title:'Lam et al. (§72022§r)', author: 'Perry Lam; Huayun Zhang; Nancy F. Chen; Berrak Sisman', display:{Lore:['[{"text": "arXiv:2209.10890", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEPIC TTS Models: Empirical Pruning Investigations Characterizing Text-To-Speech Models\\u00a7r\\n\\n\\u00a78\\u00a7oPerry Lam\\nHuayun Zhang\\nNancy F. Chen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.10890\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10626\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInterspeech 2022, 823-827 (2022)\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Sep 2022 09:47:25 GMT)\\u00a7r"}']}
{title:'Alisamir et al. (§72022§r)', author: 'Sina Alisamir; Fabien Ringeval; Francois Portet', display:{Lore:['[{"text": "arXiv:2209.11061", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-domain Voice Activity Detection with Self-Supervised Representations\\u00a7r\\n\\n\\u00a78\\u00a7oSina Alisamir\\nFabien Ringeval\\nFrancois Portet\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.11061\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Sep 2022 14:53:44 GMT)\\u00a7r"}']}
{title:'Qiao et al. (§72022§r)', author: 'Yue Qiao; Léo Guadagnin; Edgar Choueiri', display:{Lore:['[{"text": "arXiv:2209.11296", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIsolation performance metrics for personal sound zone reproduction systems\\u00a7r\\n\\n\\u00a78\\u00a7oYue Qiao\\nL\\u00e9o Guadagnin\\nEdgar Choueiri\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.11296\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0014604\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Sep 2022 20:16:49 GMT)\\u00a7r"}']}
{title:'Cai et al. (§72022§r)', author: 'Qutang Cai; Guoqiang Hong; Zhijian Ye; Ximin Li; Haizhou Li', display:{Lore:['[{"text": "arXiv:2209.11433", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Kriston AI System for the VoxCeleb Speaker Recognition Challenge 2022\\u00a7r\\n\\n\\u00a78\\u00a7oQutang Cai\\nGuoqiang Hong\\nZhijian Ye\\nXimin Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.11433\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Sep 2022 06:18:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSystem description of VoxSRC 2022: track 1, 2 and 4\\u00a7r"}']}
{title:'Cord-Landwehr et al. (§72022§r)', author: 'Tobias Cord-Landwehr; Thilo von Neumann; Christoph Boeddeker; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2209.11494", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMMS-MSG: A Multi-purpose Multi-Speaker Mixture Signal Generator\\u00a7r\\n\\n\\u00a78\\u00a7oTobias Cord-Landwehr\\nThilo von Neumann\\nChristoph Boeddeker\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.11494\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Sep 2022 09:40:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at IWAENC 2022\\u00a7r"}']}
{title:'Biswas et al. (§72022§r)', author: 'Arijit Biswas; Guanxin Jiang', display:{Lore:['[{"text": "arXiv:2209.11666", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStereo InSE-NET: Stereo Audio Quality Predictor Transfer Learned from Mono InSE-NET\\u00a7r\\n\\n\\u00a78\\u00a7oArijit Biswas\\nGuanxin Jiang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.11666\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Sep 2022 15:52:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to 153rd Audio Engineering Society (AES), New York, NY, USA,October 2022\\u00a7r"}']}
{title:'Yao et al. (§72022§r)', author: 'Jixun Yao; Qing Wang; Li Zhang; Pengcheng Guo; Yuhao Liang; Lei Xie', display:{Lore:['[{"text": "arXiv:2209.11969", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNWPU-ASLP System for the VoicePrivacy 2022 Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oJixun Yao\\nQing Wang\\nLi Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.11969\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 24 Sep 2022 09:36:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oVoicePrivacy 2022 Challenge\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Jie Wang; Yuji Liu; Binling Wang; Yiming Zhi; Song Li; Shipeng Xia; Jiayang Zhang; Feng Tong; Lin Li; Qingyang Hong', display:{Lore:['[{"text": "arXiv:2209.12002", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatial-aware Speaker Diarization for Multi-channel Multi-party Meeting\\u00a7r\\n\\n\\u00a78\\u00a7oJie Wang\\nYuji Liu\\nBinling Wang\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.12002\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-11412\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 24 Sep 2022 13:04:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2022. arXiv admin note: text overlap with arXiv:2202.05744\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Xiangyu Zhang; Shuyue Stella Li; Zhanhong He; Roberto Togneri; Leibny Paola Garcia', display:{Lore:['[{"text": "arXiv:2209.12702", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Lyrics Recognition with Self-supervised Learning\\u00a7r\\n\\n\\u00a78\\u00a7oXiangyu Zhang\\nShuyue Stella Li\\nZhanhong He\\nRoberto Togneri\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.12702\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 27 Oct 2022 03:04:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 2 figures, 3 tables\\u00a7r"}']}
{title:'Boes et al. (§72022§r)', author: 'Wim Boes; Hugo Van hamme', display:{Lore:['[{"text": "arXiv:2209.12826", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-encoder attention-based architectures for sound recognition with partial visual assistance\\u00a7r\\n\\n\\u00a78\\u00a7oWim Boes\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.12826\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1186/s13636-022-00252-9\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nEURASIP Journal on Audio, Speech, and Music Processing; 2022; 25\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Sep 2022 16:32:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to EURASIP Journal on Audio, Speech, and Music Processing\\u00a7r"}']}
{title:'Boes et al. (§72022§r)', author: 'Wim Boes; Hugo Van hamme', display:{Lore:['[{"text": "arXiv:2209.12843", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImpact of temporal resolution on convolutional recurrent networks for audio tagging and sound event detection\\u00a7r\\n\\n\\u00a78\\u00a7oWim Boes\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.12843\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 27 Sep 2022 02:20:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to DCASE 2022 Workshop\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Fuling Chen; Roberto Togneri; Murray Maybery; Diana Weiting Tan', display:{Lore:['[{"text": "arXiv:2209.13112", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomated Sex Classification of Children\'s Voices and Changes in Differentiating Factors with Age\\u00a7r\\n\\n\\u00a78\\u00a7oFuling Chen\\nRoberto Togneri\\nMurray Maybery\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.13112\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Sep 2022 02:01:01 GMT)\\u00a7r"}']}
{title:'Atmaja et al. (§72022§r)', author: 'Bagus Tris Atmaja; Akira Sasou', display:{Lore:['[{"text": "arXiv:2209.13146", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPredicting Affective Vocal Bursts with Finetuned wav2vec 2.0\\u00a7r\\n\\n\\u00a78\\u00a7oBagus Tris Atmaja\\nAkira Sasou\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.13146\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 26 Oct 2022 01:24:22 GMT)\\u00a7r"}']}
{title:'Nakashima et al. (§72022§r)', author: 'Futa Nakashima; Tomohiko Nakamura; Norihiro Takamune; Satoru Fukayama; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2209.13211", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHyperbolic Timbre Embedding for Musical Instrument Sound Synthesis Based on Variational Autoencoders\\u00a7r\\n\\n\\u00a78\\u00a7oFuta Nakashima\\nTomohiko Nakamura\\nNorihiro Takamune\\nSatoru Fukayama\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.13211\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2022 Asia Pacific Signal and Information Processing Association\\n  Annual Summit and Conference\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Sep 2022 07:28:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 4 figures, to be published in Asia PacificSignal and Information Processing AssociationAnnual Summit and Conference 2022\\u00a7r"}']}
{title:'Zhao et al. (§72022§r)', author: 'Xiao-Ying Zhao; Qiu-Shi Zhu; Jie Zhang', display:{Lore:['[{"text": "arXiv:2209.14150", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Enhancement Using Self-Supervised Pre-Trained Model and Vector Quantization\\u00a7r\\n\\n\\u00a78\\u00a7oXiao-Ying Zhao\\nQiu-Shi Zhu\\nJie Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.14150\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Sep 2022 15:00:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to APSIPA ASC 2022\\u00a7r"}']}
{title:'Deshmukh et al. (§72022§r)', author: 'Soham Deshmukh; Benjamin Elizalde; Huaming Wang', display:{Lore:['[{"text": "arXiv:2209.14275", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Retrieval with WavText5K and CLAP Training\\u00a7r\\n\\n\\u00a78\\u00a7oSoham Deshmukh\\nBenjamin Elizalde\\nHuaming Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.14275\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Sep 2022 17:39:26 GMT)\\u00a7r"}']}
{title:'Adetoyi (§72022§r)', author: 'Oluyemi E. Adetoyi', display:{Lore:['[{"text": "arXiv:2209.14335", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lText Independent Speaker Identification System for Access Control\\u00a7r\\n\\n\\u00a78\\u00a7oOluyemi E. Adetoyi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.14335\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Sep 2022 14:42:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages\\u00a7r"}']}
{title:'Kunešová et al. (§72022§r)', author: 'Marie Kunešová; Markéta Řezáčková', display:{Lore:['[{"text": "arXiv:2209.15032", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetection of Prosodic Boundaries in Speech Using Wav2Vec 2.0\\u00a7r\\n\\n\\u00a78\\u00a7oMarie Kune\\u0161ov\\u00e1\\nMark\\u00e9ta \\u0158ez\\u00e1\\u010dkov\\u00e1\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.15032\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-031-16270-1_31\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInternational Conference on Text, Speech, and Dialogue (TSD 2022),\\n  LNAI volume 13502\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 29 Sep 2022 18:12:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis preprint is a pre-review version of the paper and does not contain any post-submission improvements or corrections. The Version of Record of this contribution is published in the proceedings of the International "}','{"text": "Conference on Text, Speech, and Dialogue (TSD 2022), LNAI volume13502, and is available online at https://doi.org/10.1007/978-3-031-16270-1_31\\u00a7r"}']}
{title:'Luo et al. (§72022§r)', author: 'Yi Luo; Jianwei Yu', display:{Lore:['[{"text": "arXiv:2209.15174", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Source Separation with Band-split RNN\\u00a7r\\n\\n\\u00a78\\u00a7oYi Luo\\nJianwei Yu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.15174\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Sep 2022 01:49:52 GMT)\\u00a7r"}']}
{title:'Tokala et al. (§72022§r)', author: 'Vikas Tokala; Mike Brookes; Patrick A. Naylor', display:{Lore:['[{"text": "arXiv:2209.15472", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBinaural Speech Enhancement Using STOI-Optimal Masks\\u00a7r\\n\\n\\u00a78\\u00a7oVikas Tokala\\nMike Brookes\\nPatrick A. Naylor\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.15472\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Sep 2022 13:56:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at IWAENC 2022\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Kwangyoun Kim; Felix Wu; Yifan Peng; Jing Pan; Prashant Sridhar; Kyu J. Han; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2210.00077", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lE-Branchformer: Branchformer with Enhanced merging for speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oKwangyoun Kim\\nFelix Wu\\nYifan Peng\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.00077\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 14 Oct 2022 22:14:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to SLT 2022\\u00a7r"}']}
{title:'Sadhu et al. (§72022§r)', author: 'Samik Sadhu; Hynek Hermansky', display:{Lore:['[{"text": "arXiv:2210.00117", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBlind Signal Dereverberation for Machine Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSamik Sadhu\\nHynek Hermansky\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.00117\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Sep 2022 22:15:31 GMT)\\u00a7r"}']}
{title:'Tamm et al. (§72022§r)', author: 'Bastiaan Tamm; Helena Balabin; Rik Vandenberghe; Hugo Van hamme', display:{Lore:['[{"text": "arXiv:2210.00259", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPre-trained Speech Representations as Feature Extractors for Speech Quality Assessment in Online Conferencing Applications\\u00a7r\\n\\n\\u00a78\\u00a7oBastiaan Tamm\\nHelena Balabin\\nRik Vandenberghe\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.00259\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10147\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2022, 4083-4087\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 1 Oct 2022 11:51:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, submitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Nguyen et al. (§72022§r)', author: 'Dang-Khanh Nguyen; Sudarshan Pant; Ngoc-Huynh Ho; Guee-Sang Lee; Soo-Huyng Kim; Hyung-Jeong Yang', display:{Lore:['[{"text": "arXiv:2210.00263", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFine-tuning Wav2vec for Vocal-burst Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDang-Khanh Nguyen\\nSudarshan Pant\\nNgoc-Huynh Ho\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.00263\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 1 Oct 2022 12:03:27 GMT)\\u00a7r"}']}
{title:'Shim et al. (§72022§r)', author: 'Kyuhong Shim; Wonyong Sung', display:{Lore:['[{"text": "arXiv:2210.00367", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparison of Transformer, Convolutional, and Recurrent Neural Networks on Phoneme Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oKyuhong Shim\\nWonyong Sung\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.00367\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 1 Oct 2022 20:47:25 GMT)\\u00a7r"}']}
{title:'Heller et al. (§72022§r)', author: 'Aaron Heller; Eric Benjamin; Fernando Lopez-Lezcano', display:{Lore:['[{"text": "arXiv:2210.00378", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimized Decoders for Mixed-Order Ambisonics\\u00a7r\\n\\n\\u00a78\\u00a7oAaron Heller\\nEric Benjamin\\nFernando Lopez-Lezcano\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.00378\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPaper 10507, 150th Audio Engineering Society Convention, May 2021\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 1 Oct 2022 21:24:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 10 figures,\\u00a7r"}']}
{title:'Khan et al. (§72022§r)', author: 'Awais Khan; Khalid Mahmood Malik; James Ryan; Mikul Saravanan', display:{Lore:['[{"text": "arXiv:2210.00417", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CY\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Spoofing Countermeasures: Taxonomy, State-of-the-art, experimental analysis of generalizability, open challenges, and the way forward\\u00a7r\\n\\n\\u00a78\\u00a7oAwais Khan\\nKhalid Mahmood Malik\\nJames Ryan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.00417\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.48550/arXiv.2210.00417\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 21 Nov 2022 19:29:07 GMT)\\u00a7r"}']}
{title:'Anderson et al. (§72022§r)', author: 'Mark Anderson; Naomi Harte', display:{Lore:['[{"text": "arXiv:2210.00889", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearnable Acoustic Frontends in Bird Activity Detection\\u00a7r\\n\\n\\u00a78\\u00a7oMark Anderson\\nNaomi Harte\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.00889\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 3 Oct 2022 12:54:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted and presented at IWAENC, September 2022, 5 Pages, 1 Figure, 3 Tables\\u00a7r"}']}
{title:'Koizumi et al. (§72022§r)', author: 'Yuma Koizumi; Kohei Yatabe; Heiga Zen; Michiel Bacchiani', display:{Lore:['[{"text": "arXiv:2210.01029", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWaveFit: An Iterative and Non-autoregressive Neural Vocoder based on Fixed-Point Iteration\\u00a7r\\n\\n\\u00a78\\u00a7oYuma Koizumi\\nKohei Yatabe\\nHeiga Zen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.01029\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 3 Oct 2022 15:45:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEESLT 2022\\u00a7r"}']}
{title:'Peng et al. (§72022§r)', author: 'Junyi Peng; Oldrich Plchot; Themos Stafylakis; Ladislav Mosner; Lukas Burget; Jan Cernocky', display:{Lore:['[{"text": "arXiv:2210.01273", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn attention-based backend allowing efficient fine-tuning of transformer models for speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oJunyi Peng\\nOldrich Plchot\\nThemos Stafylakis\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.01273\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 3 Oct 2022 23:46:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by SLT2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Weiqing Wang; Xiaoyi Qin; Ming Cheng; Yucong Zhang; Kangyue Wang; Ming Li', display:{Lore:['[{"text": "arXiv:2210.01677", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe DKU-DukeECE Diarization System for the VoxCeleb Speaker Recognition Challenge 2022\\u00a7r\\n\\n\\u00a78\\u00a7oWeiqing Wang\\nXiaoyi Qin\\nMing Cheng\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.01677\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 4 Oct 2022 15:22:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2109.02002\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Yuanchao Li; Yumnah Mohamied; Peter Bell; Catherine Lai', display:{Lore:['[{"text": "arXiv:2210.02595", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploration of A Self-Supervised Speech Model: A Study on Emotional Corpora\\u00a7r\\n\\n\\u00a78\\u00a7oYuanchao Li\\nYumnah Mohamied\\nPeter Bell\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.02595\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 12 Dec 2022 16:52:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to SLT 2022\\u00a7r"}']}
{title:'Lee et al. (§72022§r)', author: 'Dongjune Lee; Minchan Kim; Sung Hwan Mun; Min Hyun Han; Nam Soo Kim', display:{Lore:['[{"text": "arXiv:2210.02732", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFully Unsupervised Training of Few-shot Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oDongjune Lee\\nMinchan Kim\\nSung Hwan Mun\\nMin Hyun Han\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.02732\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 7 Oct 2022 02:42:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE SLT 2022\\u00a7r"}']}
{title:'Horiguchi et al. (§72022§r)', author: 'Shota Horiguchi; Yuki Takashima; Shinji Watanabe; Paola Garcia', display:{Lore:['[{"text": "arXiv:2210.03459", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMutual Learning of Single- and Multi-Channel End-to-End Neural Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oShota Horiguchi\\nYuki Takashima\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.03459\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 7 Oct 2022 11:03:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEESLT 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Lei Wang; Benedict Yeoh; Jun Wah Ng', display:{Lore:['[{"text": "arXiv:2210.03581", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CR\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynthetic Voice Detection and Audio Splicing Detection using SE-Res2Net-Conformer Architecture\\u00a7r\\n\\n\\u00a78\\u00a7oLei Wang\\nBenedict Yeoh\\nJun Wah Ng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.03581\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 29 Nov 2022 16:08:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by the 13th International Symposium on Chinese Spoken Language Processing (ISCSLP 2022)\\u00a7r"}']}
{title:'Atmaja et al. (§72022§r)', author: 'Bagus Tris Atmaja; Zanjabila; Suyanto; Akira Sasou', display:{Lore:['[{"text": "arXiv:2210.05843", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-dataset COVID-19 Transfer Learning with Cough Detection, Cough Segmentation, and Data Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oBagus Tris Atmaja\\nZanjabila\\nSuyanto\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.05843\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 12 Oct 2022 00:23:13 GMT)\\u00a7r"}']}
{title:'Choi et al. (§72022§r)', author: 'Byoung Jin Choi; Myeonghun Jeong; Minchan Kim; Sung Hwan Mun; Nam Soo Kim', display:{Lore:['[{"text": "arXiv:2210.05979", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Speaker-Consistency Learning Using Untranscribed Speech Data for Zero-Shot Multi-Speaker Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oByoung Jin Choi\\nMyeonghun Jeong\\nMinchan Kim\\nSung Hwan Mun\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.05979\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 22 Nov 2022 06:09:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAPSIPA 2022\\u00a7r"}']}
{title:'Ogun et al. (§72022§r)', author: 'Sewade Ogun; Vincent Colotte; Emmanuel Vincent', display:{Lore:['[{"text": "arXiv:2210.06370", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCan we use Common Voice to train a Multi-Speaker TTS system?\\u00a7r\\n\\n\\u00a78\\u00a7oSewade Ogun\\nVincent Colotte\\nEmmanuel Vincent\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.06370\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 12 Oct 2022 16:20:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proc. SLT 2022, Jan 09-12, 2023, Doha, Qatar\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Chao-Han Huck Yang; Jun Qi; Sabato Marco Siniscalchi; Chin-Hui Lee', display:{Lore:['[{"text": "arXiv:2210.06382", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Ensemble Teacher-Student Learning Approach with Poisson Sub-sampling to Differential Privacy Preserving Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oChao-Han Huck Yang\\nJun Qi\\nSabato Marco Siniscalchi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.06382\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 12 Oct 2022 16:34:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ISCA, ISCSLP 2022, Singapore. 5 Pages\\u00a7r"}']}
{title:'Chiu et al. (§72022§r)', author: 'Ching-Yu Chiu; Meinard Müller; Matthew E. P. Davies; Alvin Wen-Yu Su; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2210.06817", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Analysis Method for Metric-Level Switching in Beat Tracking\\u00a7r\\n\\n\\u00a78\\u00a7oChing-Yu Chiu\\nMeinard M\\u00fcller\\nMatthew E. P. Davies\\nAlvin Wen-Yu Su\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.06817\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2022.3215106\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 13 Oct 2022 08:01:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEESignal Processing Letters (Oct. 2022)\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Yuxiang Zhang; Jingze Lu; Xingming Wang; Zhuo Li; Runqiu Xiao; Wenchao Wang; Ming Li; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2210.06818", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeepfake Detection System for the ADD Challenge Track 3.2 Based on Score Fusion\\u00a7r\\n\\n\\u00a78\\u00a7oYuxiang Zhang\\nJingze Lu\\nXingming Wang\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.06818\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3552466.3556528\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 13 Oct 2022 08:04:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ACM Multimedia 2022 Workshop: First International Workshopon Deepfake Detection for Audio Multimedia\\u00a7r"}']}
{title:'Baas et al. (§72022§r)', author: 'Matthew Baas; Kevin Eloff; Herman Kamper', display:{Lore:['[{"text": "arXiv:2210.07677", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransFusion: Transcribing Speech with Multinomial Diffusion\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew Baas\\nKevin Eloff\\nHerman Kamper\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.07677\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 Oct 2022 10:01:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 4 figures, 1 table. Accepted at SACAIR 2022\\u00a7r"}']}
{title:'Jia et al. (§72022§r)', author: 'Yan Jia; Mi Hong; Jingyu Hou; Kailong Ren; Sifan Ma; Jin Wang; Fangzhen Peng; Yinglin Ji; Lin Yang; Junjie Wang', display:{Lore:['[{"text": "arXiv:2210.07749", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeVoice ASR Systems for the ISCSLP 2022 Intelligent Cockpit Speech Recognition Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oYan Jia\\nMi Hong\\nJingyu Hou\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.07749\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 17 Oct 2022 03:28:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThere are experimental errors\\u00a7r"}']}
{title:'Poncelet et al. (§72022§r)', author: 'Jakob Poncelet; Hugo Van hamme', display:{Lore:['[{"text": "arXiv:2210.07771", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning to Jointly Transcribe and Subtitle for End-to-End Spontaneous Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJakob Poncelet\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.07771\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 Oct 2022 13:01:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at SLT 2022\\u00a7r"}']}
{title:'Ronchini et al. (§72022§r)', author: 'Francesca Ronchini; Samuele Cornell; Romain Serizel; Nicolas Turpault; Eduardo Fonseca; Daniel P. W. Ellis', display:{Lore:['[{"text": "arXiv:2210.07856", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDescription and analysis of novelties introduced in DCASE Task 4 2022 on the baseline system\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesca Ronchini\\nSamuele Cornell\\nRomain Serizel\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.07856\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 7th Detection and Classification of Acoustic\\n  Scenes and Events 2022 Workshop (DCASE2022)\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 Oct 2022 14:29:16 GMT)\\u00a7r"}']}
{title:'Gong et al. (§72022§r)', author: 'Yijun Gong; Shupei Liu; Xiao-Lei Zhang', display:{Lore:['[{"text": "arXiv:2210.08484", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end Two-dimensional Sound Source Localization With Ad-hoc Microphone Arrays\\u00a7r\\n\\n\\u00a78\\u00a7oYijun Gong\\nShupei Liu\\nXiao-Lei Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.08484\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 16 Oct 2022 08:28:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 4 figures, coference\\u00a7r"}']}
{title:'Singh et al. (§72022§r)', author: 'Anup Singh; Kris Demuynck; Vipul Arora', display:{Lore:['[{"text": "arXiv:2210.08624", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention-Based Audio Embeddings for Query-by-Example\\u00a7r\\n\\n\\u00a78\\u00a7oAnup Singh\\nKris Demuynck\\nVipul Arora\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.08624\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 16 Oct 2022 19:37:55 GMT)\\u00a7r"}']}
{title:'Fan et al. (§72022§r)', author: 'Ruchao Fan; Guoli Ye; Yashesh Gaur; Jinyu Li', display:{Lore:['[{"text": "arXiv:2210.08665", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic-aware Non-autoregressive Spell Correction with Mask Sample Decoding\\u00a7r\\n\\n\\u00a78\\u00a7oRuchao Fan\\nGuoli Ye\\nYashesh Gaur\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.08665\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 16 Oct 2022 23:53:58 GMT)\\u00a7r"}']}
{title:'Lv et al. (§72022§r)', author: 'Shubo Lv; Yihui Fu; Yukai Jv; Lei Xie; Weixin Zhu; Wei Rao; Yannan Wang', display:{Lore:['[{"text": "arXiv:2210.08802", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lspatial-dccrn: dccrn equipped with frame-level angle feature and hybrid filtering for multi-channel speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oShubo Lv\\nYihui Fu\\nYukai Jv\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.08802\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Oct 2022 07:35:26 GMT)\\u00a7r"}']}
{title:'Kinahan et al. (§72022§r)', author: 'Sean Kinahan; Julie Liss; Visar Berisha', display:{Lore:['[{"text": "arXiv:2210.09334", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTorchDIVA: An Extensible Computational Model of Speech Production built on an Open-Source Machine Learning Library\\u00a7r\\n\\n\\u00a78\\u00a7oSean Kinahan\\nJulie Liss\\nVisar Berisha\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.09334\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Oct 2022 18:00:52 GMT)\\u00a7r"}']}
{title:'Stafylakis et al. (§72022§r)', author: 'Themos Stafylakis; Ladislav Mosner; Sofoklis Kakouros; Oldrich Plchot; Lukas Burget; Jan Cernocky', display:{Lore:['[{"text": "arXiv:2210.09513", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExtracting speaker and emotion information from self-supervised speech models via channel-wise correlations\\u00a7r\\n\\n\\u00a78\\u00a7oThemos Stafylakis\\nLadislav Mosner\\nSofoklis Kakouros\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.09513\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 15 Oct 2022 04:15:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at IEEE-SLT 2022\\u00a7r"}']}
{title:'Boes et al. (§72022§r)', author: 'Wim Boes; Hugo Van hamme', display:{Lore:['[{"text": "arXiv:2210.10208", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimizing Temporal Resolution Of Convolutional Recurrent Neural Networks For Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oWim Boes\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.10208\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 Oct 2022 23:25:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTechnical reportof submission to DCASE 2021 Challenge Task 4\\u00a7r"}']}
{title:'Boes et al. (§72022§r)', author: 'Wim Boes; Hugo Van hamme', display:{Lore:['[{"text": "arXiv:2210.10212", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Source Transformer Architectures for Audiovisual Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oWim Boes\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.10212\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 Oct 2022 23:42:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTechnical reportof submission to DCASE 2021 Challenge Task 1B\\u00a7r"}']}
{title:'Watanabe et al. (§72022§r)', author: 'Chihiro Watanabe; Hirokazu Kameoka', display:{Lore:['[{"text": "arXiv:2210.11059", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDisC-VC: Disentangled and F0-Controllable Neural Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oChihiro Watanabe\\nHirokazu Kameoka\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.11059\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 20 Oct 2022 07:30:07 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72022§r)', author: 'Donghyeon Kim; Kyungdeuk Ko; David K. Han; Hanseok Ko', display:{Lore:['[{"text": "arXiv:2210.11519", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiscriminatory and orthogonal feature learning for noise robust keyword spotting\\u00a7r\\n\\n\\u00a78\\u00a7oDonghyeon Kim\\nKyungdeuk Ko\\nDavid K. Han\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.11519\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2022.3203911\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 20 Oct 2022 18:44:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in SPL\\u00a7r"}']}
{title:'Strauss et al. (§72022§r)', author: 'Martin Strauss; Matteo Torcoli; Bernd Edler', display:{Lore:['[{"text": "arXiv:2210.11654", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Normalizing Flow-Based Speech Enhancement using an All-pole Gammatone Filterbank for Conditional Input Representation\\u00a7r\\n\\n\\u00a78\\u00a7oMartin Strauss\\nMatteo Torcoli\\nBernd Edler\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.11654\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 Oct 2022 01:01:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for Presentation at IEEE SLT 2022\\u00a7r"}']}
{title:'Chowdhury (§72022§r)', author: 'Jatin Chowdhury', display:{Lore:['[{"text": "arXiv:2210.12554", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lchowdsp_wdf: An Advanced C++ Library for Wave Digital Circuit Modelling\\u00a7r\\n\\n\\u00a78\\u00a7oJatin Chowdhury\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.12554\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 22 Oct 2022 21:20:46 GMT)\\u00a7r"}']}
{title:'Hamsa et al. (§72022§r)', author: 'Shibani Hamsa; Ismail Shahin; Youssef Iraqi; Ernesto Damiani; Naoufel Werghi', display:{Lore:['[{"text": "arXiv:2210.12701", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Identification from emotional and noisy speech data using learned voice segregation and Speech VGG\\u00a7r\\n\\n\\u00a78\\u00a7oShibani Hamsa\\nIsmail Shahin\\nYoussef Iraqi\\nErnesto Damiani\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.12701\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 23 Oct 2022 11:20:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oJournal\\u00a7r"}']}
{title:'Yin et al. (§72022§r)', author: 'Dacheng Yin; Zhiyuan Zhao; Chuanxin Tang; Zhiwei Xiong; Chong Luo', display:{Lore:['[{"text": "arXiv:2210.12995", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTridentSE: Guiding Speech Enhancement with 32 Global Tokens\\u00a7r\\n\\n\\u00a78\\u00a7oDacheng Yin\\nZhiyuan Zhao\\nChuanxin Tang\\nZhiwei Xiong\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.12995\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Oct 2022 07:30:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 3 tables\\u00a7r"}']}
{title:'Bishnu et al. (§72022§r)', author: 'Abhijeet Bishnu; Ankit Gupta; Mandar Gogate; Kia Dashtipour; Ahsan Adeel; Amir Hussain; Mathini Sellathurai; Tharmalingam Ratnarajah', display:{Lore:['[{"text": "arXiv:2210.13127", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Novel Frame Structure for Cloud-Based Audio-Visual Speech Enhancement in Multimodal Hearing-aids\\u00a7r\\n\\n\\u00a78\\u00a7oAbhijeet Bishnu\\nAnkit Gupta\\nMandar Gogate\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.13127\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Oct 2022 11:23:35 GMT)\\u00a7r"}']}
{title:'Qi et al. (§72022§r)', author: 'Jinzi Qi; Hugo Van hamme', display:{Lore:['[{"text": "arXiv:2210.13144", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeak-Supervised Dysarthria-invariant Features for Spoken Language Understanding using an FHVAE and Adversarial Training\\u00a7r\\n\\n\\u00a78\\u00a7oJinzi Qi\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.13144\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Oct 2022 11:57:08 GMT)\\u00a7r"}']}
{title:'Pramanick et al. (§72022§r)', author: 'Pradip Pramanick; Chayan Sarkar', display:{Lore:['[{"text": "arXiv:2210.13189", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.RO\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCan Visual Context Improve Automatic Speech Recognition for an Embodied Agent?\\u00a7r\\n\\n\\u00a78\\u00a7oPradip Pramanick\\nChayan Sarkar\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.13189\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 Oct 2022 11:16:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in EMNLP \'22\\u00a7r"}']}
{title:'Bundscherer et al. (§72022§r)', author: 'Maximilian Bundscherer; Thomas H. Schmitt; Sebastian Bayerl; Thomas Auerbach; Tobias Bocklet', display:{Lore:['[{"text": "arXiv:2210.13273", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Acoustical Machine Learning Approach to Determine Abrasive Belt Wear of Wide Belt Sanders\\u00a7r\\n\\n\\u00a78\\u00a7oMaximilian Bundscherer\\nThomas H. Schmitt\\nSebastian Bayerl\\nThomas Auerbach\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.13273\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/SENSORS52175.2022.9967324\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Oct 2022 14:12:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oConference IEEESENSORS 2022\\u00a7r"}']}
{title:'Défossez et al. (§72022§r)', author: 'Alexandre Défossez; Jade Copet; Gabriel Synnaeve; Yossi Adi', display:{Lore:['[{"text": "arXiv:2210.13438", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHigh Fidelity Neural Audio Compression\\u00a7r\\n\\n\\u00a78\\u00a7oAlexandre D\\u00e9fossez\\nJade Copet\\nGabriel Synnaeve\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.13438\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Oct 2022 17:52:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint\\u00a7r"}']}
{title:'Tran et al. (§72022§r)', author: 'Viet Anh Khoa Tran; David Thulke; Yingbo Gao; Christian Herold; Hermann Ney', display:{Lore:['[{"text": "arXiv:2210.13700", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDoes Joint Training Really Help Cascaded Speech Translation?\\u00a7r\\n\\n\\u00a78\\u00a7oViet Anh Khoa Tran\\nDavid Thulke\\nYingbo Gao\\nChristian Herold\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.13700\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 24 Nov 2022 08:47:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to EMNLP 2022\\u00a7r"}']}
{title:'Lu et al. (§72022§r)', author: 'Hui Lu; Disong Wang; Xixin Wu; Zhiyong Wu; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2210.13771", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDisentangled Speech Representation Learning for One-Shot Cross-lingual Voice Conversion Using \\u03b2-VAE\\u00a7r\\n\\n\\u00a78\\u00a7oHui Lu\\nDisong Wang\\nXixin Wu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.13771\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Oct 2022 05:12:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at SLT 2022\\u00a7r"}']}
{title:'Fierro et al. (§72022§r)', author: 'Leonardo Fierro; Vesa Välimäki', display:{Lore:['[{"text": "arXiv:2210.14041", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhanced Fuzzy Decomposition of Sound Into Sines, Transients, and Noise\\u00a7r\\n\\n\\u00a78\\u00a7oLeonardo Fierro\\nVesa V\\u00e4lim\\u00e4ki\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14041\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 30 Nov 2022 13:01:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted for publication to the Journal of Audio Engineering Society on October 20th, 2022\\u00a7r"}']}
{title:'Park et al. (§72022§r)', author: 'Kyumin Park; Keon Lee; Daeyoung Kim; Dongyeop Kang', display:{Lore:['[{"text": "arXiv:2210.14406", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRedPen: Region- and Reason-Annotated Dataset of Unnatural Speech\\u00a7r\\n\\n\\u00a78\\u00a7oKyumin Park\\nKeon Lee\\nDaeyoung Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14406\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Oct 2022 01:16:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Atmaja et al. (§72022§r)', author: 'Bagus Tris Atmaja; Akira Sasou', display:{Lore:['[{"text": "arXiv:2210.14501", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEffect of different splitting criteria on the performance of speech emotion recognition\\u00a7r\\n\\n\\u00a78\\u00a7oBagus Tris Atmaja\\nAkira Sasou\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14501\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TENCON54134.2021.9707265\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nTENCON 2021, pp 760-764\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Oct 2022 06:16:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at TENCON 2021\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Hexin Liu; Haihua Xu; Leibny Paola Garcia; Andy W. H. Khong; Yi He; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:2210.14567", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReducing Language confusion for Code-switching Speech Recognition with Token-level Language Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oHexin Liu\\nHaihua Xu\\nLeibny Paola Garcia\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14567\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Oct 2022 08:55:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Qing Wang; Hang Chen; Ya Jiang; Zhe Wang; Yuyang Wang; Jun Du; Chin-Hui Lee', display:{Lore:['[{"text": "arXiv:2210.14581", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning Based Audio-Visual Multi-Speaker DOA Estimation Using Permutation-Free Loss Function\\u00a7r\\n\\n\\u00a78\\u00a7oQing Wang\\nHang Chen\\nYa Jiang\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14581\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Oct 2022 09:33:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted by ISCSLP 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Chunhui Wang; Chang Zeng; Xing He', display:{Lore:['[{"text": "arXiv:2210.14666", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lXiaoicesing 2: A High-Fidelity Singing Voice Synthesizer Based on Generative Adversarial Network\\u00a7r\\n\\n\\u00a78\\u00a7oChunhui Wang\\nChang Zeng\\nXing He\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14666\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 28 Oct 2022 09:28:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to icassp2023\\u00a7r"}']}
{title:'Koyama et al. (§72022§r)', author: 'Shoichi Koyama; Kazuyuki Arikawa', display:{Lore:['[{"text": "arXiv:2210.14711", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeighted Pressure Matching Based on Kernel Interpolation For Sound Field Reproduction\\u00a7r\\n\\n\\u00a78\\u00a7oShoichi Koyama\\nKazuyuki Arikawa\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14711\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Oct 2022 13:43:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at 24th International Congress on Acoustics (ICA) 2022\\u00a7r"}']}
{title:'Mittal et al. (§72022§r)', author: 'Trisha Mittal; Zakaria Aldeneh; Masha Fedzechkina; Anurag Ranjan; Barry-John Theobald', display:{Lore:['[{"text": "arXiv:2210.14800", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNaturalistic Head Motion Generation from Speech\\u00a7r\\n\\n\\u00a78\\u00a7oTrisha Mittal\\nZakaria Aldeneh\\nMasha Fedzechkina\\nAnurag Ranjan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14800\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Oct 2022 15:49:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Yuanzhe Chen; Ming Tu; Tang Li; Xin Li; Qiuqiang Kong; Jiaxin Li; Zhichao Wang; Qiao Tian; Yuping Wang; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2210.15158", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreaming Voice Conversion Via Intermediate Bottleneck Features And Non-streaming Teacher Guidance\\u00a7r\\n\\n\\u00a78\\u00a7oYuanzhe Chen\\nMing Tu\\nTang Li\\n+ 6 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15158\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Oct 2022 03:53:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe paper has been submitted to ICASSP2023\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Yisi Liu; Peter Wu; Alan W Black; Gopala K. Anumanchipalli', display:{Lore:['[{"text": "arXiv:2210.15272", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Fast and Accurate Pitch Estimation Algorithm Based on the Pseudo Wigner-Ville Distribution\\u00a7r\\n\\n\\u00a78\\u00a7oYisi Liu\\nPeter Wu\\nAlan W Black\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15272\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Oct 2022 09:05:04 GMT)\\u00a7r"}']}
{title:'Ragano et al. (§72022§r)', author: 'Alessandro Ragano; Emmanouil Benetos; Andrew Hines', display:{Lore:['[{"text": "arXiv:2210.15310", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Music Representations with wav2vec 2.0\\u00a7r\\n\\n\\u00a78\\u00a7oAlessandro Ragano\\nEmmanouil Benetos\\nAndrew Hines\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15310\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Oct 2022 10:45:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Zhu et al. (§72022§r)', author: 'Qiu-Shi Zhu; Long Zhou; Jie Zhang; Shu-Jie Liu; Yu-Chen Hu; Li-Rong Dai', display:{Lore:['[{"text": "arXiv:2210.15324", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Data2vec: Noise-robust Speech Representation Learning for ASR by Combining Regression and Improved Contrastive Learning\\u00a7r\\n\\n\\u00a78\\u00a7oQiu-Shi Zhu\\nLong Zhou\\nJie Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15324\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Oct 2022 11:04:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Hou et al. (§72022§r)', author: 'Yuanbo Hou; Siyang Song; Chuang Yu; Yuxin Song; Wenwu Wang; Dick Botteldooren', display:{Lore:['[{"text": "arXiv:2210.15366", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-dimensional Edge-based Audio Event Relational Graph Representation Learning for Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oYuanbo Hou\\nSiyang Song\\nChuang Yu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15366\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 1 Nov 2022 19:33:16 GMT)\\u00a7r"}']}
{title:'Tao et al. (§72022§r)', author: 'Ruijie Tao; Kong Aik Lee; Rohan Kumar Das; Ville Hautamäki; Haizhou Li', display:{Lore:['[{"text": "arXiv:2210.15385", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Training of Speaker Encoder with Multi-Modal Diverse Positive Pairs\\u00a7r\\n\\n\\u00a78\\u00a7oRuijie Tao\\nKong Aik Lee\\nRohan Kumar Das\\nVille Hautam\\u00e4ki\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15385\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Oct 2022 12:47:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages\\u00a7r"}']}
{title:'Kundu et al. (§72022§r)', author: 'Arnav Kundu; Mohammad Samragh Razlighi; Minsik Cho; Priyanka Padmanabhan; Devang Naik', display:{Lore:['[{"text": "arXiv:2210.15425", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHEiMDaL: Highly Efficient Method for Detection and Localization of wake-words\\u00a7r\\n\\n\\u00a78\\u00a7oArnav Kundu\\nMohammad Samragh Razlighi\\nMinsik Cho\\nPriyanka Padmanabhan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15425\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Oct 2022 17:26:57 GMT)\\u00a7r"}']}
{title:'Karo et al. (§72022§r)', author: 'Matan Karo; Arie Yeredor; Itshak Lapidot', display:{Lore:['[{"text": "arXiv:2210.15428", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTime-Domain Based Embeddings for Spoofed Audio Representation\\u00a7r\\n\\n\\u00a78\\u00a7oMatan Karo\\nArie Yeredor\\nItshak Lapidot\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15428\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Oct 2022 13:45:43 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Muqiao Yang; Naoyuki Kanda; Xiaofei Wang; Jian Wu; Sunit Sivasankaran; Zhuo Chen; Jinyu Li; Takuya Yoshioka', display:{Lore:['[{"text": "arXiv:2210.15715", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSimulating realistic speech overlaps improves multi-talker ASR\\u00a7r\\n\\n\\u00a78\\u00a7oMuqiao Yang\\nNaoyuki Kanda\\nXiaofei Wang\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15715\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 17 Nov 2022 19:21:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ov2: fix minor typo\\u00a7r"}']}
{title:'Baird et al. (§72022§r)', author: 'Alice Baird; Panagiotis Tzirakis; Jeffrey A. Brooks; Christopher B. Gregory; Björn Schuller; Anton Batliner; Dacher Keltner; Alan Cowen', display:{Lore:['[{"text": "arXiv:2210.15754", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProceedings of the ACII Affective Vocal Bursts Workshop and Competition 2022 (A-VB): Understanding a critically understudied modality of emotional expression\\u00a7r\\n\\n\\u00a78\\u00a7oAlice Baird\\nPanagiotis Tzirakis\\nJeffrey A. Brooks\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15754\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Oct 2022 20:19:20 GMT)\\u00a7r"}']}
{title:'Yu et al. (§72022§r)', author: 'Chin-Yun Yu; Sung-Lin Yeh; György Fazekas; Hao Tang', display:{Lore:['[{"text": "arXiv:2210.15793", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConditioning and Sampling in Variational Diffusion Models for Speech Super-Resolution\\u00a7r\\n\\n\\u00a78\\u00a7oChin-Yun Yu\\nSung-Lin Yeh\\nGy\\u00f6rgy Fazekas\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15793\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 24 Nov 2022 12:09:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Patel et al. (§72022§r)', author: 'Kashyap Patel; Anton Kovalyov; Issa Panahi', display:{Lore:['[{"text": "arXiv:2210.15822", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUX-NET: Filter-and-Process-based Improved U-Net for Real-time Time-domain Audio Separation\\u00a7r\\n\\n\\u00a78\\u00a7oKashyap Patel\\nAnton Kovalyov\\nIssa Panahi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15822\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Oct 2022 01:24:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Tao et al. (§72022§r)', author: 'Ruijie Tao; Kong Aik Lee; Zhan Shi; Haizhou Li', display:{Lore:['[{"text": "arXiv:2210.15903", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker recognition with two-step multi-modal deep cleansing\\u00a7r\\n\\n\\u00a78\\u00a7oRuijie Tao\\nKong Aik Lee\\nZhan Shi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15903\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Oct 2022 05:20:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Grósz et al. (§72022§r)', author: 'Tamás Grósz; Mittul Singh; Sudarsana Reddy Kadiri; Hemant Kathania; Mikko Kurimo', display:{Lore:['[{"text": "arXiv:2210.15978", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end Ensemble-based Feature Selection for Paralinguistics Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oTam\\u00e1s Gr\\u00f3sz\\nMittul Singh\\nSudarsana Reddy Kadiri\\nHemant Kathania\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15978\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Oct 2022 08:18:56 GMT)\\u00a7r"}']}
{title:'Bayerl et al. (§72022§r)', author: 'Sebastian P. Bayerl; Dominik Wagner; Florian Hönig; Tobias Bocklet; Elmar Nöth; Korbinian Riedhammer', display:{Lore:['[{"text": "arXiv:2210.15982", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDysfluencies Seldom Come Alone \\u2013 Detection as a Multi-Label Problem\\u00a7r\\n\\n\\u00a78\\u00a7oSebastian P. Bayerl\\nDominik Wagner\\nFlorian H\\u00f6nig\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15982\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Oct 2022 08:22:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Svirsky et al. (§72022§r)', author: 'Jonathan Svirsky; Ofir Lindenbaum', display:{Lore:['[{"text": "arXiv:2210.16022", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSG-VAD: Stochastic Gates Based Speech Activity Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJonathan Svirsky\\nOfir Lindenbaum\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16022\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Oct 2022 09:50:59 GMT)\\u00a7r"}']}
{title:'Peng et al. (§72022§r)', author: 'Junyi Peng; Themos Stafylakis; Rongzhi Gu; Oldřich Plchot; Ladislav Mošner; Lukáš Burget; Jan Černocký', display:{Lore:['[{"text": "arXiv:2210.16032", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParameter-efficient transfer learning of pre-trained Transformer models for speaker verification using adapters\\u00a7r\\n\\n\\u00a78\\u00a7oJunyi Peng\\nThemos Stafylakis\\nRongzhi Gu\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16032\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Oct 2022 10:07:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP2023\\u00a7r"}']}
{title:'Gomez-Alanis et al. (§72022§r)', author: 'Alejandro Gomez-Alanis; Lukas Drude; Andreas Schwarz; Rupak Vignesh Swaminathan; Simon Wiesler', display:{Lore:['[{"text": "arXiv:2210.16238", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContextual-Utterance Training for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAlejandro Gomez-Alanis\\nLukas Drude\\nAndreas Schwarz\\nRupak Vignesh Swaminathan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16238\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Oct 2022 08:10:44 GMT)\\u00a7r"}']}
{title:'Siriwardena et al. (§72022§r)', author: 'Yashish M. Siriwardena; Carol Espy-Wilson', display:{Lore:['[{"text": "arXiv:2210.16450", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Secret Source : Incorporating Source Features to Improve Acoustic-to-Articulatory Speech Inversion\\u00a7r\\n\\n\\u00a78\\u00a7oYashish M. Siriwardena\\nCarol Espy-Wilson\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16450\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 29 Oct 2022 00:28:29 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Yongqiang Wang; Zhehuai Chen; Chengjian Zheng; Yu Zhang; Wei Han; Parisa Haghani', display:{Lore:['[{"text": "arXiv:2210.16481", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAccelerating RNN-T Training and Inference Using CTC guidance\\u00a7r\\n\\n\\u00a78\\u00a7oYongqiang Wang\\nZhehuai Chen\\nChengjian Zheng\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16481\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 29 Oct 2022 03:39:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Zhe Li; Man-Wai Mak', display:{Lore:['[{"text": "arXiv:2210.16636", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Representation Learning via Contrastive Loss with Maximal Speaker Separability\\u00a7r\\n\\n\\u00a78\\u00a7oZhe Li\\nMan-Wai Mak\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16636\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 17 Nov 2022 03:52:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccept by APSIPA ASC 2022, 6 pages, 2 figures\\u00a7r"}']}
{title:'A et al. (§72022§r)', author: 'Arunkumar A; Mudit Batra; Umesh S', display:{Lore:['[{"text": "arXiv:2210.16739", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDuDe: Dual-Decoder Multilingual ASR for Indian Languages using Common Label Set\\u00a7r\\n\\n\\u00a78\\u00a7oArunkumar A\\nMudit Batra\\nUmesh S\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16739\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 30 Oct 2022 04:01:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Jie Wang; Menglong Xu; Jingyong Hou; Binbin Zhang; Xiao-Lei Zhang; Lei Xie; Fuping Pan', display:{Lore:['[{"text": "arXiv:2210.16743", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeKws: A production first small-footprint end-to-end Keyword Spotting Toolkit\\u00a7r\\n\\n\\u00a78\\u00a7oJie Wang\\nMenglong Xu\\nJingyong Hou\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16743\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 30 Oct 2022 04:45:07 GMT)\\u00a7r"}']}
{title:'Udupa et al. (§72022§r)', author: 'Sathvik Udupa; Siddarth C; Prasanta Kumar Ghosh', display:{Lore:['[{"text": "arXiv:2210.16871", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved acoustic-to-articulatory inversion using representations from pretrained self-supervised learning models\\u00a7r\\n\\n\\u00a78\\u00a7oSathvik Udupa\\nSiddarth C\\nPrasanta Kumar Ghosh\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16871\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 30 Oct 2022 16:24:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Udupa et al. (§72022§r)', author: 'Sathvik Udupa; Prasanta Kumar Ghosh', display:{Lore:['[{"text": "arXiv:2210.16881", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-Time MRI Video synthesis from time aligned phonemes with sequence-to-sequence networks\\u00a7r\\n\\n\\u00a78\\u00a7oSathvik Udupa\\nPrasanta Kumar Ghosh\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16881\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 30 Oct 2022 16:45:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Prakash et al. (§72022§r)', author: 'Anusha Prakash; Hema A Murthy', display:{Lore:['[{"text": "arXiv:2210.17153", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Importance of Accurate Alignments in End-to-End Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oAnusha Prakash\\nHema A Murthy\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.17153\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Oct 2022 09:05:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oVersion 1 uploaded\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Jingyu Li; Yusheng Tian; Tan Lee', display:{Lore:['[{"text": "arXiv:2210.17310", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConvolution-Based Channel-Frequency Attention for Text-Independent Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oJingyu Li\\nYusheng Tian\\nTan Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.17310\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Oct 2022 13:34:22 GMT)\\u00a7r"}']}
{title:'Scheibler et al. (§72022§r)', author: 'Robin Scheibler; Youna Ji; Soo-Whan Chung; Jaeuk Byun; Soyeon Choe; Min-Seok Choi', display:{Lore:['[{"text": "arXiv:2210.17327", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffusion-based Generative Speech Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oRobin Scheibler\\nYouna Ji\\nSoo-Whan Chung\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.17327\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 2 Nov 2022 15:44:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 2 tables. Submitted to ICASSP 2023\\u00a7r"}']}
{title:'Gaznepoglu et al. (§72022§r)', author: 'Ünal Ege Gaznepoglu; Anna Leschanowsky; Nils Peters', display:{Lore:['[{"text": "arXiv:2210.17338", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoicePrivacy 2022 System Description: Speaker Anonymization with Feature-matched F0 Trajectories\\u00a7r\\n\\n\\u00a78\\u00a7o\\u00dcnal Ege Gaznepoglu\\nAnna Leschanowsky\\nNils Peters\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.17338\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Oct 2022 13:59:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 4 figures, 2 tables, submitted to VoicePrivacy Challenge2022\\u00a7r"}']}
{title:'Tomanek et al. (§72022§r)', author: 'Katrin Tomanek; Katie Seaver; Pan-Pan Jiang; Richard Cave; Lauren Harrel; Jordan R. Green', display:{Lore:['[{"text": "arXiv:2211.00089", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn analysis of degenerating speech due to progressive dysarthria on ASR performance\\u00a7r\\n\\n\\u00a78\\u00a7oKatrin Tomanek\\nKatie Seaver\\nPan-Pan Jiang\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00089\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Oct 2022 18:42:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Cai et al. (§72022§r)', author: 'Zexin Cai; Weiqing Wang; Ming Li', display:{Lore:['[{"text": "arXiv:2211.00226", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWaveform Boundary Detection for Partially Spoofed Audio\\u00a7r\\n\\n\\u00a78\\u00a7oZexin Cai\\nWeiqing Wang\\nMing Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00226\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Nov 2022 02:31:54 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Yuhang Yang; Haihua Xu; Hao Huang; Eng Siong Chng; Sheng Li', display:{Lore:['[{"text": "arXiv:2211.00325", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech-text based multi-modal training with bidirectional attention for improved speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYuhang Yang\\nHaihua Xu\\nHao Huang\\nEng Siong Chng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00325\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Nov 2022 08:25:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 3 tables\\u00a7r"}']}
{title:'Jung et al. (§72022§r)', author: 'Jaemin Jung; Youkyum Kim; Jihwan Park; Youshin Lim; Byeong-Yeol Kim; Youngjoon Jang; Joon Son Chung', display:{Lore:['[{"text": "arXiv:2211.00439", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMetric Learning for User-defined Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oJaemin Jung\\nYoukyum Kim\\nJihwan Park\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00439\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Nov 2022 13:08:55 GMT)\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'Zili Huang; Desh Raj; Paola García; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:2211.00482", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdapting self-supervised models to multi-talker speech recognition using speaker embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oZili Huang\\nDesh Raj\\nPaola Garc\\u00eda\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00482\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Nov 2022 14:16:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Kang et al. (§72022§r)', author: 'Wei Kang; Liyong Guo; Fangjun Kuang; Long Lin; Mingshuang Luo; Zengwei Yao; Xiaoyu Yang; Piotr Żelasko; Daniel Povey', display:{Lore:['[{"text": "arXiv:2211.00484", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast and parallel decoding for transducer\\u00a7r\\n\\n\\u00a78\\u00a7oWei Kang\\nLiyong Guo\\nFangjun Kuang\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00484\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Oct 2022 07:46:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to 2023 IEEE InternationalConference on Acoustics, Speech and Signal Processing\\u00a7r"}']}
{title:'Kang et al. (§72022§r)', author: 'Wei Kang; Zengwei Yao; Fangjun Kuang; Liyong Guo; Xiaoyu Yang; Long lin; Piotr Żelasko; Daniel Povey', display:{Lore:['[{"text": "arXiv:2211.00490", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDelay-penalized transducer for low-latency streaming ASR\\u00a7r\\n\\n\\u00a78\\u00a7oWei Kang\\nZengwei Yao\\nFangjun Kuang\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00490\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Oct 2022 07:03:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to 2023 IEEE InternationalConference on Acoustics, Speech and Signal Processing\\u00a7r"}']}
{title:'Guo et al. (§72022§r)', author: 'Liyong Guo; Xiaoyu Yang; Quandong Wang; Yuxiang Kong; Zengwei Yao; Fan Cui; Fangjun Kuang; Wei Kang; Long Lin; Mingshuang Luo; Piotr Zelasko; Daniel Povey', display:{Lore:['[{"text": "arXiv:2211.00508", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPredicting Multi-Codebook Vector Quantization Indexes for Knowledge Distillation\\u00a7r\\n\\n\\u00a78\\u00a7oLiyong Guo\\nXiaoyu Yang\\nQuandong Wang\\n+ 8 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00508\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Oct 2022 07:03:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2022\\u00a7r"}']}
{title:'Chowenhill et al. (§72022§r)', author: 'Leah Chowenhill; Gaurav Satyanath; Shubhranshu Singh; Madhav Mahendra Wagh', display:{Lore:['[{"text": "arXiv:2211.00569", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFew-shot Bioacoustic Event Detection with Machine Learning Methods\\u00a7r\\n\\n\\u00a78\\u00a7oLeah Chowenhill\\nGaurav Satyanath\\nShubhranshu Singh\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00569\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Aug 2022 23:40:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 6 tables, 1 figure\\u00a7r"}']}
{title:'Ahrens (§72022§r)', author: 'Jens Ahrens', display:{Lore:['[{"text": "arXiv:2211.00583", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAmbisonic Encoding of Signals From Spherical Microphone Arrays\\u00a7r\\n\\n\\u00a78\\u00a7oJens Ahrens\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00583\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 18 Sep 2022 18:58:58 GMT)\\u00a7r"}']}
{title:'Ahrens (§72022§r)', author: 'Jens Ahrens', display:{Lore:['[{"text": "arXiv:2211.00584", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAmbisonic Encoding of Signals From Equatorial Microphone Arrays\\u00a7r\\n\\n\\u00a78\\u00a7oJens Ahrens\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00584\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 18 Sep 2022 18:57:43 GMT)\\u00a7r"}']}
{title:'Hsieh et al. (§72022§r)', author: 'Cheng-Ping Hsieh; Subhankar Ghosh; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2211.00585", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdapter-Based Extension of Multi-Speaker Text-to-Speech Model for New Speakers\\u00a7r\\n\\n\\u00a78\\u00a7oCheng-Ping Hsieh\\nSubhankar Ghosh\\nBoris Ginsburg\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00585\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Nov 2022 16:59:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Yang Liu; Yangyang Shi; Yun Li; Kaustubh Kalgaonkar; Sriram Srinivasan; Xin Lei', display:{Lore:['[{"text": "arXiv:2211.00589", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSCA: Streaming Cross-attention Alignment for Echo Cancellation\\u00a7r\\n\\n\\u00a78\\u00a7oYang Liu\\nYangyang Shi\\nYun Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00589\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Nov 2022 17:01:50 GMT)\\u00a7r"}']}
{title:'Song et al. (§72022§r)', author: 'Tongtong Song; Qiang Xu; Haoyu Lu; Longbiao Wang; Hao Shi; Yuqin Lin; Yanbing Yang; Jianwu Dang', display:{Lore:['[{"text": "arXiv:2211.01046", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMonolingual Recognizers Fusion for Code-switching Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oTongtong Song\\nQiang Xu\\nHaoyu Lu\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01046\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Nov 2022 11:24:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP2023\\u00a7r"}']}
{title:'Lee et al. (§72022§r)', author: 'Kong Aik Lee; Tomi Kinnunen; Daniele Colibro; Claudio Vair; Andreas Nautsch; Hanwu Sun; Liang He; Tianyu Liang; Qiongqiong Wang; Mickael Rouvier; Pierre-Michel Bousquet; Rohan Kumar Das; Ignacio Viñals Bailo; Meng Liu; Héctor Deldago; Xuechen Liu; Md Sahidullah; Sandro Cumani; Boning Zhang; Koji Okabe; Hitoshi Yamamoto; Ruijie Tao; Haizhou Li; Alfonso Ortega Giménez; Longbiao Wang; Luis Buera', display:{Lore:['[{"text": "arXiv:2211.01091", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lI4U System Description for NIST SRE\'20 CTS Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oKong Aik Lee\\nTomi Kinnunen\\nDaniele Colibro\\n+ 22 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01091\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Nov 2022 13:04:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSRE 2021, NIST Speaker Recognition Evaluation Workshop, CTS Speaker Recognition Challenge, 14-12 December 2021\\u00a7r"}']}
{title:'Hsieh et al. (§72022§r)', author: 'Tsun-An Hsieh; Chao-Han Huck Yang; Pin-Yu Chen; Sabato Marco Siniscalchi; Yu Tsao', display:{Lore:['[{"text": "arXiv:2211.01189", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInference and Denoise: Causal Inference-based Neural Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oTsun-An Hsieh\\nChao-Han Huck Yang\\nPin-Yu Chen\\nSabato Marco Siniscalchi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01189\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Nov 2022 15:03:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Fujimura et al. (§72022§r)', author: 'Takuya Fujimura; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2211.01198", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalysis of Noisy-target Training for DNN-based speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oTakuya Fujimura\\nTomoki Toda\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01198\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Nov 2022 15:21:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Prakash et al. (§72022§r)', author: 'Anusha Prakash; Arun Kumar; Ashish Seth; Bhagyashree Mukherjee; Ishika Gupta; Jom Kuriakose; Jordan Fernandes; K V Vikram; Mano Ranjith Kumar M; Metilda Sagaya Mary; Mohammad Wajahat; Mohana N; Mudit Batra; Navina K; Nihal John George; Nithya Ravi; Pruthwik Mishra; Sudhanshu Srivastava; Vasista Sai Lodagala; Vandan Mujadia; Kada Sai Venkata Vineeth; Vrunda Sukhadia; Dipti Sharma; Hema Murthy; Pushpak Bhattacharya; S Umesh; Rajeev Sangal', display:{Lore:['[{"text": "arXiv:2211.01338", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTechnology Pipeline for Large Scale Cross-Lingual Dubbing of Lecture Videos into Multiple Indian Languages\\u00a7r\\n\\n\\u00a78\\u00a7oAnusha Prakash\\nArun Kumar\\nAshish Seth\\n+ 23 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01338\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Nov 2022 07:06:29 GMT)\\u00a7r"}']}
{title:'Strgar et al. (§72022§r)', author: 'Luke Strgar; David Harwath', display:{Lore:['[{"text": "arXiv:2211.01461", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhoneme Segmentation Using Self-Supervised Speech Models\\u00a7r\\n\\n\\u00a78\\u00a7oLuke Strgar\\nDavid Harwath\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01461\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 2 Nov 2022 19:57:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to SLT 2022\\u00a7r"}']}
{title:'Heo et al. (§72022§r)', author: 'Jungwoo Heo; Hyun-seo Shin; Ju-ho Kim; Chan-yeong Lim; Ha-Jin Yu', display:{Lore:['[{"text": "arXiv:2211.01599", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConvolution channel separation and frequency sub-bands aggregation for music genre classification\\u00a7r\\n\\n\\u00a78\\u00a7oJungwoo Heo\\nHyun-seo Shin\\nJu-ho Kim\\nChan-yeong Lim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01599\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 3 Nov 2022 06:03:39 GMT)\\u00a7r"}']}
{title:'Nielsen et al. (§72022§r)', author: 'Christian Heider Nielsen; Zheng-Hua Tan', display:{Lore:['[{"text": "arXiv:2211.01621", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Domain Features for Detecting Adversarial Attacks Against Deep Speech Recognition in Noise\\u00a7r\\n\\n\\u00a78\\u00a7oChristian Heider Nielsen\\nZheng-Hua Tan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01621\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 3 Nov 2022 07:25:45 GMT)\\u00a7r"}']}
{title:'Kakouros et al. (§72022§r)', author: 'Sofoklis Kakouros; Themos Stafylakis; Ladislav Mosner; Lukas Burget', display:{Lore:['[{"text": "arXiv:2211.01756", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech-based emotion recognition with self-supervised models using attentive channel-wise correlations and label smoothing\\u00a7r\\n\\n\\u00a78\\u00a7oSofoklis Kakouros\\nThemos Stafylakis\\nLadislav Mosner\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01756\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 3 Nov 2022 12:37:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEE-ICASSP 2023\\u00a7r"}']}
{title:'Joglekar et al. (§72022§r)', author: 'Aditya Joglekar; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:2211.02051", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFearless Steps Challenge Phase-1 Evaluation Plan\\u00a7r\\n\\n\\u00a78\\u00a7oAditya Joglekar\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02051\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 3 Nov 2022 14:17:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDocument Generated in February 2019 for conducting the Fearless Steps Challenge Phase-1 and its associated ISCA Interspeech-2019 Special Session\\u00a7r"}']}
{title:'Jeong et al. (§72022§r)', author: 'Il-Young Jeong; Jeongsoo Park', display:{Lore:['[{"text": "arXiv:2211.02289", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCochlScene: Acquisition of acoustic scene data using crowdsourcing\\u00a7r\\n\\n\\u00a78\\u00a7oIl-Young Jeong\\nJeongsoo Park\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02289\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Nov 2022 07:01:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccept by APSIPA ASC 2022, 5 pages, 2 figures\\u00a7r"}']}
{title:'Shinohara et al. (§72022§r)', author: 'Yusuke Shinohara; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2211.02333", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMinimum Latency Training of Sequence Transducers for Streaming End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYusuke Shinohara\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02333\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Nov 2022 09:19:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at INTERSPEECH 2022\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Haici Yang; Wootaek Lim; Minje Kim', display:{Lore:['[{"text": "arXiv:2211.02506", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Feature Predictor and Discriminative Residual Coding for Low-Bitrate Speech Coding\\u00a7r\\n\\n\\u00a78\\u00a7oHaici Yang\\nWootaek Lim\\nMinje Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02506\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Nov 2022 15:07:42 GMT)\\u00a7r"}']}
{title:'Irvin et al. (§72022§r)', author: 'Bryce Irvin; Marko Stamenovic; Mikolaj Kegler; Li-Chia Yang', display:{Lore:['[{"text": "arXiv:2211.02542", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Learning for Speech Enhancement through Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oBryce Irvin\\nMarko Stamenovic\\nMikolaj Kegler\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02542\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Nov 2022 16:06:56 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Yuchen Liu; Li-Chia Yang; Alex Pawlicki; Marko Stamenovic', display:{Lore:['[{"text": "arXiv:2211.02577", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCCATMos: Convolutional Context-aware Transformer Network for Non-intrusive Speech Quality Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oYuchen Liu\\nLi-Chia Yang\\nAlex Pawlicki\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02577\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10857\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Nov 2022 16:46:11 GMT)\\u00a7r"}']}
{title:'Ding et al. (§72022§r)', author: 'Siwen Ding; You Zhang; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2211.02718", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSAMO: Speaker Attractor Multi-Center One-Class Learning for Voice Anti-Spoofing\\u00a7r\\n\\n\\u00a78\\u00a7oSiwen Ding\\nYou Zhang\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02718\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 4 Nov 2022 19:31:33 GMT)\\u00a7r"}']}
{title:'Taherian et al. (§72022§r)', author: 'Hassan Taherian; Sefik Emre Eskimez; Takuya Yoshioka', display:{Lore:['[{"text": "arXiv:2211.02944", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBreaking the trade-off in personalized speech enhancement with cross-task knowledge distillation\\u00a7r\\n\\n\\u00a78\\u00a7oHassan Taherian\\nSefik Emre Eskimez\\nTakuya Yoshioka\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02944\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 5 Nov 2022 17:02:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Yao et al. (§72022§r)', author: 'Jixun Yao; Yi Lei; Qing Wang; Pengcheng Guo; Ziqian Ning; Lei Xie; Hai Li; Junhui Liu; Danming Xie', display:{Lore:['[{"text": "arXiv:2211.03036", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPreserving background sound in noise-robust voice conversion via multi-task learning\\u00a7r\\n\\n\\u00a78\\u00a7oJixun Yao\\nYi Lei\\nQing Wang\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.03036\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 6 Nov 2022 06:00:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Yao et al. (§72022§r)', author: 'Jixun Yao; Qing Wang; Yi Lei; Pengcheng Guo; Lei Xie; Namin Wang; Jie Liu', display:{Lore:['[{"text": "arXiv:2211.03038", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDistinguishable Speaker Anonymization based on Formant and Fundamental Frequency Scaling\\u00a7r\\n\\n\\u00a78\\u00a7oJixun Yao\\nQing Wang\\nYi Lei\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.03038\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 6 Nov 2022 06:08:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Lee et al. (§72022§r)', author: 'Jihwan Lee; Jae-Sung Bae; Seongkyu Mun; Heejin Choi; Joun Yeop Lee; Hoon-Young Cho; Chanwoo Kim', display:{Lore:['[{"text": "arXiv:2211.03078", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Empirical Study on L2 Accents of Cross-lingual Text-to-Speech Systems via Vowel Space\\u00a7r\\n\\n\\u00a78\\u00a7oJihwan Lee\\nJae-Sung Bae\\nSeongkyu Mun\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.03078\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 6 Nov 2022 10:45:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Lahiri et al. (§72022§r)', author: 'Rimita Lahiri; Md Nasir; Catherine Lord; So Hyun Kim; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2211.03279", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Context-Aware Computational Approach for Measuring Vocal Entrainment in Dyadic Conversations\\u00a7r\\n\\n\\u00a78\\u00a7oRimita Lahiri\\nMd Nasir\\nCatherine Lord\\nSo Hyun Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.03279\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Nov 2022 03:07:37 GMT)\\u00a7r"}']}
{title:'Melechovsky et al. (§72022§r)', author: 'Jan Melechovsky; Ambuj Mehrish; Berrak Sisman; Dorien Herremans', display:{Lore:['[{"text": "arXiv:2211.03316", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAccented Text-to-Speech Synthesis with a Conditional Variational Autoencoder\\u00a7r\\n\\n\\u00a78\\u00a7oJan Melechovsky\\nAmbuj Mehrish\\nBerrak Sisman\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.03316\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Nov 2022 05:36:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7opreprint submitted to a conference, under review\\u00a7r"}']}
{title:'Fan et al. (§72022§r)', author: 'Xiaoran Fan; Chao Pang; Tian Yuan; He Bai; Renjie Zheng; Pengfei Zhu; Shuohuan Wang; Junkun Chen; Zeyu Chen; Liang Huang; Yu Sun; Hua Wu', display:{Lore:['[{"text": "arXiv:2211.03545", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lERNIE-SAT: Speech and Text Joint Pretraining for Cross-Lingual Multi-Speaker Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoran Fan\\nChao Pang\\nTian Yuan\\n+ 8 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.03545\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 4 Dec 2022 09:34:49 GMT)\\u00a7r"}']}
{title:'Saito et al. (§72022§r)', author: 'Koichi Saito; Naoki Murata; Toshimitsu Uesaka; Chieh-Hsin Lai; Yuhta Takida; Takao Fukui; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2211.04124", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised vocal dereverberation with diffusion-based generative models\\u00a7r\\n\\n\\u00a78\\u00a7oKoichi Saito\\nNaoki Murata\\nToshimitsu Uesaka\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.04124\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Nov 2022 09:43:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 2 figures, submitted to ICASSP 2023\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Shucong Zhang; Malcolm Chadwick; Alberto Gil C. P. Ramos; Sourav Bhattacharya', display:{Lore:['[{"text": "arXiv:2211.04346", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-Attention is all you need: Real-Time Streaming Transformers for Personalised Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oShucong Zhang\\nMalcolm Chadwick\\nAlberto Gil C. P. Ramos\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.04346\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Nov 2022 16:12:38 GMT)\\u00a7r"}']}
{title:'Ning et al. (§72022§r)', author: 'Ziqian Ning; Qicong Xie; Pengcheng Zhu; Zhichao Wang; Liumeng Xue; Jixun Yao; Lei Xie; Mengxiao Bi', display:{Lore:['[{"text": "arXiv:2211.04710", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExpressive-VC: Highly Expressive Voice Conversion with Attention Fusion of Bottleneck and Perturbation Features\\u00a7r\\n\\n\\u00a78\\u00a7oZiqian Ning\\nQicong Xie\\nPengcheng Zhu\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.04710\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 9 Nov 2022 07:03:59 GMT)\\u00a7r"}']}
{title:'Kwon et al. (§72022§r)', author: 'Youngki Kwon; Hee-Soo Heo; Bong-Jin Lee; You Jin Kim; Jee-weon Jung', display:{Lore:['[{"text": "arXiv:2211.04768", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAbsolute decision corrupts absolutely: conservative online speaker diarisation\\u00a7r\\n\\n\\u00a78\\u00a7oYoungki Kwon\\nHee-Soo Heo\\nBong-Jin Lee\\nYou Jin Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.04768\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 9 Nov 2022 09:52:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5pages, 2 figure,4 tables, submitted to ICASSP\\u00a7r"}']}
{title:'Shankar et al. (§72022§r)', author: 'Ravi Shankar; Abdouh Harouna Kenfack; Arjun Somayazulu; Archana Venkataraman', display:{Lore:['[{"text": "arXiv:2211.05047", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparative Study of Data Augmentation Techniques for Deep Learning Based Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oRavi Shankar\\nAbdouh Harouna Kenfack\\nArjun Somayazulu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.05047\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 9 Nov 2022 17:27:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder Submission\\u00a7r"}']}
{title:'Shankar et al. (§72022§r)', author: 'Ravi Shankar; Hsi-Wei Hsieh; Nicolas Charon; Archana Venkataraman', display:{Lore:['[{"text": "arXiv:2211.05071", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Diffeomorphic Flow-based Variational Framework for Multi-speaker Emotion Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oRavi Shankar\\nHsi-Wei Hsieh\\nNicolas Charon\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.05071\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 9 Nov 2022 18:03:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in IEEE Transactions on Audio, Speech and Language Processing\\u00a7r"}']}
{title:'Ma et al. (§72022§r)', author: 'Yingyi Ma; Zhe Liu; Xuedong Zhang', display:{Lore:['[{"text": "arXiv:2211.05121", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptive Multi-Corpora Language Model Training for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYingyi Ma\\nZhe Liu\\nXuedong Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.05121\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 9 Nov 2022 06:54:50 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Zhuo Chen; Naoyuki Kanda; Jian Wu; Yu Wu; Xiaofei Wang; Takuya Yoshioka; Jinyu Li; Sunit Sivasankaran; Sefik Emre Eskimez', display:{Lore:['[{"text": "arXiv:2211.05172", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech separation with large-scale self-supervised learning\\u00a7r\\n\\n\\u00a78\\u00a7oZhuo Chen\\nNaoyuki Kanda\\nJian Wu\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.05172\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 26 Nov 2022 01:31:46 GMT)\\u00a7r"}']}
{title:'Huang et al. (§72022§r)', author: 'Zili Huang; Zhuo Chen; Naoyuki Kanda; Jian Wu; Yiming Wang; Jinyu Li; Takuya Yoshioka; Xiaofei Wang; Peidong Wang', display:{Lore:['[{"text": "arXiv:2211.05564", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervised learning with bi-label masked speech prediction for streaming multi-talker speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZili Huang\\nZhuo Chen\\nNaoyuki Kanda\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.05564\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Nov 2022 13:36:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Ezzerg et al. (§72022§r)', author: 'Abdelhamid Ezzerg; Thomas Merritt; Kayoko Yanagisawa; Piotr Bilinski; Magdalena Proszewska; Kamil Pokora; Renard Korzeniowski; Roberto Barra-Chicote; Daniel Korzekwa', display:{Lore:['[{"text": "arXiv:2211.05850", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRemap, warp and attend: Non-parallel many-to-many accent conversion with Normalizing Flows\\u00a7r\\n\\n\\u00a78\\u00a7oAbdelhamid Ezzerg\\nThomas Merritt\\nKayoko Yanagisawa\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.05850\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Nov 2022 20:14:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE Spoken Language Technology Workshop 2022\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Tianrui Wang; Xie Chen; Zhuo Chen; Shu Yu; Weibin Zhu', display:{Lore:['[{"text": "arXiv:2211.06041", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Adapter based Multi-label Pre-training for Speech Separation and Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oTianrui Wang\\nXie Chen\\nZhuo Chen\\nShu Yu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.06041\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Nov 2022 07:34:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Labrador et al. (§72022§r)', author: 'Beltrán Labrador; Guanlong Zhao; Ignacio López Moreno; Angelo Scorza Scarpati; Liam Fowl; Quan Wang', display:{Lore:['[{"text": "arXiv:2211.06478", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Sequence-to-Sequence Transformer-Transducer Models for Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oBeltr\\u00e1n Labrador\\nGuanlong Zhao\\nIgnacio L\\u00f3pez Moreno\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.06478\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Nov 2022 20:41:46 GMT)\\u00a7r"}']}
{title:'Zhao et al. (§72022§r)', author: 'Guanlong Zhao; Quan Wang; Han Lu; Yiling Huang; Ignacio Lopez Moreno', display:{Lore:['[{"text": "arXiv:2211.06482", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAugmenting Transformer-Transducer Based Speaker Change Detection With Token-Level Training Loss\\u00a7r\\n\\n\\u00a78\\u00a7oGuanlong Zhao\\nQuan Wang\\nHan Lu\\nYiling Huang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.06482\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 4 Dec 2022 01:02:34 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Li-Wei Chen; Shinji Watanabe; Alexander Rudnicky', display:{Lore:['[{"text": "arXiv:2211.06535", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA unified one-shot prosody and speaker conversion system with self-supervised discrete speech units\\u00a7r\\n\\n\\u00a78\\u00a7oLi-Wei Chen\\nShinji Watanabe\\nAlexander Rudnicky\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.06535\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 12 Nov 2022 00:54:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Hajal et al. (§72022§r)', author: 'Karl El Hajal; Zihan Wu; Neil Scheidwasser-Clow; Gasser Elbanna; Milos Cernak', display:{Lore:['[{"text": "arXiv:2211.06646", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Speech Quality Assessment using Self-supervised Framewise Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oKarl El Hajal\\nZihan Wu\\nNeil Scheidwasser-Clow\\nGasser Elbanna\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.06646\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 12 Nov 2022 11:57:08 GMT)\\u00a7r"}']}
{title:'Lam et al. (§72022§r)', author: 'Perry Lam; Huayun Zhang; Nancy F. Chen; Berrak Sisman; Dorien Herremans', display:{Lore:['[{"text": "arXiv:2211.07283", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSNIPER Training: Variable Sparsity Rate Training For Text-To-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oPerry Lam\\nHuayun Zhang\\nNancy F. Chen\\nBerrak Sisman\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.07283\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Nov 2022 11:26:13 GMT)\\u00a7r"}']}
{title:'Gasparini et al. (§72022§r)', author: 'Francesca Gasparini; Alessandra Grossi', display:{Lore:['[{"text": "arXiv:2211.07307", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSentiment recognition of Italian elderly through domain adaptation on cross-corpus speech dataset\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesca Gasparini\\nAlessandra Grossi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.07307\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Nov 2022 12:39:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 3 figures, 5 tables\\u00a7r"}']}
{title:'Xue (§72022§r)', author: 'Yuqi Xue', display:{Lore:['[{"text": "arXiv:2211.07373", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Label Training for Text-Independent Speaker Identification\\u00a7r\\n\\n\\u00a78\\u00a7oYuqi Xue\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.07373\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Nov 2022 14:07:25 GMT)\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Changye Li; Trevor Cohen; Serguei Pakhomov', display:{Lore:['[{"text": "arXiv:2211.07430", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Far Side of Failure: Investigating the Impact of Speech Recognition Errors on Subsequent Dementia Classification\\u00a7r\\n\\n\\u00a78\\u00a7oChangye Li\\nTrevor Cohen\\nSerguei Pakhomov\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.07430\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Nov 2022 17:06:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted as extended abstract for ML4H 2022\\u00a7r"}']}
{title:'Panah et al. (§72022§r)', author: 'Davoud Shariat Panah; Andrew Hines; Susan McKeever', display:{Lore:['[{"text": "arXiv:2211.07445", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring the Impact of Noise and Degradations on Heart Sound Classification Models\\u00a7r\\n\\n\\u00a78\\u00a7oDavoud Shariat Panah\\nAndrew Hines\\nSusan McKeever\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.07445\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Nov 2022 15:18:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Computers in Biology and Medicine Journal\\u00a7r"}']}
{title:'Kuznetsova et al. (§72022§r)', author: 'Anastasia Kuznetsova; Aswin Sivaraman; Minje Kim', display:{Lore:['[{"text": "arXiv:2211.07493", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Potential of Neural Speech Synthesis-based Data Augmentation for Personalized Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oAnastasia Kuznetsova\\nAswin Sivaraman\\nMinje Kim\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.07493\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Nov 2022 16:20:41 GMT)\\u00a7r"}']}
{title:'Dawalatabad et al. (§72022§r)', author: 'Nauman Dawalatabad; Sameer Khurana; Antoine Laurent; James Glass', display:{Lore:['[{"text": "arXiv:2211.07795", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Unsupervised Uncertainty-Driven Speech Pseudo-Label Filtering and Model Calibration\\u00a7r\\n\\n\\u00a78\\u00a7oNauman Dawalatabad\\nSameer Khurana\\nAntoine Laurent\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.07795\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Nov 2022 23:20:36 GMT)\\u00a7r"}']}
{title:'Sandler et al. (§72022§r)', author: 'Morgan Sandler; Arun Ross', display:{Lore:['[{"text": "arXiv:2211.08213", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIs Style All You Need? Dependencies Between Emotion and GST-based Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMorgan Sandler\\nArun Ross\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08213\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Nov 2022 15:29:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Aralikatti et al. (§72022§r)', author: 'Rohith Aralikatti; Christoph Boeddeker; Gordon Wichern; Aswin Shanmugam Subramanian; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2211.08303", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReverberation as Supervision for Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oRohith Aralikatti\\nChristoph Boeddeker\\nGordon Wichern\\nAswin Shanmugam Subramanian\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08303\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Nov 2022 17:06:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 4 tables. Submitted to ICASSP 2023\\u00a7r"}']}
{title:'Rouard et al. (§72022§r)', author: 'Simon Rouard; Francisco Massa; Alexandre Défossez', display:{Lore:['[{"text": "arXiv:2211.08553", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHybrid Transformers for Music Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oSimon Rouard\\nFrancisco Massa\\nAlexandre D\\u00e9fossez\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08553\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Nov 2022 22:48:16 GMT)\\u00a7r"}']}
{title:'Hsu et al. (§72022§r)', author: 'Yicheng Hsu; Yonghan Lee; Mingsian R. Bai', display:{Lore:['[{"text": "arXiv:2211.08748", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lArray Configuration-Agnostic Personalized Speech Enhancement using Long-Short-Term Spatial Coherence\\u00a7r\\n\\n\\u00a78\\u00a7oYicheng Hsu\\nYonghan Lee\\nMingsian R. Bai\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08748\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Nov 2022 08:15:56 GMT)\\u00a7r"}']}
{title:'R et al. (§72022§r)', author: 'Gowriprasad R; R Aravind; Hema A Murthy', display:{Lore:['[{"text": "arXiv:2211.08790", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStructural Segmentation and Labeling of Tabla Solo Performances\\u00a7r\\n\\n\\u00a78\\u00a7oGowriprasad R\\nR Aravind\\nHema A Murthy\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08790\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Nov 2022 09:34:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o35 pages, 11 figures\\u00a7r"}']}
{title:'Schu et al. (§72022§r)', author: 'Guilherme Schu; Parvaneh Janbakhshi; Ina Kodrasi', display:{Lore:['[{"text": "arXiv:2211.08833", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn using the UA-Speech and TORGO databases to validate automatic dysarthric speech classification approaches\\u00a7r\\n\\n\\u00a78\\u00a7oGuilherme Schu\\nParvaneh Janbakhshi\\nIna Kodrasi\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08833\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Nov 2022 11:16:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Tomczak et al. (§72022§r)', author: 'Maciej Tomczak; Min Susan Li; Adrian Bradbury; Mark Elliott; Ryan Stables; Maria Witek; Tom Goodman; Diar Abdlkarim; Massimiliano Di Luca; Alan Wing; Jason Hockman', display:{Lore:['[{"text": "arXiv:2211.08848", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnnotation of Soft Onsets in String Ensemble Recordings\\u00a7r\\n\\n\\u00a78\\u00a7oMaciej Tomczak\\nMin Susan Li\\nAdrian Bradbury\\n+ 7 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08848\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Nov 2022 11:46:34 GMT)\\u00a7r"}']}
{title:'Bannò et al. (§72022§r)', author: 'Stefano Bannò; Kate M. Knill; Marco Matassoni; Vyas Raina; Mark J. F. Gales', display:{Lore:['[{"text": "arXiv:2211.08849", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lL2 proficiency assessment using self-supervised speech representations\\u00a7r\\n\\n\\u00a78\\u00a7oStefano Bann\\u00f2\\nKate M. Knill\\nMarco Matassoni\\nVyas Raina\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08849\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Nov 2022 11:47:20 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Yujie Yang; Changsheng Quan; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2211.08872", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMcNet: Fuse Multiple Cues for Multichannel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oYujie Yang\\nChangsheng Quan\\nXiaofei Li\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08872\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Nov 2022 12:25:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to icassp 2023\\u00a7r"}']}
{title:'Ansari et al. (§72022§r)', author: 'Md. Istiaq Ansari; Taufiq Hasan', display:{Lore:['[{"text": "arXiv:2211.09352", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpectNet : End-to-End Audio Signal Classification Using Learnable Spectrograms\\u00a7r\\n\\n\\u00a78\\u00a7oMd. Istiaq Ansari\\nTaufiq Hasan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.09352\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Nov 2022 05:29:03 GMT)\\u00a7r"}']}
{title:'Yazdani et al. (§72022§r)', author: 'Ali Yazdani; Yasser Shekofteh', display:{Lore:['[{"text": "arXiv:2211.09956", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Persian ASR-based SER: Modification of Sharif Emotional Speech Database and Investigation of Persian Text Corpora\\u00a7r\\n\\n\\u00a78\\u00a7oAli Yazdani\\nYasser Shekofteh\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.09956\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Nov 2022 10:33:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 4 figures, 8 tables\\u00a7r"}']}
{title:'Song et al. (§72022§r)', author: 'Hyungchan Song; Sanyuan Chen; Zhuo Chen; Yu Wu; Takuya Yoshioka; Min Tang; Jong Won Shin; Shujie Liu', display:{Lore:['[{"text": "arXiv:2211.09988", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring WavLM on Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oHyungchan Song\\nSanyuan Chen\\nZhuo Chen\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.09988\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Nov 2022 02:23:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE SLT 2022\\u00a7r"}']}
{title:'Yoshimura et al. (§72022§r)', author: 'Takenori Yoshimura; Shinji Takaki; Kazuhiro Nakamura; Keiichiro Oura; Yukiya Hono; Kei Hashimoto; Yoshihiko Nankaku; Keiichi Tokuda', display:{Lore:['[{"text": "arXiv:2211.11222", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmbedding a Differentiable Mel-cepstral Synthesis Filter to a Neural Speech Synthesis System\\u00a7r\\n\\n\\u00a78\\u00a7oTakenori Yoshimura\\nShinji Takaki\\nKazuhiro Nakamura\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.11222\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Nov 2022 07:35:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Ge et al. (§72022§r)', author: 'Xiaofeng Ge; Jiangyu Han; Haixin Guan; Yanhua Long', display:{Lore:['[{"text": "arXiv:2211.12097", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDynamic Acoustic Compensation and Adaptive Focal Training for Personalized Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oXiaofeng Ge\\nJiangyu Han\\nHaixin Guan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.12097\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Nov 2022 08:58:23 GMT)\\u00a7r"}']}
{title:'Guo et al. (§72022§r)', author: 'Zhifang Guo; Yichong Leng; Yihan Wu; Sheng Zhao; Xu Tan', display:{Lore:['[{"text": "arXiv:2211.12171", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPromptTTS: Controllable Text-to-Speech with Text Descriptions\\u00a7r\\n\\n\\u00a78\\u00a7oZhifang Guo\\nYichong Leng\\nYihan Wu\\nSheng Zhao\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.12171\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Nov 2022 10:58:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Liu et al. (§72022§r)', author: 'Haohe Liu; Qiuqiang Kong; Xubo Liu; Xinhao Mei; Wenwu Wang; Mark D. Plumbley', display:{Lore:['[{"text": "arXiv:2211.12195", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOntology-aware Learning and Evaluation for Audio Tagging\\u00a7r\\n\\n\\u00a78\\u00a7oHaohe Liu\\nQiuqiang Kong\\nXubo Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.12195\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-979\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Nov 2022 11:35:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023. The code is open-sourced at https://github.com/haoheliu/ontology-aware-audio-tagging\\u00a7r"}']}
{title:'Kothapally et al. (§72022§r)', author: 'Vinay Kothapally; J. H. L. Hansen', display:{Lore:['[{"text": "arXiv:2211.12623", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSkipConvGAN: Monaural Speech Dereverberation using Generative Adversarial Networks via Complex Time-Frequency Masking\\u00a7r\\n\\n\\u00a78\\u00a7oVinay Kothapally\\nJ. H. L. Hansen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.12623\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2022.3155286\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Nov 2022 23:02:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in: IEEE/ACM Transactionson Audio, Speech, and Language Processing ( Volume: 30)\\u00a7r"}']}
{title:'Kothapally et al. (§72022§r)', author: 'Vinay Kothapally; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:2211.12632", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComplex-Valued Time-Frequency Self-Attention for Speech Dereverberation\\u00a7r\\n\\n\\u00a78\\u00a7oVinay Kothapally\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.12632\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-11277\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Nov 2022 23:38:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2022: ISCA Best StudentPaper Award Finalist\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Jiacheng Zhang; Wenyi Yan; Ye Zhang', display:{Lore:['[{"text": "arXiv:2211.13377", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA new Speech Feature Fusion method with cross gate parallel CNN for Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJiacheng Zhang\\nWenyi Yan\\nYe Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.13377\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Nov 2022 02:17:43 GMT)\\u00a7r"}']}
{title:'da Silva et al. (§72022§r)', author: 'Daniel Peixoto Pinto da Silva; Edresson Casanova; Lucas Rafael Stefanel Gris; Arnaldo Candido Junior; Marcelo Finger; Flaviane Svartman; Beatriz Raposo; Marcus Vinícius Moreira Martins; Sandra Maria Aluísio; Larissa Cristina Berti; João Paulo Teixeira', display:{Lore:['[{"text": "arXiv:2211.14372", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInterpretability Analysis of Deep Models for COVID-19 Detection\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Peixoto Pinto da Silva\\nEdresson Casanova\\nLucas Rafael Stefanel Gris\\n+ 7 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.14372\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Nov 2022 20:56:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages, 4 figures\\u00a7r"}']}
{title:'Master et al. (§72022§r)', author: 'Aaron Master; Lie Lu; Nathan Swedlow', display:{Lore:['[{"text": "arXiv:2211.14378", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStereo Speech Enhancement Using Custom Mid-Side Signals and Monaural Processing\\u00a7r\\n\\n\\u00a78\\u00a7oAaron Master\\nLie Lu\\nNathan Swedlow\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.14378\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Nov 2022 21:28:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 5 figures. Submitted to the Journal of the Audio Engineering Society\\u00a7r"}']}
{title:'Tu et al. (§72022§r)', author: 'Jianhong Tu; Zeyu Cui; Xiaohuan Zhou; Siqi Zheng; Kai Hu; Ju Fan; Chang Zhou', display:{Lore:['[{"text": "arXiv:2211.14548", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContextual Expressive Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oJianhong Tu\\nZeyu Cui\\nXiaohuan Zhou\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.14548\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 26 Nov 2022 12:06:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Yoon et al. (§72022§r)', author: 'Ji Won Yoon; Beom Jun Woo; Sunghwan Ahn; Hyeonseung Lee; Nam Soo Kim', display:{Lore:['[{"text": "arXiv:2211.15075", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInter-KD: Intermediate Knowledge Distillation for CTC-Based Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJi Won Yoon\\nBeom Jun Woo\\nSunghwan Ahn\\nHyeonseung Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.15075\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Nov 2022 05:23:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by 2022 SLT Workshop\\u00a7r"}']}
{title:'Chandramouli et al. (§72022§r)', author: 'Kausthubh Chandramouli; William Sethares', display:{Lore:['[{"text": "arXiv:2211.15185", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Transcription of Drum Strokes in Carnatic Music\\u00a7r\\n\\n\\u00a78\\u00a7oKausthubh Chandramouli\\nWilliam Sethares\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.15185\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Nov 2022 09:50:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 9 figures\\u00a7r"}']}
{title:'Ma et al. (§72022§r)', author: 'Yinghao Ma; Richard M. Stern', display:{Lore:['[{"text": "arXiv:2211.15254", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearnable Front Ends Based on Temporal Modulation for Music Tagging\\u00a7r\\n\\n\\u00a78\\u00a7oYinghao Ma\\nRichard M. Stern\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.15254\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Nov 2022 12:17:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Marták et al. (§72022§r)', author: 'Lukáš Samuel Marták; Rainer Kelz; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2211.15439", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProbabilistic Modelling of Signal Mixtures with Differentiable Dictionaries\\u00a7r\\n\\n\\u00a78\\u00a7oLuk\\u00e1\\u0161 Samuel Mart\\u00e1k\\nRainer Kelz\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.15439\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.23919/EUSIPCO54536.2021.9616145\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Nov 2022 15:27:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in the Proceedings of the 29th European Signal Processing Conference (EUSIPCO 2021), Dublin, Ireland, August 23-27, 2021 (IEEE), 441-445\\u00a7r"}']}
{title:'Marták et al. (§72022§r)', author: 'Lukáš Samuel Marták; Rainer Kelz; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2211.15524", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDifferentiable Dictionary Search: Integrating Linear Mixing with Deep Non-Linear Modelling for Audio Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oLuk\\u00e1\\u0161 Samuel Mart\\u00e1k\\nRainer Kelz\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.15524\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Nov 2022 16:37:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in the Proceedings of the 24th International Congress on Acoustics (ICA 2022), Gyeongju, Korea, October 24-28, 2022\\u00a7r"}']}
{title:'Hamed et al. (§72022§r)', author: 'Injy Hamed; Amir Hussein; Oumnia Chellah; Shammur Chowdhury; Hamdy Mubarak; Sunayana Sitaram; Nizar Habash; Ahmed Ali', display:{Lore:['[{"text": "arXiv:2211.16319", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBenchmarking Evaluation Metrics for Code-Switching Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oInjy Hamed\\nAmir Hussein\\nOumnia Chellah\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.16319\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Nov 2022 08:14:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to SLT 2022\\u00a7r"}']}
{title:'Singh et al. (§72022§r)', author: 'Premjeet Singh; Shefali Waldekar; Md Sahidullah; Goutam Saha', display:{Lore:['[{"text": "arXiv:2211.16363", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalysis of constant-Q filterbank based representations for speech emotion recognition\\u00a7r\\n\\n\\u00a78\\u00a7oPremjeet Singh\\nShefali Waldekar\\nMd Sahidullah\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.16363\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.dsp.2022.103712\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nVolume 130, October 2022, 103712\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Nov 2022 16:45:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in Elsevier\'sDigital Signal Processing Journal\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Yue Li; Li Zhang; Namin Wang; Jie Liu; Lei Xie', display:{Lore:['[{"text": "arXiv:2211.16694", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMSV Challenge 2022: NPU-HC Speaker Verification System for Low-resource Indian Languages\\u00a7r\\n\\n\\u00a78\\u00a7oYue Li\\nLi Zhang\\nNamin Wang\\nJie Liu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.16694\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 2 Dec 2022 04:46:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6pages, submitted to the 9th International Workshopon Vietnamese Language and Speech Processing\\u00a7r"}']}
{title:'Choi et al. (§72022§r)', author: 'Byoung Jin Choi; Myeonghun Jeong; Joun Yeop Lee; Nam Soo Kim', display:{Lore:['[{"text": "arXiv:2211.16866", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSNAC: Speaker-normalized affine coupling layer in flow-based architecture for zero-shot multi-speaker text-to-speech\\u00a7r\\n\\n\\u00a78\\u00a7oByoung Jin Choi\\nMyeonghun Jeong\\nJoun Yeop Lee\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.16866\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2022.3226655\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Nov 2022 10:07:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEESignal Processing Letters\\u00a7r"}']}
{title:'Fierro et al. (§72022§r)', author: 'Leonardo Fierro; Alec Wright; Vesa Välimäki; Matti Hämäläinen', display:{Lore:['[{"text": "arXiv:2211.16992", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExtreme Audio Time Stretching Using Neural Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oLeonardo Fierro\\nAlec Wright\\nVesa V\\u00e4lim\\u00e4ki\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.16992\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Nov 2022 13:47:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEEInternational Conference on Acoustics, Speech and Signal Processing (ICASSP) 2023 on Oct 27, 2022\\u00a7r"}']}
{title:'Saadany et al. (§72022§r)', author: 'Hadeel Saadany; Catherine Breslin; Constantin Orăsan; Sophie Walker', display:{Lore:['[{"text": "arXiv:2211.17094", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBetter Transcription of UK Supreme Court Hearings\\u00a7r\\n\\n\\u00a78\\u00a7oHadeel Saadany\\nCatherine Breslin\\nConstantin Or\\u0103san\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.17094\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 22 Dec 2022 13:51:09 GMT)\\u00a7r"}']}
{title:'Fejgin et al. (§72022§r)', author: 'Daniel Fejgin; Simon Doclo', display:{Lore:['[{"text": "arXiv:2211.17202", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAssisted RTF-Vector-Based Binaural Direction of Arrival Estimation Exploiting a Calibrated External Microphone Array\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Fejgin\\nSimon Doclo\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.17202\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Nov 2022 17:51:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Ma et al. (§72022§r)', author: 'Biao Ma; Chengben Xu; Ye Zhang', display:{Lore:['[{"text": "arXiv:2212.00329", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Novel Speech Feature Fusion Algorithm for Text-Independent Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oBiao Ma\\nChengben Xu\\nYe Zhang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.00329\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Dec 2022 07:28:06 GMT)\\u00a7r"}']}
{title:'Xu et al. (§72022§r)', author: 'Xinmeng Xu; Weiping Tu; Yuhong Yang', display:{Lore:['[{"text": "arXiv:2212.01012", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInjecting Spatial Information for Monaural Speech Enhancement via Knowledge Distillation\\u00a7r\\n\\n\\u00a78\\u00a7oXinmeng Xu\\nWeiping Tu\\nYuhong Yang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.01012\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Dec 2022 07:49:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Leang et al. (§72022§r)', author: 'Sotheara Leang; Eric Castelli; Dominique Vaufreydaz; Sethserey Sam', display:{Lore:['[{"text": "arXiv:2212.01245", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPreliminary Study on SSCF-derived Polar Coordinate for ASR\\u00a7r\\n\\n\\u00a78\\u00a7oSotheara Leang\\nEric Castelli\\nDominique Vaufreydaz\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.01245\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nACET 2022, Dec 2022, Phnom Penh, Cambodia\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Nov 2022 14:57:28 GMT)\\u00a7r"}']}
{title:'Nespoli et al. (§72022§r)', author: 'Francesco Nespoli; Daniel Barreda; Patrick A. Naylor', display:{Lore:['[{"text": "arXiv:2212.01306", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRelative Acoustic Features for Distance Estimation in Smart-Homes\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesco Nespoli\\nDaniel Barreda\\nPatrick A. Naylor\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.01306\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInterspeech 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Dec 2022 16:58:35 GMT)\\u00a7r"}']}
{title:'Diwan et al. (§72022§r)', author: 'Anuj Diwan; Ching-Feng Yeh; Wei-Ning Hsu; Paden Tomasello; Eunsol Choi; David Harwath; Abdelrahman Mohamed', display:{Lore:['[{"text": "arXiv:2212.01393", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContinual Learning for On-Device Speech Recognition using Disentangled Conformers\\u00a7r\\n\\n\\u00a78\\u00a7oAnuj Diwan\\nChing-Feng Yeh\\nWei-Ning Hsu\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.01393\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10095484\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 13 Dec 2022 07:33:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 2 figures. Submitted to ICASSP 2023\\u00a7r"}']}
{title:'Delgado et al. (§72022§r)', author: 'Pablo M. Delgado; Jürgen Herre', display:{Lore:['[{"text": "arXiv:2212.01427", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigations on the Influence of Combined Inter-Aural Cue Distortions on Overall Audio Quality\\u00a7r\\n\\n\\u00a78\\u00a7oPablo M. Delgado\\nJ\\u00fcrgen Herre\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.01427\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nTagungsband - DAGA 2019 - 45. Jahrestagung f\\\\\\"ur Akustik\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Dec 2022 20:20:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oA previous version of this paper (minus errata) was presented at Fortschritte der Akustik - DAGA 2019 (Rostock, Germany)\\u00a7r"}']}
{title:'Delgado et al. (§72022§r)', author: 'Pablo M. Delgado; Jürgen Herre', display:{Lore:['[{"text": "arXiv:2212.01451", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lObjective Assessment of Spatial Audio Quality using Directional Loudness Maps\\u00a7r\\n\\n\\u00a78\\u00a7oPablo M. Delgado\\nJ\\u00fcrgen Herre\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.01451\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP.2019.8683810\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2019 - 2019 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Dec 2022 21:22:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted paper at ICASSP 2019\\u00a7r"}']}
{title:'Delgado et al. (§72022§r)', author: 'Pablo M. Delgado; Jürgen Herre', display:{Lore:['[{"text": "arXiv:2212.01467", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCan we still use PEAQ? A Performance Analysis of the ITU Standard for the Objective Assessment of Perceived Audio Quality\\u00a7r\\n\\n\\u00a78\\u00a7oPablo M. Delgado\\nJ\\u00fcrgen Herre\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.01467\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/QoMEX48832.2020.9123105\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2020 Twelfth International Conference on Quality of Multimedia\\n  Experience (QoMEX), 2020, pp. 1-6,\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Dec 2022 22:09:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepter manuscript for 2020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX 2020)\\u00a7r"}']}
{title:'Gody et al. (§72022§r)', author: 'Reem Gody; David Harwath', display:{Lore:['[{"text": "arXiv:2212.01661", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Fine-Tuning Data Selection for ASR Using Self-Supervised Speech Models\\u00a7r\\n\\n\\u00a78\\u00a7oReem Gody\\nDavid Harwath\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.01661\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 3 Dec 2022 18:05:08 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Yuhao Zhang; Chen Xu; Bojie Hu; Chunliang Zhang; Tong Xiao; Jingbo Zhu', display:{Lore:['[{"text": "arXiv:2212.01778", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving End-to-end Speech Translation by Leveraging Auxiliary Speech and Text Data\\u00a7r\\n\\n\\u00a78\\u00a7oYuhao Zhang\\nChen Xu\\nBojie Hu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.01778\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 4 Dec 2022 09:27:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to AAAI2023\\u00a7r"}']}
{title:'Berghi et al. (§72022§r)', author: 'Davide Berghi; Marco Volino; Philip J. B. Jackson', display:{Lore:['[{"text": "arXiv:2212.01892", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTragic Talkers: A Shakespearean Sound- and Light-Field Dataset for Audio-Visual Machine Learning Research\\u00a7r\\n\\n\\u00a78\\u00a7oDavide Berghi\\nMarco Volino\\nPhilip J. B. Jackson\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.01892\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3565516.3565522\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 4 Dec 2022 18:48:44 GMT)\\u00a7r"}']}
{title:'Reddy et al. (§72022§r)', author: 'Tadipatri Uday Kiran Reddy; Sahukari Chaitanya Varun; Kota Pranav Kumar Sankala Sreekanth; Kodukula Sri Rama Murty', display:{Lore:['[{"text": "arXiv:2212.02013", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvince the artifacts of Spoof Speech by blending Vocal Tract and Voice Source Features\\u00a7r\\n\\n\\u00a78\\u00a7oTadipatri Uday Kiran Reddy\\nSahukari Chaitanya Varun\\nKota Pranav Kumar Sankala Sreekanth\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.02013\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Dec 2022 04:02:50 GMT)\\u00a7r"}']}
{title:'Mei et al. (§72022§r)', author: 'Xinhao Mei; Xubo Liu; Jianyuan Sun; Mark D. Plumbley; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2212.02033", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Generating Diverse Audio Captions via Adversarial Training\\u00a7r\\n\\n\\u00a78\\u00a7oXinhao Mei\\nXubo Liu\\nJianyuan Sun\\nMark D. Plumbley\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.02033\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Dec 2022 05:06:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to TASLP\\u00a7r"}']}
{title:'Yang et al. (§72022§r)', author: 'Yuguang Yang; Yu Pan; Jingjing Yin; Heng Lu', display:{Lore:['[{"text": "arXiv:2212.02099", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLMEC: Learnable Multiplicative Absolute Position Embedding Based Conformer for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYuguang Yang\\nYu Pan\\nJingjing Yin\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.02099\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Dec 2022 08:36:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oNCMMSC2022\\u00a7r"}']}
{title:'Zhang et al. (§72022§r)', author: 'Jing-Xuan Zhang; Genshun Wan; Zhen-Hua Ling; Jia Pan; Jianqing Gao; Cong Liu', display:{Lore:['[{"text": "arXiv:2212.02782", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Audio-Visual Speech Representations Learning By Multimodal Self-Distillation\\u00a7r\\n\\n\\u00a78\\u00a7oJing-Xuan Zhang\\nGenshun Wan\\nZhen-Hua Ling\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.02782\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Dec 2022 06:37:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Polvani et al. (§72022§r)', author: "Niccolo' Polvani; Damien Ronssin; Milos Cernak", display:{Lore:['[{"text": "arXiv:2212.02996", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBC-VAD: A Robust Bone Conduction Voice Activity Detection\\u00a7r\\n\\n\\u00a78\\u00a7oNiccolo\' Polvani\\nDamien Ronssin\\nMilos Cernak\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.02996\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Dec 2022 14:14:00 GMT)\\u00a7r"}']}
{title:'Tan et al. (§72022§r)', author: 'Daxin Tan; Nikos Kargas; David McHardy; Constantinos Papayiannis; Antonio Bonafonte; Marek Strelec; Jonas Rohnke; Agis Oikonomou Filandras; Trevor Wood', display:{Lore:['[{"text": "arXiv:2212.03398", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalysis and Utilization of Entrainment on Acoustic and Emotion Features in User-agent Dialogue\\u00a7r\\n\\n\\u00a78\\u00a7oDaxin Tan\\nNikos Kargas\\nDavid McHardy\\n+ 5 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.03398\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Dec 2022 01:45:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis version has been removed by arXiv administrators because the submitter did not have the right toassign a license at the time of submission\\u00a7r"}']}
{title:'Fu et al. (§72022§r)', author: 'Yanjie Fu; Haoran Yin; Meng Ge; Longbiao Wang; Gaoyan Zhang; Jianwu Dang; Chengyun Deng; Fei Wang', display:{Lore:['[{"text": "arXiv:2212.03401", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMIMO-DBnet: Multi-channel Input and Multiple Outputs DOA-aware Beamforming Network for Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oYanjie Fu\\nHaoran Yin\\nMeng Ge\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.03401\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Dec 2022 01:52:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Pandey et al. (§72022§r)', author: 'Ruchi Pandey; Shreyas Jaiswal; Huy Phan; Santosh Nannuru', display:{Lore:['[{"text": "arXiv:2212.03470", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving trajectory localization accuracy via direction-of-arrival derivative estimation\\u00a7r\\n\\n\\u00a78\\u00a7oRuchi Pandey\\nShreyas Jaiswal\\nHuy Phan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.03470\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 10 Dec 2022 12:48:38 GMT)\\u00a7r"}']}
{title:'Ding et al. (§72022§r)', author: 'Fenglin Ding; Genshun Wan; Pengcheng Li; Jia Pan; Cong Liu', display:{Lore:['[{"text": "arXiv:2212.03476", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Self-Supervised Multilingual Speech Representation Learning Combined with Auxiliary Language Information\\u00a7r\\n\\n\\u00a78\\u00a7oFenglin Ding\\nGenshun Wan\\nPengcheng Li\\nJia Pan\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.03476\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Dec 2022 06:18:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubimitted to ICASSP 2023\\u00a7r"}']}
{title:'Wan et al. (§72022§r)', author: 'Genshun Wan; Tan Liu; Hang Chen; Jia Pan; Cong Liu; Zhongfu Ye', display:{Lore:['[{"text": "arXiv:2212.03480", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProgressive Multi-Scale Self-Supervised Learning for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGenshun Wan\\nTan Liu\\nHang Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.03480\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Dec 2022 06:29:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Pengcheng Li; Genshun Wan; Fenglin Ding; Hang Chen; Jianqing Gao; Jia Pan; Cong Liu', display:{Lore:['[{"text": "arXiv:2212.03482", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Speech Pre-Training with Supervision-Enhanced Acoustic Unit\\u00a7r\\n\\n\\u00a78\\u00a7oPengcheng Li\\nGenshun Wan\\nFenglin Ding\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.03482\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Dec 2022 06:31:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Pauwels et al. (§72022§r)', author: 'Johan Pauwels; Lorenzo Picinali', display:{Lore:['[{"text": "arXiv:2212.04283", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn The Relevance Of The Differences Between HRTF Measurement Setups For Machine Learning\\u00a7r\\n\\n\\u00a78\\u00a7oJohan Pauwels\\nLorenzo Picinali\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.04283\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Dec 2022 14:19:46 GMT)\\u00a7r"}']}
{title:'Radford et al. (§72022§r)', author: 'Alec Radford; Jong Wook Kim; Tao Xu; Greg Brockman; Christine McLeavey; Ilya Sutskever', display:{Lore:['[{"text": "arXiv:2212.04356", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Speech Recognition via Large-Scale Weak Supervision\\u00a7r\\n\\n\\u00a78\\u00a7oAlec Radford\\nJong Wook Kim\\nTao Xu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.04356\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Dec 2022 18:46:04 GMT)\\u00a7r"}']}
{title:'Maiti et al. (§72022§r)', author: 'Soumi Maiti; Yifan Peng; Takaaki Saeki; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2212.04559", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeechLMScore: Evaluating speech generation using speech language model\\u00a7r\\n\\n\\u00a78\\u00a7oSoumi Maiti\\nYifan Peng\\nTakaaki Saeki\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.04559\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Dec 2022 21:00:15 GMT)\\u00a7r"}']}
{title:'Delgado et al. (§72022§r)', author: 'Pablo M. Delgado; Jürgen Herre', display:{Lore:['[{"text": "arXiv:2212.04572", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Data-driven Cognitive Salience Model for Objective Perceptual Audio Quality Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oPablo M. Delgado\\nJ\\u00fcrgen Herre\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.04572\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747064.\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2022 - 2022 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP), 2022, pp. 986-990\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Dec 2022 21:40:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted version of the paper submitted to ICASSP 2020\\u00a7r"}']}
{title:'Davidson et al. (§72022§r)', author: 'Grant Davidson; Mark Vinton; Per Ekstrand; Cong Zhou; Lars Villemoes; Lie Lu', display:{Lore:['[{"text": "arXiv:2212.04583", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHigh Quality Audio Coding with MDCTNet\\u00a7r\\n\\n\\u00a78\\u00a7oGrant Davidson\\nMark Vinton\\nPer Ekstrand\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.04583\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Dec 2022 22:18:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oFive pages, five figures\\u00a7r"}']}
{title:'Kowalk et al. (§72022§r)', author: 'Ulrik Kowalk; Simon Doclo; Joerg Bitzer', display:{Lore:['[{"text": "arXiv:2212.04788", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeometry-aware DoA Estimation using a Deep Neural Network with mixed-data input features\\u00a7r\\n\\n\\u00a78\\u00a7oUlrik Kowalk\\nSimon Doclo\\nJoerg Bitzer\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.04788\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Dec 2022 11:41:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Kawamura et al. (§72022§r)', author: 'Kazuki Kawamura; Jun Rekimoto', display:{Lore:['[{"text": "arXiv:2212.04930", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDDSupport: Language Learning Support System that Displays Differences and Distances from Model Speech\\u00a7r\\n\\n\\u00a78\\u00a7oKazuki Kawamura\\nJun Rekimoto\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.04930\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2022 21st IEEE International Conference on Machine Learning and\\n  Applications (ICMLA)\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Dec 2022 05:49:15 GMT)\\u00a7r"}']}
{title:'Petermann et al. (§72022§r)', author: 'Darius Petermann; Gordon Wichern; Aswin Subramanian; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2212.05008", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHyperbolic Audio Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oDarius Petermann\\nGordon Wichern\\nAswin Subramanian\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.05008\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Dec 2022 17:47:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023, Demo page: https://darius522.github.io/hyperbolic-audio-sep/\\u00a7r"}']}
{title:'Aralikatti et al. (§72022§r)', author: 'Rohith Aralikatti; Zhenyu Tang; Dinesh Manocha', display:{Lore:['[{"text": "arXiv:2212.05360", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynthetic Wave-Geometric Impulse Responses for Improved Speech Dereverberation\\u00a7r\\n\\n\\u00a78\\u00a7oRohith Aralikatti\\nZhenyu Tang\\nDinesh Manocha\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.05360\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 10 Dec 2022 20:15:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Zhao et al. (§72022§r)', author: 'Zifeng Zhao; Ding Pan; Junyi Peng; Rongzhi Gu', display:{Lore:['[{"text": "arXiv:2212.07068", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProbing Deep Speaker Embeddings for Speaker-related Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oZifeng Zhao\\nDing Pan\\nJunyi Peng\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.07068\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Dec 2022 07:33:36 GMT)\\u00a7r"}']}
{title:'Petermann et al. (§72022§r)', author: 'Darius Petermann; Gordon Wichern; Aswin Shanmugam Subramanian; Zhong-Qiu Wang; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2212.07327", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTackling the Cocktail Fork Problem for Separation and Transcription of Real-World Soundtracks\\u00a7r\\n\\n\\u00a78\\u00a7oDarius Petermann\\nGordon Wichern\\nAswin Shanmugam Subramanian\\nZhong-Qiu Wang\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.07327\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Dec 2022 16:47:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEETASLP (In review), 13 pages, 6 figures\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Ke Li; Jay Mahadeokar; Jinxi Guo; Yangyang Shi; Gil Keren; Ozlem Kalinli; Michael L. Seltzer; Duc Le', display:{Lore:['[{"text": "arXiv:2212.07650", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Fast-slow Encoder based Transducer with Streaming Deliberation\\u00a7r\\n\\n\\u00a78\\u00a7oKe Li\\nJay Mahadeokar\\nJinxi Guo\\n+ 4 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.07650\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Dec 2022 08:16:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Yasuda et al. (§72022§r)', author: 'Yusuke Yasuda; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2212.08321", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigation of Japanese PnG BERT language model in text-to-speech synthesis for pitch accent language\\u00a7r\\n\\n\\u00a78\\u00a7oYusuke Yasuda\\nTomoki Toda\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.08321\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JSTSP.2022.3190672\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Journal of Selected Topics in Signal Processing (Volume: 16,\\n  Issue: 6, October 2022)\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 16 Dec 2022 07:47:03 GMT)\\u00a7r"}']}
{title:'Yasuda et al. (§72022§r)', author: 'Yusuke Yasuda; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2212.08329", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lText-to-speech synthesis based on latent variable conversion using diffusion probabilistic model and variational autoencoder\\u00a7r\\n\\n\\u00a78\\u00a7oYusuke Yasuda\\nTomoki Toda\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.08329\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 16 Dec 2022 08:14:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Laptev et al. (§72022§r)', author: 'Aleksandr Laptev; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2212.08703", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.IT\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a72math.IT\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast Entropy-Based Methods of Word-Level Confidence Estimation for End-To-End Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAleksandr Laptev\\nBoris Ginsburg\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.08703\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/SLT54892.2023.10022960\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 16 Dec 2022 20:27:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proc. SLT 2022, Jan 09-12, 2023, Doha, Qatar. 8 pages, 4 figures, 4 tables\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Sanyuan Chen; Yu Wu; Chengyi Wang; Shujie Liu; Daniel Tompkins; Zhuo Chen; Furu Wei', display:{Lore:['[{"text": "arXiv:2212.09058", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBEATs: Audio Pre-Training with Acoustic Tokenizers\\u00a7r\\n\\n\\u00a78\\u00a7oSanyuan Chen\\nYu Wu\\nChengyi Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.09058\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 18 Dec 2022 10:41:55 GMT)\\u00a7r"}']}
{title:'Julião et al. (§72022§r)', author: 'Mariana Julião; Alberto Abad; Helena Moniz', display:{Lore:['[{"text": "arXiv:2212.10201", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Simple Feature Method for Prosody Rhythm Comparison\\u00a7r\\n\\n\\u00a78\\u00a7oMariana Juli\\u00e3o\\nAlberto Abad\\nHelena Moniz\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.10201\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Dec 2022 12:26:44 GMT)\\u00a7r"}']}
{title:'Zhou et al. (§72022§r)', author: 'Yi Zhou; Zhizheng Wu; Mingyang Zhang; Xiaohai Tian; Haizhou Li', display:{Lore:['[{"text": "arXiv:2212.10204", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTTS-Guided Training for Accent Conversion Without Parallel Data\\u00a7r\\n\\n\\u00a78\\u00a7oYi Zhou\\nZhizheng Wu\\nMingyang Zhang\\nXiaohai Tian\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.10204\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2023.3270079\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Dec 2022 12:33:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, submitted to signal processing letter\\u00a7r"}']}
{title:'Hsu et al. (§72022§r)', author: 'Wei-Ning Hsu; Tal Remez; Bowen Shi; Jacob Donley; Yossi Adi', display:{Lore:['[{"text": "arXiv:2212.11377", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReVISE: Self-Supervised Speech Resynthesis with Visual Input for Universal and Generalized Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oWei-Ning Hsu\\nTal Remez\\nBowen Shi\\nJacob Donley\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.11377\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Dec 2022 21:36:52 GMT)\\u00a7r"}']}
{title:'Gupta et al. (§72022§r)', author: 'Ishika Gupta; Anusha Prakash; Jom Kuriakose; Hema A. Murthy', display:{Lore:['[{"text": "arXiv:2212.11982", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHMM-based data augmentation for E2E systems for building conversational speech synthesis systems\\u00a7r\\n\\n\\u00a78\\u00a7oIshika Gupta\\nAnusha Prakash\\nJom Kuriakose\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.11982\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Dec 2022 18:59:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 7 figures, 33 references\\u00a7r"}']}
{title:'Kondratenko et al. (§72022§r)', author: 'Vladimir Kondratenko; Artem Sokolov; Nikolay Karpov; Oleg Kutuzov; Nikita Savushkin; Fyodor Minkin', display:{Lore:['[{"text": "arXiv:2212.12266", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLarge Raw Emotional Dataset with Aggregation Mechanism\\u00a7r\\n\\n\\u00a78\\u00a7oVladimir Kondratenko\\nArtem Sokolov\\nNikolay Karpov\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.12266\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Dec 2022 11:31:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 1 figures, submitted to ICASSP 2023\\u00a7r"}']}
{title:'Wang et al. (§72022§r)', author: 'Lijun Wang; Suradej Duangpummet; Masashi Unoki', display:{Lore:['[{"text": "arXiv:2212.13009", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBlind estimation of room acoustic parameters from speech signals based on extended model of room impulse response\\u00a7r\\n\\n\\u00a78\\u00a7oLijun Wang\\nSuradej Duangpummet\\nMasashi Unoki\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.13009\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Dec 2022 04:28:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5-pages, 3 figures, 2 tables\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Tianyou Li; Hongji Duan; Sipei Zhao; Jing Lu; Ian S. Burnett', display:{Lore:['[{"text": "arXiv:2212.13777", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDistributed Active Noise Control System Based on a Block Diffusion FxLMS Algorithm with Bidirectional Communication\\u00a7r\\n\\n\\u00a78\\u00a7oTianyou Li\\nHongji Duan\\nSipei Zhao\\nJing Lu\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.13777\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Dec 2022 10:39:16 GMT)\\u00a7r"}']}
{title:'Li et al. (§72022§r)', author: 'Yinghao Aaron Li; Cong Han; Nima Mesgarani', display:{Lore:['[{"text": "arXiv:2212.14227", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStyleTTS-VC: One-Shot Voice Conversion by Knowledge Transfer from Style-Based TTS Models\\u00a7r\\n\\n\\u00a78\\u00a7oYinghao Aaron Li\\nCong Han\\nNima Mesgarani\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.14227\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 29 Dec 2022 08:56:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSLT 2022\\u00a7r"}']}
{title:'Chen et al. (§72022§r)', author: 'Zehua Chen; Yihan Wu; Yichong Leng; Jiawei Chen; Haohe Liu; Xu Tan; Yang Cui; Ke Wang; Lei He; Sheng Zhao; Jiang Bian; Danilo Mandic', display:{Lore:['[{"text": "arXiv:2212.14518", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lResGrad: Residual Denoising Diffusion Probabilistic Models for Text to Speech\\u00a7r\\n\\n\\u00a78\\u00a7oZehua Chen\\nYihan Wu\\nYichong Leng\\n+ 8 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.14518\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Dec 2022 02:31:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 5 figures\\u00a7r"}']}
{title:'Piya et al. (§72022§r)', author: 'Kashav Piya; Srijal Shrestha; Cameran Frank; Estephanos Jebessa; Tauheed Khan Mohd', display:{Lore:['[{"text": "arXiv:2301.00646", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MA\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAddressing the Selection Bias in Voice Assistance: Training Voice Assistance Model in Python with Equal Data Selection\\u00a7r\\n\\n\\u00a78\\u00a7oKashav Piya\\nSrijal Shrestha\\nCameran Frank\\nEstephanos Jebessa\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.00646\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Dec 2022 21:26:05 GMT)\\u00a7r"}']}
{title:'Yeh et al. (§72022§r)', author: 'Ching-Feng Yeh; Wei-Ning Hsu; Paden Tomasello; Abdelrahman Mohamed', display:{Lore:['[{"text": "arXiv:2301.00652", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Speech Representation Learning with Low-Bit Quantization\\u00a7r\\n\\n\\u00a78\\u00a7oChing-Feng Yeh\\nWei-Ning Hsu\\nPaden Tomasello\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.00652\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Dec 2022 06:09:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages\\u00a7r"}']}
{title:'Liang et al. (§72022§r)', author: 'Kailin Liang; Bin Liu; Yifan Hu; Rui Liu; Feilong Bao; Guanglai Gao', display:{Lore:['[{"text": "arXiv:2301.00657", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMnTTS2: An Open-Source Multi-Speaker Mongolian Text-to-Speech Synthesis Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oKailin Liang\\nBin Liu\\nYifan Hu\\n+ 2 others\\u00a7r\\n\\n\\u00a772022\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.00657\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 11 Dec 2022 14:55:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by NCMMSC\'2022 (https://ncmmsc2022.ustc.edu.cn/main.htm)\\u00a7r"}']}
