{title:'He et al. (§72023§r)', author: 'Yunchao He; Yujun Wang', display:{Lore:['[{"text": "arXiv:1904.05351", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRawNet: Fast End-to-End Neural Vocoder\\u00a7r\\n\\n\\u00a78\\u00a7oYunchao He\\nYujun Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1904.05351\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 10 Mar 2023 03:22:22 GMT)\\u00a7r"}']}
{title:'Baruwa et al. (§72023§r)', author: 'Ahmed Baruwa; Mojeed Abisiga; Ibrahim Gbadegesin; Afeez Fakunle', display:{Lore:['[{"text": "arXiv:1912.05946", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging End-to-End Speech Recognition with Neural Architecture Search\\u00a7r\\n\\n\\u00a78\\u00a7oAhmed Baruwa\\nMojeed Abisiga\\nIbrahim Gbadegesin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n1912.05946\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIJSER, vol 10, Issue 11, 2019, pp 1113-1119\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 20 May 2023 23:27:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oA large part of the document needs to be reviewed to meet current standards in the Automatic Speech Recognition\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Jiasong Wu; Xuan Li; Taotao Li; Fanman Meng; Youyong Kong; Guanyu Yang; Lotfi Senhadji; Huazhong Shu', display:{Lore:['[{"text": "arXiv:2007.10629", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCSLNSpeech: solving extended speech separation problem with the help of Chinese sign language\\u00a7r\\n\\n\\u00a78\\u00a7oJiasong Wu\\nXuan Li\\nTaotao Li\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.10629\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 3 Nov 2023 02:00:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 6 figures, 5 tables\\u00a7r"}']}
{title:'Hajavi et al. (§72023§r)', author: 'Amirhossein Hajavi; Ali Etemad', display:{Lore:['[{"text": "arXiv:2009.01822", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFine-grained Early Frequency Attention for Deep Speaker Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oAmirhossein Hajavi\\nAli Etemad\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2009.01822\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 24 Jan 2023 16:15:08 GMT)\\u00a7r"}']}
{title:'Qi et al. (§72023§r)', author: 'Jiajun Qi; Wu Guo; Jingjing Shi; Yafeng Chen; Tan Liu', display:{Lore:['[{"text": "arXiv:2010.06248", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Universal Speech Attributes for Speaker Verification with an Improved Cross-stitch Network\\u00a7r\\n\\n\\u00a78\\u00a7oJiajun Qi\\nWu Guo\\nJingjing Shi\\nYafeng Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.06248\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 31 May 2023 05:28:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oNot a particularly high-quality work, so werequest withdrawal\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Yafeng Chen; Wu Guo; Jingjing Shi; Jiajun Qi; Tan Liu', display:{Lore:['[{"text": "arXiv:2010.10919", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-task Metric Learning for Text-independent Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oYafeng Chen\\nWu Guo\\nJingjing Shi\\nJiajun Qi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2010.10919\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 23 Mar 2023 02:28:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oNot a particularly high-quality work, so werequest withdrawal\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Yafeng Chen; Wu Guo; Bin Gu', display:{Lore:['[{"text": "arXiv:2103.15421", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Meta-Learning Training for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oYafeng Chen\\nWu Guo\\nBin Gu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2103.15421\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 3 Aug 2023 03:09:53 GMT)\\u00a7r"}']}
{title:'Gode et al. (§72023§r)', author: 'Henri Gode; Marvin Tammen; Simon Doclo', display:{Lore:['[{"text": "arXiv:2106.01902", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Multi-Channel Dereverberation and Noise Reduction Using a Unified Convolutional Beamformer With Sparse Priors\\u00a7r\\n\\n\\u00a78\\u00a7oHenri Gode\\nMarvin Tammen\\nSimon Doclo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.01902\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 13 Mar 2023 09:42:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oITG Conference on Speech Communication\\u00a7r"}']}
{title:'Tanaka et al. (§72023§r)', author: 'Keitaro Tanaka; Ryosuke Sawata; Shusuke Takahashi', display:{Lore:['[{"text": "arXiv:2106.02331", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lManifold-Aware Deep Clustering: Maximizing Angles between Embedding Vectors Based on Regular Simplex\\u00a7r\\n\\n\\u00a78\\u00a7oKeitaro Tanaka\\nRyosuke Sawata\\nShusuke Takahashi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2106.02331\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2021-1029\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 16 Oct 2023 05:01:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2021\\u00a7r"}']}
{title:'Wolters et al. (§72023§r)', author: 'Piper Wolters; Logan Sizemore; Chris Daw; Brian Hutchinson; Lauren Phillips', display:{Lore:['[{"text": "arXiv:2107.13616", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProposal-based Few-shot Sound Event Detection for Speech and Environmental Sounds with Perceivers\\u00a7r\\n\\n\\u00a78\\u00a7oPiper Wolters\\nLogan Sizemore\\nChris Daw\\nBrian Hutchinson\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.13616\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 23 Dec 2023 18:34:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUpdated results based on additional experimentation and moved dataset generation prose to stand-alone section\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Tianrui Wang; Weibin Zhu', display:{Lore:['[{"text": "arXiv:2108.11877", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Deep Learning Loss Function based on Auditory Power Compression for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oTianrui Wang\\nWeibin Zhu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2108.11877\\u00a7r\\n\\nVersion:\\u00a77v4 (Sun, 23 Apr 2023 03:18:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work was carried over into other work and waspublished\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Donghyeon Kim; Kyungdeuk Ko; Jeonggi Kwak; David K. Han; Hanseok Ko', display:{Lore:['[{"text": "arXiv:2109.11165", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Lightweight dynamic filter for keyword spotting\\u00a7r\\n\\n\\u00a78\\u00a7oDonghyeon Kim\\nKyungdeuk Ko\\nJeonggi Kwak\\nDavid K. Han\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.11165\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 22 Dec 2023 03:06:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccept to ICASSPW2023\\u00a7r"}']}
{title:'Yen et al. (§72023§r)', author: 'Hao Yen; Pin-Jui Ku; Chao-Han Huck Yang; Hu Hu; Sabato Marco Siniscalchi; Pin-Yu Chen; Yu Tsao', display:{Lore:['[{"text": "arXiv:2110.03894", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHao Yen\\nPin-Jui Ku\\nChao-Han Huck Yang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03894\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1086\\u00a7r\\n\\nVersion:\\u00a77v5 (Mon, 30 Oct 2023 06:26:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023. Code is available at: https://github.com/dodohow1011/SpeechAdvReprogram. Selected as Best Student Paper Candidate\\u00a7r"}']}
{title:'Gitiaux et al. (§72023§r)', author: 'Xavier Gitiaux; Aditya Khant; Ebrahim Beyrami; Chandan Reddy; Jayant Gupchup; Ross Cutler', display:{Lore:['[{"text": "arXiv:2110.04391", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAura: Privacy-preserving Augmentation to Improve Test Set Diversity in Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oXavier Gitiaux\\nAditya Khant\\nEbrahim Beyrami\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04391\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 4 Apr 2023 03:39:08 GMT)\\u00a7r"}']}
{title:'Kothawade et al. (§72023§r)', author: 'Suraj Kothawade; Anmol Mekala; Chandra Sekhara D; Mayank Kothyari; Rishabh Iyer; Ganesh Ramakrishnan; Preethi Jyothi', display:{Lore:['[{"text": "arXiv:2110.04908", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDITTO: Data-efficient and Fair Targeted Subset Selection for ASR Accent Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oSuraj Kothawade\\nAnmol Mekala\\nChandra Sekhara D\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.04908\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 5 Jun 2023 18:31:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oACL 2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Li-Wei Chen; Alexander Rudnicky', display:{Lore:['[{"text": "arXiv:2110.06309", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Wav2vec 2.0 fine-tuning for improved speech emotion recognition\\u00a7r\\n\\n\\u00a78\\u00a7oLi-Wei Chen\\nAlexander Rudnicky\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.06309\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 21 Feb 2023 17:49:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Jiang et al. (§72023§r)', author: 'Ziyue Jiang; Yi Ren; Ming Lei; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2110.07216", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFedSpeech: Federated Text-to-Speech with Continual Learning\\u00a7r\\n\\n\\u00a78\\u00a7oZiyue Jiang\\nYi Ren\\nMing Lei\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.07216\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.24963/ijcai.2021/527\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2021. Main Track. Pages 3829-3835\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 22 May 2023 08:37:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IJCAI 2021\\u00a7r"}']}
{title:'Chan et al. (§72023§r)', author: 'Yun-Ju Chan; Chiang-Jen Peng; Syu-Siang Wang; Hsin-Min Wang; Yu Tsao; Tai-Shih Chi', display:{Lore:['[{"text": "arXiv:2110.09923", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Enhancement-assisted Voice Conversion in Noisy Environments\\u00a7r\\n\\n\\u00a78\\u00a7oYun-Ju Chan\\nChiang-Jen Peng\\nSyu-Siang Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.09923\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nAPSIPA 2022\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 19 Jan 2023 11:31:23 GMT)\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Jiasong Wu; Qingchun Li; Guanyu Yang; Lei Li; Lotfi Senhadji; Huazhong Shu', display:{Lore:['[{"text": "arXiv:2111.00242", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Speech Denoising Using Only Noisy Audio Signals\\u00a7r\\n\\n\\u00a78\\u00a7oJiasong Wu\\nQingchun Li\\nGuanyu Yang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.00242\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 19 Jan 2023 13:17:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 4 figures, 6 tables\\u00a7r"}']}
{title:'Mingote et al. (§72023§r)', author: 'Victoria Mingote; Antonio Miguel; Alfonso Ortega; Eduardo Lleida', display:{Lore:['[{"text": "arXiv:2111.03842", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lClass Token and Knowledge Distillation for Multi-head Self-Attention Speaker Verification Systems\\u00a7r\\n\\n\\u00a78\\u00a7oVictoria Mingote\\nAntonio Miguel\\nAlfonso Ortega\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.03842\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 10 Feb 2023 16:27:27 GMT)\\u00a7r"}']}
{title:'Kuo et al. (§72023§r)', author: 'Heng-Cheng Kuo; Yu-Peng Hsieh; Huan-Hsin Tseng; Chi-Te Wang; Shih-Hau Fang; Yu Tsao', display:{Lore:['[{"text": "arXiv:2112.02538", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lToward Real-World Voice Disorder Classification\\u00a7r\\n\\n\\u00a78\\u00a7oHeng-Cheng Kuo\\nYu-Peng Hsieh\\nHuan-Hsin Tseng\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.02538\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 26 Apr 2023 07:29:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE TBME (under an IEEE Open Access publishing Agreement)\\u00a7r"}']}
{title:'Eeckt et al. (§72023§r)', author: 'Steven Vander Eeckt; Hugo Van hamme', display:{Lore:['[{"text": "arXiv:2112.09427", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContinual Learning for Monolingual End-to-End Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSteven Vander Eeckt\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2112.09427\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 30th European Signal Processing Conference\\n  (EUSIPCO 2022), pg.459\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 9 Mar 2023 11:34:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at EUSIPCO 2022. 5 pages, 1 figure\\u00a7r"}']}
{title:'Günther et al. (§72023§r)', author: 'Michael Günther; Andreas Brendel; Walter Kellermann', display:{Lore:['[{"text": "arXiv:2201.09946", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMicrophone Utility Estimation in Acoustic Sensor Networks using Single-Channel Signal Features\\u00a7r\\n\\n\\u00a78\\u00a7oMichael G\\u00fcnther\\nAndreas Brendel\\nWalter Kellermann\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2201.09946\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 14 Jan 2023 21:44:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to EURASIP Journal on Audio, Speech, and Music Processing\\u00a7r"}']}
{title:'Elminshawi et al. (§72023§r)', author: 'Mohamed Elminshawi; Wolfgang Mack; Srikanth Raj Chetupalli; Soumitro Chakrabarty; Emanuël A. P. Habets', display:{Lore:['[{"text": "arXiv:2202.00733", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNew Insights on Target Speaker Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oMohamed Elminshawi\\nWolfgang Mack\\nSrikanth Raj Chetupalli\\nSoumitro Chakrabarty\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.00733\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 15 Sep 2023 06:15:22 GMT)\\u00a7r"}']}
{title:'Subakan et al. (§72023§r)', author: 'Cem Subakan; Mirco Ravanelli; Samuele Cornell; Francois Grondin; Mirko Bronzi', display:{Lore:['[{"text": "arXiv:2202.02884", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Self-Attention Mechanisms for Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oCem Subakan\\nMirco Ravanelli\\nSamuele Cornell\\nFrancois Grondin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.02884\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 27 May 2023 17:44:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEE/ACM Transactionson Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Karamatlı et al. (§72023§r)', author: 'Ertuğ Karamatlı; Serap Kırbız', display:{Lore:['[{"text": "arXiv:2202.03875", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMixCycle: Unsupervised Speech Separation via Cyclic Mixture Permutation Invariant Training\\u00a7r\\n\\n\\u00a78\\u00a7oErtu\\u011f Karamatl\\u0131\\nSerap K\\u0131rb\\u0131z\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.03875\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2022.3232276\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Signal Processing Letters 29 (2022) 2637-2641\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 6 Jan 2023 21:15:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages. This article has been accepted for publication in IEEE Signal Processing Letters\\u00a7r"}']}
{title:'Speckhard et al. (§72023§r)', author: 'Daniel T. Speckhard; Karolis Misiunas; Sagi Perel; Tenghui Zhu; Simon Carlile; Malcolm Slaney', display:{Lore:['[{"text": "arXiv:2202.05397", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Architecture Search for Energy Efficient Always-on Audio Models\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel T. Speckhard\\nKarolis Misiunas\\nSagi Perel\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.05397\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 1 Jun 2023 12:05:05 GMT)\\u00a7r"}']}
{title:'He et al. (§72023§r)', author: 'Qiqi He; Xiaoheng Sun; Yi Yu; Wei Li', display:{Lore:['[{"text": "arXiv:2202.06338", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDEEPCHORUS: A Hybrid Model of Multi-scale Convolution and Self-attention for Chorus Detection\\u00a7r\\n\\n\\u00a78\\u00a7oQiqi He\\nXiaoheng Sun\\nYi Yu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.06338\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746919\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 17 Aug 2023 07:26:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2022\\u00a7r"}']}
{title:'Sukhadia et al. (§72023§r)', author: 'Vrunda N. Sukhadia; S. Umesh', display:{Lore:['[{"text": "arXiv:2202.09167", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDomain Adaptation of low-resource Target-Domain models using well-trained ASR Conformer Models\\u00a7r\\n\\n\\u00a78\\u00a7oVrunda N. Sukhadia\\nS. Umesh\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2202.09167\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/SLT54892.2023.10023233\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 29 May 2023 12:15:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages,2 figures\\u00a7r"}']}
{title:'Rybakov et al. (§72023§r)', author: 'Oleg Rybakov; Marco Tagliasacchi; Yunpeng Li; Liyang Jiang; Xia Zhang; Fadi Biadsy', display:{Lore:['[{"text": "arXiv:2203.00756", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal time spectrogram inversion on mobile phone\\u00a7r\\n\\n\\u00a78\\u00a7oOleg Rybakov\\nMarco Tagliasacchi\\nYunpeng Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.00756\\u00a7r\\n\\nVersion:\\u00a77v6 (Wed, 24 May 2023 19:12:25 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Sungjae Kim; Yewon Kim; Jewoo Jun; Injung Kim', display:{Lore:['[{"text": "arXiv:2203.00931", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMuSE-SVS: Multi-Singer Emotional Singing Voice Synthesizer that Controls Emotional Intensity\\u00a7r\\n\\n\\u00a78\\u00a7oSungjae Kim\\nYewon Kim\\nJewoo Jun\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.00931\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 20 Mar 2023 08:37:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 11 figures\\u00a7r"}']}
{title:'Mishra et al. (§72023§r)', author: 'Jagabandhu Mishra; S. R. Mahadeva Prasanna', display:{Lore:['[{"text": "arXiv:2203.02680", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLanguage vs Speaker Change: A Comparative Study\\u00a7r\\n\\n\\u00a78\\u00a7oJagabandhu Mishra\\nS. R. Mahadeva Prasanna\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.02680\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 6 Oct 2023 23:09:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe work is substantially modified. The new version of the same will be submitted soon\\u00a7r"}']}
{title:'Wagner et al. (§72023§r)', author: 'Johannes Wagner; Andreas Triantafyllopoulos; Hagen Wierstorf; Maximilian Schmitt; Felix Burkhardt; Florian Eyben; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2203.07378", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDawn of the transformer era in speech emotion recognition: closing the valence gap\\u00a7r\\n\\n\\u00a78\\u00a7oJohannes Wagner\\nAndreas Triantafyllopoulos\\nHagen Wierstorf\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.07378\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TPAMI.2023.3263585\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nin IEEE Transactions on Pattern Analysis and Machine Intelligence,\\n  vol. 45, no. 9, pp. 10745-10759, 1 Sept. 2023\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 7 Sep 2023 18:53:43 GMT)\\u00a7r"}']}
{title:'Bai et al. (§72023§r)', author: 'Jisheng Bai; Jianfeng Chen; Mou Wang; Muhammad Saad Ayub', display:{Lore:['[{"text": "arXiv:2203.08350", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Squeeze-and-Excitation and Transformer based Cross-task System for Environmental Sound Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJisheng Bai\\nJianfeng Chen\\nMou Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.08350\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TCDS.2022.3222350\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 21 Nov 2023 06:40:24 GMT)\\u00a7r"}']}
{title:'Tao et al. (§72023§r)', author: 'Dehua Tao; Tan Lee; Harold Chui; Sarah Luk', display:{Lore:['[{"text": "arXiv:2203.13127", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCharacterizing Therapist\'s Speaking Style in Relation to Empathy in Psychotherapy\\u00a7r\\n\\n\\u00a78\\u00a7oDehua Tao\\nTan Lee\\nHarold Chui\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.13127\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 26 May 2023 07:37:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2022\\u00a7r"}']}
{title:'Sun et al. (§72023§r)', author: 'Haiyang Sun; Zheng Lian; Bin Liu; Ying Li; Licai Sun; Cong Cai; Jianhua Tao; Meng Wang; Yuan Cheng', display:{Lore:['[{"text": "arXiv:2203.13617", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotionNAS: Two-stream Neural Architecture Search for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHaiyang Sun\\nZheng Lian\\nBin Liu\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.13617\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 9 Jun 2023 14:45:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023\\u00a7r"}']}
{title:'Geng et al. (§72023§r)', author: 'Mengzhe Geng; Xurong Xie; Rongfeng Su; Jianwei Yu; Zengrui Jin; Tianzi Wang; Shujie Hu; Zi Ye; Helen Meng; Xunying Liu', display:{Lore:['[{"text": "arXiv:2203.14593", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn-the-Fly Feature Based Rapid Speaker Adaptation for Dysarthric and Elderly Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMengzhe Geng\\nXurong Xie\\nRongfeng Su\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14593\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 28 May 2023 13:45:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2023\\u00a7r"}']}
{title:'Hu et al. (§72023§r)', author: 'Yuchen Hu; Nana Hou; Chen Chen; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2203.14838", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDual-Path Style Learning for End-to-End Noise-Robust Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYuchen Hu\\nNana Hou\\nChen Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14838\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 27 May 2023 11:24:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, Accepted by InterSpeech 2023\\u00a7r"}']}
{title:'Peng et al. (§72023§r)', author: 'Puyuan Peng; David Harwath', display:{Lore:['[{"text": "arXiv:2203.15081", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWord Discovery in Visually Grounded, Self-Supervised Speech Models\\u00a7r\\n\\n\\u00a78\\u00a7oPuyuan Peng\\nDavid Harwath\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15081\\u00a7r\\n\\nVersion:\\u00a77v5 (Tue, 20 Jun 2023 01:55:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2022 Oral. Update Table5\\u00a7r"}']}
{title:'Ding et al. (§72023§r)', author: 'Shaojin Ding; Phoenix Meadowlark; Yanzhang He; Lukasz Lew; Shivani Agrawal; Oleg Rybakov', display:{Lore:['[{"text": "arXiv:2203.15952", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l4-bit Conformer with Native Quantization Aware Training for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oShaojin Ding\\nPhoenix Meadowlark\\nYanzhang He\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.15952\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 3 Mar 2023 01:52:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at INTERSPEECH 2022\\u00a7r"}']}
{title:'Eeckt et al. (§72023§r)', author: 'Steven Vander Eeckt; Hugo Van hamme', display:{Lore:['[{"text": "arXiv:2203.16082", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing Adapters to Overcome Catastrophic Forgetting in End-to-End Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSteven Vander Eeckt\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16082\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 9 Mar 2023 11:38:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2023. 5 pages\\u00a7r"}']}
{title:'Tao et al. (§72023§r)', author: 'Dehua Tao; Tan Lee; Harold Chui; Sarah Luk', display:{Lore:['[{"text": "arXiv:2203.16847", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierarchical Attention Network for Evaluating Therapist Empathy in Counseling Session\\u00a7r\\n\\n\\u00a78\\u00a7oDehua Tao\\nTan Lee\\nHarold Chui\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.16847\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 26 May 2023 07:43:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2022\\u00a7r"}']}
{title:'Sadhu et al. (§72023§r)', author: 'Samik Sadhu; Hynek Hermansky', display:{Lore:['[{"text": "arXiv:2204.00065", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImportance of Different Temporal Modulations of Speech: A Tale of Two Perspectives\\u00a7r\\n\\n\\u00a78\\u00a7oSamik Sadhu\\nHynek Hermansky\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00065\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 22 Mar 2023 21:03:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Casanova et al. (§72023§r)', author: 'Edresson Casanova; Christopher Shulby; Alexander Korolev; Arnaldo Candido Junior; Anderson da Silva Soares; Sandra Aluísio; Moacir Antonelli Ponti', display:{Lore:['[{"text": "arXiv:2204.00618", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lASR data augmentation in low-resource settings using cross-lingual multi-speaker TTS and cross-lingual voice conversion\\u00a7r\\n\\n\\u00a78\\u00a7oEdresson Casanova\\nChristopher Shulby\\nAlexander Korolev\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.00618\\u00a7r\\n\\nVersion:\\u00a77v5 (Sat, 20 May 2023 16:59:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper was accepted at INTERSPEECH 2023\\u00a7r"}']}
{title:'Xie et al. (§72023§r)', author: 'Yuying Xie; Thomas Arildsen; Zheng-Hua Tan', display:{Lore:['[{"text": "arXiv:2204.02195", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComplex Recurrent Variational Autoencoder with Application to Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oYuying Xie\\nThomas Arildsen\\nZheng-Hua Tan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02195\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 12 May 2023 11:50:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\\u00a7r"}']}
{title:'Ragano et al. (§72023§r)', author: 'Alessandro Ragano; Emmanouil Benetos; Michael Chinen; Helard B. Martinez; Chandan K. A. Reddy; Jan Skoglund; Andrew Hines', display:{Lore:['[{"text": "arXiv:2204.02249", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparison of Deep Learning MOS Predictors for Speech Synthesis Quality\\u00a7r\\n\\n\\u00a78\\u00a7oAlessandro Ragano\\nEmmanouil Benetos\\nMichael Chinen\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02249\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 24 Nov 2023 12:43:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted ISSC 2023\\u00a7r"}']}
{title:'Guizzo et al. (§72023§r)', author: 'Eric Guizzo; Tillman Weyde; Simone Scardapane; Danilo Comminiello', display:{Lore:['[{"text": "arXiv:2204.02385", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Speech Emotion Representations in the Quaternion Domain\\u00a7r\\n\\n\\u00a78\\u00a7oEric Guizzo\\nTillman Weyde\\nSimone Scardapane\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02385\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3250840\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 3 Mar 2023 13:16:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for Publication in IEEE/ACM Transactions on Audio, Speech and Language Processing\\u00a7r"}']}
{title:'Lemercier et al. (§72023§r)', author: 'Jean-Marie Lemercier; Joachim Thiemann; Raphael Koning; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2204.02978", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA neural network-supported two-stage algorithm for lightweight dereverberation on hearing devices\\u00a7r\\n\\n\\u00a78\\u00a7oJean-Marie Lemercier\\nJoachim Thiemann\\nRaphael Koning\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.02978\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1186/s13636-023-00285-8\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 31 May 2023 15:34:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in EURASIP Journal on Audio, Speech and Music Processing\\u00a7r"}']}
{title:'Bayerl et al. (§72023§r)', author: 'Sebastian P. Bayerl; Dominik Wagner; Ilja Baumann; Korbinian Riedhammer; Tobias Bocklet', display:{Lore:['[{"text": "arXiv:2204.03428", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetecting Vocal Fatigue with Neural Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oSebastian P. Bayerl\\nDominik Wagner\\nIlja Baumann\\nKorbinian Riedhammer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03428\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 17 Jan 2023 07:47:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for Publication in the Journal of Voice\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Qianying Liu; Zhuo Gong; Zhengdong Yang; Yuhang Yang; Sheng Li; Chenchen Ding; Nobuaki Minematsu; Hao Huang; Fei Cheng; Chenhui Chu; Sadao Kurohashi', display:{Lore:['[{"text": "arXiv:2204.03855", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierarchical Softmax for End-to-End Low-resource Multilingual Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oQianying Liu\\nZhuo Gong\\nZhengdong Yang\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.03855\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 30 Apr 2023 08:09:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, ICASSP 2023\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Lin Zhang; Xin Wang; Erica Cooper; Nicholas Evans; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2204.05177", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe PartialSpoof Database and Countermeasures for the Detection of Short Fake Speech Segments Embedded in an Utterance\\u00a7r\\n\\n\\u00a78\\u00a7oLin Zhang\\nXin Wang\\nErica Cooper\\nNicholas Evans\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.05177\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2022.3233236\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol. 31, pp. 813-825, 2023\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 30 Jan 2023 10:39:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in IEEE/ACM Transactionson Audio, Speech, and Language Processing (DOI: 10.1109/TASLP.2022.3233236)\\u00a7r"}']}
{title:'Jain et al. (§72023§r)', author: 'Rishabh Jain; Andrei Barcovschi; Mariam Yiwere; Dan Bigioi; Peter Corcoran; Horia Cucu', display:{Lore:['[{"text": "arXiv:2204.05419", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Wav2vec2-Based Experimental Study on Self-Supervised Learning Methods to Improve Child Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oRishabh Jain\\nAndrei Barcovschi\\nMariam Yiwere\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.05419\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 12 Feb 2023 04:01:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint, Submitted to IEEE Access\\u00a7r"}']}
{title:'Lu et al. (§72023§r)', author: 'Zhenxing Lu; Mengnan He; Ruixiong Zhang; Caixia Gong', display:{Lore:['[{"text": "arXiv:2204.06086", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Post Auto-regressive GAN Vocoder Focused on Spectrum Fracture\\u00a7r\\n\\n\\u00a78\\u00a7oZhenxing Lu\\nMengnan He\\nRuixiong Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.06086\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 16 Feb 2023 05:11:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oExperimental parts should be improved\\u00a7r"}']}
{title:'Caroselli et al. (§72023§r)', author: "Joseph Caroselli; Arun Narayanan; Nathan Howard; Tom O'Malley", display:{Lore:['[{"text": "arXiv:2204.11933", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCleanformer: A multichannel array configuration-invariant neural enhancement frontend for ASR in smart speakers\\u00a7r\\n\\n\\u00a78\\u00a7oJoseph Caroselli\\nArun Narayanan\\nNathan Howard\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2204.11933\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 3 May 2023 18:12:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Weng et al. (§72023§r)', author: 'Zhenzi Weng; Zhijin Qin; Xiaoming Tao; Chengkang Pan; Guangyi Liu; Geoffrey Ye Li', display:{Lore:['[{"text": "arXiv:2205.04603", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning Enabled Semantic Communications with Speech Recognition and Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oZhenzi Weng\\nZhijin Qin\\nXiaoming Tao\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.04603\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 31 Mar 2023 10:39:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2107.11190\\u00a7r"}']}
{title:'Middelberg et al. (§72023§r)', author: 'Wiebke Middelberg; Simon Doclo', display:{Lore:['[{"text": "arXiv:2205.09401", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBias Analysis of Spatial Coherence-Based RTF Vector Estimation for Acoustic Sensor Networks in a Diffuse Sound Field\\u00a7r\\n\\n\\u00a78\\u00a7oWiebke Middelberg\\nSimon Doclo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.09401\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/IWAENC53105.2022.9914715 10.1109/IWAENC53105.2022.9914715\\n  10.1109/IWAENC53105.2022.9914715 10.1109/IWAENC53105.2022.9914715\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 6 Apr 2023 11:27:57 GMT)\\u00a7r"}']}
{title:'Kang et al. (§72023§r)', author: 'Wonjune Kang; Mark Hasegawa-Johnson; Deb Roy', display:{Lore:['[{"text": "arXiv:2205.09784", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Zero-Shot Voice Conversion with Location-Variable Convolutions\\u00a7r\\n\\n\\u00a78\\u00a7oWonjune Kang\\nMark Hasegawa-Johnson\\nDeb Roy\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.09784\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-2298\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 22 May 2023 16:09:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023\\u00a7r"}']}
{title:'Lutati et al. (§72023§r)', author: 'Shahar Lutati; Eliya Nachmani; Lior Wolf', display:{Lore:['[{"text": "arXiv:2205.11801", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSepIt: Approaching a Single Channel Speech Separation Bound\\u00a7r\\n\\n\\u00a78\\u00a7oShahar Lutati\\nEliya Nachmani\\nLior Wolf\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.11801\\u00a7r\\n\\nVersion:\\u00a77v4 (Sun, 21 May 2023 09:32:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Comanducci et al. (§72023§r)', author: 'Luca Comanducci; Fabio Antonacci; Augusto Sarti', display:{Lore:['[{"text": "arXiv:2205.12872", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynthesis of Soundfields through Irregular Loudspeaker Arrays Based on Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oLuca Comanducci\\nFabio Antonacci\\nAugusto Sarti\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.12872\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 11 Sep 2023 06:34:43 GMT)\\u00a7r"}']}
{title:'Xue et al. (§72023§r)', author: 'Sihao Xue; Qianyao Shen; Guoqing Li', display:{Lore:['[{"text": "arXiv:2205.12933", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBoosting Tail Neural Network for Realtime Custom Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oSihao Xue\\nQianyao Shen\\nGuoqing Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.12933\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 7 Jun 2023 10:13:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 8 figures, 2 tables\\u00a7r"}']}
{title:'Siriwardena et al. (§72023§r)', author: 'Yashish M. Siriwardena; Ahmed Adel Attia; Ganesh Sivaraman; Carol Espy-Wilson', display:{Lore:['[{"text": "arXiv:2205.13086", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Data Augmentation for Acoustic-to-articulatory Speech Inversion using Bidirectional Gated RNNs\\u00a7r\\n\\n\\u00a78\\u00a7oYashish M. Siriwardena\\nAhmed Adel Attia\\nGanesh Sivaraman\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.13086\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 31 May 2023 21:12:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oEUSIPCO 2023\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yinghao Aaron Li; Cong Han; Nima Mesgarani', display:{Lore:['[{"text": "arXiv:2205.15439", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStyleTTS: A Style-Based Generative Model for Natural and Diverse Text-to-Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYinghao Aaron Li\\nCong Han\\nNima Mesgarani\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2205.15439\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 20 Nov 2023 04:31:13 GMT)\\u00a7r"}']}
{title:'Raju et al. (§72023§r)', author: 'Kavitha Raju; Anjaly V; Ryan Lish; Joel Mathew', display:{Lore:['[{"text": "arXiv:2206.01205", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSnow Mountain: Dataset of Audio Recordings of The Bible in Low Resource Languages\\u00a7r\\n\\n\\u00a78\\u00a7oKavitha Raju\\nAnjaly V\\nRyan Lish\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.01205\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 23 May 2023 05:58:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSee dataset at https://huggingface.co/datasets/bridgeconn/snow-mountain\\u00a7r"}']}
{title:'Jiang et al. (§72023§r)', author: 'Ziyue Jiang; Zhe Su; Zhou Zhao; Qian Yang; Yi Ren; Jinglin Liu; Zhenhui Ye', display:{Lore:['[{"text": "arXiv:2206.02147", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oZiyue Jiang\\nZhe Su\\nZhou Zhao\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.02147\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 19 Oct 2023 06:22:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ov3: fix the introduction for theconcurrent similar work of Neural Lexicon Reader (arXiv:2110.09698)\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Kwantae Kim; Shih-Chii Liu', display:{Lore:['[{"text": "arXiv:2206.02639", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContinuous-Time Analog Filters for Audio Edge Intelligence: Review on Circuit Designs\\u00a7r\\n\\n\\u00a78\\u00a7oKwantae Kim\\nShih-Chii Liu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.02639\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/MCAS.2023.3267893\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 21 Apr 2023 10:33:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o17 pages, 19 figures, 1 table\\u00a7r"}']}
{title:'Hu et al. (§72023§r)', author: 'Shujie Hu; Xurong Xie; Mengzhe Geng; Mingyu Cui; Jiajun Deng; Guinan Li; Tianzi Wang; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2206.07327", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting Cross-domain And Cross-Lingual Ultrasound Tongue Imaging Features For Elderly And Dysarthric Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oShujie Hu\\nXurong Xie\\nMengzhe Geng\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.07327\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 22 Jun 2023 06:31:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Zeng et al. (§72023§r)', author: 'Bang Zeng; Hongbing Suo; Yulong Wan; Ming Li', display:{Lore:['[{"text": "arXiv:2206.08525", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSimultaneous Speech Extraction for Multiple Target Speakers under the Meeting Scenarios\\u00a7r\\n\\n\\u00a78\\u00a7oBang Zeng\\nHongbing Suo\\nYulong Wan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.08525\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 18 Nov 2023 07:12:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 3 figures, Accepted by NCMMSC2023\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Han Zhu; Gaofeng Cheng; Jindong Wang; Wenxin Hou; Pengyuan Zhang; Yonghong Yan', display:{Lore:['[{"text": "arXiv:2206.09783", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBoosting Cross-Domain Speech Recognition with Self-Supervision\\u00a7r\\n\\n\\u00a78\\u00a7oHan Zhu\\nGaofeng Cheng\\nJindong Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.09783\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 30 Jul 2023 04:58:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE/ACM Transactionson Audio, Speech and Language Processing (TASLP), 2023\\u00a7r"}']}
{title:'Giganti et al. (§72023§r)', author: 'Antonio Giganti; Luca Cuccovillo; Paolo Bestagini; Patrick Aichroth; Stefano Tubaro', display:{Lore:['[{"text": "arXiv:2206.11640", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker-Independent Microphone Identification in Noisy Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oAntonio Giganti\\nLuca Cuccovillo\\nPaolo Bestagini\\nPatrick Aichroth\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.11640\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.23919/EUSIPCO55093.2022.9909800\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nin European Signal Processing Conference (EUSIPCO), Belgrade,\\n  Serbia, 2022, pp. 1047-1051\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 26 Jan 2023 16:34:04 GMT)\\u00a7r"}']}
{title:'Catellier et al. (§72023§r)', author: 'Andrew Catellier; Stephen Voran', display:{Lore:['[{"text": "arXiv:2206.13272", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWideband Audio Waveform Evaluation Networks: Efficient, Accurate Estimation of Speech Qualities\\u00a7r\\n\\n\\u00a78\\u00a7oAndrew Catellier\\nStephen Voran\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13272\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ACCESS.2023.3330640\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 17 Nov 2023 23:07:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been accepted to the journal IEEE Access\\u00a7r"}']}
{title:'Tesch et al. (§72023§r)', author: 'Kristina Tesch; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2206.13310", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInsights Into Deep Non-linear Filters for Improved Multi-channel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oKristina Tesch\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13310\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2022.3221046\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol. 31, pp. 563-575, 2023\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 16 Jan 2023 15:25:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted version\\u00a7r"}']}
{title:'Bak et al. (§72023§r)', author: 'Taejun Bak; Junmo Lee; Hanbin Bae; Jinhyeok Yang; Jae-Sung Bae; Young-Sun Joo', display:{Lore:['[{"text": "arXiv:2206.13404", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAvocodo: Generative Adversarial Network for Artifact-free Vocoder\\u00a7r\\n\\n\\u00a78\\u00a7oTaejun Bak\\nJunmo Lee\\nHanbin Bae\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13404\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 3 Jan 2023 06:11:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in the 37th AAAI conference on artificial intelligence (AAAI 2023)\\u00a7r"}']}
{title:'Mokrý et al. (§72023§r)', author: 'Ondřej Mokrý; Paul Magron; Thomas Oberlin; Cédric Févotte', display:{Lore:['[{"text": "arXiv:2206.13768", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAlgorithms for audio inpainting based on probabilistic nonnegative matrix factorization\\u00a7r\\n\\n\\u00a78\\u00a7oOnd\\u0159ej Mokr\\u00fd\\nPaul Magron\\nThomas Oberlin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.13768\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.sigpro.2022.108905\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 5 Jan 2023 10:34:42 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Guangyan Zhang; Ying Qin; Wenjie Zhang; Jialun Wu; Mei Li; Yutao Gai; Feijun Jiang; Tan Lee', display:{Lore:['[{"text": "arXiv:2206.14866", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7liEmoTTS: Toward Robust Cross-Speaker Emotion Transfer and Control for Speech Synthesis based on Disentanglement between Prosody and Timbre\\u00a7r\\n\\n\\u00a78\\u00a7oGuangyan Zhang\\nYing Qin\\nWenjie Zhang\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.14866\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 4 Jan 2023 06:07:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEETransactions on Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Keon Lee; Kyumin Park; Daeyoung Kim', display:{Lore:['[{"text": "arXiv:2207.01063", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oKeon Lee\\nKyumin Park\\nDaeyoung Kim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.01063\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 13 Mar 2023 02:13:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figures, 4 tables. Accepted to ICASSP 2023\\u00a7r"}']}
{title:'Gu et al. (§72023§r)', author: 'Xiangming Gu; Longshen Ou; Danielle Ong; Ye Wang', display:{Lore:['[{"text": "arXiv:2207.06127", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMM-ALT: A Multimodal Automatic Lyric Transcription System\\u00a7r\\n\\n\\u00a78\\u00a7oXiangming Gu\\nLongshen Ou\\nDanielle Ong\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.06127\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3503161.3548411\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 17 Feb 2023 12:21:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ACM Multimedia 2022. Cameraready version and appendix\\u00a7r"}']}
{title:'Drakopoulos et al. (§72023§r)', author: 'Fotios Drakopoulos; Sarah Verhulst', display:{Lore:['[{"text": "arXiv:2207.07091", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Neural-Network Framework for the Design of Individualised Hearing-Loss Compensation\\u00a7r\\n\\n\\u00a78\\u00a7oFotios Drakopoulos\\nSarah Verhulst\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.07091\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3282093\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 16 Jun 2023 20:16:42 GMT)\\u00a7r"}']}
{title:'Gao et al. (§72023§r)', author: 'Xiaoxue Gao; Chitralekha Gupta; Haizhou Li', display:{Lore:['[{"text": "arXiv:2207.07336", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPoLyScriber: Integrated Fine-tuning of Extractor and Lyrics Transcriber for Polyphonic Music\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoxue Gao\\nChitralekha Gupta\\nHaizhou Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.07336\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 5 May 2023 06:28:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTALSP\\u00a7r"}']}
{title:'Deppisch et al. (§72023§r)', author: 'Thomas Deppisch; Sebastià V. Amengual Garí; Paul Calamia; Jens Ahrens', display:{Lore:['[{"text": "arXiv:2207.09733", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDirect and Residual Subspace Decomposition of Spatial Room Impulse Responses\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Deppisch\\nSebasti\\u00e0 V. Amengual Gar\\u00ed\\nPaul Calamia\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2207.09733\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3240657\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 31 Jan 2023 10:16:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis article has been accepted for publication in the IEEE/ACM Transactionson Audio, Speech, and Language Processing. (c) 2023 IEEE\\u00a7r"}']}
{title:'Xia et al. (§72023§r)', author: 'Wei Xia; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:2208.02778", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention and DCT based Global Context Modeling for Text-independent Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oWei Xia\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.02778\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 23 Aug 2023 23:30:54 GMT)\\u00a7r"}']}
{title:'Richter et al. (§72023§r)', author: 'Julius Richter; Simon Welker; Jean-Marie Lemercier; Bunlong Lay; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2208.05830", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Enhancement and Dereverberation with Diffusion-based Generative Models\\u00a7r\\n\\n\\u00a78\\u00a7oJulius Richter\\nSimon Welker\\nJean-Marie Lemercier\\nBunlong Lay\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.05830\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 13 Jun 2023 11:30:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted version\\u00a7r"}']}
{title:'Nercessian (§72023§r)', author: 'Shahan Nercessian', display:{Lore:['[{"text": "arXiv:2208.07282", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDifferentiable WORLD Synthesizer-based Neural Vocoder With Application To End-To-End Audio Style Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oShahan Nercessian\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.07282\\u00a7r\\n\\nVersion:\\u00a77v5 (Mon, 8 May 2023 13:45:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oA revised version of this work has been accepted to the 154th AES Convention. To cite this work, please refer to the AES manuscript available at https://www.aes.org/e-lib/browse.cfm?elib=22073 ; 12 pages, 4 figures\\u00a7r"}']}
{title:'Andrusenko et al. (§72023§r)', author: 'Andrei Andrusenko; Rauf Nasretdinov; Aleksei Romanenko', display:{Lore:['[{"text": "arXiv:2208.07657", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUconv-Conformer: High Reduction of Input Sequence Length for End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAndrei Andrusenko\\nRauf Nasretdinov\\nAleksei Romanenko\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.07657\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 11 Mar 2023 10:00:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, accepted by ICASSP 2023\\u00a7r"}']}
{title:'Xiao et al. (§72023§r)', author: 'Tong Xiao; Buye Xu; Chuming Zhao', display:{Lore:['[{"text": "arXiv:2208.09997", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SY\\u00a7r, \\u00a7eeess.SY\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatially Selective Active Noise Control Systems\\u00a7r\\n\\n\\u00a78\\u00a7oTong Xiao\\nBuye Xu\\nChuming Zhao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.09997\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0019336\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJ. Acoust. Soc. Am., Vol. 153, No. 5, pp. 2733-2744, 2023\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 12 May 2023 16:36:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe following article has been submitted to the Journal of the Acoustical Society of America (JASA). It has been accepted and published in https://doi.org/10.1121/10.0019336\\u00a7r"}']}
{title:'Défossez et al. (§72023§r)', author: 'Alexandre Défossez; Charlotte Caucheteux; Jérémy Rapin; Ori Kabeli; Jean-Rémi King', display:{Lore:['[{"text": "arXiv:2208.12266", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDecoding speech perception from non-invasive brain recordings\\u00a7r\\n\\n\\u00a78\\u00a7oAlexandre D\\u00e9fossez\\nCharlotte Caucheteux\\nJ\\u00e9r\\u00e9my Rapin\\nOri Kabeli\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2208.12266\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1038/s42256-023-00714-5\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 5 Oct 2023 15:54:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oupdated version following publication in Nature Machine Intelligence (2023)\\u00a7r"}']}
{title:'Noufi et al. (§72023§r)', author: 'Camille Noufi; Dejan Markovic; Peter Dodds', display:{Lore:['[{"text": "arXiv:2209.04473", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReconstructing the Dynamic Directivity of Unconstrained Speech\\u00a7r\\n\\n\\u00a78\\u00a7oCamille Noufi\\nDejan Markovic\\nPeter Dodds\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.04473\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 5 Sep 2023 08:16:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn proceedings of I3DA 2023 - The 2023 International Conference on Immersive and 3D Audio. DOI coming soon\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Mu Yang; Andros Tjandra; Chunxi Liu; David Zhang; Duc Le; Ozlem Kalinli', display:{Lore:['[{"text": "arXiv:2209.05735", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning ASR pathways: A sparse multilingual ASR model\\u00a7r\\n\\n\\u00a78\\u00a7oMu Yang\\nAndros Tjandra\\nChunxi Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.05735\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 28 Sep 2023 20:24:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Cuccovillo et al. (§72023§r)', author: 'Luca Cuccovillo; Christoforos Papastergiopoulos; Anastasios Vafeiadis; Artem Yaroshchuk; Patrick Aichroth; Konstantinos Votis; Dimitrios Tzovaras', display:{Lore:['[{"text": "arXiv:2209.07180", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOpen Challenges in Synthetic Speech Detection\\u00a7r\\n\\n\\u00a78\\u00a7oLuca Cuccovillo\\nChristoforos Papastergiopoulos\\nAnastasios Vafeiadis\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.07180\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/WIFS55849.2022.9975433\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nin IEEE International Workshop on Information Forensics and\\n  Security (WIFS), December 12-16, 2022, Shanghai, China, pp.1-6\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 26 Jan 2023 16:40:34 GMT)\\u00a7r"}']}
{title:'Baum et al. (§72023§r)', author: 'Malte Baum; Luca Cuccovillo; Artem Yaroshchuk; Patrick Aichroth', display:{Lore:['[{"text": "arXiv:2209.07196", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnvironment Classification via Blind Roomprints Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oMalte Baum\\nLuca Cuccovillo\\nArtem Yaroshchuk\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.07196\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/WIFS55849.2022.9975411\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nin IEEE International Workshop on Information Forensics and\\n  Security (WIFS), December 12-16, 2022, Shanghai, China, pp.1-6\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 26 Jan 2023 16:34:35 GMT)\\u00a7r"}']}
{title:'Prabhu et al. (§72023§r)', author: 'Navin Raj Prabhu; Nale Lehmann-Willenbrock; Timo Gerkman', display:{Lore:['[{"text": "arXiv:2209.15449", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Label Uncertainty Modeling in Speech Emotion Recognition using Bayesian Neural Networks and Label Distribution Learning\\u00a7r\\n\\n\\u00a78\\u00a7oNavin Raj Prabhu\\nNale Lehmann-Willenbrock\\nTimo Gerkman\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.15449\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TAFFC.2023.3283595\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Transactions on Affective Computing, June 2023\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 13 Jun 2023 08:55:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted Paper at IEEE Transactions on Affective Computing, June 2023. Contains main paper with supplementary material. arXiv admin note: text overlap with arXiv:2207.12135\\u00a7r"}']}
{title:'Kuang et al. (§72023§r)', author: 'Zhihuan Kuang; Shi Zong; Jianbing Zhang; Jiajun Chen; Hongfu Liu', display:{Lore:['[{"text": "arXiv:2210.00434", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic-to-Text Synaesthesia: Generating Descriptive Text from Music Recordings\\u00a7r\\n\\n\\u00a78\\u00a7oZhihuan Kuang\\nShi Zong\\nJianbing Zhang\\nJiajun Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.00434\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 May 2023 03:09:27 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Xubo Liu; Haohe Liu; Qiuqiang Kong; Xinhao Mei; Mark D. Plumbley; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2210.00943", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSimple Pooling Front-ends For Efficient Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oXubo Liu\\nHaohe Liu\\nQiuqiang Kong\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.00943\\u00a7r\\n\\nVersion:\\u00a77v5 (Sun, 7 May 2023 02:06:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Atmaja et al. (§72023§r)', author: 'Bagus Tris Atmaja; Zanjabila; Suyanto; Akira Sasou', display:{Lore:['[{"text": "arXiv:2210.02057", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparing Hysteresis Comparator and RMS Threshold Methods for Automatic Single Cough Segmentations\\u00a7r\\n\\n\\u00a78\\u00a7oBagus Tris Atmaja\\nZanjabila\\nSuyanto\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.02057\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s41870-023-01626-8\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 12 Dec 2023 07:40:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o3 figure,s 3 tables, accepted in IJIT\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Zih-Ching Chen; Chin-Lun Fu; Chih-Ying Liu; Shang-Wen Li; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2210.06175", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Efficient-tuning Methods in Self-supervised Speech Models\\u00a7r\\n\\n\\u00a78\\u00a7oZih-Ching Chen\\nChin-Lun Fu\\nChih-Ying Liu\\nShang-Wen Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.06175\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 30 Jan 2023 12:37:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSLT 2022\\u00a7r"}']}
{title:'Fan et al. (§72023§r)', author: 'Ruchao Fan; Yiming Wang; Yashesh Gaur; Jinyu Li', display:{Lore:['[{"text": "arXiv:2210.08603", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCTCBERT: Advancing Hidden-unit BERT with CTC Objectives\\u00a7r\\n\\n\\u00a78\\u00a7oRuchao Fan\\nYiming Wang\\nYashesh Gaur\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.08603\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 28 Apr 2023 20:03:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP2023\\u00a7r"}']}
{title:'Wiepert et al. (§72023§r)', author: 'Daniela A. Wiepert; Bradley A. Malin; Joseph R. Duffy; Rene L. Utianski; John L. Stricker; David T. Jones; Hugo Botha', display:{Lore:['[{"text": "arXiv:2210.09975", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRisk of re-identification for shared clinical speech recordings\\u00a7r\\n\\n\\u00a78\\u00a7oDaniela A. Wiepert\\nBradley A. Malin\\nJoseph R. Duffy\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.09975\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 21 Aug 2023 21:10:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o24 pages, 6 figures\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Xin Wang; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2210.10570", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpoofed training data for speech spoofing countermeasure can be efficiently created using neural vocoders\\u00a7r\\n\\n\\u00a78\\u00a7oXin Wang\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.10570\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 22 Feb 2023 12:35:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023 accepted. Code: https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts/tree/master/project/09-asvspoof-vocoded-trn\\u00a7r"}']}
{title:'Zhou et al. (§72023§r)', author: 'Rui Zhou; Wenye Zhu; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2210.11089", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Dereverberation with a Reverberation Time Shortening Target\\u00a7r\\n\\n\\u00a78\\u00a7oRui Zhou\\nWenye Zhu\\nXiaofei Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.11089\\u00a7r\\n\\nVersion:\\u00a77v6 (Tue, 6 Jun 2023 01:49:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2204.08765\\u00a7r"}']}
{title:'Hsu et al. (§72023§r)', author: 'Yicheng Hsu; Chenghumg Ma; Mingsian R. Bai', display:{Lore:['[{"text": "arXiv:2210.11123", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModel-matching Principle Applied to the Design of an Array-based All-neural Binaural Rendering System for Audio Telepresence\\u00a7r\\n\\n\\u00a78\\u00a7oYicheng Hsu\\nChenghumg Ma\\nMingsian R. Bai\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.11123\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 6 Mar 2023 14:13:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by ICASSP 2023\\u00a7r"}']}
{title:'Raj et al. (§72023§r)', author: 'Desh Raj; Junteng Jia; Jay Mahadeokar; Chunyang Wu; Niko Moritz; Xiaohui Zhang; Ozlem Kalinli', display:{Lore:['[{"text": "arXiv:2210.11588", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnchored Speech Recognition with Neural Transducers\\u00a7r\\n\\n\\u00a78\\u00a7oDesh Raj\\nJunteng Jia\\nJay Mahadeokar\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.11588\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 29 Mar 2023 18:17:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at IEEE ICASSP 2023\\u00a7r"}']}
{title:'Cho et al. (§72023§r)', author: 'Cheol Jun Cho; Peter Wu; Abdelrahman Mohamed; Gopala K. Anumanchipalli', display:{Lore:['[{"text": "arXiv:2210.11723", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvidence of Vocal Tract Articulation in Self-Supervised Learning of Speech\\u00a7r\\n\\n\\u00a78\\u00a7oCheol Jun Cho\\nPeter Wu\\nAbdelrahman Mohamed\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.11723\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10094711\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 21 Jul 2023 03:12:13 GMT)\\u00a7r"}']}
{title:'Saxena et al. (§72023§r)', author: 'Kavya Ranjan Saxena; Vipul Arora', display:{Lore:['[{"text": "arXiv:2210.12532", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep domain adaptation for polyphonic melody extraction\\u00a7r\\n\\n\\u00a78\\u00a7oKavya Ranjan Saxena\\nVipul Arora\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.12532\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 5 Apr 2023 19:14:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWant to withdraw this paper because few concepts of domain adaptation are not clear in the paper\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Chunhui Wang; Chang Zeng; Jun Chen; Xing He', display:{Lore:['[{"text": "arXiv:2210.12740", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHiFi-WaveGAN: Generative Adversarial Network with Auxiliary Spectrogram-Phase Loss for High-Fidelity Singing Voice Generation\\u00a7r\\n\\n\\u00a78\\u00a7oChunhui Wang\\nChang Zeng\\nJun Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.12740\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 17 Sep 2023 08:15:45 GMT)\\u00a7r"}']}
{title:'Lavechin et al. (§72023§r)', author: 'Marvin Lavechin; Marianne Métais; Hadrien Titeux; Alodie Boissonnet; Jade Copet; Morgane Rivière; Elika Bergelson; Alejandrina Cristia; Emmanuel Dupoux; Hervé Bredin', display:{Lore:['[{"text": "arXiv:2210.13248", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBrouhaha: multi-task training for voice activity detection, speech-to-noise ratio, and C50 room acoustics estimation\\u00a7r\\n\\n\\u00a78\\u00a7oMarvin Lavechin\\nMarianne M\\u00e9tais\\nHadrien Titeux\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.13248\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 25 May 2023 11:34:30 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Yufeng Yang; Ashutosh Pandey; DeLiang Wang', display:{Lore:['[{"text": "arXiv:2210.13318", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTime-Domain Speech Enhancement for Robust Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYufeng Yang\\nAshutosh Pandey\\nDeLiang Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.13318\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 21 Jun 2023 02:22:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2023, 5 pages, 2 figures\\u00a7r"}']}
{title:'Zhou et al. (§72023§r)', author: 'Kun Zhou; Berrak Sisman; Carlos Busso; Bin Ma; Haizhou Li', display:{Lore:['[{"text": "arXiv:2210.13756", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMixed-EVC: Mixed Emotion Synthesis and Control in Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oKun Zhou\\nBerrak Sisman\\nCarlos Busso\\nBin Ma\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.13756\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 18 Sep 2023 01:54:31 GMT)\\u00a7r"}']}
{title:'Rybakov et al. (§72023§r)', author: 'Oleg Rybakov; Fadi Biadsy; Xia Zhang; Liyang Jiang; Phoenix Meadowlark; Shivani Agrawal', display:{Lore:['[{"text": "arXiv:2210.13761", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreaming Parrotron for on-device speech-to-speech conversion\\u00a7r\\n\\n\\u00a78\\u00a7oOleg Rybakov\\nFadi Biadsy\\nXia Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.13761\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 24 May 2023 20:07:17 GMT)\\u00a7r"}']}
{title:'Hauret et al. (§72023§r)', author: 'Julien Hauret; Thomas Joubaud; Véronique Zimpfer; Éric Bavu', display:{Lore:['[{"text": "arXiv:2210.14090", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEBEN: Extreme bandwidth extension network applied to speech signals captured with noise-resilient body-conduction microphones\\u00a7r\\n\\n\\u00a78\\u00a7oJulien Hauret\\nThomas Joubaud\\nV\\u00e9ronique Zimpfer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14090\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 3 Mar 2023 13:50:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, accepted to ICASSP 2023\\u00a7r"}']}
{title:'Fang et al. (§72023§r)', author: 'Zexin Fang; Bin Han; C. Clark Cao; Hans. D. Schotten', display:{Lore:['[{"text": "arXiv:2210.14321", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lArtificial ASMR: A Cyber-Psychological Approach\\u00a7r\\n\\n\\u00a78\\u00a7oZexin Fang\\nBin Han\\nC. Clark Cao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14321\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 5 Jul 2023 12:25:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE MLSP 2023\\u00a7r"}']}
{title:'Fu et al. (§72023§r)', author: 'Li Fu; Siqi Li; Qingtao Li; Liping Deng; Fangzhu Li; Lu Fan; Meng Chen; Xiaodong He', display:{Lore:['[{"text": "arXiv:2210.14515", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUFO2: A unified pre-training framework for online and offline speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oLi Fu\\nSiqi Li\\nQingtao Li\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14515\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 3 Apr 2023 08:26:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Diaz-Guerra et al. (§72023§r)', author: 'David Diaz-Guerra; Archontis Politis; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2210.14536", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPosition tracking of a varying number of sound sources with sliding permutation invariant training\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Diaz-Guerra\\nArchontis Politis\\nTuomas Virtanen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14536\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 5 Jun 2023 11:14:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at the 31st European Signal Processing Conference (EUSIPCO 2023)\\u00a7r"}']}
{title:'Jung et al. (§72023§r)', author: 'Myunghun Jung; Hoirin Kim', display:{Lore:['[{"text": "arXiv:2210.14564", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaMS: Deep Metric Learning with Adaptive Margin and Adaptive Scale for Acoustic Word Discrimination\\u00a7r\\n\\n\\u00a78\\u00a7oMyunghun Jung\\nHoirin Kim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14564\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 23 May 2023 06:42:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023\\u00a7r"}']}
{title:'Niizumi et al. (§72023§r)', author: 'Daisuke Niizumi; Daiki Takeuchi; Yasunori Ohishi; Noboru Harada; Kunio Kashino', display:{Lore:['[{"text": "arXiv:2210.14648", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMasked Modeling Duo: Learning Representations by Encouraging Both Networks to Model the Input\\u00a7r\\n\\n\\u00a78\\u00a7oDaisuke Niizumi\\nDaiki Takeuchi\\nYasunori Ohishi\\nNoboru Harada\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14648\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 2 Mar 2023 09:42:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 3 figures, and 6 tables. To appear at ICASSP2023\\u00a7r"}']}
{title:'Kunešová et al. (§72023§r)', author: 'Marie Kunešová; Zbyněk Zajíc', display:{Lore:['[{"text": "arXiv:2210.14755", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultitask Detection of Speaker Changes, Overlapping Speech and Voice Activity Using wav2vec 2.0\\u00a7r\\n\\n\\u00a78\\u00a7oMarie Kune\\u0161ov\\u00e1\\nZbyn\\u011bk Zaj\\u00edc\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14755\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10094972\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 10 Mar 2023 12:33:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at ICASSP 2023\\u00a7r"}']}
{title:'Teixeira et al. (§72023§r)', author: 'Francisco Teixeira; Alberto Abad; Bhiksha Raj; Isabel Trancoso', display:{Lore:['[{"text": "arXiv:2210.14995", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrivacy-preserving Automatic Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oFrancisco Teixeira\\nAlberto Abad\\nBhiksha Raj\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.14995\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 18 Apr 2023 10:10:05 GMT)\\u00a7r"}']}
{title:'Noufi et al. (§72023§r)', author: 'Camille Noufi; Jonathan Berger; Karen J. Parker; Daniel L. Bowling', display:{Lore:['[{"text": "arXiv:2210.15001", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustically-Driven Phoneme Removal That Preserves Vocal Affect Cues\\u00a7r\\n\\n\\u00a78\\u00a7oCamille Noufi\\nJonathan Berger\\nKaren J. Parker\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15001\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 14 Mar 2023 04:11:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be seen in proceedings of the 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (DOI coming soon)\\u00a7r"}']}
{title:'Attia et al. (§72023§r)', author: 'Ahmed Adel Attia; Carol Espy-Wilson', display:{Lore:['[{"text": "arXiv:2210.15195", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMasked Autoencoders Are Articulatory Learners\\u00a7r\\n\\n\\u00a78\\u00a7oAhmed Adel Attia\\nCarol Espy-Wilson\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15195\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10096209\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 18 May 2023 07:38:34 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'You Zhang; Yuxiang Wang; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2210.15196", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHRTF Field: Unifying Measured HRTF Magnitude Representation with Neural Fields\\u00a7r\\n\\n\\u00a78\\u00a7oYou Zhang\\nYuxiang Wang\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15196\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 23 Feb 2023 16:34:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted by ICASSP 2023\\u00a7r"}']}
{title:'Moliner et al. (§72023§r)', author: 'Eloi Moliner; Jaakko Lehtinen; Vesa Välimäki', display:{Lore:['[{"text": "arXiv:2210.15228", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSolving Audio Inverse Problems with a Diffusion Model\\u00a7r\\n\\n\\u00a78\\u00a7oEloi Moliner\\nJaakko Lehtinen\\nVesa V\\u00e4lim\\u00e4ki\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15228\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 18 Mar 2023 11:24:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccetpted at ICASSP 2023\\u00a7r"}']}
{title:'Eeckt et al. (§72023§r)', author: 'Steven Vander Eeckt; Hugo Van hamme', display:{Lore:['[{"text": "arXiv:2210.15282", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeight Averaging: A Simple Yet Effective Method to Overcome Catastrophic Forgetting in Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSteven Vander Eeckt\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15282\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 9 Mar 2023 11:41:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2023. 5 pages\\u00a7r"}']}
{title:'Wagner et al. (§72023§r)', author: 'Dominik Wagner; Ilja Baumann; Franziska Braun; Sebastian P. Bayerl; Elmar Nöth; Korbinian Riedhammer; Tobias Bocklet', display:{Lore:['[{"text": "arXiv:2210.15336", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-class Detection of Pathological Speech with Latent Features: How does it perform on unseen data?\\u00a7r\\n\\n\\u00a78\\u00a7oDominik Wagner\\nIlja Baumann\\nFranziska Braun\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15336\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 1 Aug 2023 13:20:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023\\u00a7r"}']}
{title:'Vieting et al. (§72023§r)', author: 'Peter Vieting; Christoph Lüscher; Julian Dierkes; Ralf Schlüter; Hermann Ney', display:{Lore:['[{"text": "arXiv:2210.15445", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Utilization of Large Pre-Trained Models for Low Resource ASR\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Vieting\\nChristoph L\\u00fcscher\\nJulian Dierkes\\nRalf Schl\\u00fcter\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15445\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 17 Aug 2023 13:49:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP SASB 2023\\u00a7r"}']}
{title:'Briegleb et al. (§72023§r)', author: 'Annika Briegleb; Mhd Modar Halimeh; Walter Kellermann', display:{Lore:['[{"text": "arXiv:2210.15512", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting spatial information with the informed complex-valued spatial autoencoder for target speaker extraction\\u00a7r\\n\\n\\u00a78\\u00a7oAnnika Briegleb\\nMhd Modar Halimeh\\nWalter Kellermann\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15512\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10095196\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 14 Mar 2023 15:17:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to 2023 IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece. 5 pages,2 figures\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Yujin Wang; Changli Tang; Ziyang Ma; Zhisheng Zheng; Xie Chen; Wei-Qiang Zhang', display:{Lore:['[{"text": "arXiv:2210.15631", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Effective Distillation of Self-Supervised Speech Models for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYujin Wang\\nChangli Tang\\nZiyang Ma\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15631\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 22 Oct 2023 13:03:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ASRU 2023\\u00a7r"}']}
{title:'Jia et al. (§72023§r)', author: 'Fei Jia; Nithin Rao Koluguri; Jagadeesh Balam; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2210.15781", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Compact End-to-End Model with Local and Global Context for Spoken Language Identification\\u00a7r\\n\\n\\u00a78\\u00a7oFei Jia\\nNithin Rao Koluguri\\nJagadeesh Balam\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15781\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 10 Aug 2023 23:34:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2023\\u00a7r"}']}
{title:'Lin et al. (§72023§r)', author: 'Yist Y. Lin; Tao Han; Haihua Xu; Van Tung Pham; Yerbolat Khassanov; Tze Yuang Chong; Yi He; Lu Lu; Zejun Ma', display:{Lore:['[{"text": "arXiv:2210.15876", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRandom Utterance Concatenation Based Data Augmentation for Improving Short-video Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYist Y. Lin\\nTao Han\\nHaihua Xu\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15876\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 May 2023 05:32:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 4 tables\\u00a7r"}']}
{title:'Yoneyama et al. (§72023§r)', author: 'Reo Yoneyama; Ryuichi Yamamoto; Kentaro Tachibana', display:{Lore:['[{"text": "arXiv:2210.15887", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNonparallel High-Quality Audio Super Resolution with Domain Adaptation and Resampling CycleGANs\\u00a7r\\n\\n\\u00a78\\u00a7oReo Yoneyama\\nRyuichi Yamamoto\\nKentaro Tachibana\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15887\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 27 Feb 2023 08:13:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAcceptted to ICASSP 2023\\u00a7r"}']}
{title:'Baumann et al. (§72023§r)', author: 'Ilja Baumann; Dominik Wagner; Franziska Braun; Sebastian P. Bayerl; Elmar Nöth; Korbinian Riedhammer; Tobias Bocklet', display:{Lore:['[{"text": "arXiv:2210.15941", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInfluence of Utterance and Speaker Characteristics on the Classification of Children with Cleft Lip and Palate\\u00a7r\\n\\n\\u00a78\\u00a7oIlja Baumann\\nDominik Wagner\\nFranziska Braun\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15941\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 1 Aug 2023 13:13:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023\\u00a7r"}']}
{title:'Shirahata et al. (§72023§r)', author: 'Yuma Shirahata; Ryuichi Yamamoto; Eunwoo Song; Ryo Terashima; Jae-Min Kim; Kentaro Tachibana', display:{Lore:['[{"text": "arXiv:2210.15964", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPeriod VITS: Variational Inference with Explicit Pitch Modeling for End-to-end Emotional Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYuma Shirahata\\nRyuichi Yamamoto\\nEunwoo Song\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15964\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 22 Feb 2023 03:46:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Kawamura et al. (§72023§r)', author: 'Masaya Kawamura; Yuma Shirahata; Ryuichi Yamamoto; Kentaro Tachibana', display:{Lore:['[{"text": "arXiv:2210.15975", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLightweight and High-Fidelity End-to-End Text-to-Speech with Multi-Band Generation and Inverse Short-Time Fourier Transform\\u00a7r\\n\\n\\u00a78\\u00a7oMasaya Kawamura\\nYuma Shirahata\\nRyuichi Yamamoto\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15975\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 21 Feb 2023 16:28:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Yamamoto et al. (§72023§r)', author: 'Ryuichi Yamamoto; Reo Yoneyama; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2210.15987", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNNSVS: A Neural Network-Based Singing Voice Synthesis Toolkit\\u00a7r\\n\\n\\u00a78\\u00a7oRyuichi Yamamoto\\nReo Yoneyama\\nTomoki Toda\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.15987\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 1 Mar 2023 14:39:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Lin et al. (§72023§r)', author: 'Yuke Lin; Xiaoyi Qin; Huahua Cui; Zhenyi Zhu; Ming Li', display:{Lore:['[{"text": "arXiv:2210.16028", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLaugh Betrays You? Learning Robust Speaker Representation From Speech Containing Non-Verbal Fragments\\u00a7r\\n\\n\\u00a78\\u00a7oYuke Lin\\nXiaoyi Qin\\nHuahua Cui\\nZhenyi Zhu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16028\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 20 Nov 2023 09:26:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osee 2308.07056 which is a newer version of this work\\u00a7r"}']}
{title:'Cheng et al. (§72023§r)', author: 'Ming Cheng; Weiqing Wang; Yucong Zhang; Xiaoyi Qin; Ming Li', display:{Lore:['[{"text": "arXiv:2210.16127", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTarget-Speaker Voice Activity Detection via Sequence-to-Sequence Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oMing Cheng\\nWeiqing Wang\\nYucong Zhang\\nXiaoyi Qin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16127\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 20 Feb 2023 02:21:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2023\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Xubo Liu; Qiushi Huang; Xinhao Mei; Haohe Liu; Qiuqiang Kong; Jianyuan Sun; Shengchen Li; Tom Ko; Yu Zhang; Lilian H. Tang; Mark D. Plumbley; Volkan Kılıç; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2210.16428", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisually-Aware Audio Captioning With Adaptive Audio-Visual Attention\\u00a7r\\n\\n\\u00a78\\u00a7oXubo Liu\\nQiushi Huang\\nXinhao Mei\\n+ 9 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16428\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 29 May 2023 03:53:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023\\u00a7r"}']}
{title:'Siriwardena et al. (§72023§r)', author: 'Yashish M. Siriwardena; Carol Espy-Wilson; Shihab Shamma', display:{Lore:['[{"text": "arXiv:2210.16454", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning to Compute the Articulatory Representations of Speech with the MIRRORNET\\u00a7r\\n\\n\\u00a78\\u00a7oYashish M. Siriwardena\\nCarol Espy-Wilson\\nShihab Shamma\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16454\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInterspeech 2023\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 May 2023 16:41:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023\\u00a7r"}']}
{title:'Lian et al. (§72023§r)', author: 'Jiachen Lian; Alan W Black; Yijing Lu; Louis Goldstein; Shinji Watanabe; Gopala K. Anumanchipalli', display:{Lore:['[{"text": "arXiv:2210.16498", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lArticulatory Representation Learning Via Joint Factor Analysis and Neural Matrix Factorization\\u00a7r\\n\\n\\u00a78\\u00a7oJiachen Lian\\nAlan W Black\\nYijing Lu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16498\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 20 Feb 2023 08:09:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to 2023 ICASSP. Camera Ready\\u00a7r"}']}
{title:'Kerpicci et al. (§72023§r)', author: 'Mine Kerpicci; Van Nguyen; Shuhua Zhang; Erik Visser', display:{Lore:['[{"text": "arXiv:2210.16611", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lApplication of Knowledge Distillation to Multi-task Speech Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oMine Kerpicci\\nVan Nguyen\\nShuhua Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16611\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 19 May 2023 17:16:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSpeech representation learning, multi-task training, wav2vec, HuBERT, knowledge distillation\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Zhe Li; Man-Wai Mak; Helen Mei-Ling Meng', display:{Lore:['[{"text": "arXiv:2210.16622", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiscriminative Speaker Representation via Contrastive Learning with Class-Aware Attention in Angular Space\\u00a7r\\n\\n\\u00a78\\u00a7oZhe Li\\nMan-Wai Mak\\nHelen Mei-Ling Meng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16622\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 13 Mar 2023 06:03:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023, 5 pages, 2 figures\\u00a7r"}']}
{title:'Higuchi et al. (§72023§r)', author: 'Yosuke Higuchi; Brian Yan; Siddhant Arora; Tetsuji Ogawa; Tetsunori Kobayashi; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2210.16663", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBERT Meets CTC: New Formulation of End-to-End Speech Recognition with Pre-trained Masked Language Model\\u00a7r\\n\\n\\u00a78\\u00a7oYosuke Higuchi\\nBrian Yan\\nSiddhant Arora\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16663\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 20 Apr 2023 01:23:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ov1: Accepted to Findings ofEMNLP2022, v2: Minor corrections and clearer derivation of Eq. (21)\\u00a7r"}']}
{title:'Yen et al. (§72023§r)', author: 'Hao Yen; Woojay Jeon', display:{Lore:['[{"text": "arXiv:2210.16726", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImprovements to Embedding-Matching Acoustic-to-Word ASR Using Multiple-Hypothesis Pronunciation-Based Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oHao Yen\\nWoojay Jeon\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.16726\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 19 Feb 2023 21:10:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Fuglsig et al. (§72023§r)', author: 'Andreas Jonas Fuglsig; Jesper Jensen; Zheng-Hua Tan; Lars Søndergaard Bertelsen; Jens Christian Lindof; Jan Østergaard', display:{Lore:['[{"text": "arXiv:2210.17154", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMinimum Processing Near-end Listening Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oAndreas Jonas Fuglsig\\nJesper Jensen\\nZheng-Hua Tan\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.17154\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3282094\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 May 2023 12:31:27 GMT)\\u00a7r"}']}
{title:'Han et al. (§72023§r)', author: 'Jiangyu Han; Yuhang Cao; Heng Lu; Yanhua Long', display:{Lore:['[{"text": "arXiv:2210.17189", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiaCorrect: End-to-end error correction for speaker diarization\\u00a7r\\n\\n\\u00a78\\u00a7oJiangyu Han\\nYuhang Cao\\nHeng Lu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.17189\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 18 Sep 2023 07:06:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper has been superseded by arXiv:2309.08377 (merged from arXiv:2210.17189)\\u00a7r"}']}
{title:'Sawata et al. (§72023§r)', author: 'Ryosuke Sawata; Naoki Murata; Yuhta Takida; Toshimitsu Uesaka; Takashi Shibuya; Shusuke Takahashi; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2210.17287", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffiner: A Versatile Diffusion-based Generative Refiner for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oRyosuke Sawata\\nNaoki Murata\\nYuhta Takida\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.17287\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1547\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 30 Aug 2023 10:18:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2023\\u00a7r"}']}
{title:'Olivier et al. (§72023§r)', author: 'Raphael Olivier; Bhiksha Raj', display:{Lore:['[{"text": "arXiv:2210.17316", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThere is more than one kind of robustness: Fooling Whisper with adversarial examples\\u00a7r\\n\\n\\u00a78\\u00a7oRaphael Olivier\\nBhiksha Raj\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.17316\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 10 Aug 2023 18:32:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at InterSpeech 2023\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Jingyu Li; Wei Liu; Zhaoyang Zhang; Jiong Wang; Tan Lee', display:{Lore:['[{"text": "arXiv:2210.17326", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModel Compression for DNN-based Speaker Verification Using Weight Quantization\\u00a7r\\n\\n\\u00a78\\u00a7oJingyu Li\\nWei Liu\\nZhaoyang Zhang\\nJiong Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.17326\\u00a7r\\n\\nVersion:\\u00a77v5 (Mon, 25 Sep 2023 14:29:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH2023\\u00a7r"}']}
{title:'Chern et al. (§72023§r)', author: 'I-Chun Chern; Kuo-Hsuan Hung; Yi-Ting Chen; Tassadaq Hussain; Mandar Gogate; Amir Hussain; Yu Tsao; Jen-Cheng Hou', display:{Lore:['[{"text": "arXiv:2210.17456", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-Visual Speech Enhancement and Separation by Utilizing Multi-Modal Self-Supervised Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oI-Chun Chern\\nKuo-Hsuan Hung\\nYi-Ting Chen\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.17456\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 1 Jun 2023 03:43:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP AMHAT 2023\\u00a7r"}']}
{title:'Pan et al. (§72023§r)', author: 'Zexu Pan; Wupeng Wang; Marvin Borsdorf; Haizhou Li', display:{Lore:['[{"text": "arXiv:2211.00109", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImagineNET: Target Speaker Extraction with Intermittent Visual Cue through Embedding Inpainting\\u00a7r\\n\\n\\u00a78\\u00a7oZexu Pan\\nWupeng Wang\\nMarvin Borsdorf\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00109\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 9 Mar 2023 06:16:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2023\\u00a7r"}']}
{title:'Nam et al. (§72023§r)', author: 'Kihyun Nam; Youkyum Kim; Jaesung Huh; Hee Soo Heo; Jee-weon Jung; Joon Son Chung', display:{Lore:['[{"text": "arXiv:2211.00437", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDisentangled representation learning for multilingual speaker recognition\\u00a7r\\n\\n\\u00a78\\u00a7oKihyun Nam\\nYoukyum Kim\\nJaesung Huh\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00437\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 6 Jun 2023 19:28:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023\\u00a7r"}']}
{title:'Shi et al. (§72023§r)', author: 'Mohan Shi; Jie Zhang; Zhihao Du; Fan Yu; Qian Chen; Shiliang Zhang; Li-Rong Dai', display:{Lore:['[{"text": "arXiv:2211.00511", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparative Study on Multichannel Speaker-Attributed Automatic Speech Recognition in Multi-party Meetings\\u00a7r\\n\\n\\u00a78\\u00a7oMohan Shi\\nJie Zhang\\nZhihao Du\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00511\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 2 Mar 2023 03:15:44 GMT)\\u00a7r"}']}
{title:'Higuchi et al. (§72023§r)', author: 'Yosuke Higuchi; Tetsuji Ogawa; Tetsunori Kobayashi; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2211.00792", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBECTRA: Transducer-based End-to-End ASR with BERT-Enhanced Encoder\\u00a7r\\n\\n\\u00a78\\u00a7oYosuke Higuchi\\nTetsuji Ogawa\\nTetsunori Kobayashi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00792\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 17 Mar 2023 01:52:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP2023\\u00a7r"}']}
{title:'Higuchi et al. (§72023§r)', author: 'Yosuke Higuchi; Tetsuji Ogawa; Tetsunori Kobayashi; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2211.00795", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInterMPL: Momentum Pseudo-Labeling with Intermediate CTC Loss\\u00a7r\\n\\n\\u00a78\\u00a7oYosuke Higuchi\\nTetsuji Ogawa\\nTetsunori Kobayashi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00795\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 17 Mar 2023 01:56:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Xing Chen; Jie Wang; Xiao-Lei Zhang; Wei-Qiang Zhang; Kunde Yang', display:{Lore:['[{"text": "arXiv:2211.00825", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLMD: A Learnable Mask Network to Detect Adversarial Examples for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oXing Chen\\nJie Wang\\nXiao-Lei Zhang\\nWei-Qiang Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00825\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 14 Jun 2023 13:56:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 9 figures\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Jin Woo Lee; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2211.00878", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Fourier Shift for Binaural Speech Rendering\\u00a7r\\n\\n\\u00a78\\u00a7oJin Woo Lee\\nKyogu Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00878\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 1 May 2023 10:57:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Le et al. (§72023§r)', author: 'Duc Le; Frank Seide; Yuhao Wang; Yang Li; Kjell Schubert; Ozlem Kalinli; Michael L. Seltzer', display:{Lore:['[{"text": "arXiv:2211.00896", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFactorized Blank Thresholding for Improved Runtime Efficiency of Neural Transducers\\u00a7r\\n\\n\\u00a78\\u00a7oDuc Le\\nFrank Seide\\nYuhao Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00896\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 4 Mar 2023 22:08:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at ICASSP 2023\\u00a7r"}']}
{title:'Wright et al. (§72023§r)', author: 'Alec Wright; Vesa Välimäki; Lauri Juvela', display:{Lore:['[{"text": "arXiv:2211.00943", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Guitar Amplifier Modelling With Unpaired Data\\u00a7r\\n\\n\\u00a78\\u00a7oAlec Wright\\nVesa V\\u00e4lim\\u00e4ki\\nLauri Juvela\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.00943\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 20 Mar 2023 07:55:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Lodagala et al. (§72023§r)', author: 'Vasista Sai Lodagala; Sreyan Ghosh; S. Umesh', display:{Lore:['[{"text": "arXiv:2211.01246", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7ldata2vec-aqc: Search for the right Teaching Assistant in the Teacher-Student training setup\\u00a7r\\n\\n\\u00a78\\u00a7oVasista Sai Lodagala\\nSreyan Ghosh\\nS. Umesh\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01246\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 13 May 2023 21:16:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Pan et al. (§72023§r)', author: 'Zexu Pan; Gordon Wichern; François G. Germain; Aswin Subramanian; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2211.01299", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLate Audio-Visual Fusion for In-The-Wild Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oZexu Pan\\nGordon Wichern\\nFran\\u00e7ois G. Germain\\nAswin Subramanian\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01299\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 27 Sep 2023 12:47:35 GMT)\\u00a7r"}']}
{title:'Swietojanski et al. (§72023§r)', author: 'Pawel Swietojanski; Stefan Braun; Dogan Can; Thiago Fraga da Silva; Arnab Ghoshal; Takaaki Hori; Roger Hsiao; Henry Mason; Erik McDermott; Honza Silovsky; Ruchir Travadi; Xiaodan Zhuang', display:{Lore:['[{"text": "arXiv:2211.01438", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVariable Attention Masking for Configurable Transformer Transducer Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oPawel Swietojanski\\nStefan Braun\\nDogan Can\\n+ 8 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01438\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInternational Conference on Acoustics, Speech, and Signal\\n  Processing, 2023 International Conference on Acoustics, Speech, and Signal\\n  Processing International Conference on Acoustics, Speech, and Signal\\n  Processing\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 18 Apr 2023 09:59:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP 2023\\u00a7r"}']}
{title:'Ghosh et al. (§72023§r)', author: 'Sreyan Ghosh; Ashish Seth; S. Umesh; Dinesh Manocha', display:{Lore:['[{"text": "arXiv:2211.01515", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMAST: Multiscale Audio Spectrogram Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oSreyan Ghosh\\nAshish Seth\\nS. Umesh\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01515\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 18 May 2023 01:35:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Seth et al. (§72023§r)', author: 'Ashish Seth; Sreyan Ghosh; S. Umesh; Dinesh Manocha', display:{Lore:['[{"text": "arXiv:2211.01519", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSLICER: Learning universal audio representations using low-resource self-supervised pre-training\\u00a7r\\n\\n\\u00a78\\u00a7oAshish Seth\\nSreyan Ghosh\\nS. Umesh\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01519\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 18 May 2023 01:31:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Li Li; Dongxing Xu; Haoran Wei; Yanhua Long', display:{Lore:['[{"text": "arXiv:2211.01571", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhonetic-assisted Multi-Target Units Modeling for Improving Conformer-Transducer ASR system\\u00a7r\\n\\n\\u00a78\\u00a7oLi Li\\nDongxing Xu\\nHaoran Wei\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01571\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 7 Jul 2023 10:55:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2023\\u00a7r"}']}
{title:'Jin et al. (§72023§r)', author: 'Zengrui Jin; Xurong Xie; Mengzhe Geng; Tianzi Wang; Shujie Hu; Jiajun Deng; Guinan Li; Xunying Liu', display:{Lore:['[{"text": "arXiv:2211.01646", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Data Augmentation Using VAE-GAN for Disordered Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZengrui Jin\\nXurong Xie\\nMengzhe Geng\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01646\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 19 Mar 2023 11:39:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Sukhadia et al. (§72023§r)', author: 'Vrunda N. Sukhadia; A. Arunkumar; S. Umesh', display:{Lore:['[{"text": "arXiv:2211.01669", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChannel-Aware Pretraining of Joint Encoder-Decoder Self-Supervised Model for Telephonic-Speech ASR\\u00a7r\\n\\n\\u00a78\\u00a7oVrunda N. Sukhadia\\nA. Arunkumar\\nS. Umesh\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01669\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 3 Jun 2023 19:58:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures\\u00a7r"}']}
{title:'Wissbrock et al. (§72023§r)', author: 'Peter Wissbrock; David Pelkmann; Yvonne Richter', display:{Lore:['[{"text": "arXiv:2211.01716", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiscussion of Features for Acoustic Anomaly Detection under Industrial Disturbing Noise in an End-of-Line Test of Geared Motors\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Wissbrock\\nDavid Pelkmann\\nYvonne Richter\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.01716\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 26 May 2023 08:26:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oo\\u0327p\\u0327y\\u0327\\u0157i\\u0327\\u0123\\u1e29\\u0163 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for "}','{"text": "advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Ma et al. (§72023§r)', author: 'Pingchuan Ma; Niko Moritz; Stavros Petridis; Christian Fuegen; Maja Pantic', display:{Lore:['[{"text": "arXiv:2211.02133", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreaming Audio-Visual Speech Recognition with Alignment Regularization\\u00a7r\\n\\n\\u00a78\\u00a7oPingchuan Ma\\nNiko Moritz\\nStavros Petridis\\nChristian Fuegen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02133\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 2 Jul 2023 00:33:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Ju-ho Kim; Jungwoo Heo; Hyun-seo Shin; Chan-yeong Lim; Ha-Jin Yu', display:{Lore:['[{"text": "arXiv:2211.02227", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntegrated Parameter-Efficient Tuning for General-Purpose Audio Models\\u00a7r\\n\\n\\u00a78\\u00a7oJu-ho Kim\\nJungwoo Heo\\nHyun-seo Shin\\nChan-yeong Lim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02227\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 2 Mar 2023 04:53:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Koo et al. (§72023§r)', author: 'Junghyun Koo; Marco A. Martínez-Ramírez; Wei-Hsiang Liao; Stefan Uhlich; Kyogu Lee; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2211.02247", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Mixing Style Transfer: A Contrastive Learning Approach to Disentangle Audio Effects\\u00a7r\\n\\n\\u00a78\\u00a7oJunghyun Koo\\nMarco A. Mart\\u00ednez-Ram\\u00edrez\\nWei-Hsiang Liao\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02247\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 11 Apr 2023 07:53:14 GMT)\\u00a7r"}']}
{title:'Lemercier et al. (§72023§r)', author: 'Jean-Marie Lemercier; Julius Richter; Simon Welker; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2211.02397", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalysing Diffusion-based Generative Approaches versus Discriminative Approaches for Speech Restoration\\u00a7r\\n\\n\\u00a78\\u00a7oJean-Marie Lemercier\\nJulius Richter\\nSimon Welker\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02397\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2023 - IEEE International Conference on Acoustics, Speech\\n  and Signal Processing\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 16 Mar 2023 15:44:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oo\\u0327p\\u0327y\\u0327\\u0157i\\u0327\\u0123\\u1e29\\u0163 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for "}','{"text": "advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Tesch et al. (§72023§r)', author: 'Kristina Tesch; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2211.02420", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatially Selective Deep Non-linear Filters for Speaker Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oKristina Tesch\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02420\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2023 - IEEE International Conference on Acoustics, Speech\\n  and Signal Processing\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 23 Mar 2023 08:31:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or "}','{"text": "promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Didier et al. (§72023§r)', author: 'Paul Didier; Toon van Waterschoot; Simon Doclo; Marc Moonen', display:{Lore:['[{"text": "arXiv:2211.02489", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSampling Rate Offset Estimation and Compensation for Distributed Adaptive Node-Specific Signal Estimation in Wireless Acoustic Sensor Networks\\u00a7r\\n\\n\\u00a78\\u00a7oPaul Didier\\nToon van Waterschoot\\nSimon Doclo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02489\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/OJSP.2023.3243851\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 9 Feb 2023 20:12:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 6 figures\\u00a7r"}']}
{title:'Yen et al. (§72023§r)', author: 'Hao Yen; François G. Germain; Gordon Wichern; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2211.02527", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCold Diffusion for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oHao Yen\\nFran\\u00e7ois G. Germain\\nGordon Wichern\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02527\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 23 May 2023 17:56:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, 1 table, 3 algorithms. To appearin ICASSP 2023. With corrected references\\u00a7r"}']}
{title:'Tengan et al. (§72023§r)', author: 'Elisa Tengan; Thomas Dietzen; Santiago Ruiz; Mansour Alkmim; João Cardenuto; Toon van Waterschoot', display:{Lore:['[{"text": "arXiv:2211.02690", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech enhancement using ego-noise references with a microphone array embedded in an unmanned aerial vehicle\\u00a7r\\n\\n\\u00a78\\u00a7oElisa Tengan\\nThomas Dietzen\\nSantiago Ruiz\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02690\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 24th International Congress on Acoustics (ICA),\\n  Gyeongju, South Korea, 24 Oct 2022-28 Oct 2022\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 16 Aug 2023 16:26:30 GMT)\\u00a7r"}']}
{title:'Eskimez et al. (§72023§r)', author: 'Sefik Emre Eskimez; Takuya Yoshioka; Alex Ju; Min Tang; Tanel Parnamaa; Huaming Wang', display:{Lore:['[{"text": "arXiv:2211.02773", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-Time Joint Personalized Speech Enhancement and Acoustic Echo Cancellation\\u00a7r\\n\\n\\u00a78\\u00a7oSefik Emre Eskimez\\nTakuya Yoshioka\\nAlex Ju\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.02773\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 May 2023 23:44:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023\\u00a7r"}']}
{title:'Tian et al. (§72023§r)', author: 'Zhengkun Tian; Hongyu Xiang; Min Li; Feifei Lin; Ke Ding; Guanglu Wan', display:{Lore:['[{"text": "arXiv:2211.03284", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPeak-First CTC: Reducing the Peak Latency of CTC Models by Applying Peak-First Regularization\\u00a7r\\n\\n\\u00a78\\u00a7oZhengkun Tian\\nHongyu Xiang\\nMin Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.03284\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 16 Mar 2023 03:21:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023(5 pages, 2 figures)\\u00a7r"}']}
{title:'Xie et al. (§72023§r)', author: 'Huang Xie; Okko Räsänen; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2211.04070", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Negative Sampling for Contrastive Audio-Text Retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oHuang Xie\\nOkko R\\u00e4s\\u00e4nen\\nTuomas Virtanen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.04070\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 17 Feb 2023 12:37:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Yafeng Chen; Siqi Zheng; Hui Wang; Luyao Cheng; Qian Chen', display:{Lore:['[{"text": "arXiv:2211.04168", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPushing the limits of self-supervised speaker verification using regularized distillation framework\\u00a7r\\n\\n\\u00a78\\u00a7oYafeng Chen\\nSiqi Zheng\\nHui Wang\\nLuyao Cheng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.04168\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 3 Aug 2023 03:47:52 GMT)\\u00a7r"}']}
{title:'Peer et al. (§72023§r)', author: 'Tal Peer; Simon Welker; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2211.04332", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffPhase: Generative Diffusion-based STFT Phase Retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oTal Peer\\nSimon Welker\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.04332\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10095396\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2023 IEEE International Conference on Acoustics, Speech and Signal\\n  Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 2 Jun 2023 08:50:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Junhyeok Lee; Seungu Han; Hyunjae Cho; Wonbin Jung', display:{Lore:['[{"text": "arXiv:2211.04610", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhaseAug: A Differentiable Augmentation for Speech Synthesis to Simulate One-to-Many Mapping\\u00a7r\\n\\n\\u00a78\\u00a7oJunhyeok Lee\\nSeungu Han\\nHyunjae Cho\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.04610\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10096374\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 13 Mar 2023 23:29:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Bartley et al. (§72023§r)', author: 'Travis M. Bartley; Fei Jia; Krishna C. Puvvada; Samuel Kriman; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2211.05103", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAccidental Learners: Spoken Language Identification in Multilingual Self-Supervised Models\\u00a7r\\n\\n\\u00a78\\u00a7oTravis M. Bartley\\nFei Jia\\nKrishna C. Puvvada\\nSamuel Kriman\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.05103\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 13 Mar 2023 13:35:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Shanshan Wang; Soumya Tripathy; Annamaria Mesaros', display:{Lore:['[{"text": "arXiv:2211.05442", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervised learning of audio representations using angular contrastive loss\\u00a7r\\n\\n\\u00a78\\u00a7oShanshan Wang\\nSoumya Tripathy\\nAnnamaria Mesaros\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.05442\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 5 Mar 2023 10:46:34 GMT)\\u00a7r"}']}
{title:'Oh et al. (§72023§r)', author: 'Yoori Oh; Juheon Lee; Yoseob Han; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2211.06160", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-supervised learning for continuous emotional intensity controllable speech synthesis with disentangled representations\\u00a7r\\n\\n\\u00a78\\u00a7oYoori Oh\\nJuheon Lee\\nYoseob Han\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.06160\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 29 May 2023 06:40:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2023\\u00a7r"}']}
{title:'Zhou et al. (§72023§r)', author: 'Wei Zhou; Haotian Wu; Jingjing Xu; Mohammad Zeineldeen; Christoph Lüscher; Ralf Schlüter; Hermann Ney', display:{Lore:['[{"text": "arXiv:2211.06369", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing and Adversarial: Improve ASR with Speaker Labels\\u00a7r\\n\\n\\u00a78\\u00a7oWei Zhou\\nHaotian Wu\\nJingjing Xu\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.06369\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10096722\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 24 Feb 2023 09:21:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at ICASSP 2023\\u00a7r"}']}
{title:'Solanki et al. (§72023§r)', author: 'Mohammad Shaique Solanki; Ashutosh M Bharadwaj; Jeevan K; Prasanta Kumar Ghosh', display:{Lore:['[{"text": "arXiv:2211.06371", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVocal Breath Sound Based Gender Classification\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Shaique Solanki\\nAshutosh M Bharadwaj\\nJeevan K\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.06371\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 May 2023 10:49:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSome updates in the paper. Will new version after updares\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Xiaofei Wang; Zhuo Chen; Yu Shi; Jian Wu; Naoyuki Kanda; Takuya Yoshioka', display:{Lore:['[{"text": "arXiv:2211.06493", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHandling Trade-Offs in Speech Separation with Sparsely-Gated Mixture of Experts\\u00a7r\\n\\n\\u00a78\\u00a7oXiaofei Wang\\nZhuo Chen\\nYu Shi\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.06493\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 31 May 2023 02:36:44 GMT)\\u00a7r"}']}
{title:'Kothinti et al. (§72023§r)', author: 'Sandeep Kothinti; Dimitra Emmanouilidou', display:{Lore:['[{"text": "arXiv:2211.06547", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigations in Audio Captioning: Addressing Vocabulary Imbalance and Evaluating Suitability of Language-Centric Performance Metrics\\u00a7r\\n\\n\\u00a78\\u00a7oSandeep Kothinti\\nDimitra Emmanouilidou\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.06547\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 3 May 2023 17:53:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to EUSIPCO 2023\\u00a7r"}']}
{title:'Landini et al. (§72023§r)', author: 'Federico Landini; Mireia Diez; Alicia Lozano-Diez; Lukáš Burget', display:{Lore:['[{"text": "arXiv:2211.06750", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Speaker and Wide-Band Simulated Conversations as Training Data for End-to-End Neural Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oFederico Landini\\nMireia Diez\\nAlicia Lozano-Diez\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.06750\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 24 Feb 2023 10:52:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Mehta et al. (§72023§r)', author: 'Shivam Mehta; Ambika Kirkland; Harm Lameris; Jonas Beskow; Éva Székely; Gustav Eje Henter', display:{Lore:['[{"text": "arXiv:2211.06892", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOverFlow: Putting flows on top of neural transducers for better TTS\\u00a7r\\n\\n\\u00a78\\u00a7oShivam Mehta\\nAmbika Kirkland\\nHarm Lameris\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.06892\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1996\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 29 May 2023 14:23:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures. Accepted for publication at Interspeech 2023\\u00a7r"}']}
{title:'Liao et al. (§72023§r)', author: 'Dexin Liao; Tao Jiang; Feng Wang; Lin Li; Qingyang Hong', display:{Lore:['[{"text": "arXiv:2211.07201", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards A Unified Conformer Structure: from ASR to ASV Task\\u00a7r\\n\\n\\u00a78\\u00a7oDexin Liao\\nTao Jiang\\nFeng Wang\\nLin Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.07201\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 16 Jan 2023 03:31:59 GMT)\\u00a7r"}']}
{title:'Cappellazzo et al. (§72023§r)', author: 'Umberto Cappellazzo; Daniele Falavigna; Alessio Brutti', display:{Lore:['[{"text": "arXiv:2211.08161", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Investigation of the Combination of Rehearsal and Knowledge Distillation in Continual Learning for Spoken Language Understanding\\u00a7r\\n\\n\\u00a78\\u00a7oUmberto Cappellazzo\\nDaniele Falavigna\\nAlessio Brutti\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08161\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 23 May 2023 10:04:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2023. Code available here: https://github.com/umbertocappellazzo/CL_SLU\\u00a7r"}']}
{title:'Xie et al. (§72023§r)', author: 'Yuying Xie; Thomas Arildsen; Zheng-Hua Tan', display:{Lore:['[{"text": "arXiv:2211.08191", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved disentangled speech representations using contrastive learning in factorized hierarchical variational autoencoder\\u00a7r\\n\\n\\u00a78\\u00a7oYuying Xie\\nThomas Arildsen\\nZheng-Hua Tan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08191\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 14 Jun 2023 07:34:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by EUSIPCO 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Zhichao Wang; Xinsheng Wang; Lei Xie; Yuanzhe Chen; Qiao Tian; Yuping Wang', display:{Lore:['[{"text": "arXiv:2211.08857", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDelivering Speaking Style in Low-resource Voice Conversion with Multi-factor Constraints\\u00a7r\\n\\n\\u00a78\\u00a7oZhichao Wang\\nXinsheng Wang\\nLei Xie\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.08857\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 14 Mar 2023 01:42:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Xiang et al. (§72023§r)', author: 'Yang Xiang; Jesper Lisby Højvang; Morten Højfeldt Rasmussen; Mads Græsbøll Christensen', display:{Lore:['[{"text": "arXiv:2211.09166", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Two-Stage Deep Representation Learning-Based Speech Enhancement Method Using Variational Autoencoder and Adversarial Training\\u00a7r\\n\\n\\u00a78\\u00a7oYang Xiang\\nJesper Lisby H\\u00f8jvang\\nMorten H\\u00f8jfeldt Rasmussen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.09166\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 27 Sep 2023 08:28:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE/ACM Transactionson Audio, Speech and Language Processing\\u00a7r"}']}
{title:'Xie et al. (§72023§r)', author: 'Xurong Xie; Xunying Liu; Hui Chen; Hongan Wang', display:{Lore:['[{"text": "arXiv:2211.09313", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Model-based speaker adaptation of end-to-end lattice-free MMI model for speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXurong Xie\\nXunying Liu\\nHui Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.09313\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 6 Jan 2023 08:08:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 2 figures, submitted to ICASSP 2023\\u00a7r"}']}
{title:'Kang et al. (§72023§r)', author: 'Minki Kang; Dongchan Min; Sung Ju Hwang', display:{Lore:['[{"text": "arXiv:2211.09383", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGrad-StyleSpeech: Any-speaker Adaptive Text-to-Speech Synthesis with Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oMinki Kang\\nDongchan Min\\nSung Ju Hwang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.09383\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 14 Mar 2023 02:42:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Guo et al. (§72023§r)', author: 'Yiwei Guo; Chenpeng Du; Xie Chen; Kai Yu', display:{Lore:['[{"text": "arXiv:2211.09496", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmoDiff: Intensity Controllable Emotional Text-to-Speech with Soft-Label Guidance\\u00a7r\\n\\n\\u00a78\\u00a7oYiwei Guo\\nChenpeng Du\\nXie Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.09496\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 16 Feb 2023 06:30:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP2023\\u00a7r"}']}
{title:'Gao et al. (§72023§r)', author: 'Xiaoxue Gao; Xianghu Yue; Haizhou Li', display:{Lore:['[{"text": "arXiv:2211.10152", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Transriber: Few-shot Lyrics Transcription with Self-training\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoxue Gao\\nXianghu Yue\\nHaizhou Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.10152\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 2 Mar 2023 09:12:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Saijo et al. (§72023§r)', author: 'Kohei Saijo; Tetsuji Ogawa', display:{Lore:['[{"text": "arXiv:2211.10194", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Remixing: Unsupervised Speech Separation via Separation and Remixing\\u00a7r\\n\\n\\u00a78\\u00a7oKohei Saijo\\nTetsuji Ogawa\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.10194\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 1 Sep 2023 10:13:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2023, 5pages, 2figures, 2tables\\u00a7r"}']}
{title:'Boes et al. (§72023§r)', author: 'Wim Boes; Hugo Van hamme', display:{Lore:['[{"text": "arXiv:2211.10539", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImpact of visual assistance for automated audio captioning\\u00a7r\\n\\n\\u00a78\\u00a7oWim Boes\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.10539\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 20 Feb 2023 22:27:16 GMT)\\u00a7r"}']}
{title:'López-Espejo et al. (§72023§r)', author: 'Iván López-Espejo; Ram C. M. C. Shekar; Zheng-Hua Tan; Jesper Jensen; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:2211.10565", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFilterbank Learning for Noise-Robust Small-Footprint Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oIv\\u00e1n L\\u00f3pez-Espejo\\nRam C. M. C. Shekar\\nZheng-Hua Tan\\nJesper Jensen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.10565\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 23 Feb 2023 21:38:32 GMT)\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Xinfa Zhu; Yi Lei; Kun Song; Yongmao Zhang; Tao Li; Lei Xie', display:{Lore:['[{"text": "arXiv:2211.10568", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Speaker Expressive Speech Synthesis via Multiple Factors Decoupling\\u00a7r\\n\\n\\u00a78\\u00a7oXinfa Zhu\\nYi Lei\\nKun Song\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.10568\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 14 Mar 2023 07:20:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2023\\u00a7r"}']}
{title:'Singh et al. (§72023§r)', author: 'Anup Singh; Kris Demuynck; Vipul Arora', display:{Lore:['[{"text": "arXiv:2211.11060", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSimultaneously Learning Robust Audio Embeddings and balanced Hash codes for Query-by-Example\\u00a7r\\n\\n\\u00a78\\u00a7oAnup Singh\\nKris Demuynck\\nVipul Arora\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.11060\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 18 Jan 2023 19:28:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWe need to rewrite the subsection \'Efficiency\' section under section 4to make it more easy to follow for the readers and appreciate our results\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Qiushi Zhu; Long Zhou; Ziqiang Zhang; Shujie Liu; Binxing Jiao; Jie Zhang; Lirong Dai; Daxin Jiang; Jinyu Li; Furu Wei', display:{Lore:['[{"text": "arXiv:2211.11275", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oQiushi Zhu\\nLong Zhou\\nZiqiang Zhang\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.11275\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TMM.2023.3275873\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 19 May 2023 10:03:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, Accepted by IEEE Transactions on Multimedia\\u00a7r"}']}
{title:'Kothapally et al. (§72023§r)', author: 'Vinay Kothapally; Yong Xu; Meng Yu; Shi-Xiong Zhang; Dong Yu', display:{Lore:['[{"text": "arXiv:2211.12590", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Neural Mel-Subband Beamformer for In-car Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oVinay Kothapally\\nYong Xu\\nMeng Yu\\nShi-Xiong Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.12590\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 11 Mar 2023 20:38:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Lameris et al. (§72023§r)', author: 'Harm Lameris; Shivam Mehta; Gustav Eje Henter; Joakim Gustafson; Éva Székely', display:{Lore:['[{"text": "arXiv:2211.13533", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProsody-controllable spontaneous TTS with neural HMMs\\u00a7r\\n\\n\\u00a78\\u00a7oHarm Lameris\\nShivam Mehta\\nGustav Eje Henter\\nJoakim Gustafson\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.13533\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10097200\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 1 Jun 2023 10:51:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, Published at ICASSP 2023\\u00a7r"}']}
{title:'Carneiro et al. (§72023§r)', author: 'Hugo Carneiro; Cornelius Weber; Stefan Wermter', display:{Lore:['[{"text": "arXiv:2211.15377", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhose Emotion Matters? Speaking Activity Localisation without Prior Knowledge\\u00a7r\\n\\n\\u00a78\\u00a7oHugo Carneiro\\nCornelius Weber\\nStefan Wermter\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.15377\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.neucom.2023.126271\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nNeurocomputing (2023); Volume 545; 126271\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 15 Aug 2023 17:33:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o17 pages, 8 figures, 7 tables, Published in Neurocomputing\\u00a7r"}']}
{title:'Nakamura et al. (§72023§r)', author: 'Tomohiko Nakamura; Shinnosuke Takamichi; Naoko Tanji; Satoru Fukayama; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2211.16028", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJaCappella Corpus: A Japanese a Cappella Vocal Ensemble Corpus\\u00a7r\\n\\n\\u00a78\\u00a7oTomohiko Nakamura\\nShinnosuke Takamichi\\nNaoko Tanji\\nSatoru Fukayama\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.16028\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10095569\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE International Conference on Acoustics, Speech, and Signal\\n  Processing (ICASSP), Jun. 2023, 5 pages\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 24 Feb 2023 08:06:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP2023\\u00a7r"}']}
{title:'Minixhofer et al. (§72023§r)', author: 'Christoph Minixhofer; Ondřej Klejch; Peter Bell', display:{Lore:['[{"text": "arXiv:2211.16049", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluating and reducing the distance between synthetic and real speech distributions\\u00a7r\\n\\n\\u00a78\\u00a7oChristoph Minixhofer\\nOnd\\u0159ej Klejch\\nPeter Bell\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.16049\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 May 2023 08:41:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be presented at INTERSPEECH 2023\\u00a7r"}']}
{title:'Noé et al. (§72023§r)', author: 'Paul-Gauthier Noé; Xiaoxiao Miao; Xin Wang; Junichi Yamagishi; Jean-François Bonastre; Driss Matrouf', display:{Lore:['[{"text": "arXiv:2211.16065", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHiding speaker\'s sex in speech using zero-evidence speaker representation in an analysis/synthesis pipeline\\u00a7r\\n\\n\\u00a78\\u00a7oPaul-Gauthier No\\u00e9\\nXiaoxiao Miao\\nXin Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.16065\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 24 Mar 2023 12:13:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'von Neumann et al. (§72023§r)', author: 'Thilo von Neumann; Christoph Boeddeker; Keisuke Kinoshita; Marc Delcroix; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2211.16112", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Word Error Rate Definitions and their Efficient Computation for Multi-Speaker Speech Recognition Systems\\u00a7r\\n\\n\\u00a78\\u00a7oThilo von Neumann\\nChristoph Boeddeker\\nKeisuke Kinoshita\\nMarc Delcroix\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.16112\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10094784\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 21 Jul 2023 07:28:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at ICASSP 2023\\u00a7r"}']}
{title:'Yu et al. (§72023§r)', author: 'Jianwei Yu; Yi Luo; Hangting Chen; Rongzhi Gu; Chao Weng', display:{Lore:['[{"text": "arXiv:2212.00406", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHigh Fidelity Speech Enhancement with Band-split RNN\\u00a7r\\n\\n\\u00a78\\u00a7oJianwei Yu\\nYi Luo\\nHangting Chen\\nRongzhi Gu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.00406\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 6 Jun 2023 05:30:55 GMT)\\u00a7r"}']}
{title:'Shen et al. (§72023§r)', author: 'Pengjie Shen; Shulin He; Xueliang Zhang', display:{Lore:['[{"text": "arXiv:2212.01106", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExARN: self-attending RNN for target speaker extraction\\u00a7r\\n\\n\\u00a78\\u00a7oPengjie Shen\\nShulin He\\nXueliang Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.01106\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 12 Mar 2023 07:42:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe overall quality of the article is not good enough\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Zih-Ching Chen; Yu-Shun Sung; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2212.01282", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCHAPTER: Exploiting Convolutional Neural Network Adapters for Self-supervised Speech Models\\u00a7r\\n\\n\\u00a78\\u00a7oZih-Ching Chen\\nYu-Shun Sung\\nHung-yi Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.01282\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 20 Jan 2023 15:53:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2023. Under review\\u00a7r"}']}
{title:'Ruaud et al. (§72023§r)', author: 'Elise Ruaud; Guillaume Dutilleux', display:{Lore:['[{"text": "arXiv:2212.02616", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a75physics.class-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound emergence as a predictor of short-term annoyance from wind turbine noise\\u00a7r\\n\\n\\u00a78\\u00a7oElise Ruaud\\nGuillaume Dutilleux\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.02616\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0017112\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 25 Jan 2023 16:23:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in the Journal or the Acoustical Society of America. 17 pages, 8 figures, 3 tables\\u00a7r"}']}
{title:'Cwitkowitz et al. (§72023§r)', author: 'Frank Cwitkowitz; Toni Hirvonen; Anssi Klapuri', display:{Lore:['[{"text": "arXiv:2212.03023", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFretNet: Continuous-Valued Pitch Contour Streaming for Polyphonic Guitar Tablature Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oFrank Cwitkowitz\\nToni Hirvonen\\nAnssi Klapuri\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.03023\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10094825\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 14 Mar 2023 23:06:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Xu et al. (§72023§r)', author: 'Xinmeng Xu; Weiping Tu; Yuhong Yang', display:{Lore:['[{"text": "arXiv:2212.03408", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelector-Enhancer: Learning Dynamic Selection of Local and Non-local Attention Operation for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oXinmeng Xu\\nWeiping Tu\\nYuhong Yang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.03408\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 13 Jan 2023 09:13:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by AAAI 2023\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Zijian Yang; Wei Zhou; Ralf Schlüter; Hermann Ney', display:{Lore:['[{"text": "arXiv:2212.04325", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLattice-Free Sequence Discriminative Training for Phoneme-Based Neural Transducers\\u00a7r\\n\\n\\u00a78\\u00a7oZijian Yang\\nWei Zhou\\nRalf Schl\\u00fcter\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.04325\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 25 May 2023 15:54:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at ICASSP 2023\\u00a7r"}']}
{title:'Valin et al. (§72023§r)', author: 'Jean-Marc Valin; Jan Büthe; Ahmed Mustafa', display:{Lore:['[{"text": "arXiv:2212.04453", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Bitrate Redundancy Coding of Speech Using a Rate-Distortion-Optimized Variational Autoencoder\\u00a7r\\n\\n\\u00a78\\u00a7oJean-Marc Valin\\nJan B\\u00fcthe\\nAhmed Mustafa\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.04453\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 24 Feb 2023 19:51:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProc. ICASSP 2023, 5 pages\\u00a7r"}']}
{title:'Mustafa et al. (§72023§r)', author: 'Ahmed Mustafa; Jean-Marc Valin; Jan Büthe; Paris Smaragdis; Mike Goodwin', display:{Lore:['[{"text": "arXiv:2212.04532", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFramewise WaveGAN: High Speed Adversarial Vocoder in Time Domain with Very Low Computational Complexity\\u00a7r\\n\\n\\u00a78\\u00a7oAhmed Mustafa\\nJean-Marc Valin\\nJan B\\u00fcthe\\nParis Smaragdis\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.04532\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 2 Mar 2023 00:45:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023, demo: https://ahmed-fau.github.io/fwgan_demo/\\u00a7r"}']}
{title:'Fang et al. (§72023§r)', author: 'Huajian Fang; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2212.04831", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUncertainty Estimation in Deep Speech Enhancement Using Complex Gaussian Mixture Models\\u00a7r\\n\\n\\u00a78\\u00a7oHuajian Fang\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.04831\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2023 - IEEE International Conference on Acoustics, Speech\\n  and Signal Processing\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 15 May 2023 14:32:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or "}','{"text": "promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Raj et al. (§72023§r)', author: 'Desh Raj; Daniel Povey; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:2212.05271", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGPU-accelerated Guided Source Separation for Meeting Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oDesh Raj\\nDaniel Povey\\nSanjeev Khudanpur\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.05271\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 13 Aug 2023 18:30:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 4 figures. To appear at InterSpeech 2023. Code available at https://github.com/desh2608/gss\\u00a7r"}']}
{title:'Jia et al. (§72023§r)', author: 'Dongya Jia; Qiao Tian; Kainan Peng; Jiaxin Li; Yuanzhe Chen; Mingbo Ma; Yuping Wang; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2212.05751", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero-Shot Accent Conversion using Pseudo Siamese Disentanglement Network\\u00a7r\\n\\n\\u00a78\\u00a7oDongya Jia\\nQiao Tian\\nKainan Peng\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.05751\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 10 Aug 2023 11:49:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Jinglin Liu; Zhenhui Ye; Qian Chen; Siqi Zheng; Wen Wang; Qinglin Zhang; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2212.07000", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDopplerBAS: Binaural Audio Synthesis Addressing Doppler Effect\\u00a7r\\n\\n\\u00a78\\u00a7oJinglin Liu\\nZhenhui Ye\\nQian Chen\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.07000\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 1 Jun 2023 07:35:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ACL 2023 short paper; key words: binaural audio, stereophonic sound\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Dongheon Lee; Jung-Woo Choi', display:{Lore:['[{"text": "arXiv:2212.07570", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeFT-AN: Dense Frequency-Time Attentive Network for Multichannel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oDongheon Lee\\nJung-Woo Choi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.07570\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2023.3244428\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Signal Processing Letters, Vol. 30, pp. 155-159, 2023\\u00a7r\\n\\nVersion:\\u00a77v5 (Mon, 6 Mar 2023 06:20:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 3 tables. This article has been published by IEEE Signal Processing Letters. This version is the authors\' version and may vary from the final publication in details\\u00a7r"}']}
{title:'Shon et al. (§72023§r)', author: 'Suwon Shon; Felix Wu; Kwangyoun Kim; Prashant Sridhar; Karen Livescu; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2212.08542", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContext-aware Fine-tuning of Self-supervised Speech Models\\u00a7r\\n\\n\\u00a78\\u00a7oSuwon Shon\\nFelix Wu\\nKwangyoun Kim\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.08542\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 28 Mar 2023 21:20:11 GMT)\\u00a7r"}']}
{title:'Hao et al. (§72023§r)', author: 'Xiang Hao; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2212.09019", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast FullSubNet: Accelerate Full-band and Sub-band Fusion Model for Single-channel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oXiang Hao\\nXiaofei Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.09019\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 7 Mar 2023 00:49:05 GMT)\\u00a7r"}']}
{title:'Raitio et al. (§72023§r)', author: 'Tuomo Raitio; Javier Latorre; Andrea Davis; Tuuli Morrill; Ladan Golipour', display:{Lore:['[{"text": "arXiv:2212.10075", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving the quality of neural TTS using long-form content and multi-speaker multi-style modeling\\u00a7r\\n\\n\\u00a78\\u00a7oTuomo Raitio\\nJavier Latorre\\nAndrea Davis\\nTuuli Morrill\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.10075\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 28 Jun 2023 04:15:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to 12thISCA Speech Synthesis Workshop (SSW)\\u00a7r"}']}
{title:'Hono et al. (§72023§r)', author: 'Yukiya Hono; Kei Hashimoto; Yoshihiko Nankaku; Keiichi Tokuda', display:{Lore:['[{"text": "arXiv:2212.13703", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSinging Voice Synthesis Based on a Musical Note Position-Aware Attention Mechanism\\u00a7r\\n\\n\\u00a78\\u00a7oYukiya Hono\\nKei Hashimoto\\nYoshihiko Nankaku\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.13703\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 14 Mar 2023 18:16:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, 2 tables, accepted to ICASSP 2023\\u00a7r"}']}
{title:'Cao et al. (§72023§r)', author: 'Lixin Cao; Jun Wang; Ben Yang; Dan Su; Dong Yu', display:{Lore:['[{"text": "arXiv:2301.00656", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTriNet: stabilizing self-supervised learning from complete or slow collapse on ASR\\u00a7r\\n\\n\\u00a78\\u00a7oLixin Cao\\nJun Wang\\nBen Yang\\nDan Su\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.00656\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 14 Mar 2023 12:23:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Tang et al. (§72023§r)', author: 'Kun Tang; Yuqi Wang; Shaobo Wang; Da Gao; Haojie Li; Xindong Liang; Patrick Sebbah; Yibin Li; Jin Zhang; Junhui Shi', display:{Lore:['[{"text": "arXiv:2301.00833", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a75physics.app-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHyperuniform disordered parametric loudspeaker array\\u00a7r\\n\\n\\u00a78\\u00a7oKun Tang\\nYuqi Wang\\nShaobo Wang\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.00833\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 13 Apr 2023 07:32:24 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Daiyu Zhang; Ju-Chiang Wang; Katerina Kosta; Jordan B. L. Smith; Shicen Zhou', display:{Lore:['[{"text": "arXiv:2301.01361", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModeling the Rhythm from Lyrics for Melody Generation of Pop Song\\u00a7r\\n\\n\\u00a78\\u00a7oDaiyu Zhang\\nJu-Chiang Wang\\nKaterina Kosta\\nJordan B. L. Smith\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.01361\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 3 Jan 2023 21:30:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in ISMIR 2022\\u00a7r"}']}
{title:'Holighaus et al. (§72023§r)', author: 'Nicki Holighaus; Günther Koliander; Clara Hollomey; Friedrich Pillichshammer', display:{Lore:['[{"text": "arXiv:2301.01640", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.NA\\u00a7r, \\u00a72math.FA\\u00a7r, \\u00a72math.NA\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGrid-Based Decimation for Wavelet Transforms with Stably Invertible Implementation\\u00a7r\\n\\n\\u00a78\\u00a7oNicki Holighaus\\nG\\u00fcnther Koliander\\nClara Hollomey\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.01640\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3235197\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech and Language Processing,\\n  31:789--801, January 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 4 Jan 2023 14:32:09 GMT)\\u00a7r"}']}
{title:'Jiang et al. (§72023§r)', author: 'Zifan Jiang; Adrian Soldati; Isaac Schamberg; Adriano R. Lameira; Steven Moran', display:{Lore:['[{"text": "arXiv:2301.02214", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Sound Event Detection and Classification of Great Ape Calls Using Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oZifan Jiang\\nAdrian Soldati\\nIsaac Schamberg\\nAdriano R. Lameira\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.02214\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 12 Sep 2023 09:17:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper is published as: Jiang, Zifan, Adrian Soldati, Isaac Schamberg, Adriano R. Lameira and Steven Moran. Automatic Sound Event Detection and Classification of Great Ape Calls Using Neural Networks. In Proceedings"}','{"text": "of the 20th International Congressof Phonetic Sciences (ICPhS 2023), 3100-3104, Prague, Czech Republic\\u00a7r"}']}
{title:'Nishihara et al. (§72023§r)', author: 'Miku Nishihara; Yukiya Hono; Kei Hashimoto; Yoshihiko Nankaku; Keiichi Tokuda', display:{Lore:['[{"text": "arXiv:2301.02262", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSinging voice synthesis based on frame-level sequence-to-sequence models considering vocal timing deviation\\u00a7r\\n\\n\\u00a78\\u00a7oMiku Nishihara\\nYukiya Hono\\nKei Hashimoto\\nYoshihiko Nankaku\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.02262\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 22 Feb 2023 05:12:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures\\u00a7r"}']}
{title:'Chan et al. (§72023§r)', author: 'David M. Chan; Shalini Ghosh; Ariya Rastrow; Björn Hoffmeister', display:{Lore:['[{"text": "arXiv:2301.02736", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing External Off-Policy Speech-To-Text Mappings in Contextual End-To-End Automated Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDavid M. Chan\\nShalini Ghosh\\nAriya Rastrow\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.02736\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 Jan 2023 22:32:50 GMT)\\u00a7r"}']}
{title:'Tinchev et al. (§72023§r)', author: 'Georgi Tinchev; Marta Czarnowska; Kamil Deja; Kayoko Yanagisawa; Marius Cotescu', display:{Lore:['[{"text": "arXiv:2301.04606", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModelling low-resource accents without accent-specific TTS frontend\\u00a7r\\n\\n\\u00a78\\u00a7oGeorgi Tinchev\\nMarta Czarnowska\\nKamil Deja\\nKayoko Yanagisawa\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.04606\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 11 Jan 2023 18:00:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe first two authors contributed equally to this work. In Review. Samples available on https://bit.ly/3V52ZrF\\u00a7r"}']}
{title:'Casco-Rodriguez (§72023§r)', author: 'Josue Casco-Rodriguez', display:{Lore:['[{"text": "arXiv:2301.05295", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRock Guitar Tablature Generation via Natural Language Processing\\u00a7r\\n\\n\\u00a78\\u00a7oJosue Casco-Rodriguez\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.05295\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 1 Feb 2023 17:32:09 GMT)\\u00a7r"}']}
{title:'Luz et al. (§72023§r)', author: 'Saturnino Luz; Fasih Haider; Davida Fromm; Ioulietta Lazarou; Ioannis Kompatsiaris; Brian MacWhinney', display:{Lore:['[{"text": "arXiv:2301.05562", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultilingual Alzheimer\'s Dementia Recognition through Spontaneous Speech: a Signal Processing Grand Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oSaturnino Luz\\nFasih Haider\\nDavida Fromm\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.05562\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 13 Jan 2023 14:09:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023 SPGC description\\u00a7r"}']}
{title:'Singh et al. (§72023§r)', author: 'Premjeet Singh; Md Sahidullah; Goutam Saha', display:{Lore:['[{"text": "arXiv:2301.05868", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModulation spectral features for speech emotion recognition using deep neural networks\\u00a7r\\n\\n\\u00a78\\u00a7oPremjeet Singh\\nMd Sahidullah\\nGoutam Saha\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.05868\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.specom.2022.11.005\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nVolume 146, January 2023, Pages 53-69\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 14 Jan 2023 09:36:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in Elsevier\'sSpeech Communication Journal\\u00a7r"}']}
{title:'Taherian et al. (§72023§r)', author: 'Hassan Taherian; DeLiang Wang', display:{Lore:['[{"text": "arXiv:2301.06458", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-resolution location-based training for multi-channel continuous speech separation\\u00a7r\\n\\n\\u00a78\\u00a7oHassan Taherian\\nDeLiang Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.06458\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 16 Jan 2023 15:02:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 23\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Young-Eun Lee; Seo-Hyun Lee; Sang-Ho Kim; Seong-Whan Lee', display:{Lore:['[{"text": "arXiv:2301.07173", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Voice Reconstruction from EEG during Imagined Speech\\u00a7r\\n\\n\\u00a78\\u00a7oYoung-Eun Lee\\nSeo-Hyun Lee\\nSang-Ho Kim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.07173\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 2 Jan 2023 05:10:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 4 figures, accepted paper of AAAI2023 in main track\\u00a7r"}']}
{title:'Lohmann et al. (§72023§r)', author: 'Anselm Lohmann; Toon van Waterschoot; Joerg Bitzer; Simon Doclo', display:{Lore:['[{"text": "arXiv:2301.07649", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDereverberation in Acoustic Sensor Networks Using Weighted Prediction Error With Microphone-dependent Prediction Delays\\u00a7r\\n\\n\\u00a78\\u00a7oAnselm Lohmann\\nToon van Waterschoot\\nJoerg Bitzer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.07649\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 18 Jan 2023 16:52:07 GMT)\\u00a7r"}']}
{title:'Williams et al. (§72023§r)', author: 'Jennifer Williams; Karla Pizzi; Shuvayanti Das; Paul-Gauthier Noe', display:{Lore:['[{"text": "arXiv:2301.08925", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNew Challenges for Content Privacy in Speech and Audio\\u00a7r\\n\\n\\u00a78\\u00a7oJennifer Williams\\nKarla Pizzi\\nShuvayanti Das\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.08925\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/SPSC.2022-1\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 21 Jan 2023 09:16:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in ISCA SPSCSymposium 2022\\u00a7r"}']}
{title:'Baeg et al. (§72023§r)', author: 'Kwangje Baeg; Yeong-Gwan Kim; Young-Sub Han; Byoung-Ki Jeon', display:{Lore:['[{"text": "arXiv:2301.09058", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Speaker Embeddings with Adversarial Multi-task Learning for Age Group Classification\\u00a7r\\n\\n\\u00a78\\u00a7oKwangje Baeg\\nYeong-Gwan Kim\\nYoung-Sub Han\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.09058\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 22 Jan 2023 05:01:13 GMT)\\u00a7r"}']}
{title:'Yu et al. (§72023§r)', author: 'Wangyang Yu; W. Bastiaan Kleijn', display:{Lore:['[{"text": "arXiv:2301.09198", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEstimation of Source and Receiver Positions, Room Geometry and Reflection Coefficients From a Single Room Impulse Response\\u00a7r\\n\\n\\u00a78\\u00a7oWangyang Yu\\nW. Bastiaan Kleijn\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.09198\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 22 Jan 2023 20:46:10 GMT)\\u00a7r"}']}
{title:'Riedel et al. (§72023§r)', author: 'Stefan Riedel; Matthias Frank; Franz Zotter', display:{Lore:['[{"text": "arXiv:2301.10210", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerceptual evaluation of listener envelopment using spatial granular synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oStefan Riedel\\nMatthias Frank\\nFranz Zotter\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.10210\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.17743/jaes.2022.0088\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 30 Jan 2023 15:36:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to the Journal of the Audio Engineering Society (JAES)\\u00a7r"}']}
{title:'Lutati et al. (§72023§r)', author: 'Shahar Lutati; Eliya Nachmani; Lior Wolf', display:{Lore:['[{"text": "arXiv:2301.10752", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSeparate And Diffuse: Using a Pretrained Diffusion Model for Improving Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oShahar Lutati\\nEliya Nachmani\\nLior Wolf\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.10752\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 24 Jun 2023 05:28:19 GMT)\\u00a7r"}']}
{title:'Morikawa et al. (§72023§r)', author: 'Masahiro Morikawa; Akika Nakamichi', display:{Lore:['[{"text": "arXiv:2301.11176", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a75physics.class-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA simple model for pink noise from amplitude modulations\\u00a7r\\n\\n\\u00a78\\u00a7oMasahiro Morikawa\\nAkika Nakamichi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.11176\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 26 Jan 2023 15:33:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 9 figures\\u00a7r"}']}
{title:'Rieger (§72023§r)', author: 'Will Rieger', display:{Lore:['[{"text": "arXiv:2301.11276", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBayesSpeech: A Bayesian Transformer Network for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oWill Rieger\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.11276\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 16 Jan 2023 16:19:04 GMT)\\u00a7r"}']}
{title:'Babianski et al. (§72023§r)', author: 'Mikolaj Babianski; Kamil Pokora; Raahil Shah; Rafal Sienkiewicz; Daniel Korzekwa; Viacheslav Klimkov', display:{Lore:['[{"text": "arXiv:2301.11446", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn granularity of prosodic representations in expressive text-to-speech\\u00a7r\\n\\n\\u00a78\\u00a7oMikolaj Babianski\\nKamil Pokora\\nRaahil Shah\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.11446\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/SLT54892.2023.10022793\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2022 IEEE Spoken Language Technology Workshop (SLT), pp. 892-899\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 26 Jan 2023 22:24:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEESLT 2022\\u00a7r"}']}
{title:'Morrison et al. (§72023§r)', author: 'Max Morrison; Caedon Hsieh; Nathan Pruyne; Bryan Pardo', display:{Lore:['[{"text": "arXiv:2301.12258", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-domain Neural Pitch and Periodicity Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oMax Morrison\\nCaedon Hsieh\\nNathan Pruyne\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.12258\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 9 Jun 2023 22:12:53 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Yixuan Zhang; Meng Yu; Hao Zhang; Dong Yu; DeLiang Wang', display:{Lore:['[{"text": "arXiv:2301.12363", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeuralKalman: A Learnable Kalman Filter for Acoustic Echo Cancellation\\u00a7r\\n\\n\\u00a78\\u00a7oYixuan Zhang\\nMeng Yu\\nHao Zhang\\nDong Yu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.12363\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 26 Dec 2023 07:05:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe term of the algorithm is renamed because it conflicts with an existing KalmanNet algorithm proposed by Revach et. al. (arXiv:2107.10043); Accepted by ASRU 2023\\u00a7r"}']}
{title:'Saeki et al. (§72023§r)', author: 'Takaaki Saeki; Soumi Maiti; Xinjian Li; Shinji Watanabe; Shinnosuke Takamichi; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2301.12596", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning to Speak from Text: Zero-Shot Multilingual Text-to-Speech with Unsupervised Text Pretraining\\u00a7r\\n\\n\\u00a78\\u00a7oTakaaki Saeki\\nSoumi Maiti\\nXinjian Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.12596\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 27 May 2023 15:15:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in IJCAI 2023\\u00a7r"}']}
{title:'Yin et al. (§72023§r)', author: 'Jun Yin; Stefano Damiano; Marian Verhelst; Toon van Waterschoot; Andre Guntoro', display:{Lore:['[{"text": "arXiv:2301.12808", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-Time Acoustic Perception for Automotive Applications\\u00a7r\\n\\n\\u00a78\\u00a7oJun Yin\\nStefano Damiano\\nMarian Verhelst\\nToon van Waterschoot\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.12808\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 Jan 2023 12:00:20 GMT)\\u00a7r"}']}
{title:'Dietzen et al. (§72023§r)', author: 'Thomas Dietzen; Randall Ali; Maja Taseska; Toon van Waterschoot', display:{Lore:['[{"text": "arXiv:2301.13057", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMYRiAD: A Multi-Array Room Acoustic Database\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Dietzen\\nRandall Ali\\nMaja Taseska\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.13057\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1186/s13636-023-00284-9\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nEURASIP J. Audio Speech Music Process., vol. 2023, no. 17, pp.\\n  1-14, Apr. 2023\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 12 May 2023 15:05:53 GMT)\\u00a7r"}']}
{title:'Zmolikova et al. (§72023§r)', author: 'Katerina Zmolikova; Marc Delcroix; Tsubasa Ochiai; Keisuke Kinoshita; Jan Černocký; Dong Yu', display:{Lore:['[{"text": "arXiv:2301.13341", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Target Speech Extraction: An Overview\\u00a7r\\n\\n\\u00a78\\u00a7oKaterina Zmolikova\\nMarc Delcroix\\nTsubasa Ochiai\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.13341\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/MSP.2023.3240008\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 31 Jan 2023 00:26:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEESignal Processing Magazine on Apr. 25, 2022, and accepted on Jan. 12, 2023\\u00a7r"}']}
{title:'Puffay et al. (§72023§r)', author: 'Corentin Puffay; Bernd Accou; Lies Bollens; Mohammad Jalilpour Monesi; Jonas Vanthornhout; Hugo Van hamme; Tom Francart', display:{Lore:['[{"text": "arXiv:2302.01736", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRelating EEG to continuous speech using deep neural networks: a review\\u00a7r\\n\\n\\u00a78\\u00a7oCorentin Puffay\\nBernd Accou\\nLies Bollens\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.01736\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 16 Jun 2023 15:50:26 GMT)\\u00a7r"}']}
{title:'Michaloliakos et al. (§72023§r)', author: 'Anargyros Michaloliakos; Chongan Wang; Alexander F. Vakakis', display:{Lore:['[{"text": "arXiv:2302.01746", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMachine Learning Extreme Acoustic Non-reciprocity in a Linear Waveguide with Multiple Nonlinear Asymmetric Gates\\u00a7r\\n\\n\\u00a78\\u00a7oAnargyros Michaloliakos\\nChongan Wang\\nAlexander F. Vakakis\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.01746\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 Feb 2023 17:28:04 GMT)\\u00a7r"}']}
{title:'Luo et al. (§72023§r)', author: 'Jiachen Luo; Huy Phan; Joshua Reiss', display:{Lore:['[{"text": "arXiv:2302.02447", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lcross-modal fusion techniques for utterance-level emotion recognition from text and speech\\u00a7r\\n\\n\\u00a78\\u00a7oJiachen Luo\\nHuy Phan\\nJoshua Reiss\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.02447\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 5 Feb 2023 18:16:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 2 figures\\u00a7r"}']}
{title:'Stan (§72023§r)', author: 'Adriana Stan', display:{Lore:['[{"text": "arXiv:2302.02742", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lResidual Information in Deep Speaker Embedding Architectures\\u00a7r\\n\\n\\u00a78\\u00a7oAdriana Stan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.02742\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3390/math10213927\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nMathematics 2022, 10(21), 3927\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 Feb 2023 12:37:57 GMT)\\u00a7r"}']}
{title:'Ditthapron et al. (§72023§r)', author: 'Apiwat Ditthapron; Emmanuel O. Agu; Adam C. Lammert', display:{Lore:['[{"text": "arXiv:2302.04161", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMasking Kernel for Learning Energy-Efficient Representations for Speaker Recognition and Mobile Health\\u00a7r\\n\\n\\u00a78\\u00a7oApiwat Ditthapron\\nEmmanuel O. Agu\\nAdam C. Lammert\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.04161\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1026\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. INTERSPEECH 2023, 2843-2847\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 15 Aug 2023 16:43:59 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Li-Wei Chen; Shinji Watanabe; Alexander Rudnicky', display:{Lore:['[{"text": "arXiv:2302.04215", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech\\u00a7r\\n\\n\\u00a78\\u00a7oLi-Wei Chen\\nShinji Watanabe\\nAlexander Rudnicky\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.04215\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 8 Feb 2023 17:34:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to AAAI2023\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yuying Li; Yuchen Liu; Donald S. Williamson', display:{Lore:['[{"text": "arXiv:2302.04932", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Composite T60 Regression and Classification Approach for Speech Dereverberation\\u00a7r\\n\\n\\u00a78\\u00a7oYuying Li\\nYuchen Liu\\nDonald S. Williamson\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.04932\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Feb 2023 20:56:09 GMT)\\u00a7r"}']}
{title:'Dey et al. (§72023§r)', author: 'Spandan Dey; Md Sahidullah; Goutam Saha', display:{Lore:['[{"text": "arXiv:2302.05110", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-Corpora Spoken Language Identification with Domain Diversification and Generalization\\u00a7r\\n\\n\\u00a78\\u00a7oSpandan Dey\\nMd Sahidullah\\nGoutam Saha\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.05110\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.csl.2023.101489\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 10 Feb 2023 08:21:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in Elsevier Computer Speech Language\\u00a7r"}']}
{title:'Mishra et al. (§72023§r)', author: 'Jagabandhu Mishra; S. R. Mahadeva Prasanna', display:{Lore:['[{"text": "arXiv:2302.05265", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpoken language change detection inspired by speaker change detection\\u00a7r\\n\\n\\u00a78\\u00a7oJagabandhu Mishra\\nS. R. Mahadeva Prasanna\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.05265\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 10 Feb 2023 14:25:49 GMT)\\u00a7r"}']}
{title:'Yuen et al. (§72023§r)', author: 'Daniel Hao Xian Yuen; Andrew Yong Chen Pang; Zhou Yang; Chun Yong Chong; Mei Kuan Lim; David Lo', display:{Lore:['[{"text": "arXiv:2302.05582", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7acs.SE\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lASDF: A Differential Testing Framework for Automatic Speech Recognition Systems\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Hao Xian Yuen\\nAndrew Yong Chen Pang\\nZhou Yang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.05582\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 11 Feb 2023 02:53:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccpeted by ICST 2023 Tool Demo Track\\u00a7r"}']}
{title:'Han et al. (§72023§r)', author: 'Cong Han; Vishal Choudhari; Yinghao Aaron Li; Nima Mesgarani', display:{Lore:['[{"text": "arXiv:2302.05756", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Decoding of Attentional Selection in Multi-Talker Environments with Self-Supervised Learned Speech Representation\\u00a7r\\n\\n\\u00a78\\u00a7oCong Han\\nVishal Choudhari\\nYinghao Aaron Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.05756\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 11 Feb 2023 18:33:42 GMT)\\u00a7r"}']}
{title:'Srivastava et al. (§72023§r)', author: 'Sudhanshu Srivastava; Ishika Gupta; Anusha Prakash; Jom Kuriakose; Hema A. Murthy', display:{Lore:['[{"text": "arXiv:2302.06227", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast and small footprint Hybrid HMM-HiFiGAN based system for speech synthesis in Indian languages\\u00a7r\\n\\n\\u00a78\\u00a7oSudhanshu Srivastava\\nIshika Gupta\\nAnusha Prakash\\nJom Kuriakose\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.06227\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Feb 2023 10:01:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Peter Wu; Li-Wei Chen; Cheol Jun Cho; Shinji Watanabe; Louis Goldstein; Alan W Black; Gopala K. Anumanchipalli', display:{Lore:['[{"text": "arXiv:2302.06774", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker-Independent Acoustic-to-Articulatory Speech Inversion\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Wu\\nLi-Wei Chen\\nCheol Jun Cho\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.06774\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 24 Jul 2023 23:49:07 GMT)\\u00a7r"}']}
{title:'Garoufis et al. (§72023§r)', author: 'Christos Garoufis; Athanasia Zlatintsi; Petros Maragos', display:{Lore:['[{"text": "arXiv:2302.07077", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Source Contrastive Learning from Musical Audio\\u00a7r\\n\\n\\u00a78\\u00a7oChristos Garoufis\\nAthanasia Zlatintsi\\nPetros Maragos\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.07077\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 10 May 2023 23:31:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 4 figures, 3 tables. Camera-ready submission at SMC23\\u00a7r"}']}
{title:'Hebbar et al. (§72023§r)', author: 'Rajat Hebbar; Digbalay Bose; Krishna Somandepalli; Veena Vijai; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2302.07315", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA dataset for Audio-Visual Sound Event Detection in Movies\\u00a7r\\n\\n\\u00a78\\u00a7oRajat Hebbar\\nDigbalay Bose\\nKrishna Somandepalli\\nVeena Vijai\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.07315\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Feb 2023 19:55:39 GMT)\\u00a7r"}']}
{title:'Deng et al. (§72023§r)', author: 'Jiajun Deng; Xurong Xie; Tianzi Wang; Mingyu Cui; Boyang Xue; Zengrui Jin; Guinan Li; Shujie Hu; Xunying Liu', display:{Lore:['[{"text": "arXiv:2302.07521", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConfidence Score Based Speaker Adaptation of Conformer Speech Recognition Systems\\u00a7r\\n\\n\\u00a78\\u00a7oJiajun Deng\\nXurong Xie\\nTianzi Wang\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.07521\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Feb 2023 08:29:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE/ACM Transactions on Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Dong Yang; Mingle Liu; Muyong Cao', display:{Lore:['[{"text": "arXiv:2302.07584", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.IT\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a72math.IT\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast and Blind Speech Copy-Move Detection and Localization in Noise\\u00a7r\\n\\n\\u00a78\\u00a7oDong Yang\\nMingle Liu\\nMuyong Cao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.07584\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 8 Sep 2023 08:41:18 GMT)\\u00a7r"}']}
{title:'Cornell et al. (§72023§r)', author: 'Samuele Cornell; Zhong-Qiu Wang; Yoshiki Masuyama; Shinji Watanabe; Manuel Pariente; Nobutaka Ono', display:{Lore:['[{"text": "arXiv:2302.07928", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oSamuele Cornell\\nZhong-Qiu Wang\\nYoshiki Masuyama\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.07928\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Feb 2023 20:08:21 GMT)\\u00a7r"}']}
{title:'Master et al. (§72023§r)', author: 'Aaron Master; Lie Lu; Jonas Samuelsson; Heidi-Maria Lehtonen; Scott Norcross; Nathan Swedlow; Audrey Howard', display:{Lore:['[{"text": "arXiv:2302.08202", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeepSpace: Dynamic Spatial and Source Cue Based Source Separation for Dialog Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oAaron Master\\nLie Lu\\nJonas Samuelsson\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.08202\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 22 Feb 2023 09:58:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures. To be published in ICASSP 2023\\u00a7r"}']}
{title:'Zhao et al. (§72023§r)', author: 'Xiao-Ying Zhao; Qiu-Shi Zhu; Jie Zhang', display:{Lore:['[{"text": "arXiv:2302.08342", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Enhancement with Multi-granularity Vector Quantization\\u00a7r\\n\\n\\u00a78\\u00a7oXiao-Ying Zhao\\nQiu-Shi Zhu\\nJie Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.08342\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Feb 2023 14:53:41 GMT)\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Jian Wu; Zhuo Chen; Min Hu; Xiong Xiao; Jinyu Li', display:{Lore:['[{"text": "arXiv:2302.08549", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Change Detection for Transformer Transducer ASR\\u00a7r\\n\\n\\u00a78\\u00a7oJian Wu\\nZhuo Chen\\nMin Hu\\nXiong Xiao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.08549\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Feb 2023 19:55:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, accepted by ICASSP 2023\\u00a7r"}']}
{title:'Deng et al. (§72023§r)', author: 'Keqi Deng; Philip C. Woodland', display:{Lore:['[{"text": "arXiv:2302.08579", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptable End-to-End ASR Models using Replaceable Internal LMs and Residual Softmax\\u00a7r\\n\\n\\u00a78\\u00a7oKeqi Deng\\nPhilip C. Woodland\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.08579\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 14 Mar 2023 23:13:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2023\\u00a7r"}']}
{title:'Meng et al. (§72023§r)', author: 'Zhong Meng; Weiran Wang; Rohit Prabhavalkar; Tara N. Sainath; Tongzhou Chen; Ehsan Variani; Yu Zhang; Bo Li; Andrew Rosenberg; Bhuvana Ramabhadran', display:{Lore:['[{"text": "arXiv:2302.08583", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJEIT: Joint End-to-End Model and Internal Language Model Training for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhong Meng\\nWeiran Wang\\nRohit Prabhavalkar\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.08583\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2023 IEEE International Conference on Acoustics, Speech and Signal\\n  Processing (ICASSP), Rhodes island, Greece\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Feb 2023 21:07:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, in ICASSP 2023\\u00a7r"}']}
{title:'Sang et al. (§72023§r)', author: 'Mufan Sang; Yong Zhao; Gang Liu; John H. L. Hansen; Jian Wu', display:{Lore:['[{"text": "arXiv:2302.08639", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Transformer-based Networks With Locality For Automatic Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oMufan Sang\\nYong Zhao\\nGang Liu\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.08639\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 28 Feb 2023 23:32:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Hao Zhang; Meng Yu; Dong Yu', display:{Lore:['[{"text": "arXiv:2302.09252", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep AHS: A Deep Learning Approach to Acoustic Howling Suppression\\u00a7r\\n\\n\\u00a78\\u00a7oHao Zhang\\nMeng Yu\\nDong Yu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.09252\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 17 Aug 2023 17:06:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in 2023 ICASSP\\u00a7r"}']}
{title:'Xiao et al. (§72023§r)', author: 'Shengchang Xiao; Xueshuai Zhang; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2302.09256", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-dimensional frequency dynamic convolution with confident mean teacher for sound event detection\\u00a7r\\n\\n\\u00a78\\u00a7oShengchang Xiao\\nXueshuai Zhang\\nPengyuan Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.09256\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 21 Feb 2023 08:52:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to ICASSP 2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Xie Chen; Ziyang Ma; Changli Tang; Yujin Wang; Zhisheng Zheng', display:{Lore:['[{"text": "arXiv:2302.09331", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFront-End Adapter: Adapting Front-End Input of Speech based Self-Supervised Learning for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXie Chen\\nZiyang Ma\\nChangli Tang\\nYujin Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.09331\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 18 Feb 2023 13:46:12 GMT)\\u00a7r"}']}
{title:'Berns et al. (§72023§r)', author: 'Tijn Berns; Nik Vaessen; David A. van Leeuwen', display:{Lore:['[{"text": "arXiv:2302.09381", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker and Language Change Detection using Wav2vec2 and Whisper\\u00a7r\\n\\n\\u00a78\\u00a7oTijn Berns\\nNik Vaessen\\nDavid A. van Leeuwen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.09381\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 18 Feb 2023 16:45:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oGoing to be submitted, but rules do not allow to indicate where\\u00a7r"}']}
{title:'Guimarães et al. (§72023§r)', author: 'Heitor R. Guimarães; Arthur Pimentel; Anderson R. Avila; Mehdi Rezagholizadeh; Boxing Chen; Tiago H. Falk', display:{Lore:['[{"text": "arXiv:2302.09437", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobustDistiller: Compressing Universal Speech Representations for Enhanced Environment Robustness\\u00a7r\\n\\n\\u00a78\\u00a7oHeitor R. Guimar\\u00e3es\\nArthur Pimentel\\nAnderson R. Avila\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.09437\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 23 Feb 2023 03:22:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Sholokhov et al. (§72023§r)', author: 'Alexey Sholokhov; Nikita Kuzmin; Kong Aik Lee; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2302.09523", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProbabilistic Back-ends for Online Speaker Recognition and Clustering\\u00a7r\\n\\n\\u00a78\\u00a7oAlexey Sholokhov\\nNikita Kuzmin\\nKong Aik Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.09523\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 19 Feb 2023 09:48:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Heller et al. (§72023§r)', author: 'Laurie M. Heller; Benjamin Elizalde; Bhiksha Raj; Soham Deshmukh', display:{Lore:['[{"text": "arXiv:2302.09719", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynergy between human and machine approaches to sound/scene recognition and processing: An overview of ICASSP special session\\u00a7r\\n\\n\\u00a78\\u00a7oLaurie M. Heller\\nBenjamin Elizalde\\nBhiksha Raj\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.09719\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 24 Feb 2023 01:57:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages. Summary of Special Session planned for 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). https://2023.ieeeicassp.org/ Secondversion has corrected spelling of an author\'s "}','{"text": "name\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Wei Liu; Kaiqi Fu; Xiaohai Tian; Shuju Shi; Wei Li; Zejun Ma; Tan Lee', display:{Lore:['[{"text": "arXiv:2302.09928", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn ASR-free Fluency Scoring Approach with Self-Supervised Learning\\u00a7r\\n\\n\\u00a78\\u00a7oWei Liu\\nKaiqi Fu\\nXiaohai Tian\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.09928\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 13 Mar 2023 05:58:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Le et al. (§72023§r)', author: 'Xiaohuai Le; Li Chen; Chao He; Yiqing Guo; Cheng Chen; Xianjun Xia; Jing Lu', display:{Lore:['[{"text": "arXiv:2302.09953", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPersonalized speech enhancement combining band-split RNN and speaker attentive module\\u00a7r\\n\\n\\u00a78\\u00a7oXiaohuai Le\\nLi Chen\\nChao He\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.09953\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 16 Mar 2023 04:25:55 GMT)\\u00a7r"}']}
{title:'Anderson et al. (§72023§r)', author: 'Mark Anderson; Tomi Kinnunen; Naomi Harte', display:{Lore:['[{"text": "arXiv:2302.10014", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearnable Frontends that do not Learn: Quantifying Sensitivity to Filterbank Initialisation\\u00a7r\\n\\n\\u00a78\\u00a7oMark Anderson\\nTomi Kinnunen\\nNaomi Harte\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.10014\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Feb 2023 14:43:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2023, 5 pages, 2 figures, 2 tables\\u00a7r"}']}
{title:'Moumen et al. (§72023§r)', author: 'Adel Moumen; Titouan Parcollet', display:{Lore:['[{"text": "arXiv:2302.10144", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStabilising and accelerating light gated recurrent units for automatic speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAdel Moumen\\nTitouan Parcollet\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.10144\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Feb 2023 16:18:58 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Kuan-Lin Chen; Ching-Hua Lee; Bhaskar D. Rao; Harinath Garudadri', display:{Lore:['[{"text": "arXiv:2302.10147", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA DNN based Normalized Time-frequency Weighted Criterion for Robust Wideband DoA Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oKuan-Lin Chen\\nChing-Hua Lee\\nBhaskar D. Rao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.10147\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Feb 2023 18:26:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages. Accepted at ICASSP 2023\\u00a7r"}']}
{title:'Singla et al. (§72023§r)', author: 'Karan Singla; Yeon-Jun Kim; Srinivas Bangalore', display:{Lore:['[{"text": "arXiv:2302.10186", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lE2E Spoken Entity Extraction for Virtual Agents\\u00a7r\\n\\n\\u00a78\\u00a7oKaran Singla\\nYeon-Jun Kim\\nSrinivas Bangalore\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.10186\\u00a7r\\n\\nVersion:\\u00a77v7 (Thu, 9 Nov 2023 20:51:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at EMNLP 2023 Industry Track\\u00a7r"}']}
{title:'Ioannides et al. (§72023§r)', author: 'Georgios Ioannides; Vasilios Rallis', display:{Lore:['[{"text": "arXiv:2302.10313", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-Time Speech Enhancement Using Spectral Subtraction with Minimum Statistics and Spectral Floor\\u00a7r\\n\\n\\u00a78\\u00a7oGeorgios Ioannides\\nVasilios Rallis\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.10313\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Feb 2023 20:55:53 GMT)\\u00a7r"}']}
{title:'Zheng et al. (§72023§r)', author: 'Chengyu Zheng; Yuan Zhou; Xiulian Peng; Yuan Zhang; Yan Lu', display:{Lore:['[{"text": "arXiv:2302.10377", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-time speech enhancement with dynamic attention span\\u00a7r\\n\\n\\u00a78\\u00a7oChengyu Zheng\\nYuan Zhou\\nXiulian Peng\\nYuan Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.10377\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Feb 2023 00:53:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023 (Accepted)\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Wei Liu; Kaiqi Fu; Xiaohai Tian; Shuju Shi; Wei Li; Zejun Ma; Tan Lee', display:{Lore:['[{"text": "arXiv:2302.10444", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging phone-level linguistic-acoustic similarity for utterance-level pronunciation scoring\\u00a7r\\n\\n\\u00a78\\u00a7oWei Liu\\nKaiqi Fu\\nXiaohai Tian\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.10444\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 13 Mar 2023 06:02:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Nguyen et al. (§72023§r)', author: 'Tuan Nguyen; Salima Mdhaffar; Natalia Tomashenko; Jean-François Bonastre; Yannick Estève', display:{Lore:['[{"text": "arXiv:2302.10790", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFederated Learning for ASR based on Wav2vec 2.0\\u00a7r\\n\\n\\u00a78\\u00a7oTuan Nguyen\\nSalima Mdhaffar\\nNatalia Tomashenko\\nJean-Fran\\u00e7ois Bonastre\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.10790\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Feb 2023 18:36:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted in ICASSP 2023\\u00a7r"}']}
{title:'Hu et al. (§72023§r)', author: 'Yuchen Hu; Chen Chen; Heqing Zou; Xionghu Zhong; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2302.11131", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnifying Speech Enhancement and Separation with Gradient Modulation for End-to-End Noise-Robust Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oYuchen Hu\\nChen Chen\\nHeqing Zou\\nXionghu Zhong\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.11131\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 Feb 2023 03:54:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, Accepted by ICASSP 2023\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Chao Zhang; Bo Li; Tara N. Sainath; Trevor Strohman; Shuo-yiin Chang', display:{Lore:['[{"text": "arXiv:2302.11186", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUML: A Universal Monolingual Output Layer for Multilingual ASR\\u00a7r\\n\\n\\u00a78\\u00a7oChao Zhang\\nBo Li\\nTara N. Sainath\\nTrevor Strohman\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.11186\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 Feb 2023 07:40:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished as a conference paper at ICASSP 2023\\u00a7r"}']}
{title:'Götz et al. (§72023§r)', author: 'Philipp Götz; Cagdas Tuna; Andreas Walther; Emanuël A. P. Habets', display:{Lore:['[{"text": "arXiv:2302.11205", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContrastive Representation Learning for Acoustic Parameter Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oPhilipp G\\u00f6tz\\nCagdas Tuna\\nAndreas Walther\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.11205\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 13 Mar 2023 07:25:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP 2023, Camera-ready version\\u00a7r"}']}
{title:'Hu et al. (§72023§r)', author: 'Yuchen Hu; Chen Chen; Ruizhe Li; Qiushi Zhu; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2302.11362", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGradient Remedy for Multi-Task Learning in End-to-End Noise-Robust Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYuchen Hu\\nChen Chen\\nRuizhe Li\\nQiushi Zhu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.11362\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 3 May 2023 05:06:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, Accepted by ICASSP 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Qiongqiong Wang; Kong Aik Lee; Tianchi Liu', display:{Lore:['[{"text": "arXiv:2302.11763", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIncorporating Uncertainty from Speaker Embedding Estimation to Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oQiongqiong Wang\\nKong Aik Lee\\nTianchi Liu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.11763\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Feb 2023 03:42:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ICASSP 2023 conference\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Zhepei Wang; Ritwik Giri; Devansh Shah; Jean-Marc Valin; Michael M. Goodwin; Paris Smaragdis', display:{Lore:['[{"text": "arXiv:2302.11768", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Framework for Unified Real-time Personalized and Non-Personalized Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oZhepei Wang\\nRitwik Giri\\nDevansh Shah\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.11768\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Feb 2023 04:10:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Tao et al. (§72023§r)', author: 'Shuai Tao; Himavanth Reddy; Jesper Rindom Jensen; Mads Græsbøll Christensen', display:{Lore:['[{"text": "arXiv:2302.12048", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrequency bin-wise single channel speech presence probability estimation using multiple DNNs\\u00a7r\\n\\n\\u00a78\\u00a7oShuai Tao\\nHimavanth Reddy\\nJesper Rindom Jensen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.12048\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Feb 2023 14:20:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP 2023\\u00a7r"}']}
{title:'Kanda et al. (§72023§r)', author: 'Naoyuki Kanda; Takuya Yoshioka; Yang Liu', display:{Lore:['[{"text": "arXiv:2302.12369", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFactual Consistency Oriented Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oNaoyuki Kanda\\nTakuya Yoshioka\\nYang Liu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.12369\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Feb 2023 00:01:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, 3 tables\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Junhyeok Lee; Wonbin Jung; Hyunjae Cho; Jaeyeon Kim; Jaehwan Kim', display:{Lore:['[{"text": "arXiv:2302.12391", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPITS: Variational Pitch Inference without Fundamental Frequency for End-to-End Pitch-controllable TTS\\u00a7r\\n\\n\\u00a78\\u00a7oJunhyeok Lee\\nWonbin Jung\\nHyunjae Cho\\nJaeyeon Kim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.12391\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 6 Jun 2023 05:34:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, preprint\\u00a7r"}']}
{title:'Huang et al. (§72023§r)', author: 'Kuan-Po Huang; Tzu-hsun Feng; Yu-Kuan Fu; Tsu-Yuan Hsu; Po-Chieh Yen; Wei-Cheng Tseng; Kai-Wei Chang; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2302.12757", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnsemble knowledge distillation of self-supervised speech models\\u00a7r\\n\\n\\u00a78\\u00a7oKuan-Po Huang\\nTzu-hsun Feng\\nYu-Kuan Fu\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.12757\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 24 Feb 2023 17:15:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Zheng et al. (§72023§r)', author: 'Chengyu Zheng; Yuan Zhou; Xiulian Peng; Yuan Zhang; Yan Lu', display:{Lore:['[{"text": "arXiv:2302.13063", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTime-Variance Aware Real-Time Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oChengyu Zheng\\nYuan Zhou\\nXiulian Peng\\nYuan Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.13063\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 25 Feb 2023 11:37:35 GMT)\\u00a7r"}']}
{title:'Mishra et al. (§72023§r)', author: 'Jagabandhu Mishra; Mrinmoy Bhattacharjee; S. R. Mahadeva Prasanna', display:{Lore:['[{"text": "arXiv:2302.13209", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lI-MSV 2022: Indic-Multilingual and Multi-sensor Speaker Verification Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oJagabandhu Mishra\\nMrinmoy Bhattacharjee\\nS. R. Mahadeva Prasanna\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.13209\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 26 Feb 2023 02:26:02 GMT)\\u00a7r"}']}
{title:'Kovalyov et al. (§72023§r)', author: 'Anton Kovalyov; Kashyap Patel; Issa Panahi', display:{Lore:['[{"text": "arXiv:2302.13407", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDFSNet: A Steerable Neural Beamformer Invariant to Microphone Array Configuration for Real-Time, Low-Latency Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oAnton Kovalyov\\nKashyap Patel\\nIssa Panahi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.13407\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 26 Feb 2023 21:30:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, 2 tables\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Yoonhyung Lee; Jinhyeok Yang; Kyomin Jung', display:{Lore:['[{"text": "arXiv:2302.13458", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVarianceflow: High-Quality and Controllable Text-to-Speech using Variance Information via Normalizing Flow\\u00a7r\\n\\n\\u00a78\\u00a7oYoonhyung Lee\\nJinhyeok Yang\\nKyomin Jung\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.13458\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747050\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Feb 2023 01:12:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP 2022\\u00a7r"}']}
{title:'Atlas et al. (§72023§r)', author: 'Les Atlas; Nicholas Rasmussen; Felix Schwock; Mert Pilanci', display:{Lore:['[{"text": "arXiv:2302.13527", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComplex Clipping for Improved Generalization in Machine Learning\\u00a7r\\n\\n\\u00a78\\u00a7oLes Atlas\\nNicholas Rasmussen\\nFelix Schwock\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.13527\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Feb 2023 05:48:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEESignal Processing Letters\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Dong Yang; Tomoki Koriyama; Yuki Saito; Takaaki Saeki; Detai Xin; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2302.13652", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDuration-aware pause insertion using pre-trained language model for multi-speaker text-to-speech\\u00a7r\\n\\n\\u00a78\\u00a7oDong Yang\\nTomoki Koriyama\\nYuki Saito\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.13652\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Feb 2023 10:40:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2023\\u00a7r"}']}
{title:'Kwon et al. (§72023§r)', author: 'Yoohwan Kwon; Soo-Whan Chung', display:{Lore:['[{"text": "arXiv:2302.13750", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMoLE : Mixture of Language Experts for Multi-Lingual Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYoohwan Kwon\\nSoo-Whan Chung\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.13750\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Feb 2023 13:26:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Bataev et al. (§72023§r)', author: 'Vladimir Bataev; Roman Korostik; Evgeny Shabalin; Vitaly Lavrukhin; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2302.14036", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lText-only domain adaptation for end-to-end ASR using integrated text-to-mel-spectrogram generator\\u00a7r\\n\\n\\u00a78\\u00a7oVladimir Bataev\\nRoman Korostik\\nEvgeny Shabalin\\nVitaly Lavrukhin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.14036\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 16 Aug 2023 12:03:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2023\\u00a7r"}']}
{title:'Saon et al. (§72023§r)', author: 'George Saon; Ankit Gupta; Xiaodong Cui', display:{Lore:['[{"text": "arXiv:2302.14120", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiagonal State Space Augmented Transformers for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGeorge Saon\\nAnkit Gupta\\nXiaodong Cui\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.14120\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Feb 2023 20:08:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oto be presented at ICASSP 2023\\u00a7r"}']}
{title:'Martín-Morató et al. (§72023§r)', author: 'Irene Martín-Morató; Manu Harju; Paul Ahokas; Annamaria Mesaros', display:{Lore:['[{"text": "arXiv:2302.14572", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTraining sound event detection with soft labels from crowdsourced annotations\\u00a7r\\n\\n\\u00a78\\u00a7oIrene Mart\\u00edn-Morat\\u00f3\\nManu Harju\\nPaul Ahokas\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.14572\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Feb 2023 13:51:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Weidong Chen; Xiaofen Xing; Xiangmin Xu; Jianxin Pang; Lan Du', display:{Lore:['[{"text": "arXiv:2302.14638", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeechFormer++: A Hierarchical Efficient Framework for Paralinguistic Speech Processing\\u00a7r\\n\\n\\u00a78\\u00a7oWeidong Chen\\nXiaofen Xing\\nXiangmin Xu\\nJianxin Pang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.14638\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3235194\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Feb 2023 11:48:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages, 7 figures, 14 tables, TASLP 2023 paper\\u00a7r"}']}
{title:'Lay et al. (§72023§r)', author: 'Bunlong Lay; Simon Welker; Julius Richter; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2302.14748", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReducing the Prior Mismatch of Stochastic Differential Equations for Diffusion-based Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oBunlong Lay\\nSimon Welker\\nJulius Richter\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.14748\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 May 2023 13:05:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, Accepted to Interspeech 20223\\u00a7r"}']}
{title:'Mulimani et al. (§72023§r)', author: 'Manjunath Mulimani; Annamaria Mesaros', display:{Lore:['[{"text": "arXiv:2302.14815", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIncremental Learning of Acoustic Scenes and Sound Events\\u00a7r\\n\\n\\u00a78\\u00a7oManjunath Mulimani\\nAnnamaria Mesaros\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.14815\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 24 Aug 2023 12:35:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to DCASE2023 Workshop\\u00a7r"}']}
{title:'Huh et al. (§72023§r)', author: 'Jaeyoung Huh; Sangjoon Park; Jeong Eun Lee; Jong Chul Ye', display:{Lore:['[{"text": "arXiv:2303.00091", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Medical Speech-to-Text Accuracy with Vision-Language Pre-training Model\\u00a7r\\n\\n\\u00a78\\u00a7oJaeyoung Huh\\nSangjoon Park\\nJeong Eun Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.00091\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Feb 2023 08:06:04 GMT)\\u00a7r"}']}
{title:'Harada et al. (§72023§r)', author: 'Noboru Harada; Daisuke Niizumi; Yasunori Ohishi; Daiki Takeuchi; Masahiro Yasuda', display:{Lore:['[{"text": "arXiv:2303.00455", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFirst-shot anomaly sound detection for machine condition monitoring: A domain generalization baseline\\u00a7r\\n\\n\\u00a78\\u00a7oNoboru Harada\\nDaisuke Niizumi\\nYasunori Ohishi\\nDaiki Takeuchi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.00455\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Mar 2023 12:29:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures\\u00a7r"}']}
{title:'Lemercier et al. (§72023§r)', author: 'Jean-Marie Lemercier; Julian Tobergte; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2303.00529", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExtending DNN-based Multiplicative Masking to Deep Subband Filtering for Improved Dereverberation\\u00a7r\\n\\n\\u00a78\\u00a7oJean-Marie Lemercier\\nJulian Tobergte\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.00529\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 31 May 2023 08:45:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ISCAInterspeech 2023\\u00a7r"}']}
{title:'Ahmad et al. (§72023§r)', author: 'Rehan Ahmad; Md Asif Jalal; Muhammad Umar Farooq; Anna Ollerenshaw; Thomas Hain', display:{Lore:['[{"text": "arXiv:2303.00550", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards domain generalisation in ASR with elitist sampling and ensemble knowledge distillation\\u00a7r\\n\\n\\u00a78\\u00a7oRehan Ahmad\\nMd Asif Jalal\\nMuhammad Umar Farooq\\nAnna Ollerenshaw\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.00550\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Mar 2023 14:49:53 GMT)\\u00a7r"}']}
{title:'Wierstorf et al. (§72023§r)', author: 'Hagen Wierstorf; Johannes Wagner; Florian Eyben; Felix Burkhardt; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2303.00645", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7laudb \\u2013 Sharing and Versioning of Audio and Annotation Data in Python\\u00a7r\\n\\n\\u00a78\\u00a7oHagen Wierstorf\\nJohannes Wagner\\nFlorian Eyben\\nFelix Burkhardt\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.00645\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 10 May 2023 14:53:39 GMT)\\u00a7r"}']}
{title:'Chang et al. (§72023§r)', author: 'Feng-Ju Chang; Anastasios Alexandridis; Rupak Vignesh Swaminathan; Martin Radfar; Harish Mallidi; Maurizio Omologo; Athanasios Mouchtaris; Brian King; Roland Maas', display:{Lore:['[{"text": "arXiv:2303.00692", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Redundancy in Multiple Audio Signals for Far-Field Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oFeng-Ju Chang\\nAnastasios Alexandridis\\nRupak Vignesh Swaminathan\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.00692\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Mar 2023 17:39:46 GMT)\\u00a7r"}']}
{title:'Chang et al. (§72023§r)', author: 'Kai-Wei Chang; Yu-Kai Wang; Hua Shen; Iu-thing Kang; Wei-Cheng Tseng; Shang-Wen Li; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2303.00733", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeechPrompt v2: Prompt Tuning for Speech Classification Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oKai-Wei Chang\\nYu-Kai Wang\\nHua Shen\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.00733\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Mar 2023 18:47:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProject website: https://ga642381.github.io/SpeechPrompt\\u00a7r"}']}
{title:'Lagacé et al. (§72023§r)', author: 'Pierre-Olivier Lagacé; François Ferland; François Grondin', display:{Lore:['[{"text": "arXiv:2303.00829", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEgo-noise reduction of a mobile robot using noise spatial covariance matrix learning and minimum variance distortionless response\\u00a7r\\n\\n\\u00a78\\u00a7oPierre-Olivier Lagac\\u00e9\\nFran\\u00e7ois Ferland\\nFran\\u00e7ois Grondin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.00829\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 7 Mar 2023 03:54:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IROS2023\\u00a7r"}']}
{title:'Baghel et al. (§72023§r)', author: 'Shikha Baghel; Shreyas Ramoji; Sidharth; Ranjana H; Prachi Singh; Somil Jain; Pratik Roy Chowdhuri; Kaustubh Kulkarni; Swapnil Padhi; Deepu Vijayasenan; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:2303.00830", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDISPLACE Challenge: DIarization of SPeaker and LAnguage in Conversational Environments\\u00a7r\\n\\n\\u00a78\\u00a7oShikha Baghel\\nShreyas Ramoji\\nSidharth\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.00830\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 5 Jun 2023 07:24:02 GMT)\\u00a7r"}']}
{title:'Kealey et al. (§72023§r)', author: 'Jacob Kealey; Anthony Gosselin; Étienne Deshaies-Samson; Francis Cardinal; Félix Ducharme-Turcotte; Olivier Bergeron; Amélie Rioux-Joyal; Jérémy Bélec; François Grondin', display:{Lore:['[{"text": "arXiv:2303.00949", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-time Audio Video Enhancement \\nwith a Microphone Array and Headphones\\u00a7r\\n\\n\\u00a78\\u00a7oJacob Kealey\\nAnthony Gosselin\\n\\u00c9tienne Deshaies-Samson\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.00949\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 Mar 2023 04:01:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IROS2023\\u00a7r"}']}
{title:'Jeoung et al. (§72023§r)', author: 'Ye-Rin Jeoung; Joon-Young Yang; Jeong-Hwan Choi; Joon-Hyuk Chang', display:{Lore:['[{"text": "arXiv:2303.01192", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Transformer-based End-to-End Speaker Diarization by Assigning Auxiliary Losses to Attention Heads\\u00a7r\\n\\n\\u00a78\\u00a7oYe-Rin Jeoung\\nJoon-Young Yang\\nJeong-Hwan Choi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.01192\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 Mar 2023 12:15:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, ICASSP2023 (Accepted)\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Haolin Chen; Philip N. Garner', display:{Lore:['[{"text": "arXiv:2303.01849", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn investigation into the adaptability of a diffusion-based TTS model\\u00a7r\\n\\n\\u00a78\\u00a7oHaolin Chen\\nPhilip N. Garner\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.01849\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 3 Mar 2023 11:06:20 GMT)\\u00a7r"}']}
{title:'Macha et al. (§72023§r)', author: 'Sashank Macha; Om Oza; Alex Escott; Francesco Caliva; Robbie Armitano; Santosh Kumar Cheekatmalla; Sree Hari Krishnan Parthasarathi; Yuzong Liu', display:{Lore:['[{"text": "arXiv:2303.02284", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFixed-point quantization aware training for on-device keyword-spotting\\u00a7r\\n\\n\\u00a78\\u00a7oSashank Macha\\nOm Oza\\nAlex Escott\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.02284\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 4 Mar 2023 01:06:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 4 tables\\u00a7r"}']}
{title:'Kaufmann et al. (§72023§r)', author: 'Thomas B. Kaufmann; Mehdi Foroogozar; Julie Liss; Visar Berisha', display:{Lore:['[{"text": "arXiv:2303.02523", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRequirements for Mass Adoption of Assistive Listening Technology by the General Public\\u00a7r\\n\\n\\u00a78\\u00a7oThomas B. Kaufmann\\nMehdi Foroogozar\\nJulie Liss\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.02523\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 3 May 2023 05:03:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Siyang Wang; Gustav Eje Henter; Joakim Gustafson; Éva Székely', display:{Lore:['[{"text": "arXiv:2303.02719", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparative Study of Self-Supervised Speech Representations in Read and Spontaneous TTS\\u00a7r\\n\\n\\u00a78\\u00a7oSiyang Wang\\nGustav Eje Henter\\nJoakim Gustafson\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.02719\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 2023 IEEE International Conference on\\n  Acoustics, Speech, and Signal Processing Workshops (ICASSPW)\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 10 Jul 2023 15:15:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures. ICASSP Workshop SASB (Self-Supervision in Audio, Speech and Beyond)2023\\u00a7r"}']}
{title:'Xue et al. (§72023§r)', author: 'Ruiqing Xue; Yanqing Liu; Lei He; Xu Tan; Linquan Liu; Edward Lin; Sheng Zhao', display:{Lore:['[{"text": "arXiv:2303.02939", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model\\u00a7r\\n\\n\\u00a78\\u00a7oRuiqing Xue\\nYanqing Liu\\nLei He\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.02939\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 8 Mar 2023 03:06:47 GMT)\\u00a7r"}']}
{title:'Tamm et al. (§72023§r)', author: 'Bastiaan Tamm; Rik Vandenberghe; Hugo Van hamme', display:{Lore:['[{"text": "arXiv:2303.03049", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-Lingual Transfer Learning for Alzheimer\'s Detection From Spontaneous Speech\\u00a7r\\n\\n\\u00a78\\u00a7oBastiaan Tamm\\nRik Vandenberghe\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.03049\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 Mar 2023 11:46:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2 pages, submitted to ICASSP 2023\\u00a7r"}']}
{title:'Mitra et al. (§72023§r)', author: 'Vikramjit Mitra; Vasudha Kowtha; Hsiang-Yun Sherry Chien; Erdrin Azemi; Carlos Avendano', display:{Lore:['[{"text": "arXiv:2303.03177", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPre-trained Model Representations and their Robustness against Noise for Speech Emotion Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oVikramjit Mitra\\nVasudha Kowtha\\nHsiang-Yun Sherry Chien\\nErdrin Azemi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.03177\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 3 Mar 2023 18:22:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, conference\\u00a7r"}']}
{title:'Prabhavalkar et al. (§72023§r)', author: 'Rohit Prabhavalkar; Takaaki Hori; Tara N. Sainath; Ralf Schlüter; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2303.03329", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Speech Recognition: A Survey\\u00a7r\\n\\n\\u00a78\\u00a7oRohit Prabhavalkar\\nTakaaki Hori\\nTara N. Sainath\\nRalf Schl\\u00fcter\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.03329\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 3 Mar 2023 01:46:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEE/ACM Transactionson Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Kataria et al. (§72023§r)', author: 'Saurabh Kataria; Jesús Villalba; Laureano Moro-Velázquez; Thomas Thebaud; Najim Dehak', display:{Lore:['[{"text": "arXiv:2303.03657", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-FiLM: Conditioning GANs with self-supervised representations for bandwidth extension based speaker recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSaurabh Kataria\\nJes\\u00fas Villalba\\nLaureano Moro-Vel\\u00e1zquez\\nThomas Thebaud\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.03657\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Mar 2023 05:28:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Kang Li; Yan Song; Li-Rong Dai; Ian McLoughlin; Xin Fang; Lin Liu', display:{Lore:['[{"text": "arXiv:2303.03689", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAST-SED: An Effective Sound Event Detection Method Based on Audio Spectrogram Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oKang Li\\nYan Song\\nLi-Rong Dai\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.03689\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Mar 2023 07:13:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to ICASSP 2023\\u00a7r"}']}
{title:'Ribeiro et al. (§72023§r)', author: 'Juliano G. C. Ribeiro; Shoichi Koyama; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2303.03869", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKernel interpolation of acoustic transfer functions with adaptive kernel for directed and residual reverberations\\u00a7r\\n\\n\\u00a78\\u00a7oJuliano G. C. Ribeiro\\nShoichi Koyama\\nHiroshi Saruwatari\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.03869\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2023 IEEE International Conference on Acoustics, Speech and Signal\\n  Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Mar 2023 13:14:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP 2023\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Kai Liu; Ziqing Du; Xucheng Wan; Huan Zhou', display:{Lore:['[{"text": "arXiv:2303.05023", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lX-SepFormer: End-to-end Speaker Extraction Network with Explicit Optimization on Speaker Confusion\\u00a7r\\n\\n\\u00a78\\u00a7oKai Liu\\nZiqing Du\\nXucheng Wan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.05023\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Mar 2023 04:00:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Seth et al. (§72023§r)', author: 'Ashish Seth; Sreyan Ghosh; S. Umesh; Dinesh Manocha', display:{Lore:['[{"text": "arXiv:2303.05668", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUNFUSED: UNsupervised Finetuning Using SElf supervised Distillation\\u00a7r\\n\\n\\u00a78\\u00a7oAshish Seth\\nSreyan Ghosh\\nS. Umesh\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.05668\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 18 May 2023 01:28:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023 SASB Workshop\\u00a7r"}']}
{title:'Shor et al. (§72023§r)', author: 'Joel Shor; Ruyue Agnes Bi; Subhashini Venugopalan; Steven Ibara; Roman Goldenberg; Ehud Rivlin', display:{Lore:['[{"text": "arXiv:2303.05737", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lClinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings\\u00a7r\\n\\n\\u00a78\\u00a7oJoel Shor\\nRuyue Agnes Bi\\nSubhashini Venugopalan\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.05737\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.18653/v1/2023.clinicalnlp-1.1\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nClinical NLP Workshop, ACL 2023\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 28 Apr 2023 21:40:15 GMT)\\u00a7r"}']}
{title:'T. et al. (§72023§r)', author: 'Pedro J. Villasana T.; Janusz Klejsa; Lars Villemoes; Per Hedelin', display:{Lore:['[{"text": "arXiv:2303.05896", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDistribution Preserving Source Separation With Time Frequency Predictive Models\\u00a7r\\n\\n\\u00a78\\u00a7oPedro J. Villasana T.\\nJanusz Klejsa\\nLars Villemoes\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.05896\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 10 Mar 2023 13:05:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, pre-review version submitted to EUSIPCO 2023\\u00a7r"}']}
{title:'Fleck et al. (§72023§r)', author: 'Michael Fleck; Wolfgang Göderle', display:{Lore:['[{"text": "arXiv:2303.06026", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lwav2vec and its current potential to Automatic Speech Recognition in German for the usage in Digital History: A comparative assessment of available ASR-technologies for the use in cultural heritage contexts\\u00a7r\\n\\n\\u00a78\\u00a7oMichael Fleck\\nWolfgang G\\u00f6derle\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.06026\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 Mar 2023 22:24:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 2 tables\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Liu Chen; Michael Deisher; Munir Georges', display:{Lore:['[{"text": "arXiv:2303.06078", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.NE\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn End-to-End Neural Network for Image-to-Audio Transformation\\u00a7r\\n\\n\\u00a78\\u00a7oLiu Chen\\nMichael Deisher\\nMunir Georges\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.06078\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 10 Mar 2023 16:56:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 2023 IEEE Conference on Acoustics, Speech, and Signal Processing\\u00a7r"}']}
{title:'Guo et al. (§72023§r)', author: 'Pengcheng Guo; He Wang; Bingshen Mu; Ao Zhang; Peikun Chen', display:{Lore:['[{"text": "arXiv:2303.06341", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe NPU-ASLP System for Audio-Visual Speech Recognition in MISP 2022 Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oPengcheng Guo\\nHe Wang\\nBingshen Mu\\nAo Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.06341\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 11 Mar 2023 08:10:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2 pages, accepted by ICASSP 2023\\u00a7r"}']}
{title:'Sun et al. (§72023§r)', author: 'Jiayao Sun; Dawei Luo; Zhaoxia Li; Jindong Li; Yukai Ju; Yang Li', display:{Lore:['[{"text": "arXiv:2303.06404", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Task Sub-Band Network For Deep Residual Echo Suppression\\u00a7r\\n\\n\\u00a78\\u00a7oJiayao Sun\\nDawei Luo\\nZhaoxia Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.06404\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 11 Mar 2023 12:51:12 GMT)\\u00a7r"}']}
{title:'Thornton et al. (§72023§r)', author: 'Mike Thornton; Danilo Mandic; Tobias Reichenbach', display:{Lore:['[{"text": "arXiv:2303.06435", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRelating EEG recordings to speech using envelope tracking and the speech-FFR\\u00a7r\\n\\n\\u00a78\\u00a7oMike Thornton\\nDanilo Mandic\\nTobias Reichenbach\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.06435\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 11 Mar 2023 16:12:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2 pages, 3 figures. Accepted for ICASSP 2023 (challenge track)\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Ge Zhu; Yujia Yan; Juan-Pablo Caceres; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2303.06475", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTranscription free filler word detection with Neural semi-CRFs\\u00a7r\\n\\n\\u00a78\\u00a7oGe Zhu\\nYujia Yan\\nJuan-Pablo Caceres\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.06475\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 11 Mar 2023 18:17:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Cutler et al. (§72023§r)', author: 'Ross Cutler; Ando Saabas; Babak Naderi; Nicolae-Cătălin Ristea; Sebastian Braun; Solomiya Branets', display:{Lore:['[{"text": "arXiv:2303.06566", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lICASSP 2023 Speech Signal Improvement Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oRoss Cutler\\nAndo Saabas\\nBabak Naderi\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.06566\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 13 Oct 2023 12:35:21 GMT)\\u00a7r"}']}
{title:'Zaiem et al. (§72023§r)', author: 'Salah Zaiem; Robin Algayres; Titouan Parcollet; Slim Essid; Mirco Ravanelli', display:{Lore:['[{"text": "arXiv:2303.06740", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFine-tuning Strategies for Faster Inference using Speech Self-Supervised Models: A Comparative Study\\u00a7r\\n\\n\\u00a78\\u00a7oSalah Zaiem\\nRobin Algayres\\nTitouan Parcollet\\nSlim Essid\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.06740\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 12 Mar 2023 19:52:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP \\"Self-supervision in Audio, Speech and Beyond\\" workshop\\u00a7r"}']}
{title:'Fujita et al. (§72023§r)', author: 'Yusuke Fujita; Tatsuya Komatsu; Robin Scheibler; Yusuke Kida; Tetsuji Ogawa', display:{Lore:['[{"text": "arXiv:2303.06806", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Diarization with Non-autoregressive Intermediate Attractors\\u00a7r\\n\\n\\u00a78\\u00a7oYusuke Fujita\\nTatsuya Komatsu\\nRobin Scheibler\\nYusuke Kida\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.06806\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Mar 2023 01:28:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Yan et al. (§72023§r)', author: 'Xiaopeng Yan; Yindi Yang; Zhihao Guo; Liangliang Peng; Lei Xie', display:{Lore:['[{"text": "arXiv:2303.06811", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe NPU-Elevoc Personalized Speech Enhancement System for ICASSP2023 DNS Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oXiaopeng Yan\\nYindi Yang\\nZhihao Guo\\nLiangliang Peng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.06811\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 15 Mar 2023 04:01:18 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Zihan Zhang; Shimin Zhang; Mingshuai Liu; Yanhong Leng; Zhe Han; Li Chen; Lei Xie', display:{Lore:['[{"text": "arXiv:2303.06828", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTwo-step Band-split Neural Network Approach for Full-band Residual Echo Suppression\\u00a7r\\n\\n\\u00a78\\u00a7oZihan Zhang\\nShimin Zhang\\nMingshuai Liu\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.06828\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Mar 2023 03:07:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Hsu et al. (§72023§r)', author: 'Yicheng Hsu; Mingsian Bai', display:{Lore:['[{"text": "arXiv:2303.06867", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning-based Robust Speaker Counting and Separation with the Aid of Spatial Coherence\\u00a7r\\n\\n\\u00a78\\u00a7oYicheng Hsu\\nMingsian Bai\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.06867\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 7 Aug 2023 15:33:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o20 pages, 17 figures\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Zirun Zhu; Hemin Yang; Min Tang; Ziyi Yang; Sefik Emre Eskimez; Huaming Wang', display:{Lore:['[{"text": "arXiv:2303.07005", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-Time Audio-Visual End-to-End Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oZirun Zhu\\nHemin Yang\\nMin Tang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07005\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Mar 2023 11:01:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Gode et al. (§72023§r)', author: 'Henri Gode; Simon Doclo', display:{Lore:['[{"text": "arXiv:2303.07027", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptive Dereverberation, Noise and Interferer Reduction Using Sparse Weighted Linearly Constrained Minimum Power Beamforming\\u00a7r\\n\\n\\u00a78\\u00a7oHenri Gode\\nSimon Doclo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07027\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.23919/EUSIPCO55093.2022.9909809\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Mar 2023 11:44:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o30th European Signal Processing Conference (EUSIPCO 2022)\\u00a7r"}']}
{title:'Ge et al. (§72023§r)', author: 'Wanying Ge; Hemlata Tak; Massimiliano Todisco; Nicholas Evans', display:{Lore:['[{"text": "arXiv:2303.07073", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCan spoofing countermeasure and speaker verification systems be jointly optimised?\\u00a7r\\n\\n\\u00a78\\u00a7oWanying Ge\\nHemlata Tak\\nMassimiliano Todisco\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07073\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 20 Dec 2023 13:45:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Wechsler et al. (§72023§r)', author: 'Julian Wechsler; Srikanth Raj Chetupalli; Wolfgang Mack; Emanuël A. P. Habets', display:{Lore:['[{"text": "arXiv:2303.07143", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Microphone Speaker Separation by Spatial Regions\\u00a7r\\n\\n\\u00a78\\u00a7oJulian Wechsler\\nSrikanth Raj Chetupalli\\nWolfgang Mack\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07143\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Mar 2023 14:11:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to the 2023 IEEE InternationalConference on Acoustics, Speech, and Signal Processing\\u00a7r"}']}
{title:'Zarazaga et al. (§72023§r)', author: 'Pablo Perez Zarazaga; Gustav Eje Henter; Zofia Malisz', display:{Lore:['[{"text": "arXiv:2303.07442", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA processing framework to access large quantities of whispered speech found in ASMR\\u00a7r\\n\\n\\u00a78\\u00a7oPablo Perez Zarazaga\\nGustav Eje Henter\\nZofia Malisz\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07442\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Mar 2023 19:50:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2023, 5 pages, 2 figures, 2 tables\\u00a7r"}']}
{title:'Ick et al. (§72023§r)', author: 'Christopher Ick; Adib Mehrabi; Wenyu Jin', display:{Lore:['[{"text": "arXiv:2303.07449", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBlind Acoustic Room Parameter Estimation Using Phase Features\\u00a7r\\n\\n\\u00a78\\u00a7oChristopher Ick\\nAdib Mehrabi\\nWenyu Jin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07449\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Mar 2023 20:05:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages + 1 page bibliography, 3 figures, to be published in proceedings of ICASSP 2023\\u00a7r"}']}
{title:'Han et al. (§72023§r)', author: 'Cong Han; Nima Mesgarani', display:{Lore:['[{"text": "arXiv:2303.07458", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOnline Binaural Speech Separation of Moving Speakers With a Wavesplit Network\\u00a7r\\n\\n\\u00a78\\u00a7oCong Han\\nNima Mesgarani\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07458\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Mar 2023 20:38:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP 2023\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Yang Yang; Shao-Fu Shih; Hakan Erdogan; Jamie Menjay Lin; Chehung Lee; Yunpeng Li; George Sung; Matthias Grundmann', display:{Lore:['[{"text": "arXiv:2303.07486", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGuided Speech Enhancement Network\\u00a7r\\n\\n\\u00a78\\u00a7oYang Yang\\nShao-Fu Shih\\nHakan Erdogan\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07486\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Mar 2023 21:48:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Venugopalan et al. (§72023§r)', author: 'Subhashini Venugopalan; Jimmy Tobin; Samuel J. Yang; Katie Seaver; Richard J. N. Cave; Pan-Pan Jiang; Neil Zeghidour; Rus Heywood; Jordan Green; Michael P. Brenner', display:{Lore:['[{"text": "arXiv:2303.07533", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Intelligibility Classifiers from 550k Disordered Speech Samples\\u00a7r\\n\\n\\u00a78\\u00a7oSubhashini Venugopalan\\nJimmy Tobin\\nSamuel J. Yang\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07533\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 15 Mar 2023 22:54:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023 camera-ready\\u00a7r"}']}
{title:'Neri et al. (§72023§r)', author: 'Julian Neri; Sebastian Braun', display:{Lore:['[{"text": "arXiv:2303.07569", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Real-Time Single-Channel Speech Separation in Noisy and Reverberant Environments\\u00a7r\\n\\n\\u00a78\\u00a7oJulian Neri\\nSebastian Braun\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07569\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 17 Apr 2023 17:47:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oto appear in ICASSP 2023\\u00a7r"}']}
{title:'Lim et al. (§72023§r)', author: 'Hyungjun Lim; Younggwan Kim; Kiho Yeom; Eunjoo Seo; Hoodong Lee; Stanley Jungkyu Choi; Honglak Lee', display:{Lore:['[{"text": "arXiv:2303.07592", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLightweight feature encoder for wake-up word detection based on self-supervised speech representation\\u00a7r\\n\\n\\u00a78\\u00a7oHyungjun Lim\\nYounggwan Kim\\nKiho Yeom\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07592\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Mar 2023 02:31:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Mingshuai Liu; Shubo Lv; Zihan Zhang; Runduo Han; Xiang Hao; Xianjun Xia; Li Chen; Yijian Xiao; Lei Xie', display:{Lore:['[{"text": "arXiv:2303.07621", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTwo-stage Neural Network for ICASSP 2023 Speech Signal Improvement Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oMingshuai Liu\\nShubo Lv\\nZihan Zhang\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07621\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Mar 2023 04:19:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Ju et al. (§72023§r)', author: 'Yukai Ju; Jun Chen; Shimin Zhang; Shulin He; Wei Rao; Weixin Zhu; Yannan Wang; Tao Yu; Shidong Shang', display:{Lore:['[{"text": "arXiv:2303.07704", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTEA-PSE 3.0: Tencent-Ethereal-Audio-Lab Personalized Speech Enhancement System For ICASSP 2023 DNS Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oYukai Ju\\nJun Chen\\nShimin Zhang\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07704\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Mar 2023 08:41:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Dai et al. (§72023§r)', author: 'Wang Dai; Archontis Politis; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2303.07816", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Channel Masking with Learnable Filterbank for Sound Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oWang Dai\\nArchontis Politis\\nTuomas Virtanen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.07816\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Mar 2023 11:46:47 GMT)\\u00a7r"}']}
{title:'Petermann et al. (§72023§r)', author: 'Darius Petermann; Inseon Jang; Minje Kim', display:{Lore:['[{"text": "arXiv:2303.08005", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNative Multi-Band Audio Coding within Hyper-Autoencoded Reconstruction Propagation Networks\\u00a7r\\n\\n\\u00a78\\u00a7oDarius Petermann\\nInseon Jang\\nMinje Kim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08005\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Mar 2023 15:53:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023. For resources and examples, see https://saige.sice.indiana.edu/research-projects/HARP-Net/\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Jinchao Li; Kaitao Song; Junan Li; Bo Zheng; Dongsheng Li; Xixin Wu; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2303.08019", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Pretrained Representations with Task-related Keywords for Alzheimer\'s Disease Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJinchao Li\\nKaitao Song\\nJunan Li\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08019\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Mar 2023 16:03:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 3 tables\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Jinchao Li; Xixin Wu; Kaitao Song; Dongsheng Li; Xunying Liu; Helen Meng', display:{Lore:['[{"text": "arXiv:2303.08027", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Hierarchical Regression Chain Framework for Affective Vocal Burst Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJinchao Li\\nXixin Wu\\nKaitao Song\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08027\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Mar 2023 16:08:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 5 tables\\u00a7r"}']}
{title:'Briegleb et al. (§72023§r)', author: 'Annika Briegleb; Thomas Haubner; Vasileios Belagiannis; Walter Kellermann', display:{Lore:['[{"text": "arXiv:2303.08052", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLocalizing Spatial Information in Neural Spatiospectral Filters\\u00a7r\\n\\n\\u00a78\\u00a7oAnnika Briegleb\\nThomas Haubner\\nVasileios Belagiannis\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08052\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 3 Jul 2023 13:04:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the 31st European Signal Processing Conference (EUSIPCO 2023), Helsinki, Finland. 5 pages, 3 figures\\u00a7r"}']}
{title:'Hernandez et al. (§72023§r)', author: 'Steven M. Hernandez; Ding Zhao; Shaojin Ding; Antoine Bruguier; Rohit Prabhavalkar; Tara N. Sainath; Yanzhang He; Ian McGraw', display:{Lore:['[{"text": "arXiv:2303.08343", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSharing Low Rank Conformer Weights for Tiny Always-On Ambient Speech Recognition Models\\u00a7r\\n\\n\\u00a78\\u00a7oSteven M. Hernandez\\nDing Zhao\\nShaojin Ding\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08343\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Mar 2023 03:21:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEICASSP 2023\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Chenda Li; Yao Qian; Zhuo Chen; Dongmei Wang; Takuya Yoshioka; Shujie Liu; Yanmin Qian; Michael Zeng', display:{Lore:['[{"text": "arXiv:2303.08372", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTarget Sound Extraction with Variable Cross-modality Clues\\u00a7r\\n\\n\\u00a78\\u00a7oChenda Li\\nYao Qian\\nZhuo Chen\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08372\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Mar 2023 05:17:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Tan et al. (§72023§r)', author: 'Ee-Leng Tan; Santi Peksi; Woon-Seng Gan', display:{Lore:['[{"text": "arXiv:2303.08379", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImplementing Continuous HRTF Measurement in Near-Field\\u00a7r\\n\\n\\u00a78\\u00a7oEe-Leng Tan\\nSanti Peksi\\nWoon-Seng Gan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08379\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10096536\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 16 Jun 2023 02:17:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 9 figures, Submitted to 2023 IEEE International Conference on Acoustics, Speech and Signal Processing\\u00a7r"}']}
{title:'Shen et al. (§72023§r)', author: 'Xiaoyi Shen; Dongyuan Shi; Zhengding Luo; Junwei Ji; Woon-Seng Gan', display:{Lore:['[{"text": "arXiv:2303.08397", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Momentum Two-gradient Direction Algorithm with Variable Step Size Applied to Solve Practical Output Constraint Issue for Active Noise Control\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoyi Shen\\nDongyuan Shi\\nZhengding Luo\\nJunwei Ji\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08397\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Mar 2023 07:01:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper is submitted and accepted by ICASSP2023\\u00a7r"}']}
{title:'Ji et al. (§72023§r)', author: 'Junwei Ji; Dongyuan Shi; Zhengding Luo; Xiaoyi Shen; Woon-Seng Gan', display:{Lore:['[{"text": "arXiv:2303.08411", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA practical distributed active noise control algorithm overcoming communication restrictions\\u00a7r\\n\\n\\u00a78\\u00a7oJunwei Ji\\nDongyuan Shi\\nZhengding Luo\\nXiaoyi Shen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08411\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Mar 2023 07:25:16 GMT)\\u00a7r"}']}
{title:'Cobos et al. (§72023§r)', author: 'Maximo Cobos; Mirco Pezzoli; Fabio Antonacci; Augusto Sarti', display:{Lore:['[{"text": "arXiv:2303.08480", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic source localization in the spherical harmonics domain exploiting low-rank approximations\\u00a7r\\n\\n\\u00a78\\u00a7oMaximo Cobos\\nMirco Pezzoli\\nFabio Antonacci\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08480\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10095324\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2023 IEEE International Conference on Acoustics, Speech and Signal\\n  Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Mar 2023 09:35:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP 2023\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Yuguang Yang; Yu Pan; Jingjing Yin; Jiangyu Han; Lei Ma; Heng Lu', display:{Lore:['[{"text": "arXiv:2303.08636", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHYBRIDFORMER: improving SqueezeFormer with hybrid attention and NSR mechanism\\u00a7r\\n\\n\\u00a78\\u00a7oYuguang Yang\\nYu Pan\\nJingjing Yin\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08636\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Mar 2023 14:03:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2023\\u00a7r"}']}
{title:'Richter et al. (§72023§r)', author: 'Julius Richter; Simon Welker; Jean-Marie Lemercier; Bunlong Lay; Tal Peer; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2303.08674", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Signal Improvement Using Causal Generative Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oJulius Richter\\nSimon Welker\\nJean-Marie Lemercier\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08674\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Mar 2023 14:58:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Elminshawi et al. (§72023§r)', author: 'Mohamed Elminshawi; Srikanth Raj Chetupalli; Emanuël A. P. Habets', display:{Lore:['[{"text": "arXiv:2303.08702", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBeamformer-Guided Target Speaker Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oMohamed Elminshawi\\nSrikanth Raj Chetupalli\\nEmanu\\u00ebl A. P. Habets\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08702\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Mar 2023 15:37:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to the 2023 IEEE InternationalConference on Acoustics, Speech, and Signal Processing (ICASSP 2023)\\u00a7r"}']}
{title:'Hafezi et al. (§72023§r)', author: 'Sina Hafezi; Alastair H. Moore; Pierre Guiraud; Patrick A. Naylor; Jacob Donley; Vladimir Tourbabin; Thomas Lunner', display:{Lore:['[{"text": "arXiv:2303.08967", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSubspace Hybrid Beamforming for Head-worn Microphone Arrays\\u00a7r\\n\\n\\u00a78\\u00a7oSina Hafezi\\nAlastair H. Moore\\nPierre Guiraud\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.08967\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Mar 2023 22:33:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, accepted for ICASSP 2023\\u00a7r"}']}
{title:'Park et al. (§72023§r)', author: 'Hyun Joon Park; Seok Woo Yang; Jin Sob Kim; Wooseok Shin; Sung Won Han', display:{Lore:['[{"text": "arXiv:2303.09057", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTriAAN-VC: Triple Adaptive Attention Normalization for Any-to-Any Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oHyun Joon Park\\nSeok Woo Yang\\nJin Sob Kim\\nWooseok Shin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.09057\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Mar 2023 03:13:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP 2023\\u00a7r"}']}
{title:'Fu et al. (§72023§r)', author: 'Yanzhe Fu; Yueteng Kang; Songjun Cao; Long Ma', display:{Lore:['[{"text": "arXiv:2303.09278", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDistillW2V2: A Small and Streaming Wav2vec 2.0 Based ASR Model\\u00a7r\\n\\n\\u00a78\\u00a7oYanzhe Fu\\nYueteng Kang\\nSongjun Cao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.09278\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Mar 2023 12:59:17 GMT)\\u00a7r"}']}
{title:'Lin et al. (§72023§r)', author: 'Xiaoyu Lin; Xiaoyu Bie; Simon Leglaive; Laurent Girin; Xavier Alameda-Pineda', display:{Lore:['[{"text": "arXiv:2303.09404", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Modeling with a Hierarchical Transformer Dynamical VAE\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoyu Lin\\nXiaoyu Bie\\nSimon Leglaive\\nLaurent Girin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.09404\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 10 May 2023 13:55:59 GMT)\\u00a7r"}']}
{title:'Arend et al. (§72023§r)', author: 'Johannes M. Arend; Christoph Pörschmann; Stefan Weinzierl; Fabian Brinkmann', display:{Lore:['[{"text": "arXiv:2303.09966", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMagnitude-Corrected and Time-Aligned Interpolation of Head-Related Transfer Functions\\u00a7r\\n\\n\\u00a78\\u00a7oJohannes M. Arend\\nChristoph P\\u00f6rschmann\\nStefan Weinzierl\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.09966\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3313908\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Trans. Audio Speech and Lang. Proc., 31, 3783--3799\\n  (2023)\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 17 Mar 2023 13:39:41 GMT)\\u00a7r"}']}
{title:'Hauret et al. (§72023§r)', author: 'Julien Hauret; Thomas Joubaud; Véronique Zimpfer; Éric Bavu', display:{Lore:['[{"text": "arXiv:2303.10008", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConfigurable EBEN: Extreme Bandwidth Extension Network to enhance body-conducted speech capture\\u00a7r\\n\\n\\u00a78\\u00a7oJulien Hauret\\nThomas Joubaud\\nV\\u00e9ronique Zimpfer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.10008\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3313433\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 12 Sep 2023 18:33:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in IEEE/ACM Transactionson Audio, Speech and Language Processing on 14/08/2023\\u00a7r"}']}
{title:'Kumar et al. (§72023§r)', author: 'Vanya Bannihatti Kumar; Shanbo Cheng; Ningxin Peng; Yuchen Zhang', display:{Lore:['[{"text": "arXiv:2303.10160", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisual Information Matters for ASR Error Correction\\u00a7r\\n\\n\\u00a78\\u00a7oVanya Bannihatti Kumar\\nShanbo Cheng\\nNingxin Peng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.10160\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 26 May 2023 08:37:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2023\\u00a7r"}']}
{title:'Laptev et al. (§72023§r)', author: 'Aleksandr Laptev; Vladimir Bataev; Igor Gitman; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2303.10384", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPowerful and Extensible WFST Framework for RNN-Transducer Losses\\u00a7r\\n\\n\\u00a78\\u00a7oAleksandr Laptev\\nVladimir Bataev\\nIgor Gitman\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.10384\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10096679\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 18 Mar 2023 10:36:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proc. ICASSP 2023, June 04-10, 2023,Rhodes island, Greece. 5 pages, 5 figures, 3 tables\\u00a7r"}']}
{title:'Ge et al. (§72023§r)', author: 'Zirui Ge; Haiyan Guo; Zhen Yang', display:{Lore:['[{"text": "arXiv:2303.10556", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Graph feature fusion technique for speaker recognition based on wav2vec2.0 framework\\u00a7r\\n\\n\\u00a78\\u00a7oZirui Ge\\nHaiyan Guo\\nZhen Yang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.10556\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 19 Mar 2023 03:50:16 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Xiaoyu Yang; Qiujia Li; Chao Zhang; Philip C. Woodland', display:{Lore:['[{"text": "arXiv:2303.10917", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKnowledge Distillation from Multiple Foundation Models for End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoyu Yang\\nQiujia Li\\nChao Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.10917\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Mar 2023 07:18:18 GMT)\\u00a7r"}']}
{title:'Yu et al. (§72023§r)', author: 'Haibin Yu; Yuxuan Hu; Yao Qian; Ma Jin; Linquan Liu; Shujie Liu; Yu Shi; Yanmin Qian; Edward Lin; Michael Zeng', display:{Lore:['[{"text": "arXiv:2303.10949", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCode-Switching Text Generation and Injection in Mandarin-English ASR\\u00a7r\\n\\n\\u00a78\\u00a7oHaibin Yu\\nYuxuan Hu\\nYao Qian\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.10949\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Mar 2023 09:13:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Ren et al. (§72023§r)', author: 'Xiaoming Ren; Chao Li; Shenjian Wang; Biao Li', display:{Lore:['[{"text": "arXiv:2303.12187", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPractice of the conformer enhanced AUDIO-VISUAL HUBERT on Mandarin and English\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoming Ren\\nChao Li\\nShenjian Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.12187\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Feb 2023 02:10:13 GMT)\\u00a7r"}']}
{title:'Jayashankar et al. (§72023§r)', author: 'Tejas Jayashankar; Jilong Wu; Leda Sari; David Kant; Vimal Manohar; Qing He', display:{Lore:['[{"text": "arXiv:2303.12197", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Representations for Singing Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oTejas Jayashankar\\nJilong Wu\\nLeda Sari\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.12197\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Mar 2023 21:04:03 GMT)\\u00a7r"}']}
{title:'Sadhu et al. (§72023§r)', author: 'Samik Sadhu; Hynek Hermansky', display:{Lore:['[{"text": "arXiv:2303.12908", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervised Learning with Speech Modulation Dropout\\u00a7r\\n\\n\\u00a78\\u00a7oSamik Sadhu\\nHynek Hermansky\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.12908\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 Mar 2023 20:55:54 GMT)\\u00a7r"}']}
{title:'Koyama et al. (§72023§r)', author: 'Shoichi Koyama; Keisuke Kimura; Natsuki Ueno', display:{Lore:['[{"text": "arXiv:2303.13027", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeighted Pressure and Mode Matching for Sound Field Reproduction: Theoretical and Experimental Comparisons\\u00a7r\\n\\n\\u00a78\\u00a7oShoichi Koyama\\nKeisuke Kimura\\nNatsuki Ueno\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.13027\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Mar 2023 04:26:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Journal of Audio Engineering Society, Special Issue on SpatialAudio\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Kai Liu; Hailiang Xiong; Gangqiang Yang; Zhengfeng Du; Yewen Cao; Danyal Shah', display:{Lore:['[{"text": "arXiv:2303.13243", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPyramid Multi-branch Fusion DCNN with Multi-Head Self-Attention for Mandarin Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oKai Liu\\nHailiang Xiong\\nGangqiang Yang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.13243\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Mar 2023 13:18:54 GMT)\\u00a7r"}']}
{title:'Min et al. (§72023§r)', author: 'Do June Min; Andreas Stolcke; Anirudh Raju; Colin Vaz; Di He; Venkatesh Ravichandran; Viet Anh Trinh', display:{Lore:['[{"text": "arXiv:2303.13407", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptive Endpointing with Deep Contextual Multi-armed Bandits\\u00a7r\\n\\n\\u00a78\\u00a7oDo June Min\\nAndreas Stolcke\\nAnirudh Raju\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.13407\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10097142\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. IEEE ICASSP, June 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Mar 2023 16:28:26 GMT)\\u00a7r"}']}
{title:'Torcoli et al. (§72023§r)', author: 'Matteo Torcoli; Emanuël A. P. Habets', display:{Lore:['[{"text": "arXiv:2303.13453", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBetter Together: Dialogue Separation and Voice Activity Detection for Audio Personalization in TV\\u00a7r\\n\\n\\u00a78\\u00a7oMatteo Torcoli\\nEmanu\\u00ebl A. P. Habets\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.13453\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Mar 2023 17:19:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper accepted to the 2023 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2023), Rhodes, Greece\\u00a7r"}']}
{title:'Nayem et al. (§72023§r)', author: 'Khandokar Md. Nayem; Donald S. Williamson', display:{Lore:['[{"text": "arXiv:2303.13685", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention-based Speech Enhancement Using Human Quality Perception Modelling\\u00a7r\\n\\n\\u00a78\\u00a7oKhandokar Md. Nayem\\nDonald S. Williamson\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.13685\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Mar 2023 21:32:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 4 figures, 3 tables, submitted in journal TASLP 2023\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Karren Yang; Ting-Yao Hu; Jen-Hao Rick Chang; Hema Swetha Koppula; Oncel Tuzel', display:{Lore:['[{"text": "arXiv:2303.14885", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lText is All You Need: Personalizing ASR Models using Controllable Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oKarren Yang\\nTing-Yao Hu\\nJen-Hao Rick Chang\\nHema Swetha Koppula\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.14885\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Mar 2023 02:50:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Fang et al. (§72023§r)', author: 'Huajian Fang; Niklas Wittmer; Johannes Twiefel; Stefan Wermter; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2303.15042", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPartially Adaptive Multichannel Joint Reduction of Ego-noise and Environmental Noise\\u00a7r\\n\\n\\u00a78\\u00a7oHuajian Fang\\nNiklas Wittmer\\nJohannes Twiefel\\nStefan Wermter\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.15042\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2023 - 2023 IEEE International Conference on Acoustics,\\n  Speech, and Signal Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Mar 2023 09:40:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the 2023 IEEE InternationalConference on Acoustics, Speech, and Signal Processing (ICASSP 2023)\\u00a7r"}']}
{title:'Tankasala et al. (§72023§r)', author: 'Srinath Tankasala; Long Chen; Andreas Stolcke; Anirudh Raju; Qianli Deng; Chander Chandak; Aparna Khare; Roland Maas; Venkatesh Ravichandran', display:{Lore:['[{"text": "arXiv:2303.15132", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-utterance ASR Rescoring with Graph-based Label Propagation\\u00a7r\\n\\n\\u00a78\\u00a7oSrinath Tankasala\\nLong Chen\\nAndreas Stolcke\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.15132\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10096820\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. IEEE ICASSP, June 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Mar 2023 12:08:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in IEEE ICASSP 2023\\u00a7r"}']}
{title:'Mavandadi et al. (§72023§r)', author: 'Sepand Mavandadi; Tara N. Sainath; Ke Hu; Zelin Wu', display:{Lore:['[{"text": "arXiv:2303.15293", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Deliberation-based Joint Acoustic and Text Decoder\\u00a7r\\n\\n\\u00a78\\u00a7oSepand Mavandadi\\nTara N. Sainath\\nKe Hu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.15293\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Mar 2023 18:02:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2021\\u00a7r"}']}
{title:'Park et al. (§72023§r)', author: 'Seongyeon Park; Myungseo Song; Bohyung Kim; Tae-Hyun Oh', display:{Lore:['[{"text": "arXiv:2303.15669", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Pre-Training For Data-Efficient Text-to-Speech On Low Resource Languages\\u00a7r\\n\\n\\u00a78\\u00a7oSeongyeon Park\\nMyungseo Song\\nBohyung Kim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.15669\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Mar 2023 01:26:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Jin Sob Kim; Hyun Joon Park; Wooseok Shin; Sung Won Han', display:{Lore:['[{"text": "arXiv:2303.15703", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAD-YOLO: You Look Only Once in Training Multiple Sound Event Localization and Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJin Sob Kim\\nHyun Joon Park\\nWooseok Shin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.15703\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 10 May 2023 04:36:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted for publication in IEEE ICASSP 2023\\u00a7r"}']}
{title:'Arikawa et al. (§72023§r)', author: 'Kazuyuki Arikawa; Shoichi Koyama; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2303.16021", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatial Active Noise Control Method Based On Sound Field Interpolation From Reference Microphone Signals\\u00a7r\\n\\n\\u00a78\\u00a7oKazuyuki Arikawa\\nShoichi Koyama\\nHiroshi Saruwatari\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.16021\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Mar 2023 14:50:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2023\\u00a7r"}']}
{title:'Arikawa et al. (§72023§r)', author: 'Kazuyuki Arikawa; Shoichi Koyama; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2303.16389", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKernel-interpolation-based spatial active noise control with exterior radiation suppression\\u00a7r\\n\\n\\u00a78\\u00a7oKazuyuki Arikawa\\nShoichi Koyama\\nHiroshi Saruwatari\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.16389\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 31 Mar 2023 04:29:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at International Congresson Acoustics (ICA) 2022\\u00a7r"}']}
{title:'Park et al. (§72023§r)', author: 'Jinseok Park; Hyung Yong Kim; Jihwan Park; Byeong-Yeol Kim; Shukjae Choi; Yunkyu Lim', display:{Lore:['[{"text": "arXiv:2303.16511", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint unsupervised and supervised learning for context-aware language identification\\u00a7r\\n\\n\\u00a78\\u00a7oJinseok Park\\nHyung Yong Kim\\nJihwan Park\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.16511\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 14 Apr 2023 07:27:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Pandey et al. (§72023§r)', author: 'Rahul Pandey; Roger Ren; Qi Luo; Jing Liu; Ariya Rastrow; Ankur Gandhe; Denis Filimonov; Grant Strimel; Andreas Stolcke; Ivan Bulyko', display:{Lore:['[{"text": "arXiv:2303.17131", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPROCTER: PROnunciation-aware ConTextual adaptER for personalized speech recognition in neural transducers\\u00a7r\\n\\n\\u00a78\\u00a7oRahul Pandey\\nRoger Ren\\nQi Luo\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.17131\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10096062\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. IEEE ICASSP, June 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Mar 2023 03:36:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proc. IEEE ICASSP\\u00a7r"}']}
{title:'Mei et al. (§72023§r)', author: 'Xinhao Mei; Chutong Meng; Haohe Liu; Qiuqiang Kong; Tom Ko; Chengqi Zhao; Mark D. Plumbley; Yuexian Zou; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2303.17395", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research\\u00a7r\\n\\n\\u00a78\\u00a7oXinhao Mei\\nChutong Meng\\nHaohe Liu\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.17395\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Mar 2023 14:07:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Minkyu Kim; Kim Sung-Bin; Tae-Hyun Oh', display:{Lore:['[{"text": "arXiv:2303.17489", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrefix tuning for automated audio captioning\\u00a7r\\n\\n\\u00a78\\u00a7oMinkyu Kim\\nKim Sung-Bin\\nTae-Hyun Oh\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.17489\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 4 Apr 2023 07:39:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Gunawardhana et al. (§72023§r)', author: 'Malitha Gunawardhana; Chathuki Navanjana; Dinithi Fernando; Nipuna Upeksha; Anjula De Silva', display:{Lore:['[{"text": "arXiv:2303.17829", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluation of Noise Reduction Methods for Sentence Recognition by Sinhala Speaking Listeners\\u00a7r\\n\\n\\u00a78\\u00a7oMalitha Gunawardhana\\nChathuki Navanjana\\nDinithi Fernando\\nNipuna Upeksha\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.17829\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 27 Jun 2023 16:50:38 GMT)\\u00a7r"}']}
{title:'Záviška et al. (§72023§r)', author: 'Pavel Záviška; Pavel Rajmic; Ondřej Mokrý', display:{Lore:['[{"text": "arXiv:2303.18023", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultiple Hankel matrix rank minimization for audio inpainting\\u00a7r\\n\\n\\u00a78\\u00a7oPavel Z\\u00e1vi\\u0161ka\\nPavel Rajmic\\nOnd\\u0159ej Mokr\\u00fd\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.18023\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TSP59544.2023.10197741\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2023 46th International Conference on Telecommunications and\\n  Signal Processing (TSP)\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 14 Jul 2023 11:22:43 GMT)\\u00a7r"}']}
{title:'Fu et al. (§72023§r)', author: 'Szu-Wei Fu; Yaran Fan; Yasaman Hosseinkashi; Jayant Gupchup; Ross Cutler', display:{Lore:['[{"text": "arXiv:2304.00658", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Meeting Inclusiveness using Speech Interruption Analysis\\u00a7r\\n\\n\\u00a78\\u00a7oSzu-Wei Fu\\nYaran Fan\\nYasaman Hosseinkashi\\nJayant Gupchup\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.00658\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 5 Apr 2023 02:17:35 GMT)\\u00a7r"}']}
{title:'Teh et al. (§72023§r)', author: 'Tian Huey Teh; Vivian Hu; Devang S Ram Mohan; Zack Hodari; Christopher G. R. Wallis; Tomás Gomez Ibarrondo; Alexandra Torresquintero; James Leoni; Mark Gales; Simon King', display:{Lore:['[{"text": "arXiv:2304.00714", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnsemble prosody prediction for expressive speech synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oTian Huey Teh\\nVivian Hu\\nDevang S Ram Mohan\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.00714\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 3 Apr 2023 04:21:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yuang Li; Xianrui Zheng; Philip C. Woodland', display:{Lore:['[{"text": "arXiv:2304.00871", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Learning-Based Source Separation for Meeting Data\\u00a7r\\n\\n\\u00a78\\u00a7oYuang Li\\nXianrui Zheng\\nPhilip C. Woodland\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.00871\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 3 Apr 2023 10:51:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proc. ICASSP2023\\u00a7r"}']}
{title:'Fuchs et al. (§72023§r)', author: 'Tzeviya Sylvia Fuchs; Yedid Hoshen', display:{Lore:['[{"text": "arXiv:2304.00993", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Word Segmentation Using Temporal Gradient Pseudo-Labels\\u00a7r\\n\\n\\u00a78\\u00a7oTzeviya Sylvia Fuchs\\nYedid Hoshen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.00993\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Mar 2023 17:59:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Kumar et al. (§72023§r)', author: 'Anurag Kumar; Ke Tan; Zhaoheng Ni; Pranay Manocha; Xiaohui Zhang; Ethan Henderson; Buye Xu', display:{Lore:['[{"text": "arXiv:2304.01448", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTorchAudio-Squim: Reference-less Speech Quality and Intelligibility measures in TorchAudio\\u00a7r\\n\\n\\u00a78\\u00a7oAnurag Kumar\\nKe Tan\\nZhaoheng Ni\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.01448\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 4 Apr 2023 01:44:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Koldovský et al. (§72023§r)', author: "Zbyněk Koldovský; Jaroslav Čmejla; Tülay Adalı; Stephen O'Regan", display:{Lore:['[{"text": "arXiv:2304.01778", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIndependent Vector Extraction Constrained on Manifold of Half-Length Filters\\u00a7r\\n\\n\\u00a78\\u00a7oZbyn\\u011bk Koldovsk\\u00fd\\nJaroslav \\u010cmejla\\nT\\u00fclay Adal\\u0131\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.01778\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 4 Apr 2023 13:11:33 GMT)\\u00a7r"}']}
{title:'Thienpondt et al. (§72023§r)', author: 'Jenthe Thienpondt; Nilesh Madhu; Kris Demuynck', display:{Lore:['[{"text": "arXiv:2304.03515", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMargin-Mixup: A Method for Robust Speaker Verification in Multi-Speaker Audio\\u00a7r\\n\\n\\u00a78\\u00a7oJenthe Thienpondt\\nNilesh Madhu\\nKris Demuynck\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.03515\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 7 Apr 2023 07:19:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oproceedings of ICASSP 2023\\u00a7r"}']}
{title:'Yadav et al. (§72023§r)', author: 'Shivani Yadav; Dipanjan Gope; Uma Maheswari K.; Prasanta Kumar Ghosh', display:{Lore:['[{"text": "arXiv:2304.03758", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn unsupervised segmentation of vocal breath sounds\\u00a7r\\n\\n\\u00a78\\u00a7oShivani Yadav\\nDipanjan Gope\\nUma Maheswari K.\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.03758\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 7 Apr 2023 17:53:08 GMT)\\u00a7r"}']}
{title:'Vadapalli (§72023§r)', author: 'Anandaswarup Vadapalli', display:{Lore:['[{"text": "arXiv:2304.04157", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn investigation of speaker independent phrase break models in End-to-End TTS systems\\u00a7r\\n\\n\\u00a78\\u00a7oAnandaswarup Vadapalli\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.04157\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 21 Apr 2023 05:03:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted for review to IEEE Access\\u00a7r"}']}
{title:'Hawley et al. (§72023§r)', author: 'Scott H. Hawley; Christian J. Steinmetz', display:{Lore:['[{"text": "arXiv:2304.04394", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Neural Representations for Audio Manipulation\\u00a7r\\n\\n\\u00a78\\u00a7oScott H. Hawley\\nChristian J. Steinmetz\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.04394\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 Apr 2023 05:44:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted as Express Paper for AESEurope 2023, https://aeseurope.com/\\u00a7r"}']}
{title:'Sandhan et al. (§72023§r)', author: 'Tushar Sandhan; Sukanya Sonowal; Jin Young Choi', display:{Lore:['[{"text": "arXiv:2304.05067", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Bank: A High-Level Acoustic Signal Representation for Audio Event Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oTushar Sandhan\\nSukanya Sonowal\\nJin Young Choi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.05067\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Apr 2023 09:03:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 9 figures, published in IEEE International Conf ICCAS 2014 (Best paperaward)\\u00a7r"}']}
{title:'Zheng et al. (§72023§r)', author: 'Rui-Chen Zheng; Yang Ai; Zhen-Hua Ling', display:{Lore:['[{"text": "arXiv:2304.05574", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Reconstruction from Silent Tongue and Lip Articulation By Pseudo Target Generation and Domain Adversarial Training\\u00a7r\\n\\n\\u00a78\\u00a7oRui-Chen Zheng\\nYang Ai\\nZhen-Hua Ling\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.05574\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 12 Apr 2023 02:24:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in ICASSP2023\\u00a7r"}']}
{title:'Zhao et al. (§72023§r)', author: 'Zhiyuan Zhao; Lijun Wu; Chuanxin Tang; Dacheng Yin; Yucheng Zhao; Chong Luo', display:{Lore:['[{"text": "arXiv:2304.05922", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFiller Word Detection with Hard Category Mining and Inter-Category Focal Loss\\u00a7r\\n\\n\\u00a78\\u00a7oZhiyuan Zhao\\nLijun Wu\\nChuanxin Tang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.05922\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 12 Apr 2023 15:40:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by ICASSP23\\u00a7r"}']}
{title:'Bhati et al. (§72023§r)', author: 'Saurabhchand Bhati; Jesús Villalba; Piotr Żelasko; Laureano Moro-Velazquez; Najim Dehak', display:{Lore:['[{"text": "arXiv:2304.05974", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRegularizing Contrastive Predictive Coding for Speech Applications\\u00a7r\\n\\n\\u00a78\\u00a7oSaurabhchand Bhati\\nJes\\u00fas Villalba\\nPiotr \\u017belasko\\nLaureano Moro-Velazquez\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.05974\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 26 Apr 2023 16:29:27 GMT)\\u00a7r"}']}
{title:'Kelley (§72023§r)', author: 'Matthew C. Kelley', display:{Lore:['[{"text": "arXiv:2304.06183", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic absement in detail: Quantifying acoustic differences across time-series representations of speech data\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew C. Kelley\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.06183\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 14 Apr 2023 18:26:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, accepted for ICPhS 2023; Julia reference corrected in v2\\u00a7r"}']}
{title:'Hohmann (§72023§r)', author: 'Volker Hohmann', display:{Lore:['[{"text": "arXiv:2304.06786", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe future of hearing aid technology\\u00a7r\\n\\n\\u00a78\\u00a7oVolker Hohmann\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.06786\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s00391-023-02179-y\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 27 Apr 2023 16:47:10 GMT)\\u00a7r"}']}
{title:'Xu et al. (§72023§r)', author: 'Hainan Xu; Fei Jia; Somshubra Majumdar; He Huang; Shinji Watanabe; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2304.06795", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Sequence Transduction by Jointly Predicting Tokens and Durations\\u00a7r\\n\\n\\u00a78\\u00a7oHainan Xu\\nFei Jia\\nSomshubra Majumdar\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.06795\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 29 May 2023 21:08:11 GMT)\\u00a7r"}']}
{title:'Kreuzer et al. (§72023§r)', author: 'Matthias Kreuzer; Walter Kellermann', display:{Lore:['[{"text": "arXiv:2304.07305", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l1-D Residual Convolutional Neural Network coupled with Data Augmentation and Regularization for the ICPHM 2023 Data Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oMatthias Kreuzer\\nWalter Kellermann\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.07305\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 24 May 2023 10:11:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the IEEE Conference on Prognostics and HealthManagement (ICPHM) 2023\\u00a7r"}']}
{title:'Kreuzer et al. (§72023§r)', author: 'Matthias Kreuzer; David Schmidt; Simon Wokusch; Walter Kellermann', display:{Lore:['[{"text": "arXiv:2304.07307", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAirborne Sound Analysis for the Detection of Bearing Faults in Railway Vehicles with Real-World Data\\u00a7r\\n\\n\\u00a78\\u00a7oMatthias Kreuzer\\nDavid Schmidt\\nSimon Wokusch\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.07307\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 24 May 2023 10:08:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the ICPHM 2023\\u00a7r"}']}
{title:'Feng et al. (§72023§r)', author: 'Linfeng Feng; Yijun Gong; Xiao-Lei Zhang', display:{Lore:['[{"text": "arXiv:2304.07512", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSoft Label Coding for End-to-end Sound Source Localization With Ad-hoc Microphone Arrays\\u00a7r\\n\\n\\u00a78\\u00a7oLinfeng Feng\\nYijun Gong\\nXiao-Lei Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.07512\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 15 Apr 2023 08:58:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4pages, 2figures, conference\\u00a7r"}']}
{title:'Zuluaga-Gomez et al. (§72023§r)', author: 'Juan Zuluaga-Gomez; Amrutha Prasad; Iuliia Nigmatulina; Petr Motlicek; Matthias Kleinert', display:{Lore:['[{"text": "arXiv:2304.07842", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.HC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Virtual Simulation-Pilot Agent for Training of Air Traffic Controllers\\u00a7r\\n\\n\\u00a78\\u00a7oJuan Zuluaga-Gomez\\nAmrutha Prasad\\nIuliia Nigmatulina\\nPetr Motlicek\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.07842\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 16 Apr 2023 17:45:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review\\u00a7r"}']}
{title:'Jo et al. (§72023§r)', author: 'Byeongho Jo; Seungkwon Beack; Taejin Lee', display:{Lore:['[{"text": "arXiv:2304.08076", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio coding with unified noise shaping and phase contrast control\\u00a7r\\n\\n\\u00a78\\u00a7oByeongho Jo\\nSeungkwon Beack\\nTaejin Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.08076\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Apr 2023 08:41:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted and accepted in ICASSP (International Conference on Acoustics, Speech, and Signal Processing) 2023\\u00a7r"}']}
{title:'Delalez et al. (§72023§r)', author: 'Samuel Delalez; Ludi Akue', display:{Lore:['[{"text": "arXiv:2304.08209", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural TTS in French: Comparing Graphemic and Phonetic Inputs Using the SynPaFlex-Corpus and Tacotron2\\u00a7r\\n\\n\\u00a78\\u00a7oSamuel Delalez\\nLudi Akue\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.08209\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 18 Apr 2023 09:50:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 3 figures\\u00a7r"}']}
{title:'Kreuzer et al. (§72023§r)', author: 'Matthias Kreuzer; Alexander Schmidt; Walter Kellermann', display:{Lore:['[{"text": "arXiv:2304.08249", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNovel features for the detection of bearing faults in railway vehicles\\u00a7r\\n\\n\\u00a78\\u00a7oMatthias Kreuzer\\nAlexander Schmidt\\nWalter Kellermann\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.08249\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3397/IN-2021-2537\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 Apr 2023 10:09:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Inter-Noise 2021\\u00a7r"}']}
{title:'Ray et al. (§72023§r)', author: 'Subhajit Ray; Xinghua Sun; Nolan Tremelling; Maria Gordiyenko; Peter Kinget', display:{Lore:['[{"text": "arXiv:2304.08541", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHow Tiny Can Analog Filterbank Features Be Made for Ultra-low-power On-device Keyword Spotting?\\u00a7r\\n\\n\\u00a78\\u00a7oSubhajit Ray\\nXinghua Sun\\nNolan Tremelling\\nMaria Gordiyenko\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.08541\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Apr 2023 18:15:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted as a full paper by the TinyMLResearch Symposium 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Zhong-Qiu Wang; Samuele Cornell; Shukjae Choi; Younglo Lee; Byeong-Yeol Kim; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2304.08707", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated Full- and Sub-Band Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oZhong-Qiu Wang\\nSamuele Cornell\\nShukjae Choi\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.08707\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 Apr 2023 02:37:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oin ICASSP 2023\\u00a7r"}']}
{title:'Hsu et al. (§72023§r)', author: 'Yicheng Hsu; Mingsian R. Bai', display:{Lore:['[{"text": "arXiv:2304.08887", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lArray Configuration-Agnostic Personal Voice Activity Detection Based on Spatial Coherence\\u00a7r\\n\\n\\u00a78\\u00a7oYicheng Hsu\\nMingsian R. Bai\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.08887\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 Apr 2023 10:27:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTER-NOISE 2023. arXiv admin note: text overlap with arXiv:2211.08748\\u00a7r"}']}
{title:'Shen et al. (§72023§r)', author: 'Kai Shen; Zeqian Ju; Xu Tan; Yanqing Liu; Yichong Leng; Lei He; Tao Qin; Sheng Zhao; Jiang Bian', display:{Lore:['[{"text": "arXiv:2304.09116", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers\\u00a7r\\n\\n\\u00a78\\u00a7oKai Shen\\nZeqian Ju\\nXu Tan\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.09116\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 30 May 2023 16:09:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oA large-scale text-to-speech and singing voice synthesis system with latent diffusion models. Update: NaturalSpeech 2 extension to voice conversion and speechenhancement\\u00a7r"}']}
{title:'Xu et al. (§72023§r)', author: 'Ziyi Xu; Ziyue Zhao; Tim Fingscheidt', display:{Lore:['[{"text": "arXiv:2304.09226", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCoded Speech Quality Measurement by a Non-Intrusive PESQ-DNN\\u00a7r\\n\\n\\u00a78\\u00a7oZiyi Xu\\nZiyue Zhao\\nTim Fingscheidt\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.09226\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 Apr 2023 18:26:56 GMT)\\u00a7r"}']}
{title:'Chiodi et al. (§72023§r)', author: 'Túlio Chiodi; Arthur dos Santos; Pedro Martins; Bruno Masiero', display:{Lore:['[{"text": "arXiv:2304.09318", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAIRCADE: an Anechoic and IR Convolution-based Auralization Data-compilation Ensemble\\u00a7r\\n\\n\\u00a78\\u00a7oT\\u00falio Chiodi\\nArthur dos Santos\\nPedro Martins\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.09318\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5281/zenodo.7818761\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 24 Apr 2023 01:11:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Xilai Li; Goeric Huybrechts; Srikanth Ronanki; Jeff Farris; Sravan Bodapati', display:{Lore:['[{"text": "arXiv:2304.09325", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDynamic Chunk Convolution for Unified Streaming and Non-Streaming Conformer ASR\\u00a7r\\n\\n\\u00a78\\u00a7oXilai Li\\nGoeric Huybrechts\\nSrikanth Ronanki\\nJeff Farris\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.09325\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 25 Apr 2023 19:36:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2023)\\u00a7r"}']}
{title:'Reuter et al. (§72023§r)', author: 'Paul M. Reuter; Christian Rollwage; Bernd T. Meyer', display:{Lore:['[{"text": "arXiv:2304.09585", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultilingual Query-by-Example Keyword Spotting with Metric Learning and Phoneme-to-Embedding Mapping\\u00a7r\\n\\n\\u00a78\\u00a7oPaul M. Reuter\\nChristian Rollwage\\nBernd T. Meyer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.09585\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 19 Apr 2023 11:47:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Guo et al. (§72023§r)', author: 'Feng Guo; Zheng Sun; Yuxuan Chen; Lei Ju', display:{Lore:['[{"text": "arXiv:2304.10088", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards the Universal Defense for Query-Based Audio Adversarial Attacks\\u00a7r\\n\\n\\u00a78\\u00a7oFeng Guo\\nZheng Sun\\nYuxuan Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.10088\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 20 Apr 2023 04:50:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Cybersecurity journal\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Mohan Li; Rama Doddipatla', display:{Lore:['[{"text": "arXiv:2304.10869", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-autoregressive End-to-end Approaches for Joint Automatic Speech Recognition and Spoken Language Understanding\\u00a7r\\n\\n\\u00a78\\u00a7oMohan Li\\nRama Doddipatla\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.10869\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 Apr 2023 10:31:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 1 figure, accepted at IEEE SLT2023\\u00a7r"}']}
{title:'Stuchbury-Wass et al. (§72023§r)', author: 'Jake Stuchbury-Wass; Erika Bondareva; Kayla-Jade Butkow; Sanja Scepanovic; Zoran Radivojevic; Cecilia Mascolo', display:{Lore:['[{"text": "arXiv:2304.11020", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHeart Rate Extraction from Abdominal Audio Signals\\u00a7r\\n\\n\\u00a78\\u00a7oJake Stuchbury-Wass\\nErika Bondareva\\nKayla-Jade Butkow\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.11020\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 Apr 2023 15:08:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Jacqmin et al. (§72023§r)', author: 'Léo Jacqmin; Lucas Druart; Yannick Estève; Benoît Favre; Lina Maria Rojas-Barahona; Valentin Vielzeuf', display:{Lore:['[{"text": "arXiv:2304.11073", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOLISIA: a Cascade System for Spoken Dialogue State Tracking\\u00a7r\\n\\n\\u00a78\\u00a7oL\\u00e9o Jacqmin\\nLucas Druart\\nYannick Est\\u00e8ve\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.11073\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 31 Aug 2023 08:51:33 GMT)\\u00a7r"}']}
{title:'Phukan et al. (§72023§r)', author: 'Orchid Chetia Phukan; Arun Balaji Buduru; Rajesh Sharma', display:{Lore:['[{"text": "arXiv:2304.11472", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparative Study of Pre-trained Speech and Audio Embeddings for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oOrchid Chetia Phukan\\nArun Balaji Buduru\\nRajesh Sharma\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.11472\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 22 Apr 2023 19:56:35 GMT)\\u00a7r"}']}
{title:'Gupta et al. (§72023§r)', author: 'Chitralekha Gupta; Purnima Kamath; Yize Wei; Zhuoyao Li; Suranga Nanayakkara; Lonce Wyse', display:{Lore:['[{"text": "arXiv:2304.11648", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Controllable Audio Texture Morphing\\u00a7r\\n\\n\\u00a78\\u00a7oChitralekha Gupta\\nPurnima Kamath\\nYize Wei\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.11648\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 23 Apr 2023 13:32:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to ICASSP 2023\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Zhijun Liu; Yiwei Guo; Kai Yu', display:{Lore:['[{"text": "arXiv:2304.11750", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffVoice: Text-to-Speech with Latent Diffusion\\u00a7r\\n\\n\\u00a78\\u00a7oZhijun Liu\\nYiwei Guo\\nKai Yu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.11750\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 23 Apr 2023 21:05:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP2023\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Mohan Li; Rama Doddipatla; Catalin Zorila', display:{Lore:['[{"text": "arXiv:2304.11985", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-regularised Minimum Latency Training for Streaming Transformer-based Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMohan Li\\nRama Doddipatla\\nCatalin Zorila\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.11985\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Apr 2023 10:38:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, accepted at Interspeech2022\\u00a7r"}']}
{title:'Tesch et al. (§72023§r)', author: 'Kristina Tesch; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2304.12023", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-channel Speech Separation Using Spatially Selective Deep Non-linear Filters\\u00a7r\\n\\n\\u00a78\\u00a7oKristina Tesch\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.12023\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3334101\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol.32, pp. 542-553, 2024\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 21 Nov 2023 14:59:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted version\\u00a7r"}']}
{title:'Fukuda et al. (§72023§r)', author: 'Ryo Fukuda; Katsuhito Sudoh; Satoshi Nakamura', display:{Lore:['[{"text": "arXiv:2304.12659", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Speech Translation Accuracy and Time Efficiency with Fine-tuned wav2vec 2.0-based Speech Segmentation\\u00a7r\\n\\n\\u00a78\\u00a7oRyo Fukuda\\nKatsuhito Sudoh\\nSatoshi Nakamura\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.12659\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3343614\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 18 Dec 2023 08:32:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE/ACM Transactionson Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Khandelwal et al. (§72023§r)', author: 'Tanmay Khandelwal; Rohan Kumar Das; Andrew Koh; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2304.12688", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Audio-Tagging Assisted Sound Event Detection using Weakified Strong Labels and Frequency Dynamic Convolutions\\u00a7r\\n\\n\\u00a78\\u00a7oTanmay Khandelwal\\nRohan Kumar Das\\nAndrew Koh\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.12688\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Apr 2023 09:43:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for Publication in IEEE-Statistical Signal Processing (SSP) Workshop 2023\\u00a7r"}']}
{title:'Lu et al. (§72023§r)', author: 'Ye-Xin Lu; Yang Ai; Zhen-Hua Ling', display:{Lore:['[{"text": "arXiv:2304.13270", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSource-Filter-Based Generative Adversarial Neural Vocoder for High Fidelity Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYe-Xin Lu\\nYang Ai\\nZhen-Hua Ling\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.13270\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-981-99-2401-1_6\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nMan-Machine Speech Communication, 2022, pp.68-80\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Apr 2023 03:43:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by NCMMSC 2022\\u00a7r"}']}
{title:'Xu et al. (§72023§r)', author: 'Xinmeng Xu; Weiping Tu; Chang Han; Yuhong Yang', display:{Lore:['[{"text": "arXiv:2304.13439", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAll Information is Necessary: Integrating Speech Positive and Negative Information by Contrastive Learning for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oXinmeng Xu\\nWeiping Tu\\nChang Han\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.13439\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Apr 2023 10:50:05 GMT)\\u00a7r"}']}
{title:'Ghosal et al. (§72023§r)', author: 'Deepanway Ghosal; Navonil Majumder; Ambuj Mehrish; Soujanya Poria', display:{Lore:['[{"text": "arXiv:2304.13731", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lText-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model\\u00a7r\\n\\n\\u00a78\\u00a7oDeepanway Ghosal\\nNavonil Majumder\\nAmbuj Mehrish\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.13731\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 29 May 2023 12:09:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ohttps://github.com/declare-lab/tango\\u00a7r"}']}
{title:'Fan et al. (§72023§r)', author: 'Ruchao Fan; Yunzheng Zhu; Jinhan Wang; Abeer Alwan', display:{Lore:['[{"text": "arXiv:2305.00115", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Better Domain Adaptation for Self-supervised Models: A Case Study of Child ASR\\u00a7r\\n\\n\\u00a78\\u00a7oRuchao Fan\\nYunzheng Zhu\\nJinhan Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.00115\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JSTSP.2022.3200910\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Apr 2023 22:26:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in IEEEJournal of Selected Topics in Signal Processing, ICASSP Journal Poster Presentation\\u00a7r"}']}
{title:'Mehrish et al. (§72023§r)', author: 'Ambuj Mehrish; Navonil Majumder; Rishabh Bhardwaj; Rada Mihalcea; Soujanya Poria', display:{Lore:['[{"text": "arXiv:2305.00359", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Review of Deep Learning Techniques for Speech Processing\\u00a7r\\n\\n\\u00a78\\u00a7oAmbuj Mehrish\\nNavonil Majumder\\nRishabh Bhardwaj\\nRada Mihalcea\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.00359\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 30 May 2023 05:17:15 GMT)\\u00a7r"}']}
{title:'Zuluaga-Gomez et al. (§72023§r)', author: 'Juan Zuluaga-Gomez; Iuliia Nigmatulina; Amrutha Prasad; Petr Motlicek; Driss Khalil; Srikanth Madikeri; Allan Tart; Igor Szoke; Vincent Lenders; Mickael Rigault; Khalid Choukri', display:{Lore:['[{"text": "arXiv:2305.01155", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLessons Learned in ATCO2: 5000 hours of Air Traffic Control Communications for Robust Automatic Speech Recognition and Understanding\\u00a7r\\n\\n\\u00a78\\u00a7oJuan Zuluaga-Gomez\\nIuliia Nigmatulina\\nAmrutha Prasad\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.01155\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 May 2023 02:04:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oManuscript under review\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Hao Zhang; Meng Yu; Dong Yu', display:{Lore:['[{"text": "arXiv:2305.01637", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning for Joint Acoustic Echo and Acoustic Howling Suppression in Hybrid Meetings\\u00a7r\\n\\n\\u00a78\\u00a7oHao Zhang\\nMeng Yu\\nDong Yu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.01637\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 4 May 2023 06:40:00 GMT)\\u00a7r"}']}
{title:'Ludvigsen et al. (§72023§r)', author: 'Martin Ludvigsen; Markus Grasmair', display:{Lore:['[{"text": "arXiv:2305.01758", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NA\\u00a7r, \\u00a72math.NA\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial Generative NMF for Single Channel Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oMartin Ludvigsen\\nMarkus Grasmair\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.01758\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Apr 2023 09:26:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o24 pages, 4 figures\\u00a7r"}']}
{title:'Nourtel et al. (§72023§r)', author: 'Hubert Nourtel; Pierre Champion; Denis Jouvet; Anthony Larcher; Marie Tahon', display:{Lore:['[{"text": "arXiv:2305.01759", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluation of Speaker Anonymization on Emotional Speech\\u00a7r\\n\\n\\u00a78\\u00a7oHubert Nourtel\\nPierre Champion\\nDenis Jouvet\\nAnthony Larcher\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.01759\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/SPSC.2021-13\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. 2021 ISCA Symposium on Security and Privacy in Speech\\n  Communication (62-66)\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 15 Apr 2023 20:50:29 GMT)\\u00a7r"}']}
{title:'Yadav et al. (§72023§r)', author: 'Manuj Yadav; Densil Cabrera; Jungsoo Kim; Janina Fels; Richard de Dear', display:{Lore:['[{"text": "arXiv:2305.01762", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound in occupied open-plan offices: Objective metrics with a review of historical perspectives\\u00a7r\\n\\n\\u00a78\\u00a7oManuj Yadav\\nDensil Cabrera\\nJungsoo Kim\\nJanina Fels\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.01762\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.apacoust.2021.107943\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 Apr 2023 16:47:50 GMT)\\u00a7r"}']}
{title:'López-Espejo et al. (§72023§r)', author: 'Iván López-Espejo; Santi Prieto; Alfonso Ortega; Eduardo Lleida', display:{Lore:['[{"text": "arXiv:2305.02147", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Vocal Effort Transfer Vector Estimation for Vocal Effort-Robust Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oIv\\u00e1n L\\u00f3pez-Espejo\\nSanti Prieto\\nAlfonso Ortega\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.02147\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 4 Jul 2023 17:23:46 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Hao Zhang; Meng Yu; Yuzhong Wu; Tao Yu; Dong Yu', display:{Lore:['[{"text": "arXiv:2305.02583", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHybrid AHS: A Hybrid of Kalman Filter and Deep Learning for Acoustic Howling Suppression\\u00a7r\\n\\n\\u00a78\\u00a7oHao Zhang\\nMeng Yu\\nYuzhong Wu\\nTao Yu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.02583\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 May 2023 06:36:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to INTERSPEECH 2023. arXiv admin note: text overlap with arXiv:2302.09252\\u00a7r"}']}
{title:'Delabie et al. (§72023§r)', author: 'Daan Delabie; Chesney Buyle; Bert Cox; Liesbet Van der Perre; Lieven De Strycker', display:{Lore:['[{"text": "arXiv:2305.02715", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Acoustic Simulation Framework to Support Indoor Positioning and Data Driven Signal Processing Assessments\\u00a7r\\n\\n\\u00a78\\u00a7oDaan Delabie\\nChesney Buyle\\nBert Cox\\nLiesbet Van der Perre\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.02715\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 21 Jun 2023 14:28:57 GMT)\\u00a7r"}']}
{title:'Saeed et al. (§72023§r)', author: 'Aaqib Saeed; Vasileios Tsouvalas', display:{Lore:['[{"text": "arXiv:2305.03058", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPlug-and-Play Multilingual Few-shot Spoken Words Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAaqib Saeed\\nVasileios Tsouvalas\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.03058\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 May 2023 18:58:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCode: https://github.com/FewshotML/plix\\u00a7r"}']}
{title:'Baktash et al. (§72023§r)', author: 'Jawid Ahmad Baktash; Mursal Dawodi', display:{Lore:['[{"text": "arXiv:2305.03200", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmploying Hybrid Deep Neural Networks on Dari Speech\\u00a7r\\n\\n\\u00a78\\u00a7oJawid Ahmad Baktash\\nMursal Dawodi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.03200\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 May 2023 23:10:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ohttps://nlpai2023.org/papers?fbclid=IwAR2v29d3nFUaIx9U-rnfN8pqJu1tXBS9P9OV1IJnsbJ0QHN9JZAMPhZA7Ds\\u00a7r"}']}
{title:'Guan et al. (§72023§r)', author: 'Jian Guan; Youde Liu; Qiaoxi Zhu; Tieran Zheng; Jiqing Han; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2305.03328", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTime-weighted Frequency Domain Audio Representation with GMM Estimator for Anomalous Sound Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJian Guan\\nYoude Liu\\nQiaoxi Zhu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.03328\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 May 2023 07:17:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at ICASSP 2023\\u00a7r"}']}
{title:'Kitić et al. (§72023§r)', author: 'Srđan Kitić; Jérôme Daniel', display:{Lore:['[{"text": "arXiv:2305.03558", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBlind identification of Ambisonic reduced room impulse response\\u00a7r\\n\\n\\u00a78\\u00a7oSr\\u0111an Kiti\\u0107\\nJ\\u00e9r\\u00f4me Daniel\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.03558\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3332546\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 6 Nov 2023 22:03:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at the IEEE/ACM Transactionson Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Das et al. (§72023§r)', author: 'Nilaksh Das; Monica Sunkara; Sravan Bodapati; Jinglun Cai; Devang Kulshreshtha; Jeff Farris; Katrin Kirchhoff', display:{Lore:['[{"text": "arXiv:2305.03837", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMask The Bias: Improving Domain-Adaptive Generalization of CTC-based ASR with Internal Language Model Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oNilaksh Das\\nMonica Sunkara\\nSravan Bodapati\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.03837\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 May 2023 20:35:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Strimel et al. (§72023§r)', author: 'Grant P. Strimel; Yi Xie; Brian King; Martin Radfar; Ariya Rastrow; Athanasios Mouchtaris', display:{Lore:['[{"text": "arXiv:2305.04159", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLookahead When It Matters: Adaptive Non-causal Transformers for Streaming Neural Transducers\\u00a7r\\n\\n\\u00a78\\u00a7oGrant P. Strimel\\nYi Xie\\nBrian King\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.04159\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 May 2023 19:08:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICML2023\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Ruiqi Li; Rongjie Huang; Lichao Zhang; Jinglin Liu; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2305.04476", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment\\u00a7r\\n\\n\\u00a78\\u00a7oRuiqi Li\\nRongjie Huang\\nLichao Zhang\\nJinglin Liu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.04476\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 24 May 2023 16:37:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oFindings of ACL 2023\\u00a7r"}']}
{title:'Zhou et al. (§72023§r)', author: 'Xuehao Zhou; Mingyang Zhang; Yi Zhou; Zhizheng Wu; Haizhou Li', display:{Lore:['[{"text": "arXiv:2305.04816", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAccented Text-to-Speech Synthesis with Limited Data\\u00a7r\\n\\n\\u00a78\\u00a7oXuehao Zhou\\nMingyang Zhang\\nYi Zhou\\nZhizheng Wu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.04816\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 May 2023 16:15:39 GMT)\\u00a7r"}']}
{title:'Rekesh et al. (§72023§r)', author: 'Dima Rekesh; Nithin Rao Koluguri; Samuel Kriman; Somshubra Majumdar; Vahid Noroozi; He Huang; Oleksii Hrinchuk; Krishna Puvvada; Ankur Kumar; Jagadeesh Balam; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2305.05084", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast Conformer with Linearly Scalable Attention for Efficient Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDima Rekesh\\nNithin Rao Koluguri\\nSamuel Kriman\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.05084\\u00a7r\\n\\nVersion:\\u00a77v6 (Sat, 30 Sep 2023 20:59:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ASRU 2023\\u00a7r"}']}
{title:'Bäckström (§72023§r)', author: 'Tom Bäckström', display:{Lore:['[{"text": "arXiv:2305.05227", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrivacy in Speech Technology\\u00a7r\\n\\n\\u00a78\\u00a7oTom B\\u00e4ckstr\\u00f6m\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.05227\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 May 2023 07:41:36 GMT)\\u00a7r"}']}
{title:'Guimarães et al. (§72023§r)', author: 'Heitor Guimarães; Arthur Pimentel; Anderson Avila; Mehdi Rezagholizadeh; Tiago H. Falk', display:{Lore:['[{"text": "arXiv:2305.05443", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Exploration into the Performance of Unsupervised Cross-Task Speech Representations for \\"In the Wild\\u201d Edge Applications\\u00a7r\\n\\n\\u00a78\\u00a7oHeitor Guimar\\u00e3es\\nArthur Pimentel\\nAnderson Avila\\nMehdi Rezagholizadeh\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.05443\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 May 2023 13:37:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oExtended Abstract accepted in the Edge Intelligence Workshop (EIW) 2022\\u00a7r"}']}
{title:'Kovalyov et al. (§72023§r)', author: 'Anton Kovalyov; Kashyap Patel; Issa Panahi', display:{Lore:['[{"text": "arXiv:2305.05630", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAccurate Real-Time Estimation of 2-Dimensional Direction of Arrival using a 3-Microphone Array\\u00a7r\\n\\n\\u00a78\\u00a7oAnton Kovalyov\\nKashyap Patel\\nIssa Panahi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.05630\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 May 2023 17:20:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 6 figures\\u00a7r"}']}
{title:'Hirano et al. (§72023§r)', author: 'Masato Hirano; Kazuki Shimada; Yuichiro Koyama; Shusuke Takahashi; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2305.05857", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffusion-based Signal Refiner for Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oMasato Hirano\\nKazuki Shimada\\nYuichiro Koyama\\nShusuke Takahashi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.05857\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 12 May 2023 05:02:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review\\u00a7r"}']}
{title:'Ren et al. (§72023§r)', author: 'Yuwei Ren; Matt Zivney; Yin Huang; Eddie Choy; Chirag Patel; Hao Xu', display:{Lore:['[{"text": "arXiv:2305.06640", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.IT\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a72math.IT\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Diaphragm Excursion Prediction: deep attention and online adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oYuwei Ren\\nMatt Zivney\\nYin Huang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.06640\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 May 2023 08:17:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, ICASSP 2023\\u00a7r"}']}
{title:'Harere et al. (§72023§r)', author: 'Ahmad Al Harere; Khloud Al Jallad', display:{Lore:['[{"text": "arXiv:2305.07034", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQuran Recitation Recognition using End-to-End Deep Learning\\u00a7r\\n\\n\\u00a78\\u00a7oAhmad Al Harere\\nKhloud Al Jallad\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.07034\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 May 2023 18:40:01 GMT)\\u00a7r"}']}
{title:'Kheir et al. (§72023§r)', author: 'Yassine El Kheir; Fouad Khnaisser; Shammur Absar Chowdhury; Hamdy Mubarak; Shazia Afzal; Ahmed Ali', display:{Lore:['[{"text": "arXiv:2305.07445", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQVoice: Arabic Speech Pronunciation Learning Application\\u00a7r\\n\\n\\u00a78\\u00a7oYassine El Kheir\\nFouad Khnaisser\\nShammur Absar Chowdhury\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.07445\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInterSpeech 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 May 2023 07:21:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2 pages, Accepted InterSpeech23 Show Tell Demo Session\\u00a7r"}']}
{title:'Sawata et al. (§72023§r)', author: 'Ryosuke Sawata; Naoya Takahashi; Stefan Uhlich; Shusuke Takahashi; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2305.07855", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Whole Is Greater than the Sum of Its Parts: Improving DNN-based Music Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oRyosuke Sawata\\nNaoya Takahashi\\nStefan Uhlich\\nShusuke Takahashi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.07855\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 13 May 2023 07:28:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEETASLP (under review), 11 pages, 8 figures\\u00a7r"}']}
{title:'Sandler et al. (§72023§r)', author: 'Morgan Sandler; Arun Ross', display:{Lore:['[{"text": "arXiv:2305.07997", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVocal Style Factorization for Effective Speaker Recognition in Affective Scenarios\\u00a7r\\n\\n\\u00a78\\u00a7oMorgan Sandler\\nArun Ross\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.07997\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 3 Aug 2023 17:28:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of the IEEE 2023 International Joint Conference on Biometrics (IJCB)\\u00a7r"}']}
{title:'Schröter et al. (§72023§r)', author: 'Hendrik Schröter; Tobias Rosenkranz; Alberto N. Escalante-B.; Andreas Maier', display:{Lore:['[{"text": "arXiv:2305.08225", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Multi-Frame Filtering for Hearing Aids\\u00a7r\\n\\n\\u00a78\\u00a7oHendrik Schr\\u00f6ter\\nTobias Rosenkranz\\nAlberto N. Escalante-B.\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.08225\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 14 May 2023 18:59:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2023\\u00a7r"}']}
{title:'Schröter et al. (§72023§r)', author: 'Hendrik Schröter; Tobias Rosenkranz; Alberto N. Escalante-B.; Andreas Maier', display:{Lore:['[{"text": "arXiv:2305.08227", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeepFilterNet: Perceptually Motivated Real-Time Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oHendrik Schr\\u00f6ter\\nTobias Rosenkranz\\nAlberto N. Escalante-B.\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.08227\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 14 May 2023 19:09:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted as show and tell demo to interspeech 2023\\u00a7r"}']}
{title:'Fang et al. (§72023§r)', author: 'Huajian Fang; Dennis Becker; Stefan Wermter; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2305.08744", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntegrating Uncertainty into Neural Network-based Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oHuajian Fang\\nDennis Becker\\nStefan Wermter\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.08744\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3265202\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol. 31, pp. 1587-1600, 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 May 2023 15:55:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted version\\u00a7r"}']}
{title:'Sun et al. (§72023§r)', author: 'Xiaoheng Sun; Yuejie Gao; Hanyao Lin; Huaping Liu', display:{Lore:['[{"text": "arXiv:2305.09127", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTG-Critic: A Timbre-Guided Model for Reference-Independent Singing Evaluation\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoheng Sun\\nYuejie Gao\\nHanyao Lin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.09127\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10096309\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 May 2023 03:15:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe annotations for datasets used in this paper and further experimental results are available at https://github.com/YuejieGao/TG-CRITIC\\u00a7r"}']}
{title:'Hu et al. (§72023§r)', author: 'Yuchen Hu; Ruizhe Li; Chen Chen; Heqing Zou; Qiushi Zhu; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2305.09212", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-Modal Global Interaction and Local Alignment for Audio-Visual Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYuchen Hu\\nRuizhe Li\\nChen Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.09212\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 May 2023 06:41:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 5 figures, Accepted by IJCAI 2023\\u00a7r"}']}
{title:'Plantinga et al. (§72023§r)', author: 'Peter Plantinga; Jaekwon Yoo; Chandra Dhir', display:{Lore:['[{"text": "arXiv:2305.09681", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContinual Learning for End-to-End ASR by Averaging Domain Experts\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Plantinga\\nJaekwon Yoo\\nChandra Dhir\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.09681\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 May 2023 16:19:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2023\\u00a7r"}']}
{title:'Rakib et al. (§72023§r)', author: 'Fazle Rabbi Rakib; Souhardya Saha Dip; Samiul Alam; Nazia Tasnim; Md. Istiak Hossain Shihab; Md. Nazmuddoha Ansary; Syed Mobassir Hossen; Marsia Haque Meghla; Mamunur Mamun; Farig Sadeque; Sayma Sultana Chowdhury; Tahsin Reasat; Asif Sushmit; Ahmed Imtiaz Humayun', display:{Lore:['[{"text": "arXiv:2305.09688", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOOD-Speech: A Large Bengali Speech Recognition Dataset for Out-of-Distribution Benchmarking\\u00a7r\\n\\n\\u00a78\\u00a7oFazle Rabbi Rakib\\nSouhardya Saha Dip\\nSamiul Alam\\n+ 10 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.09688\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 May 2023 18:00:39 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Jie Zhang; Qing-Tian Xu; Qiu-Shi Zhu; Zhen-Hua Ling', display:{Lore:['[{"text": "arXiv:2305.09994", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBASEN: Time-Domain Brain-Assisted Speech Enhancement Network with Convolutional Cross Attention in Multi-talker Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oJie Zhang\\nQing-Tian Xu\\nQiu-Shi Zhu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.09994\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 May 2023 06:40:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ISCAInterspeech 2023\\u00a7r"}']}
{title:'Peng et al. (§72023§r)', author: 'Junyi Peng; Oldřich Plchot; Themos Stafylakis; Ladislav Mošner; Lukáš Burget; Jan Černocký', display:{Lore:['[{"text": "arXiv:2305.10517", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Speaker Verification with Self-Pretrained Transformer Models\\u00a7r\\n\\n\\u00a78\\u00a7oJunyi Peng\\nOld\\u0159ich Plchot\\nThemos Stafylakis\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10517\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 May 2023 18:52:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023\\u00a7r"}']}
{title:'Cooper et al. (§72023§r)', author: 'Erica Cooper; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2305.10608", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating Range-Equalizing Bias in Mean Opinion Score Ratings of Synthesized Speech\\u00a7r\\n\\n\\u00a78\\u00a7oErica Cooper\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10608\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 7 Oct 2023 03:58:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of Interspeech 2023. DOI: 10.21437/Interspeech.2023-1076\\u00a7r"}']}
{title:'Geng et al. (§72023§r)', author: 'Mengzhe Geng; Zengrui Jin; Tianzi Wang; Shujie Hu; Jiajun Deng; Mingyu Cui; Guinan Li; Jianwei Yu; Xurong Xie; Xunying Liu', display:{Lore:['[{"text": "arXiv:2305.10659", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUse of Speech Impairment Severity for Dysarthric Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMengzhe Geng\\nZengrui Jin\\nTianzi Wang\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10659\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 May 2023 02:42:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH2023\\u00a7r"}']}
{title:'Tanna et al. (§72023§r)', author: 'Avani Tanna; Michael Saxon; Amr El Abbadi; William Yang Wang', display:{Lore:['[{"text": "arXiv:2305.10684", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData Augmentation for Diverse Voice Conversion in Noisy Environments\\u00a7r\\n\\n\\u00a78\\u00a7oAvani Tanna\\nMichael Saxon\\nAmr El Abbadi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10684\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 May 2023 03:54:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023 Show and Tell, 2 pp\\u00a7r"}']}
{title:'Khandelwal et al. (§72023§r)', author: 'Tanmay Khandelwal; Rohan Kumar Das', display:{Lore:['[{"text": "arXiv:2305.10729", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Multi-Task Learning Framework for Sound Event Detection using High-level Acoustic Characteristics of Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oTanmay Khandelwal\\nRohan Kumar Das\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10729\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 May 2023 05:57:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for Publication at INTERSPEECH 2023\\u00a7r"}']}
{title:'Attia et al. (§72023§r)', author: 'Ahmed Adel Attia; Mark Tiede; Carol Y. Espy-Wilson', display:{Lore:['[{"text": "arXiv:2305.10775", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Speech Articulation Analysis using a Geometric Transformation of the X-ray Microbeam Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oAhmed Adel Attia\\nMark Tiede\\nCarol Y. Espy-Wilson\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10775\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-2211\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 28 Sep 2023 16:44:10 GMT)\\u00a7r"}']}
{title:'Wilkinghoff et al. (§72023§r)', author: 'Kevin Wilkinghoff; Alessia Cornaggia-Urrigshardt', display:{Lore:['[{"text": "arXiv:2305.10816", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTACos: Learning Temporally Structured Embeddings for Few-Shot Keyword Spotting with Dynamic Time Warping\\u00a7r\\n\\n\\u00a78\\u00a7oKevin Wilkinghoff\\nAlessia Cornaggia-Urrigshardt\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10816\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 14 Dec 2023 15:44:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for presentation at IEEE ICASSP 2024\\u00a7r"}']}
{title:'Fu et al. (§72023§r)', author: 'Yanjie Fu; Meng Ge; Honglong Wang; Nan Li; Haoran Yin; Longbiao Wang; Gaoyan Zhang; Jianwu Dang; Chengyun Deng; Fei Wang', display:{Lore:['[{"text": "arXiv:2305.10821", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLocate and Beamform: Two-dimensional Locating All-neural Beamformer for Multi-channel Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oYanjie Fu\\nMeng Ge\\nHonglong Wang\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10821\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 2 Jun 2023 11:46:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2023. arXiv admin note: substantial text overlap with arXiv:2212.03401\\u00a7r"}']}
{title:'Jang et al. (§72023§r)', author: 'Won Jang; Dan Lim; Heayoung Park', display:{Lore:['[{"text": "arXiv:2305.10823", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFastFit: Towards Real-Time Iterative Neural Vocoder by Replacing U-Net Encoder With Multiple STFTs\\u00a7r\\n\\n\\u00a78\\u00a7oWon Jang\\nDan Lim\\nHeayoung Park\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10823\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 May 2023 09:05:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2023\\u00a7r"}']}
{title:'Tian et al. (§72023§r)', author: 'Yusheng Tian; Wei Liu; Tan Lee', display:{Lore:['[{"text": "arXiv:2305.10891", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffusion-Based Mel-Spectrogram Enhancement for Personalized Speech Synthesis with Found Data\\u00a7r\\n\\n\\u00a78\\u00a7oYusheng Tian\\nWei Liu\\nTan Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10891\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 30 Sep 2023 00:55:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ASRU 2023\\u00a7r"}']}
{title:'Zeng et al. (§72023§r)', author: 'Chang Zeng; Xin Wang; Xiaoxiao Miao; Erica Cooper; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2305.10940", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Generalization Ability of Countermeasures for New Mismatch Scenario by Combining Multiple Advanced Regularization Terms\\u00a7r\\n\\n\\u00a78\\u00a7oChang Zeng\\nXin Wang\\nXiaoxiao Miao\\nErica Cooper\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10940\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 May 2023 12:58:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by interspeech2023\\u00a7r"}']}
{title:'Peng et al. (§72023§r)', author: 'Puyuan Peng; Brian Yan; Shinji Watanabe; David Harwath', display:{Lore:['[{"text": "arXiv:2305.11095", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization\\u00a7r\\n\\n\\u00a78\\u00a7oPuyuan Peng\\nBrian Yan\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11095\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 16 Aug 2023 00:57:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023\\u00a7r"}']}
{title:'Shuai et al. (§72023§r)', author: 'Chenhao Shuai; Chaohua Shi; Lu Gan; Hongqing Liu', display:{Lore:['[{"text": "arXiv:2305.11104", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lmdctGAN: Taming transformer-based GAN for speech super-resolution with Modified DCT spectra\\u00a7r\\n\\n\\u00a78\\u00a7oChenhao Shuai\\nChaohua Shi\\nLu Gan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11104\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 19 May 2023 07:26:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, INTERSPEECH 2023\\u00a7r"}']}
{title:'Arasteh et al. (§72023§r)', author: 'Soroosh Tayebi Arasteh; Cristian David Rios-Urrego; Elmar Noeth; Andreas Maier; Seung Hee Yang; Jan Rusz; Juan Rafael Orozco-Arroyave', display:{Lore:['[{"text": "arXiv:2305.11284", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFederated learning for secure development of AI models for Parkinson\'s disease detection using speech from different languages\\u00a7r\\n\\n\\u00a78\\u00a7oSoroosh Tayebi Arasteh\\nCristian David Rios-Urrego\\nElmar Noeth\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11284\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-2108\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nINTERSPEECH 2023\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 21 Aug 2023 09:35:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023, pp. 5003\\u20135007, Dublin, Ireland\\u00a7r"}']}
{title:'Cao et al. (§72023§r)', author: 'Faxian Cao; Yongqiang Cheng; Adil Mehmood Khan; Zhijing Yang', display:{Lore:['[{"text": "arXiv:2305.11397", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAre Microphone Signals Alone Sufficient for Self-Positioning?\\u00a7r\\n\\n\\u00a78\\u00a7oFaxian Cao\\nYongqiang Cheng\\nAdil Mehmood Khan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11397\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 6 Jul 2023 20:57:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o1 figure, including 3 sub-figures\\u00a7r"}']}
{title:'Peng et al. (§72023§r)', author: 'Puyuan Peng; Shang-Wen Li; Okko Räsänen; Abdelrahman Mohamed; David Harwath', display:{Lore:['[{"text": "arXiv:2305.11435", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSyllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Model\\u00a7r\\n\\n\\u00a78\\u00a7oPuyuan Peng\\nShang-Wen Li\\nOkko R\\u00e4s\\u00e4nen\\nAbdelrahman Mohamed\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11435\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 23 Jul 2023 05:32:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023. Code Model: https://github.com/jasonppy/syllable-discovery\\u00a7r"}']}
{title:'Yao et al. (§72023§r)', author: 'Zengwei Yao; Wei Kang; Fangjun Kuang; Liyong Guo; Xiaoyu Yang; Yifan Yang; Long Lin; Daniel Povey', display:{Lore:['[{"text": "arXiv:2305.11539", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDelay-penalized CTC implemented based on Finite State Transducer\\u00a7r\\n\\n\\u00a78\\u00a7oZengwei Yao\\nWei Kang\\nFangjun Kuang\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11539\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 May 2023 09:16:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in INTERSPEECH 2023\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Yifan Yang; Xiaoyu Yang; Liyong Guo; Zengwei Yao; Wei Kang; Fangjun Kuang; Long Lin; Xie Chen; Daniel Povey', display:{Lore:['[{"text": "arXiv:2305.11558", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBlank-regularized CTC for Frame Skipping in Neural Transducer\\u00a7r\\n\\n\\u00a78\\u00a7oYifan Yang\\nXiaoyu Yang\\nLiyong Guo\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11558\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 May 2023 09:56:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in INTERSPEECH 2023\\u00a7r"}']}
{title:'Feng et al. (§72023§r)', author: 'Siyuan Feng; Ming Tu; Rui Xia; Chuanzeng Huang; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2305.11569", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLanguage-Universal Phonetic Representation in Multilingual Speech Pretraining for Low-Resource Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSiyuan Feng\\nMing Tu\\nRui Xia\\nChuanzeng Huang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11569\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 May 2023 10:15:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in INTERSPEECH 2023\\u00a7r"}']}
{title:'An et al. (§72023§r)', author: 'Keyu An; Xian Shi; Shiliang Zhang', display:{Lore:['[{"text": "arXiv:2305.11571", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBAT: Boundary aware transducer for memory-efficient and low-latency ASR\\u00a7r\\n\\n\\u00a78\\u00a7oKeyu An\\nXian Shi\\nShiliang Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11571\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 May 2023 10:17:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted into INTERSPEECH2023\\u00a7r"}']}
{title:'Feng et al. (§72023§r)', author: 'Siyuan Feng; Ming Tu; Rui Xia; Chuanzeng Huang; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2305.11576", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLanguage-universal phonetic encoder for low-resource speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSiyuan Feng\\nMing Tu\\nRui Xia\\nChuanzeng Huang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11576\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 May 2023 10:24:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in INTERSPEECH 2023\\u00a7r"}']}
{title:'Jang et al. (§72023§r)', author: 'Kangwook Jang; Sungnyun Kim; Se-Young Yun; Hoirin Kim', display:{Lore:['[{"text": "arXiv:2305.11685", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRecycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation\\u00a7r\\n\\n\\u00a78\\u00a7oKangwook Jang\\nSungnyun Kim\\nSe-Young Yun\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11685\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1329\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 26 Oct 2023 10:43:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of Interspeech 2023. Code URL: https://github.com/sungnyun/ARMHuBERT\\u00a7r"}']}
{title:'Kakouros et al. (§72023§r)', author: 'Sofoklis Kakouros; Katri Hiovain-Asikainen', display:{Lore:['[{"text": "arXiv:2305.11864", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNorth S\\u00e1mi Dialect Identification with Self-supervised Speech Models\\u00a7r\\n\\n\\u00a78\\u00a7oSofoklis Kakouros\\nKatri Hiovain-Asikainen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11864\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 May 2023 17:53:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2023\\u00a7r"}']}
{title:'Zeng et al. (§72023§r)', author: 'Xiao-Min Zeng; Yan Song; Zhu Zhuo; Yu Zhou; Yu-Hong Li; Hui Xue; Li-Rong Dai; Ian McLoughlin', display:{Lore:['[{"text": "arXiv:2305.12111", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Generative-Contrastive Representation Learning for Anomalous Sound Detection\\u00a7r\\n\\n\\u00a78\\u00a7oXiao-Min Zeng\\nYan Song\\nZhu Zhuo\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12111\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 20 May 2023 06:10:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2023\\u00a7r"}']}
{title:'Lv et al. (§72023§r)', author: 'Shubo Lv; Xiong Wang; Sining Sun; Long Ma; Lei Xie', display:{Lore:['[{"text": "arXiv:2305.12331", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDCCRN-KWS: an audio bias based model for noise robust small-footprint keyword spotting\\u00a7r\\n\\n\\u00a78\\u00a7oShubo Lv\\nXiong Wang\\nSining Sun\\nLong Ma\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12331\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 13 Jun 2023 01:07:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH2023\\u00a7r"}']}
{title:'Ning et al. (§72023§r)', author: 'Ziqian Ning; Yuepeng Jiang; Pengcheng Zhu; Jixun Yao; Shuai Wang; Lei Xie; Mengxiao Bi', display:{Lore:['[{"text": "arXiv:2305.12425", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDualVC: Dual-mode Voice Conversion using Intra-model Knowledge Distillation and Hybrid Predictive Coding\\u00a7r\\n\\n\\u00a78\\u00a7oZiqian Ning\\nYuepeng Jiang\\nPengcheng Zhu\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12425\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 31 May 2023 01:17:27 GMT)\\u00a7r"}']}
{title:'Shi et al. (§72023§r)', author: 'Mohan Shi; Yuchun Shu; Lingyun Zuo; Qian Chen; Shiliang Zhang; Jie Zhang; Li-Rong Dai', display:{Lore:['[{"text": "arXiv:2305.12450", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemantic VAD: Low-Latency Voice Activity Detection for Speech Interaction\\u00a7r\\n\\n\\u00a78\\u00a7oMohan Shi\\nYuchun Shu\\nLingyun Zuo\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12450\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 21 May 2023 13:02:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech2023\\u00a7r"}']}
{title:'Shi et al. (§72023§r)', author: 'Mohan Shi; Zhihao Du; Qian Chen; Fan Yu; Yangze Li; Shiliang Zhang; Jie Zhang; Li-Rong Dai', display:{Lore:['[{"text": "arXiv:2305.12459", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCASA-ASR: Context-Aware Speaker-Attributed ASR\\u00a7r\\n\\n\\u00a78\\u00a7oMohan Shi\\nZhihao Du\\nQian Chen\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12459\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 21 May 2023 13:32:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech2023\\u00a7r"}']}
{title:'Huang et al. (§72023§r)', author: 'Kaixun Huang; Ao Zhang; Zhanheng Yang; Pengcheng Guo; Bingshen Mu; Tianyi Xu; Lei Xie', display:{Lore:['[{"text": "arXiv:2305.12493", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network\\u00a7r\\n\\n\\u00a78\\u00a7oKaixun Huang\\nAo Zhang\\nZhanheng Yang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12493\\u00a7r\\n\\nVersion:\\u00a77v5 (Wed, 12 Jul 2023 17:41:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by interspeech2023\\u00a7r"}']}
{title:'Fathullah et al. (§72023§r)', author: 'Yassir Fathullah; Chunyang Wu; Yuan Shangguan; Junteng Jia; Wenhan Xiong; Jay Mahadeokar; Chunxi Liu; Yangyang Shi; Ozlem Kalinli; Mike Seltzer; Mark J. F. Gales', display:{Lore:['[{"text": "arXiv:2305.12498", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Head State Space Model for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYassir Fathullah\\nChunyang Wu\\nYuan Shangguan\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12498\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 May 2023 21:55:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Jialu Li; Mark Hasegawa-Johnson; Nancy L. McElwain', display:{Lore:['[{"text": "arXiv:2305.12530", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Robust Family-Infant Audio Analysis Based on Unsupervised Pretraining of Wav2vec 2.0 on Large-Scale Unlabeled Family Audio\\u00a7r\\n\\n\\u00a78\\u00a7oJialu Li\\nMark Hasegawa-Johnson\\nNancy L. McElwain\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12530\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-460\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 8 Dec 2023 21:22:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of Interspeech 2023; v4 version updates: correction of W2V2-base pretrained on 960-hour of LibriSpeech and number of families participated for LENA home recordings\\u00a7r"}']}
{title:'Bansal et al. (§72023§r)', author: 'Lokesh Bansal; S. Pavankumar Dubagunta; Malolan Chetlur; Pushpak Jagtap; Aravind Ganapathiraju', display:{Lore:['[{"text": "arXiv:2305.12540", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Efficacy and Noise-Robustness of Jointly Learned Speech Emotion and Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oLokesh Bansal\\nS. Pavankumar Dubagunta\\nMalolan Chetlur\\nPushpak Jagtap\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12540\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 May 2023 19:33:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to be part of INTERSPEECH 2023\\u00a7r"}']}
{title:'Bhattacharya et al. (§72023§r)', author: 'Debarpan Bhattacharya; Neeraj Kumar Sharma; Debottam Dutta; Srikanth Raj Chetupalli; Pravin Mote; Sriram Ganapathy; Chandrakiran C; Sahiti Nori; Suhail K K; Sadhana Gonuguntla; Murali Alagesan', display:{Lore:['[{"text": "arXiv:2305.12741", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCoswara: A respiratory sounds and symptoms dataset for remote screening of SARS-CoV-2 infection\\u00a7r\\n\\n\\u00a78\\u00a7oDebarpan Bhattacharya\\nNeeraj Kumar Sharma\\nDebottam Dutta\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12741\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 May 2023 06:09:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publiation in Nature Scientific Data\\u00a7r"}']}
{title:'Jiang et al. (§72023§r)', author: 'Yidi Jiang; Ruijie Tao; Zexu Pan; Haizhou Li', display:{Lore:['[{"text": "arXiv:2305.12831", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTarget Active Speaker Detection with Audio-visual Cues\\u00a7r\\n\\n\\u00a78\\u00a7oYidi Jiang\\nRuijie Tao\\nZexu Pan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12831\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 12 Jun 2023 06:41:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Yafeng Chen; Siqi Zheng; Hui Wang; Luyao Cheng; Qian Chen; Jiajun Qi', display:{Lore:['[{"text": "arXiv:2305.12838", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Enhanced Res2Net with Local and Global Feature Fusion for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oYafeng Chen\\nSiqi Zheng\\nHui Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12838\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 3 Aug 2023 04:00:16 GMT)\\u00a7r"}']}
{title:'Fares et al. (§72023§r)', author: 'Mireille Fares; Catherine Pelachaud; Nicolas Obin', display:{Lore:['[{"text": "arXiv:2305.12887", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZS-MSTM: Zero-Shot Style Transfer for Gesture Animation driven by Text and Speech using Adversarial Disentanglement of Multimodal Style Encoding\\u00a7r\\n\\n\\u00a78\\u00a7oMireille Fares\\nCatherine Pelachaud\\nNicolas Obin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12887\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 May 2023 10:10:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2208.01917\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Eungbeom Kim; Yunkee Chae; Jaeheon Sim; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2305.13108", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDebiased Automatic Speech Recognition for Dysarthric Speech via Sample Reweighting with Sample Affinity Test\\u00a7r\\n\\n\\u00a78\\u00a7oEungbeom Kim\\nYunkee Chae\\nJaeheon Sim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13108\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 27 Jun 2023 13:19:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2023\\u00a7r"}']}
{title:'Tang et al. (§72023§r)', author: 'Jiyang Tang; William Chen; Xuankai Chang; Shinji Watanabe; Brian MacWhinney', display:{Lore:['[{"text": "arXiv:2305.13331", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA New Benchmark of Aphasia Speech Recognition and Detection Based on E-Branchformer and Multi-task Learning\\u00a7r\\n\\n\\u00a78\\u00a7oJiyang Tang\\nWilliam Chen\\nXuankai Chang\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13331\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 May 2023 15:10:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2023. Code: https://github.com/espnet/espnet\\u00a7r"}']}
{title:'Meneses et al. (§72023§r)', author: 'Michel Meneses; Bruno Iwami', display:{Lore:['[{"text": "arXiv:2305.13332", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConditional Online Learning for Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oMichel Meneses\\nBruno Iwami\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13332\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 May 2023 15:46:31 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Qiujia Li; Bo Li; Dongseong Hwang; Tara N. Sainath; Pedro M. Mengibar', display:{Lore:['[{"text": "arXiv:2305.13408", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModular Domain Adaptation for Conformer-Based Streaming ASR\\u00a7r\\n\\n\\u00a78\\u00a7oQiujia Li\\nBo Li\\nDongseong Hwang\\nTara N. Sainath\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13408\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 May 2023 18:49:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023\\u00a7r"}']}
{title:'Delcroix et al. (§72023§r)', author: 'Marc Delcroix; Naohiro Tawara; Mireia Diez; Federico Landini; Anna Silnova; Atsunori Ogawa; Tomohiro Nakatani; Lukas Burget; Shoko Araki', display:{Lore:['[{"text": "arXiv:2305.13580", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Stream Extension of Variational Bayesian HMM Clustering (MS-VBx) for Combined End-to-End and Vector Clustering-based Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oMarc Delcroix\\nNaohiro Tawara\\nMireia Diez\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13580\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 May 2023 01:19:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2023\\u00a7r"}']}
{title:'Xue et al. (§72023§r)', author: 'Hongfei Xue; Qijie Shao; Peikun Chen; Pengcheng Guo; Lei Xie; Jie Liu', display:{Lore:['[{"text": "arXiv:2305.13629", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTranUSR: Phoneme-to-word Transcoder Based Unified Speech Representation Learning for Cross-lingual Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHongfei Xue\\nQijie Shao\\nPeikun Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13629\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-746\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 8 Oct 2023 14:18:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures. Accepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Lu et al. (§72023§r)', author: 'Ye-Xin Lu; Yang Ai; Zhen-Hua Ling', display:{Lore:['[{"text": "arXiv:2305.13686", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMP-SENet: A Speech Enhancement Model with Parallel Denoising of Magnitude and Phase Spectra\\u00a7r\\n\\n\\u00a78\\u00a7oYe-Xin Lu\\nYang Ai\\nZhen-Hua Ling\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13686\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1441\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 May 2023 04:48:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2023\\u00a7r"}']}
{title:'Cappellazzo et al. (§72023§r)', author: 'Umberto Cappellazzo; Muqiao Yang; Daniele Falavigna; Alessio Brutti', display:{Lore:['[{"text": "arXiv:2305.13899", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken Language Understanding\\u00a7r\\n\\n\\u00a78\\u00a7oUmberto Cappellazzo\\nMuqiao Yang\\nDaniele Falavigna\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13899\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 31 Jul 2023 19:02:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2023. Code (will be) available at https://github.com/umbertocappellazzo/SLURP-SeqKD\\u00a7r"}']}
{title:'Atienza (§72023§r)', author: 'Rowel Atienza', display:{Lore:['[{"text": "arXiv:2305.13905", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficientSpeech: An On-Device Text to Speech Model\\u00a7r\\n\\n\\u00a78\\u00a7oRowel Atienza\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13905\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 May 2023 10:28:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be presented at ICASSP 2023\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Qiushi Zhu; Xiaoying Zhao; Jie Zhang; Yu Gu; Chao Weng; Yuchen Hu', display:{Lore:['[{"text": "arXiv:2305.13957", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEeg2vec: Self-Supervised Electroencephalographic Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oQiushi Zhu\\nXiaoying Zhao\\nJie Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13957\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 May 2023 11:34:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Bae et al. (§72023§r)', author: 'Sangmin Bae; June-Woo Kim; Won-Yang Cho; Hyerim Baek; Soyoun Son; Byungjo Lee; Changwan Ha; Kyongpil Tae; Sungnyun Kim; Se-Young Yun', display:{Lore:['[{"text": "arXiv:2305.14032", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPatch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification\\u00a7r\\n\\n\\u00a78\\u00a7oSangmin Bae\\nJune-Woo Kim\\nWon-Yang Cho\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.14032\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1426\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 22 Nov 2023 07:01:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023, Code URL: https://github.com/raymin0223/patch-mix_contrastive_learning\\u00a7r"}']}
{title:'Niizumi et al. (§72023§r)', author: 'Daisuke Niizumi; Daiki Takeuchi; Yasunori Ohishi; Noboru Harada; Kunio Kashino', display:{Lore:['[{"text": "arXiv:2305.14079", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMasked Modeling Duo for Speech: Specializing General-Purpose Audio Representation to Speech using Denoising Distillation\\u00a7r\\n\\n\\u00a78\\u00a7oDaisuke Niizumi\\nDaiki Takeuchi\\nYasunori Ohishi\\nNoboru Harada\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.14079\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 3 Aug 2023 06:18:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023; 5+2 pages, 2 figures, 6+6 tables, Code: https://github.com/nttcslab/m2d/tree/master/speech\\u00a7r"}']}
{title:'Xu et al. (§72023§r)', author: 'Anfeng Xu; Rajat Hebbar; Rimita Lahiri; Tiantian Feng; Lindsay Butler; Lue Shen; Helen Tager-Flusberg; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2305.14117", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnderstanding Spoken Language Development of Children with ASD Using Pre-trained Speech Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oAnfeng Xu\\nRajat Hebbar\\nRimita Lahiri\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.14117\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 31 May 2023 22:32:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023, 5 pages\\u00a7r"}']}
{title:'Kashiwagi et al. (§72023§r)', author: 'Sara Kashiwagi; Keitaro Tanaka; Qi Feng; Shigeo Morishima', display:{Lore:['[{"text": "arXiv:2305.14203", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving the Gap in Visual Speech Recognition Between Normal and Silent Speech Based on Metric Learning\\u00a7r\\n\\n\\u00a78\\u00a7oSara Kashiwagi\\nKeitaro Tanaka\\nQi Feng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.14203\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-370\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 16 Oct 2023 05:06:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Chemudupati et al. (§72023§r)', author: 'Vamsikrishna Chemudupati; Marzieh Tahaei; Heitor Guimaraes; Arthur Pimentel; Anderson Avila; Mehdi Rezagholizadeh; Boxing Chen; Tiago Falk', display:{Lore:['[{"text": "arXiv:2305.14546", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Transferability of Whisper-based Representations for \\"In-the-Wild\\" Cross-Task Downstream Speech Applications\\u00a7r\\n\\n\\u00a78\\u00a7oVamsikrishna Chemudupati\\nMarzieh Tahaei\\nHeitor Guimaraes\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.14546\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 May 2023 22:02:55 GMT)\\u00a7r"}']}
{title:'Sato et al. (§72023§r)', author: 'Hiroshi Sato; Ryo Masumura; Tsubasa Ochiai; Marc Delcroix; Takafumi Moriya; Takanori Ashihara; Kentaro Shinayama; Saki Mizuno; Mana Ihori; Tomohiro Tanaka; Nobukatsu Hojo', display:{Lore:['[{"text": "arXiv:2305.14723", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDownstream Task Agnostic Speech Enhancement with Self-Supervised Representation Loss\\u00a7r\\n\\n\\u00a78\\u00a7oHiroshi Sato\\nRyo Masumura\\nTsubasa Ochiai\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.14723\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 May 2023 05:00:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages , 2 figures, Accepted to Interspeech 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Xiyuan Wang; Fangyuan Wang; Bo Xu; Liang Xu; Jing Xiao', display:{Lore:['[{"text": "arXiv:2305.14778", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lP-vectors: A Parallel-Coupled TDNN/Transformer Network for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oXiyuan Wang\\nFangyuan Wang\\nBo Xu\\nLiang Xu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.14778\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 May 2023 07:40:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Zheng et al. (§72023§r)', author: 'Rui-Chen Zheng; Yang Ai; Zhen-Hua Ling', display:{Lore:['[{"text": "arXiv:2305.14933", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIncorporating Ultrasound Tongue Images for Audio-Visual Speech Enhancement through Knowledge Distillation\\u00a7r\\n\\n\\u00a78\\u00a7oRui-Chen Zheng\\nYang Ai\\nZhen-Hua Ling\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.14933\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-780\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. INTERSPEECH 2023, 844-848 (2023)\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 20 Nov 2023 06:17:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in InterSpeech 2023\\u00a7r"}']}
{title:'Clarke (§72023§r)', author: 'Christopher Johann Clarke', display:{Lore:['[{"text": "arXiv:2305.14948", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Representing Corpus Virtual: An Open Sourced Library for Explorative Music Generation, Sound Design, and Instrument Creation with Artificial Intelligence and Machine Learning\\u00a7r\\n\\n\\u00a78\\u00a7oChristopher Johann Clarke\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.14948\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 May 2023 09:36:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages\\u00a7r"}']}
{title:'Moliner et al. (§72023§r)', author: 'Eloi Moliner; Vesa Välimäki', display:{Lore:['[{"text": "arXiv:2305.15266", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffusion-Based Audio Inpainting\\u00a7r\\n\\n\\u00a78\\u00a7oEloi Moliner\\nVesa V\\u00e4lim\\u00e4ki\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.15266\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.17743/jaes.2022.0129\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJournal of the Audio Engineering Society 72, no. 3 (2024): 100-113\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Sep 2023 20:16:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted for publication to the Journal of Audio Engineering Society on January 30th, 2023\\u00a7r"}']}
{title:'Ito et al. (§72023§r)', author: 'Aoi Ito; Shota Horiguchi', display:{Lore:['[{"text": "arXiv:2305.15518", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpoofing Attacker Also Benefits from Self-Supervised Pretrained Model\\u00a7r\\n\\n\\u00a78\\u00a7oAoi Ito\\nShota Horiguchi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.15518\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 May 2023 19:15:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2023\\u00a7r"}']}
{title:'Qiu et al. (§72023§r)', author: 'David Qiu; David Rim; Shaojin Ding; Oleg Rybakov; Yanzhang He', display:{Lore:['[{"text": "arXiv:2305.15536", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRAND: Robustness Aware Norm Decay For Quantized Seq2seq Models\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Qiu\\nDavid Rim\\nShaojin Ding\\nOleg Rybakov\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.15536\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 May 2023 19:45:56 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Qiongqiong Wang; Koji Okabe; Kong Aik Lee; Takafumi Koshinaka', display:{Lore:['[{"text": "arXiv:2305.15567", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeneralized domain adaptation framework for parametric back-end in speaker recognition\\u00a7r\\n\\n\\u00a78\\u00a7oQiongqiong Wang\\nKoji Okabe\\nKong Aik Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.15567\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 May 2023 20:59:31 GMT)\\u00a7r"}']}
{title:'Yeshpanov et al. (§72023§r)', author: 'Rustem Yeshpanov; Saida Mussakhojayeva; Yerbolat Khassanov', display:{Lore:['[{"text": "arXiv:2305.15749", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultilingual Text-to-Speech Synthesis for Turkic Languages Using Transliteration\\u00a7r\\n\\n\\u00a78\\u00a7oRustem Yeshpanov\\nSaida Mussakhojayeva\\nYerbolat Khassanov\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.15749\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 May 2023 05:57:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, 3 tables, accepted to Interspeech\\u00a7r"}']}
{title:'Choi et al. (§72023§r)', author: 'Ha-Yeong Choi; Sang-Hoon Lee; Seong-Whan Lee', display:{Lore:['[{"text": "arXiv:2305.15816", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDDDM-VC: Decoupled Denoising Diffusion Models with Disentangled Representation and Prior Mixup for Verified Robust Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oHa-Yeong Choi\\nSang-Hoon Lee\\nSeong-Whan Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.15816\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 May 2023 07:59:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o23 pages, 10 figures, 17 tables, under review\\u00a7r"}']}
{title:'Moriya et al. (§72023§r)', author: 'Takafumi Moriya; Takanori Ashihara; Hiroshi Sato; Kohei Matsuura; Tomohiro Tanaka; Ryo Masumura', display:{Lore:['[{"text": "arXiv:2305.15958", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Scheduled Sampling for Neural Transducer-based ASR\\u00a7r\\n\\n\\u00a78\\u00a7oTakafumi Moriya\\nTakanori Ashihara\\nHiroshi Sato\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.15958\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 May 2023 11:56:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Moriya et al. (§72023§r)', author: 'Takafumi Moriya; Hiroshi Sato; Tsubasa Ochiai; Marc Delcroix; Takanori Ashihara; Kohei Matsuura; Tomohiro Tanaka; Ryo Masumura; Atsunori Ogawa; Taichi Asami', display:{Lore:['[{"text": "arXiv:2305.15971", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKnowledge Distillation for Neural Transducer-based Target-Speaker ASR: Exploiting Parallel Mixture/Single-Talker Speech Data\\u00a7r\\n\\n\\u00a78\\u00a7oTakafumi Moriya\\nHiroshi Sato\\nTsubasa Ochiai\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.15971\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 May 2023 12:10:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023\\u00a7r"}']}
{title:'Vainio et al. (§72023§r)', author: 'Martti Vainio; Antti Suni; Juraj Šimko; Sofoklis Kakouros', display:{Lore:['[{"text": "arXiv:2305.16040", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Power of Prosody and Prosody of Power: An Acoustic Analysis of Finnish Parliamentary Speech\\u00a7r\\n\\n\\u00a78\\u00a7oMartti Vainio\\nAntti Suni\\nJuraj \\u0160imko\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.16040\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 May 2023 13:18:41 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yuanchao Li; Zeyu Zhao; Ondrej Klejch; Peter Bell; Catherine Lai', display:{Lore:['[{"text": "arXiv:2305.16065", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lASR and Emotional Speech: A Word-Level Investigation of the Mutual Impact of Speech and Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYuanchao Li\\nZeyu Zhao\\nOndrej Klejch\\nPeter Bell\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.16065\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 28 May 2023 17:26:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2023\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yuanchao Li; Peter Bell; Catherine Lai', display:{Lore:['[{"text": "arXiv:2305.16076", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransfer Learning for Personality Perception via Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYuanchao Li\\nPeter Bell\\nCatherine Lai\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.16076\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 28 May 2023 17:22:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2023\\u00a7r"}']}
{title:'Benway et al. (§72023§r)', author: 'Nina R Benway; Yashish M Siriwardena; Jonathan L Preston; Elaine Hitchcock; Tara McAllister; Carol Espy-Wilson', display:{Lore:['[{"text": "arXiv:2305.16085", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic-to-Articulatory Speech Inversion Features for Mispronunciation Detection of /r/ in Child Speech Sound Disorders\\u00a7r\\n\\n\\u00a78\\u00a7oNina R Benway\\nYashish M Siriwardena\\nJonathan L Preston\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.16085\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1924\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. INTERSPEECH 2023, 4568-4572\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 May 2023 14:19:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o*denotes equal contribution. To appearin Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2023\\u00a7r"}']}
{title:'Benway et al. (§72023§r)', author: 'Nina R Benway; Jonathan L Preston; Asif Salekin; Yi Xiao; Harshit Sharma; Tara McAllister', display:{Lore:['[{"text": "arXiv:2305.16111", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lClassifying Rhoticity of /r/ in Speech Sound Disorder using Age-and-Sex Normalized Formants\\u00a7r\\n\\n\\u00a78\\u00a7oNina R Benway\\nJonathan L Preston\\nAsif Salekin\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.16111\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-312\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. INTERSPEECH 2023, 4563-4567\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 May 2023 14:42:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2023\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Wangyou Zhang; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2305.16286", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeakly-Supervised Speech Pre-training: A Case Study on Target Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oWangyou Zhang\\nYanmin Qian\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.16286\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 29 Jun 2023 14:42:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech; 5 pages, 1figure, 3 tables\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Yi-Chiao Wu; Israel D. Gebru; Dejan Marković; Alexander Richard', display:{Lore:['[{"text": "arXiv:2305.16608", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudioDec: An Open-source Streaming High-fidelity Neural Audio Codec\\u00a7r\\n\\n\\u00a78\\u00a7oYi-Chiao Wu\\nIsrael D. Gebru\\nDejan Markovi\\u0107\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.16608\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10096509\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 May 2023 04:01:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, 5 tables. Proc. ICASSP, 2023\\u00a7r"}']}
{title:'Rybakov et al. (§72023§r)', author: 'Oleg Rybakov; Phoenix Meadowlark; Shaojin Ding; David Qiu; Jian Li; David Rim; Yanzhang He', display:{Lore:['[{"text": "arXiv:2305.16619", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l2-bit Conformer quantization for automatic speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oOleg Rybakov\\nPhoenix Meadowlark\\nShaojin Ding\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.16619\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 May 2023 04:26:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to Interspeech\\u00a7r"}']}
{title:'Wan et al. (§72023§r)', author: 'Yixin Wan; Yuan Zhou; Xiulian Peng; Kai-Wei Chang; Yan Lu', display:{Lore:['[{"text": "arXiv:2305.16665", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lABC-KD: Attention-Based-Compression Knowledge Distillation for Deep Learning-Based Noise Suppression\\u00a7r\\n\\n\\u00a78\\u00a7oYixin Wan\\nYuan Zhou\\nXiulian Peng\\nKai-Wei Chang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.16665\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-971\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of INTERSPEECH 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 May 2023 06:29:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper was accepted to Interspeech 2023 Main Conference\\u00a7r"}']}
{title:'Tao et al. (§72023§r)', author: 'Dehua Tao; Tan Lee; Harold Chui; Sarah Luk', display:{Lore:['[{"text": "arXiv:2305.16690", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Representation of Therapist Empathy in Counseling Conversation Using Siamese Hierarchical Attention Network\\u00a7r\\n\\n\\u00a78\\u00a7oDehua Tao\\nTan Lee\\nHarold Chui\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.16690\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 May 2023 07:23:15 GMT)\\u00a7r"}']}
{title:'Park et al. (§72023§r)', author: 'Seongyeon Park; Bohyung Kim; Tae-hyun Oh', display:{Lore:['[{"text": "arXiv:2305.16699", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Tuning of Loss Trade-offs without Hyper-parameter Search in End-to-End Zero-Shot Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oSeongyeon Park\\nBohyung Kim\\nTae-hyun Oh\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.16699\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 May 2023 07:39:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023\\u00a7r"}']}
{title:'Huang et al. (§72023§r)', author: 'Enoch Hsin-Ho Huang; Rong Chao; Yu Tsao; Chao-Min Wu', display:{Lore:['[{"text": "arXiv:2305.16753", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lElectrodeNet \\u2013 A Deep Learning Based Sound Coding Strategy for Cochlear Implants\\u00a7r\\n\\n\\u00a78\\u00a7oEnoch Hsin-Ho Huang\\nRong Chao\\nYu Tsao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.16753\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TCDS.2023.3275587\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 May 2023 09:06:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages and 7 figures. Preprint version; IEEE Transactions on Cognitive and Developmental Systems (accepted)\\u00a7r"}']}
{title:'Mikkonen et al. (§72023§r)', author: 'Otto Mikkonen; Alec Wright; Eloi Moliner; Vesa Välimäki', display:{Lore:['[{"text": "arXiv:2305.16862", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural modeling of magnetic tape recorders\\u00a7r\\n\\n\\u00a78\\u00a7oOtto Mikkonen\\nAlec Wright\\nEloi Moliner\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.16862\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 May 2023 12:15:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to DAFx 2023. For accompanying web page, see http://research.spa.aalto.fi/publications/papers/dafx23-neural-tape/\\u00a7r"}']}
{title:'Heo et al. (§72023§r)', author: 'Jungwoo Heo; Chan-yeong Lim; Ju-ho Kim; Hyun-seo Shin; Ha-Jin Yu', display:{Lore:['[{"text": "arXiv:2305.17394", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOne-Step Knowledge Distillation and Fine-Tuning in Using Large Pre-Trained Self-Supervised Learning Models for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oJungwoo Heo\\nChan-yeong Lim\\nJu-ho Kim\\nHyun-seo Shin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.17394\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 8 Jun 2023 03:41:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oISCA INTERSPEECH 2023\\u00a7r"}']}
{title:'Tian et al. (§72023§r)', author: 'Yusheng Tian; Guangyan Zhang; Tan Lee', display:{Lore:['[{"text": "arXiv:2305.17436", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCreating Personalized Synthetic Voices from Post-Glossectomy Speech with Guided Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oYusheng Tian\\nGuangyan Zhang\\nTan Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.17436\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1639\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nINTERSPEECH 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 27 May 2023 10:17:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to INTERSPEECH 2023\\u00a7r"}']}
{title:'Liang et al. (§72023§r)', author: 'Jinhua Liang; Xubo Liu; Haohe Liu; Huy Phan; Emmanouil Benetos; Mark D. Plumbley; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2305.17719", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdapting Language-Audio Models as Few-Shot Audio Learners\\u00a7r\\n\\n\\u00a78\\u00a7oJinhua Liang\\nXubo Liu\\nHaohe Liu\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.17719\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 28 May 2023 13:17:10 GMT)\\u00a7r"}']}
{title:'Ogun et al. (§72023§r)', author: 'Sewade Ogun; Vincent Colotte; Emmanuel Vincent', display:{Lore:['[{"text": "arXiv:2305.17724", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStochastic Pitch Prediction Improves the Diversity and Naturalness of Speech in Glow-TTS\\u00a7r\\n\\n\\u00a78\\u00a7oSewade Ogun\\nVincent Colotte\\nEmmanuel Vincent\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.17724\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 28 May 2023 13:44:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages with 3 figures, InterSpeech 2023\\u00a7r"}']}
{title:'Ekstedt et al. (§72023§r)', author: 'Erik Ekstedt; Siyang Wang; Éva Székely; Joakim Gustafson; Gabriel Skantze', display:{Lore:['[{"text": "arXiv:2305.17971", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Evaluation of Turn-taking Cues in Conversational Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oErik Ekstedt\\nSiyang Wang\\n\\u00c9va Sz\\u00e9kely\\nJoakim Gustafson\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.17971\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 May 2023 09:29:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2023, 5 pages, 2 figures, 4 tables\\u00a7r"}']}
{title:'Serafini et al. (§72023§r)', author: 'Luca Serafini; Samuele Cornell; Giovanni Morrone; Enrico Zovato; Alessio Brutti; Stefano Squartini', display:{Lore:['[{"text": "arXiv:2305.18074", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Experimental Review of Speaker Diarization methods with application to Two-Speaker Conversational Telephone Speech recordings\\u00a7r\\n\\n\\u00a78\\u00a7oLuca Serafini\\nSamuele Cornell\\nGiovanni Morrone\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.18074\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 May 2023 13:19:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o52 pages, 10 figures\\u00a7r"}']}
{title:'Chao et al. (§72023§r)', author: 'Fu-An Chao; Tien-Hong Lo; Tzu-I Wu; Yao-Ting Sung; Berlin Chen', display:{Lore:['[{"text": "arXiv:2305.18146", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Hierarchical Context-aware Modeling Approach for Multi-aspect and Multi-granular Pronunciation Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oFu-An Chao\\nTien-Hong Lo\\nTzu-I Wu\\nYao-Ting Sung\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.18146\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 7 Jun 2023 15:41:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Xiaoming Wang; Chen Liang; Yulin Mei', display:{Lore:['[{"text": "arXiv:2305.18298", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a75physics.app-ph\\u00a7r, \\u00a75physics.class-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimization design of a micro-perforated panel absorber with 8.6 octave bands\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoming Wang\\nChen Liang\\nYulin Mei\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.18298\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 23 Apr 2023 15:21:03 GMT)\\u00a7r"}']}
{title:'Jiang et al. (§72023§r)', author: 'Xilin Jiang; Yinghao Aaron Li; Nima Mesgarani', display:{Lore:['[{"text": "arXiv:2305.18441", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeCoR: Defy Knowledge Forgetting by Predicting Earlier Audio Codes\\u00a7r\\n\\n\\u00a78\\u00a7oXilin Jiang\\nYinghao Aaron Li\\nNima Mesgarani\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.18441\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-2297\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. INTERSPEECH 2023, pp.2818--2822\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 May 2023 02:25:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023\\u00a7r"}']}
{title:'Phukan et al. (§72023§r)', author: 'Orchid Chetia Phukan; Arun Balaji Buduru; Rajesh Sharma', display:{Lore:['[{"text": "arXiv:2305.18640", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransforming the Embeddings: A Lightweight Technique for Speech Emotion Recognition Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oOrchid Chetia Phukan\\nArun Balaji Buduru\\nRajesh Sharma\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.18640\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 29 May 2023 22:27:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023\\u00a7r"}']}
{title:'Byun et al. (§72023§r)', author: 'Jaeuk Byun; Youna Ji; Soo Whan Chung; Soyeon Choe; Min Seok Choi', display:{Lore:['[{"text": "arXiv:2305.18739", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn empirical study on speech restoration guided by self supervised speech representation\\u00a7r\\n\\n\\u00a78\\u00a7oJaeuk Byun\\nYouna Ji\\nSoo Whan Chung\\nSoyeon Choe\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.18739\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10095881\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 May 2023 04:26:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be presented at ICASSP 2023\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Chenda Li; Yao Qian; Zhuo Chen; Naoyuki Kanda; Dongmei Wang; Takuya Yoshioka; Yanmin Qian; Michael Zeng', display:{Lore:['[{"text": "arXiv:2305.18747", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdapting Multi-Lingual ASR Models for Handling Multiple Talkers\\u00a7r\\n\\n\\u00a78\\u00a7oChenda Li\\nYao Qian\\nZhuo Chen\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.18747\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 May 2023 05:05:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2023\\u00a7r"}']}
{title:'Sun et al. (§72023§r)', author: 'Jianyuan Sun; Xubo Liu; Xinhao Mei; Volkan Kılıç; Mark D. Plumbley; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2305.18753", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDual Transformer Decoder based Features Fusion Network for Automated Audio Captioning\\u00a7r\\n\\n\\u00a78\\u00a7oJianyuan Sun\\nXubo Liu\\nXinhao Mei\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.18753\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 May 2023 05:28:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023. arXiv admin note: substantial text overlap with arXiv:2210.05037\\u00a7r"}']}
{title:'Koizumi et al. (§72023§r)', author: 'Yuma Koizumi; Heiga Zen; Shigeki Karita; Yifan Ding; Kohei Yatabe; Nobuyuki Morioka; Michiel Bacchiani; Yu Zhang; Wei Han; Ankur Bapna', display:{Lore:['[{"text": "arXiv:2305.18802", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLibriTTS-R: A Restored Multi-Speaker Text-to-Speech Corpus\\u00a7r\\n\\n\\u00a78\\u00a7oYuma Koizumi\\nHeiga Zen\\nShigeki Karita\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.18802\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 May 2023 07:30:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023\\u00a7r"}']}
{title:'Chua et al. (§72023§r)', author: 'Victoria Y. H. Chua; Hexin Liu; Leibny Paola Garcia Perera; Fei Ting Woon; Jinyi Wong; Xiangyu Zhang; Sanjeev Khudanpur; Andy W. H. Khong; Justin Dauwels; Suzy J. Styles', display:{Lore:['[{"text": "arXiv:2305.18881", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMERLIon CCS Challenge: A English-Mandarin code-switching child-directed speech corpus for language identification and diarization\\u00a7r\\n\\n\\u00a78\\u00a7oVictoria Y. H. Chua\\nHexin Liu\\nLeibny Paola Garcia Perera\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.18881\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 May 2023 09:26:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2023, 5 pages, 2 figures, 3 tables\\u00a7r"}']}
{title:'Styles et al. (§72023§r)', author: 'Suzy J. Styles; Victoria Y. H. Chua; Fei Ting Woon; Hexin Liu; Leibny Paola Garcia Perera; Sanjeev Khudanpur; Andy W. H. Khong; Justin Dauwels', display:{Lore:['[{"text": "arXiv:2305.18925", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating model performance in language identification: beyond simple error statistics\\u00a7r\\n\\n\\u00a78\\u00a7oSuzy J. Styles\\nVictoria Y. H. Chua\\nFei Ting Woon\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.18925\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 May 2023 10:32:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023, 5 pages, 5 figures\\u00a7r"}']}
{title:'Baas et al. (§72023§r)', author: 'Matthew Baas; Benjamin van Niekerk; Herman Kamper', display:{Lore:['[{"text": "arXiv:2305.18975", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Conversion With Just Nearest Neighbors\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew Baas\\nBenjamin van Niekerk\\nHerman Kamper\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.18975\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 May 2023 12:19:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 page, 1 table, 2 figures. Accepted at Interspeech 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Yu-Hsiang Wang; Huang-Yu Chen; Kai-Wei Chang; Winston Hsu; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2305.19011", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMiniSUPERB: Lightweight Benchmark for Self-supervised Speech Models\\u00a7r\\n\\n\\u00a78\\u00a7oYu-Hsiang Wang\\nHuang-Yu Chen\\nKai-Wei Chang\\nWinston Hsu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19011\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 14 Nov 2023 21:22:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEASRU 2023\\u00a7r"}']}
{title:'Mun et al. (§72023§r)', author: 'Sung Hwan Mun; Hye-jin Shim; Hemlata Tak; Xin Wang; Xuechen Liu; Md Sahidullah; Myeonghun Jeong; Min Hyun Han; Massimiliano Todisco; Kong Aik Lee; Junichi Yamagishi; Nicholas Evans; Tomi Kinnunen; Nam Soo Kim; Jee-weon Jung', display:{Lore:['[{"text": "arXiv:2305.19051", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards single integrated spoofing-aware speaker verification embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oSung Hwan Mun\\nHye-jin Shim\\nHemlata Tak\\n+ 11 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19051\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 1 Jun 2023 11:18:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023. Code and models are available in https://github.com/sasv-challenge/ASVSpoof5-SASVBaseline\\u00a7r"}']}
{title:'Benway et al. (§72023§r)', author: 'Nina R Benway; Jonathan L Preston', display:{Lore:['[{"text": "arXiv:2305.19090", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProspective Validation of Motor-Based Intervention with Automated Mispronunciation Detection of Rhotics in Residual Speech Sound Disorders\\u00a7r\\n\\n\\u00a78\\u00a7oNina R Benway\\nJonathan L Preston\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19090\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1882\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. INTERSPEECH 2023, 4558-4562\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 May 2023 14:53:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2023\\u00a7r"}']}
{title:'Resti et al. (§72023§r)', author: 'Luca Resti; Martin Strauss; Matteo Torcoli; Emanuël Habets; Bernd Edler', display:{Lore:['[{"text": "arXiv:2305.19100", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPredicting Preferred Dialogue-to-Background Loudness Difference in Dialogue-Separated Audio\\u00a7r\\n\\n\\u00a78\\u00a7oLuca Resti\\nMartin Strauss\\nMatteo Torcoli\\nEmanu\\u00ebl Habets\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19100\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 31 May 2023 13:22:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper accepted at the 15th International Conference on Quality of Multimedia Experience (QoMEX), 4 pages, 2 figures\\u00a7r"}']}
{title:'de Oliveira et al. (§72023§r)', author: 'Danilo de Oliveira; Navin Raj Prabhu; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2305.19184", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Semantic Information for Efficient Self-Supervised Emotion Recognition with Audio-Textual Distilled Models\\u00a7r\\n\\n\\u00a78\\u00a7oDanilo de Oliveira\\nNavin Raj Prabhu\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19184\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1758\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 May 2023 16:29:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2023\\u00a7r"}']}
{title:'Bayerl et al. (§72023§r)', author: 'Sebastian P. Bayerl; Dominik Wagner; Ilja Baumann; Florian Hönig; Tobias Bocklet; Elmar Nöth; Korbinian Riedhammer', display:{Lore:['[{"text": "arXiv:2305.19255", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Stutter Seldom Comes Alone \\u2013 Cross-Corpus Stuttering Detection as a Multi-label Problem\\u00a7r\\n\\n\\u00a78\\u00a7oSebastian P. Bayerl\\nDominik Wagner\\nIlja Baumann\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19255\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 May 2023 17:42:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for presentation at Interspeech 2023. arXiv admin note: substantial text overlap with arXiv:2210.15982\\u00a7r"}']}
{title:'Huang et al. (§72023§r)', author: 'Rongjie Huang; Chunlei Zhang; Yongqi Wang; Dongchao Yang; Luping Liu; Zhenhui Ye; Ziyue Jiang; Chao Weng; Zhou Zhao; Dong Yu', display:{Lore:['[{"text": "arXiv:2305.19269", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMake-A-Voice: Unified Voice Synthesis With Discrete Representation\\u00a7r\\n\\n\\u00a78\\u00a7oRongjie Huang\\nChunlei Zhang\\nYongqi Wang\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19269\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 May 2023 17:59:26 GMT)\\u00a7r"}']}
{title:'Do et al. (§72023§r)', author: 'Phat Do; Matt Coler; Jelske Dijkstra; Esther Klabbers', display:{Lore:['[{"text": "arXiv:2305.19396", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lResource-Efficient Fine-Tuning Strategies for Automatic MOS Prediction in Text-to-Speech for Low-Resource Languages\\u00a7r\\n\\n\\u00a78\\u00a7oPhat Do\\nMatt Coler\\nJelske Dijkstra\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19396\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 May 2023 20:19:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2023\\u00a7r"}']}
{title:'Perera et al. (§72023§r)', author: 'Leibny Paola Garcia Perera; Y. H. Victoria Chua; Hexin Liu; Fei Ting Woon; Andy W. H. Khong; Justin Dauwels; Sanjeev Khudanpur; Suzy J. Styles', display:{Lore:['[{"text": "arXiv:2305.19493", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMERLIon CCS Challenge Evaluation Plan\\u00a7r\\n\\n\\u00a78\\u00a7oLeibny Paola Garcia Perera\\nY. H. Victoria Chua\\nHexin Liu\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19493\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 May 2023 02:05:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oEvaluation plan for Interspeech 2023 special session \\"MERLIon\\"\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yanxiong Li; Wenchang Cao; Wei Xie; Jialong Li; Emmanouil Benetos', display:{Lore:['[{"text": "arXiv:2305.19539", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFew-shot Class-incremental Audio Classification Using Dynamically Expanded Classifier with Self-attention Modified Prototypes\\u00a7r\\n\\n\\u00a78\\u00a7oYanxiong Li\\nWenchang Cao\\nWei Xie\\nJialong Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19539\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TMM.2023.3280011\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 May 2023 03:59:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 8 figures, 12 tables. Accepted for publication in IEEE TMM\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yanxiong Li; Hao Chen; Wenchang Cao; Qisheng Huang; Qianhua He', display:{Lore:['[{"text": "arXiv:2305.19541", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFew-Shot Speaker Identification Using Lightweight Prototypical Network with Feature Grouping and Interaction\\u00a7r\\n\\n\\u00a78\\u00a7oYanxiong Li\\nHao Chen\\nWenchang Cao\\nQisheng Huang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19541\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TMM.2023.3253301\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 May 2023 04:09:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 4 figures, 12 tables. Accepted for publication in IEEE TMM\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Yabo Wang; Bing Yang; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2305.19610", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization\\u00a7r\\n\\n\\u00a78\\u00a7oYabo Wang\\nBing Yang\\nXiaofei Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19610\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 May 2023 07:27:03 GMT)\\u00a7r"}']}
{title:'Ni et al. (§72023§r)', author: 'Ziyi Ni; Minglun Han; Feilong Chen; Linghui Meng; Jing Shi; Pin Lv; Bo Xu', display:{Lore:['[{"text": "arXiv:2305.19972", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVILAS: Exploring the Effects of Vision and Language Context in Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZiyi Ni\\nMinglun Han\\nFeilong Chen\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.19972\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 18 Dec 2023 12:29:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Martel et al. (§72023§r)', author: 'Héctor Martel; Julius Richter; Kai Li; Xiaolin Hu; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2306.00160", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-Visual Speech Separation in Noisy Environments with a Lightweight Iterative Model\\u00a7r\\n\\n\\u00a78\\u00a7oH\\u00e9ctor Martel\\nJulius Richter\\nKai Li\\nXiaolin Hu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00160\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 May 2023 20:09:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2023\\u00a7r"}']}
{title:'Siriwardena et al. (§72023§r)', author: 'Yashish M. Siriwardena; Carol Espy-Wilson; Suzanne Boyce; Mark K. Tiede; Liran Oren', display:{Lore:['[{"text": "arXiv:2306.00203", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker-independent Speech Inversion for Estimation of Nasalance\\u00a7r\\n\\n\\u00a78\\u00a7oYashish M. Siriwardena\\nCarol Espy-Wilson\\nSuzanne Boyce\\nMark K. Tiede\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00203\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 May 2023 21:47:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023\\u00a7r"}']}
{title:'Ku et al. (§72023§r)', author: 'Pin-Jui Ku; Chao-Han Huck Yang; Sabato Marco Siniscalchi; Chin-Hui Lee', display:{Lore:['[{"text": "arXiv:2306.00331", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7acs.SY\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a7eeess.SY\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Multi-dimensional Deep Structured State Space Approach to Speech Enhancement Using Small-footprint Models\\u00a7r\\n\\n\\u00a78\\u00a7oPin-Jui Ku\\nChao-Han Huck Yang\\nSabato Marco Siniscalchi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00331\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1084\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jun 2023 04:19:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023. Code will be released athttps://github.com/Kuray107/S4ND-U-Net_speech_enhancement\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yanxiong Li; Zhongjie Jiang; Wenchang Cao; Qisheng Huang', display:{Lore:['[{"text": "arXiv:2306.00426", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker verification using attentive multi-scale convolutional recurrent network\\u00a7r\\n\\n\\u00a78\\u00a7oYanxiong Li\\nZhongjie Jiang\\nWenchang Cao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00426\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jun 2023 08:05:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o21 pages, 6 figures, 8 tables. Accepted for publication in Applied Soft Computing\\u00a7r"}']}
{title:'Zaiem et al. (§72023§r)', author: 'Salah Zaiem; Youcef Kemiche; Titouan Parcollet; Slim Essid; Mirco Ravanelli', display:{Lore:['[{"text": "arXiv:2306.00452", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Self-Supervised Representation Benchmarking: Are We Doing it Right?\\u00a7r\\n\\n\\u00a78\\u00a7oSalah Zaiem\\nYoucef Kemiche\\nTitouan Parcollet\\nSlim Essid\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00452\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nINTERSPEECH 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jun 2023 08:51:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages\\u00a7r"}']}
{title:'Zaiem et al. (§72023§r)', author: 'Salah Zaiem; Titouan Parcollet; Slim Essid', display:{Lore:['[{"text": "arXiv:2306.00481", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Data Augmentation for Domain Adapted Fine-Tuning of Self-Supervised Speech Representations\\u00a7r\\n\\n\\u00a78\\u00a7oSalah Zaiem\\nTitouan Parcollet\\nSlim Essid\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00481\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jun 2023 09:30:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages,INTERSPEECH 2023\\u00a7r"}']}
{title:'Cord-Landwehr et al. (§72023§r)', author: 'Tobias Cord-Landwehr; Christoph Boeddeker; Cătălin Zorilă; Rama Doddipatla; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2306.00625", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrame-wise and overlap-robust speaker embeddings for meeting diarization\\u00a7r\\n\\n\\u00a78\\u00a7oTobias Cord-Landwehr\\nChristoph Boeddeker\\nC\\u0103t\\u0103lin Zoril\\u0103\\nRama Doddipatla\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00625\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jun 2023 12:47:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2023\\u00a7r"}']}
{title:'Cord-Landwehr et al. (§72023§r)', author: 'Tobias Cord-Landwehr; Christoph Boeddeker; Cătălin Zorilă; Rama Doddipatla; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2306.00634", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Teacher-Student approach for extracting informative speaker embeddings from speech mixtures\\u00a7r\\n\\n\\u00a78\\u00a7oTobias Cord-Landwehr\\nChristoph Boeddeker\\nC\\u0103t\\u0103lin Zoril\\u0103\\nRama Doddipatla\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00634\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1379\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 19 Sep 2023 14:34:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH\\u00a7r"}']}
{title:'Gupta et al. (§72023§r)', author: 'Shashi Kant Gupta; Sushant Hiray; Prashant Kukde', display:{Lore:['[{"text": "arXiv:2306.00736", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpoken Language Identification System for English-Mandarin Code-Switching Child-Directed Speech\\u00a7r\\n\\n\\u00a78\\u00a7oShashi Kant Gupta\\nSushant Hiray\\nPrashant Kukde\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00736\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1335\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. INTERSPEECH 2023, 4114--4118\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jun 2023 14:30:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2023, 5 pages, 1 figure, 4 tables\\u00a7r"}']}
{title:'Le et al. (§72023§r)', author: 'Xiaohuai Le; Tong Lei; Li Chen; Yiqing Guo; Chao He; Cheng Chen; Xianjun Xia; Hua Gao; Yijian Xiao; Piao Ding; Shenyi Song; Jing Lu', display:{Lore:['[{"text": "arXiv:2306.00812", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHarmonic enhancement using learnable comb filter for light-weight full-band speech enhancement model\\u00a7r\\n\\n\\u00a78\\u00a7oXiaohuai Le\\nTong Lei\\nLi Chen\\n+ 8 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00812\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jun 2023 15:39:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by Interspeech 2023\\u00a7r"}']}
{title:'Chaubey et al. (§72023§r)', author: 'Ashutosh Chaubey; Sparsh Sinha; Susmita Ghose', display:{Lore:['[{"text": "arXiv:2306.00952", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMeta-Learning Framework for End-to-End Imposter Identification in Unseen Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAshutosh Chaubey\\nSparsh Sinha\\nSusmita Ghose\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00952\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 30 Sep 2023 19:35:49 GMT)\\u00a7r"}']}
{title:'Kouzelis et al. (§72023§r)', author: 'Theodoros Kouzelis; Georgios Paraskevopoulos; Athanasios Katsamanis; Vassilis Katsouros', display:{Lore:['[{"text": "arXiv:2306.00996", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWeakly-supervised forced alignment of disfluent speech using phoneme-level modeling\\u00a7r\\n\\n\\u00a78\\u00a7oTheodoros Kouzelis\\nGeorgios Paraskevopoulos\\nAthanasios Katsamanis\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00996\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 May 2023 09:57:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Shuo Liu; Leda Sarı; Chunyang Wu; Gil Keren; Yuan Shangguan; Jay Mahadeokar; Ozlem Kalinli', display:{Lore:['[{"text": "arXiv:2306.00998", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Selection of Text-to-speech Data to Augment ASR Training\\u00a7r\\n\\n\\u00a78\\u00a7oShuo Liu\\nLeda Sar\\u0131\\nChunyang Wu\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.00998\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 May 2023 17:24:28 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Bohan Wang; Damien Ronssin; Milos Cernak', display:{Lore:['[{"text": "arXiv:2306.01100", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lALO-VC: Any-to-any Low-latency One-shot Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oBohan Wang\\nDamien Ronssin\\nMilos Cernak\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01100\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jun 2023 19:23:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023. Some audio samples areavailable at https://bohan7.github.io/ALO-VC-demo/\\u00a7r"}']}
{title:'Ma et al. (§72023§r)', author: 'Rao Ma; Mengjie Qian; Mark J. F. Gales; Kate M. Knill', display:{Lore:['[{"text": "arXiv:2306.01208", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdapting an Unadaptable ASR System\\u00a7r\\n\\n\\u00a78\\u00a7oRao Ma\\nMengjie Qian\\nMark J. F. Gales\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01208\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1899\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 10 Oct 2023 09:44:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH\\u00a7r"}']}
{title:'Kashiwagi et al. (§72023§r)', author: 'Yosuke Kashiwagi; Siddhant Arora; Hayato Futami; Jessica Huynh; Shih-Lun Wu; Yifan Peng; Brian Yan; Emiru Tsunoo; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2306.01247", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTensor decomposition for minimization of E2E SLU model toward on-device processing\\u00a7r\\n\\n\\u00a78\\u00a7oYosuke Kashiwagi\\nSiddhant Arora\\nHayato Futami\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01247\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Jun 2023 03:14:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Hanbyul Kim; Seunghyun Seo; Lukas Lee; Seolki Baek', display:{Lore:['[{"text": "arXiv:2306.01296", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Training for End-to-End Streaming Automatic Speech Recognition Model with Punctuation\\u00a7r\\n\\n\\u00a78\\u00a7oHanbyul Kim\\nSeunghyun Seo\\nLukas Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01296\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-361\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. INTERSPEECH 2023, 1653-1657\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Jun 2023 06:46:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2023\\u00a7r"}']}
{title:'Carson et al. (§72023§r)', author: 'Alistair Carson; Cassia Valentini-Botinhao; Simon King; Stefan Bilbao', display:{Lore:['[{"text": "arXiv:2306.01332", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDifferentiable Grey-box Modelling of Phaser Effects using Frame-based Spectral Processing\\u00a7r\\n\\n\\u00a78\\u00a7oAlistair Carson\\nCassia Valentini-Botinhao\\nSimon King\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01332\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Jun 2023 07:53:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in Proc. DAFx23, Copenhagen, Denmark, September 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Haoyu Wang; Siyuan Wang; Wei-Qiang Zhang; Hongbin Suo; Yulong Wan', display:{Lore:['[{"text": "arXiv:2306.01385", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTask-Agnostic Structured Pruning of Speech Representation Models\\u00a7r\\n\\n\\u00a78\\u00a7oHaoyu Wang\\nSiyuan Wang\\nWei-Qiang Zhang\\nHongbin Suo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01385\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 9 Jul 2023 06:31:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Doyeon Kim; Soo-Whan Chung; Hyewon Han; Youna Ji; Hong-Goo Kang', display:{Lore:['[{"text": "arXiv:2306.01411", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHD-DEMUCS: General Speech Restoration with Heterogeneous Decoders\\u00a7r\\n\\n\\u00a78\\u00a7oDoyeon Kim\\nSoo-Whan Chung\\nHyewon Han\\nYouna Ji\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01411\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Jun 2023 10:03:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Shi et al. (§72023§r)', author: 'Dongyuan Shi; Bhan Lam; Woon-Seng Gan; Jordan Cheer; Stephen J. Elliott', display:{Lore:['[{"text": "arXiv:2306.01425", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SY\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a7eeess.SY\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lActive Noise Control in The New Century: The Role and Prospect of Signal Processing\\u00a7r\\n\\n\\u00a78\\u00a7oDongyuan Shi\\nBhan Lam\\nWoon-Seng Gan\\nJordan Cheer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01425\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 6 Jul 2023 17:32:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to inter.noise 2023, Chiba, Japan\\u00a7r"}']}
{title:'Richter et al. (§72023§r)', author: 'Julius Richter; Simone Frintrop; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2306.01432", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-Visual Speech Enhancement with Score-Based Generative Models\\u00a7r\\n\\n\\u00a78\\u00a7oJulius Richter\\nSimone Frintrop\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01432\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Jun 2023 10:43:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ITG Conference on Speech Communication\\u00a7r"}']}
{title:'Irino et al. (§72023§r)', author: 'Toshio Irino; Shintaro Doan', display:{Lore:['[{"text": "arXiv:2306.01522", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAuditory Representation Effective for Estimating Vocal Tract Information\\u00a7r\\n\\n\\u00a78\\u00a7oToshio Irino\\nShintaro Doan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01522\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Sep 2023 05:04:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis manuscript is a revised version after acceptance for publication in Proc. APSIPA ASC 2023 on August 25, 2023\\u00a7r"}']}
{title:'Suárez et al. (§72023§r)', author: 'Angélica S. Z. Suárez; Clément Laroche; Line H. Clemmensen; Sneha Das', display:{Lore:['[{"text": "arXiv:2306.01538", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Crowdsourcing-design with Comparison Category Rating for Evaluating Speech Enhancement Algorithms\\u00a7r\\n\\n\\u00a78\\u00a7oAng\\u00e9lica S. Z. Su\\u00e1rez\\nCl\\u00e9ment Laroche\\nLine H. Clemmensen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01538\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Jun 2023 13:41:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished at ICASSP 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Jinhan Wang; Vijay Ravi; Abeer Alwan', display:{Lore:['[{"text": "arXiv:2306.01861", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-uniform Speaker Disentanglement For Depression Detection From Raw Speech Signals\\u00a7r\\n\\n\\u00a78\\u00a7oJinhan Wang\\nVijay Ravi\\nAbeer Alwan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01861\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 6 Jun 2023 02:09:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023\\u00a7r"}']}
{title:'Prabhu et al. (§72023§r)', author: 'Navin Raj Prabhu; Nale Lehmann-Willenbrock; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2306.01916", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIn-the-wild Speech Emotion Conversion Using Disentangled Self-Supervised Representations and Neural Vocoder-based Resynthesis\\u00a7r\\n\\n\\u00a78\\u00a7oNavin Raj Prabhu\\nNale Lehmann-Willenbrock\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01916\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Jun 2023 21:02:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to 15thITG Conference on Speech Communication\\u00a7r"}']}
{title:'Zarazaga et al. (§72023§r)', author: 'Pablo Pérez Zarazaga; Zofia Malisz; Gustav Eje Henter; Lauri Juvela', display:{Lore:['[{"text": "arXiv:2306.01957", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker-independent neural formant synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oPablo P\\u00e9rez Zarazaga\\nZofia Malisz\\nGustav Eje Henter\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01957\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Jun 2023 23:33:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures. Article accepted at INTERSPEECH 2023\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Changhun Kim; Joonhyung Park; Hajin Shim; Eunho Yang', display:{Lore:['[{"text": "arXiv:2306.01981", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization\\u00a7r\\n\\n\\u00a78\\u00a7oChanghun Kim\\nJoonhyung Park\\nHajin Shim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01981\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 21 Jun 2023 11:13:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023 Oral Presentation;Code is available at https://github.com/drumpt/SGEM\\u00a7r"}']}
{title:'Chiang et al. (§72023§r)', author: 'Cheng-Han Chiang; Wei-Ping Huang; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2306.02044", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhy We Should Report the Details in Subjective Evaluation of TTS More Rigorously\\u00a7r\\n\\n\\u00a78\\u00a7oCheng-Han Chiang\\nWei-Ping Huang\\nHung-yi Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.02044\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 3 Jun 2023 07:52:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023 camera-ready version\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yanxiong Li; Wenchang Cao; Jialong Li; Wei Xie; Qianhua He', display:{Lore:['[{"text": "arXiv:2306.02053", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFew-shot Class-incremental Audio Classification Using Stochastic Classifier\\u00a7r\\n\\n\\u00a78\\u00a7oYanxiong Li\\nWenchang Cao\\nJialong Li\\nWei Xie\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.02053\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 3 Jun 2023 08:59:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 4 tables. Accepted for publication in INTERSPEECH 2023\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yanxiong Li; Wenchang Cao; Wei Xie; Qisheng Huang; Wenfeng Pang; Qianhua He', display:{Lore:['[{"text": "arXiv:2306.02054", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Complexity Acoustic Scene Classification Using Data Augmentation and Lightweight ResNet\\u00a7r\\n\\n\\u00a78\\u00a7oYanxiong Li\\nWenchang Cao\\nWei Xie\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.02054\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 3 Jun 2023 09:05:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, 4 tables. Accepted for publication in the 16th IEEE International Conference on Signal Processing (IEEE ICSP)\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Haibin Wu; Kai-Wei Chang; Yuan-Kuei Wu; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2306.02207", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts\\u00a7r\\n\\n\\u00a78\\u00a7oHaibin Wu\\nKai-Wei Chang\\nYuan-Kuei Wu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.02207\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 25 Aug 2023 16:10:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWork inprogress. The first three authors contributed equally\\u00a7r"}']}
{title:'Song et al. (§72023§r)', author: 'Siyuan Song; Stijn Kindt; Jasper Maes; Alexander Bohlender. Nilesh Madhu', display:{Lore:['[{"text": "arXiv:2306.02344", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInfluence of Lossy Speech Codecs on Hearing-aid, Binaural Sound Source Localisation using DNNs\\u00a7r\\n\\n\\u00a78\\u00a7oSiyuan Song\\nStijn Kindt\\nJasper Maes\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.02344\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 4 Jun 2023 12:34:46 GMT)\\u00a7r"}']}
{title:'Heydari et al. (§72023§r)', author: 'Mojtaba Heydari; Ju-Chiang Wang; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2306.02372", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSingNet: A Real-time Singing Voice Beat and Downbeat Tracking System\\u00a7r\\n\\n\\u00a78\\u00a7oMojtaba Heydari\\nJu-Chiang Wang\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.02372\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 4 Jun 2023 15:09:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for 2023 International Conference on Acoustics, Speech, and Signal Processing (ICASSP-2023)\\u00a7r"}']}
{title:'Haubner et al. (§72023§r)', author: 'Thomas Haubner; Andreas Brendel; Walter Kellermann', display:{Lore:['[{"text": "arXiv:2306.02450", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-To-End Deep Learning-based Adaptation Control for Linear Acoustic Echo Cancellation\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Haubner\\nAndreas Brendel\\nWalter Kellermann\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.02450\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 4 Jun 2023 19:44:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis article has been submitted to IEEE/ACM Transactions on Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Fu et al. (§72023§r)', author: 'Li Fu; Siqi Li; Qingtao Li; Fangzhu Li; Liping Deng; Lu Fan; Meng Chen; Youzheng Wu; Xiaodong He', display:{Lore:['[{"text": "arXiv:2306.02541", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOTF: Optimal Transport based Fusion of Supervised and Self-Supervised Learning Models for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oLi Fu\\nSiqi Li\\nQingtao Li\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.02541\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Jun 2023 02:24:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2023\\u00a7r"}']}
{title:'Shul et al. (§72023§r)', author: 'Yusun Shul; Byeong-Yun Ko; Jung-Woo Choi', display:{Lore:['[{"text": "arXiv:2306.02591", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDivided spectro-temporal attention for sound event localization and detection in real scenes for DCASE2023 challenge\\u00a7r\\n\\n\\u00a78\\u00a7oYusun Shul\\nByeong-Yun Ko\\nJung-Woo Choi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.02591\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Jun 2023 04:39:34 GMT)\\u00a7r"}']}
{title:'Gao et al. (§72023§r)', author: 'Lufei Gao; Shan Huang; Li Liu', display:{Lore:['[{"text": "arXiv:2306.02596", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Novel Interpretable and Generalizable Re-synchronization Model for Cued Speech based on a Multi-Cuer Corpus\\u00a7r\\n\\n\\u00a78\\u00a7oLufei Gao\\nShan Huang\\nLi Liu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.02596\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Jun 2023 05:03:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, Accepted to INTERSPEECH2023\\u00a7r"}']}
{title:'Sach et al. (§72023§r)', author: 'Marvin Sach; Jan Franzen; Bruno Defraene; Kristoff Fluyt; Maximilian Strake; Wouter Tirry; Tim Fingscheidt', display:{Lore:['[{"text": "arXiv:2306.02778", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEffCRN: An Efficient Convolutional Recurrent Network for High-Performance Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oMarvin Sach\\nJan Franzen\\nBruno Defraene\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.02778\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Jun 2023 11:03:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, accepted for Interspeech 2023\\u00a7r"}']}
{title:'Panariello et al. (§72023§r)', author: 'Michele Panariello; Massimiliano Todisco; Nicholas Evans', display:{Lore:['[{"text": "arXiv:2306.02892", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVocoder drift in x-vector-based speaker anonymization\\u00a7r\\n\\n\\u00a78\\u00a7oMichele Panariello\\nMassimiliano Todisco\\nNicholas Evans\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.02892\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Jun 2023 14:01:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2023\\u00a7r"}']}
{title:'Khorrami et al. (§72023§r)', author: 'Khazar Khorrami; María Andrea Cruz Blandón; Tuomas Virtanen; Okko Räsänen', display:{Lore:['[{"text": "arXiv:2306.02972", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSimultaneous or Sequential Training? How Speech Representations Cooperate in a Multi-Task Self-Supervised Learning System\\u00a7r\\n\\n\\u00a78\\u00a7oKhazar Khorrami\\nMar\\u00eda Andrea Cruz Bland\\u00f3n\\nTuomas Virtanen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.02972\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.23919/EUSIPCO58844.2023.10290051\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Jun 2023 15:35:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted by EUSIPCO 2023\\u00a7r"}']}
{title:'de Oliveira et al. (§72023§r)', author: 'Danilo de Oliveira; Julius Richter; Jean-Marie Lemercier; Tal Peer; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2306.03014", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Behavior of Intrusive and Non-intrusive Speech Enhancement Metrics in Predictive and Generative Settings\\u00a7r\\n\\n\\u00a78\\u00a7oDanilo de Oliveira\\nJulius Richter\\nJean-Marie Lemercier\\nTal Peer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.03014\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Jun 2023 16:30:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ITG Conference on Speech Communication\\u00a7r"}']}
{title:'Mori et al. (§72023§r)', author: 'Hiroki Mori; Shunya Kimura', display:{Lore:['[{"text": "arXiv:2306.03465", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA generative framework for conversational laughter: Its \'language model\' and laughter sound synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oHiroki Mori\\nShunya Kimura\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.03465\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-2453\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2023 (2023) 3372-3376\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Jun 2023 07:35:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH\\u00a7r"}']}
{title:'Jiang et al. (§72023§r)', author: 'Ziyue Jiang; Yi Ren; Zhenhui Ye; Jinglin Liu; Chen Zhang; Qian Yang; Shengpeng Ji; Rongjie Huang; Chunfeng Wang; Xiang Yin; Zejun Ma; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2306.03509", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias\\u00a7r\\n\\n\\u00a78\\u00a7oZiyue Jiang\\nYi Ren\\nZhenhui Ye\\n+ 8 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.03509\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Jun 2023 08:54:49 GMT)\\u00a7r"}']}
{title:'Lepage et al. (§72023§r)', author: 'Theo Lepage; Reda Dehak', display:{Lore:['[{"text": "arXiv:2306.03664", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExperimenting with Additive Margins for Contrastive Self-Supervised Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oTheo Lepage\\nReda Dehak\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.03664\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1479\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. INTERSPEECH 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Jun 2023 13:26:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at INTERSPEECH 2023, 20th-24th August 2023,Dublin, Ireland\\u00a7r"}']}
{title:'Maison et al. (§72023§r)', author: 'Lucas Maison; Yannick Estève', display:{Lore:['[{"text": "arXiv:2306.03773", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSome voices are too common: Building fair speech recognition systems using the Common Voice dataset\\u00a7r\\n\\n\\u00a78\\u00a7oLucas Maison\\nYannick Est\\u00e8ve\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.03773\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jun 2023 11:42:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures. Accepted to Interspeech 2023\\u00a7r"}']}
{title:'Filimonov et al. (§72023§r)', author: 'Denis Filimonov; Prabhat Pandey; Ariya Rastrow; Ankur Gandhe; Andreas Stolcke', display:{Lore:['[{"text": "arXiv:2306.03778", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreaming Speech-to-Confusion Network Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDenis Filimonov\\nPrabhat Pandey\\nAriya Rastrow\\nAnkur Gandhe\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.03778\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-486\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech, Aug. 2023, pp. 4099-4103\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Jun 2023 20:28:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Interspeech 2023\\u00a7r"}']}
{title:'Sullivan et al. (§72023§r)', author: 'Peter Sullivan; AbdelRahim Elmadany; Muhammad Abdul-Mageed', display:{Lore:['[{"text": "arXiv:2306.03789", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Robustness of Arabic Speech Dialect Identification\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Sullivan\\nAbdelRahim Elmadany\\nMuhammad Abdul-Mageed\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.03789\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Jun 2023 21:31:00 GMT)\\u00a7r"}']}
{title:'Sagar et al. (§72023§r)', author: 'Sangeet Sagar; Mirco Ravanelli; Bernd Kiefer; Ivana Kruijff Korbayova; Josef van Genabith', display:{Lore:['[{"text": "arXiv:2306.04054", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRescueSpeech: A German Corpus for Speech Recognition in Search and Rescue Domain\\u00a7r\\n\\n\\u00a78\\u00a7oSangeet Sagar\\nMirco Ravanelli\\nBernd Kiefer\\nIvana Kruijff Korbayova\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.04054\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 25 Sep 2023 08:00:05 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Xian Li; Nian Shao; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2306.04186", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervised Audio Teacher-Student Transformer for Both Clip-level and Frame-level Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oXian Li\\nNian Shao\\nXiaofei Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.04186\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 7 Nov 2023 09:59:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEETASLP. arXiv admin note:text overlap with arXiv:2204.12076\\u00a7r"}']}
{title:'Yin et al. (§72023§r)', author: 'Han Yin; Jisheng Bai; Mou Wang; Siwei Huang; Yafei Jia; Jianfeng Chen', display:{Lore:['[{"text": "arXiv:2306.04987", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConvolutional Recurrent Neural Network with Attention for 3D Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oHan Yin\\nJisheng Bai\\nMou Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.04987\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 20 Nov 2023 03:00:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished on IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC 2023)\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Junhyeok Lee; Hyeonuk Nam; Yong-Hwa Park', display:{Lore:['[{"text": "arXiv:2306.05004", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVIFS: An End-to-End Variational Inference for Foley Sound Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oJunhyeok Lee\\nHyeonuk Nam\\nYong-Hwa Park\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.05004\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Jun 2023 07:48:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDCASE 2023 Challenge Task 7\\u00a7r"}']}
{title:'Nishu et al. (§72023§r)', author: 'Kumari Nishu; Minsik Cho; Devang Naik', display:{Lore:['[{"text": "arXiv:2306.05245", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMatching Latent Encoding for Audio-Text based Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oKumari Nishu\\nMinsik Cho\\nDevang Naik\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.05245\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Jun 2023 14:44:23 GMT)\\u00a7r"}']}
{title:'Lea et al. (§72023§r)', author: 'Colin Lea; Dianna Yee; Jaya Narain; Zifang Huang; Lauren Tooley; Jeffrey P. Bigham; Leah Findlater', display:{Lore:['[{"text": "arXiv:2306.05446", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLatent Phrase Matching for Dysarthric Speech\\u00a7r\\n\\n\\u00a78\\u00a7oColin Lea\\nDianna Yee\\nJaya Narain\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.05446\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Jun 2023 17:28:28 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yanxiong Li; Mingle Liu; Wucheng Wang; Yuhan Zhang; Qianhua He', display:{Lore:['[{"text": "arXiv:2306.05621", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic Scene Clustering Using Joint Optimization of Deep Embedding Learning and Clustering Iteration\\u00a7r\\n\\n\\u00a78\\u00a7oYanxiong Li\\nMingle Liu\\nWucheng Wang\\nYuhan Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.05621\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Jun 2023 01:51:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 6 figures, 11 tables. Accepted for publication in IEEE TMM\\u00a7r"}']}
{title:'Zeng et al. (§72023§r)', author: 'Yufei Zeng; Yanxiong Li; Zhenfeng Zhou; Ruiqi Wang; Difeng Lu', display:{Lore:['[{"text": "arXiv:2306.05624", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDomestic Activities Classification from Audio Recordings Using Multi-scale Dilated Depthwise Separable Convolutional Network\\u00a7r\\n\\n\\u00a78\\u00a7oYufei Zeng\\nYanxiong Li\\nZhenfeng Zhou\\nRuiqi Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.05624\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Jun 2023 02:15:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 4 tables. Accepted for publication in IEEE MMSP2021\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Shijun Wang; Jón Guðnason; Damian Borth', display:{Lore:['[{"text": "arXiv:2306.05709", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Emotional Representations from Imbalanced Speech Data for Speech Emotion Recognition and Emotional Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oShijun Wang\\nJ\\u00f3n Gu\\u00f0nason\\nDamian Borth\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.05709\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Jun 2023 07:04:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH2023\\u00a7r"}']}
{title:'Wang (§72023§r)', author: 'Junyu Wang', display:{Lore:['[{"text": "arXiv:2306.05861", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Encoder-Decoder and Dual-Path Conformer for Comprehensive Feature Learning in Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oJunyu Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.05861\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Jun 2023 12:52:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech2023\\u00a7r"}']}
{title:'Wang (§72023§r)', author: 'Junyu Wang', display:{Lore:['[{"text": "arXiv:2306.05887", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Efficient Speech Separation Network Based on Recurrent Fusion Dilated Convolution and Channel Attention\\u00a7r\\n\\n\\u00a78\\u00a7oJunyu Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.05887\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Jun 2023 13:30:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2023\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Zihan Wu; Neil Scheidwasser-Clow; Karl El Hajal; Milos Cernak', display:{Lore:['[{"text": "arXiv:2306.05915", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Embeddings as Individuality Proxy for Voice Stress Detection\\u00a7r\\n\\n\\u00a78\\u00a7oZihan Wu\\nNeil Scheidwasser-Clow\\nKarl El Hajal\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.05915\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Jun 2023 14:11:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures. Accepted at Interspeech 2023\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Ji Won Kim; Sang Won Son; Yoonah Song; Hong Kook Kim; Il Hoon Song; Jeong Eun Lim', display:{Lore:['[{"text": "arXiv:2306.06461", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-supervsied Learning-based Sound Event Detection using Freuqency Dynamic Convolution with Large Kernel Attention for DCASE Challenge 2023 Task 4\\u00a7r\\n\\n\\u00a78\\u00a7oJi Won Kim\\nSang Won Son\\nYoonah Song\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.06461\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 10 Jun 2023 15:05:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDCASE 2023 Challenge Task 4A,5 pages\\u00a7r"}']}
{title:'Yoshinaga et al. (§72023§r)', author: 'Tomoya Yoshinaga; Keitaro Tanaka; Shigeo Morishima', display:{Lore:['[{"text": "arXiv:2306.06495", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-Visual Speech Enhancement With Selective Off-Screen Speech Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oTomoya Yoshinaga\\nKeitaro Tanaka\\nShigeo Morishima\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.06495\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 10 Jun 2023 17:37:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by EUSIPCO 2023\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Mu Yang; Ram C. M. C. Shekar; Okim Kang; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:2306.06524", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhat Can an Accent Identifier Learn? Probing Phonetic and Prosodic Information in a Wav2vec2-based Accent Identification Model\\u00a7r\\n\\n\\u00a78\\u00a7oMu Yang\\nRam C. M. C. Shekar\\nOkim Kang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.06524\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 10 Jun 2023 21:20:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2023\\u00a7r"}']}
{title:'Hwang et al. (§72023§r)', author: 'Ji-Sang Hwang; Sang-Hoon Lee; Seong-Whan Lee', display:{Lore:['[{"text": "arXiv:2306.06814", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHiddenSinger: High-Quality Singing Voice Synthesis via Neural Audio Codec and Latent Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oJi-Sang Hwang\\nSang-Hoon Lee\\nSeong-Whan Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.06814\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 12 Jun 2023 01:21:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 5 figures, 5 tables, under review\\u00a7r"}']}
{title:'Alastruey et al. (§72023§r)', author: 'Belen Alastruey; Lukas Drude; Jahn Heymann; Simon Wiesler', display:{Lore:['[{"text": "arXiv:2306.06954", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-View Frequency-Attention Alternative to CNN Frontends for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oBelen Alastruey\\nLukas Drude\\nJahn Heymann\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.06954\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 12 Jun 2023 08:37:36 GMT)\\u00a7r"}']}
{title:'Qi et al. (§72023§r)', author: 'Jinzi Qi; Hugo Van hamme', display:{Lore:['[{"text": "arXiv:2306.07090", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParameter-efficient Dysarthric Speech Recognition Using Adapter Fusion and Householder Transformation\\u00a7r\\n\\n\\u00a78\\u00a7oJinzi Qi\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.07090\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 12 Jun 2023 13:06:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2023\\u00a7r"}']}
{title:'Hwang et al. (§72023§r)', author: 'Ji-Sang Hwang; Sang-Hoon Lee; Seong-Whan Lee', display:{Lore:['[{"text": "arXiv:2306.07489", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPauseSpeech: Natural Speech Synthesis via Pre-trained Language Model and Pause-based Prosody Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oJi-Sang Hwang\\nSang-Hoon Lee\\nSeong-Whan Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.07489\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Jun 2023 01:36:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 4 figures, 3 tables, under reivew\\u00a7r"}']}
{title:'Singh et al. (§72023§r)', author: 'Vishwanath Pratap Singh; Md Sahidullah; Tomi Kinnunen', display:{Lore:['[{"text": "arXiv:2306.07501", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Verification Across Ages: Investigating Deep Speaker Embedding Sensitivity to Age Mismatch in Enrollment and Test Speech\\u00a7r\\n\\n\\u00a78\\u00a7oVishwanath Pratap Singh\\nMd Sahidullah\\nTomi Kinnunen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.07501\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInterspeech 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Jun 2023 02:23:55 GMT)\\u00a7r"}']}
{title:'Panariello et al. (§72023§r)', author: 'Michele Panariello; Wanying Ge; Hemlata Tak; Massimiliano Todisco; Nicholas Evans', display:{Lore:['[{"text": "arXiv:2306.07655", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMalafide: a novel adversarial convolutive noise attack against deepfake and spoofing detection systems\\u00a7r\\n\\n\\u00a78\\u00a7oMichele Panariello\\nWanying Ge\\nHemlata Tak\\nMassimiliano Todisco\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.07655\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Jun 2023 09:52:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2023\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yinghao Aaron Li; Cong Han; Vinay S. Raghavan; Gavin Mischler; Nima Mesgarani', display:{Lore:['[{"text": "arXiv:2306.07691", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oYinghao Aaron Li\\nCong Han\\nVinay S. Raghavan\\nGavin Mischler\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.07691\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 20 Nov 2023 04:23:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oNeurIPS 2023\\u00a7r"}']}
{title:'Lin et al. (§72023§r)', author: 'Xiaoyu Lin; Simon Leglaive; Laurent Girin; Xavier Alameda-Pineda', display:{Lore:['[{"text": "arXiv:2306.07820", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised speech enhancement with deep dynamical generative speech and noise models\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoyu Lin\\nSimon Leglaive\\nLaurent Girin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.07820\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Jun 2023 14:52:35 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Liming Wang; Mark Hasegawa-Johnson; Chang D. Yoo', display:{Lore:['[{"text": "arXiv:2306.07926", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Theory of Unsupervised Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oLiming Wang\\nMark Hasegawa-Johnson\\nChang D. Yoo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.07926\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Jun 2023 08:12:27 GMT)\\u00a7r"}']}
{title:'Baali et al. (§72023§r)', author: 'Massa Baali; Ahmed Ali', display:{Lore:['[{"text": "arXiv:2306.07936", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFOOCTTS: Generating Arabic Speech with Acoustic Environment for Football Commentator\\u00a7r\\n\\n\\u00a78\\u00a7oMassa Baali\\nAhmed Ali\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.07936\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Jun 2023 12:33:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Interspeech 2023 ShowTell Demo Session\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Mingqiu Wang; Izhak Shafran; Hagen Soltau; Wei Han; Yuan Cao; Dian Yu; Laurent El Shafey', display:{Lore:['[{"text": "arXiv:2306.07944", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech-to-Text Adapter and Speech-to-Entity Retriever Augmented LLMs for Speech Understanding\\u00a7r\\n\\n\\u00a78\\u00a7oMingqiu Wang\\nIzhak Shafran\\nHagen Soltau\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.07944\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Jun 2023 22:33:22 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Xianzhao Chen; Yist Y. Lin; Kang Wang; Yi He; Zejun Ma', display:{Lore:['[{"text": "arXiv:2306.07949", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Frame-level Classifier for Word Timings with Non-peaky CTC in End-to-End Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXianzhao Chen\\nYist Y. Lin\\nKang Wang\\nYi He\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.07949\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Jun 2023 03:36:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in the proceedings of INTERSPEECH 2023\\u00a7r"}']}
{title:'Yadav et al. (§72023§r)', author: 'Manuj Yadav; Markus Georgi; Larissa Leist; Maria Klatte; Sabine J. Schlittmeier; Janina Fels', display:{Lore:['[{"text": "arXiv:2306.08051", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCognitive performance in open-plan office acoustic simulations: Effects of room acoustics and semantics but not spatial separation of sound sources\\u00a7r\\n\\n\\u00a78\\u00a7oManuj Yadav\\nMarkus Georgi\\nLarissa Leist\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08051\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.apacoust.2023.109559\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nApplied Acoustics, 211, 109559 (2023)\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 8 Aug 2023 11:26:01 GMT)\\u00a7r"}']}
{title:'Watcharasupat et al. (§72023§r)', author: 'Karn N. Watcharasupat; Alexander Lerch', display:{Lore:['[{"text": "arXiv:2306.08053", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQuantifying Spatial Audio Quality Impairment\\u00a7r\\n\\n\\u00a78\\u00a7oKarn N. Watcharasupat\\nAlexander Lerch\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08053\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 14 Dec 2023 18:58:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the 2024 IEEE InternationalConference on Acoustics, Speech, and Signal Processing (ICASSP)\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Nanxin Chen; Izhak Shafran; Yu Zhang; Chung-Cheng Chiu; Hagen Soltau; James Qin; Yonghui Wu', display:{Lore:['[{"text": "arXiv:2306.08131", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Adapters for Giant Speech Models\\u00a7r\\n\\n\\u00a78\\u00a7oNanxin Chen\\nIzhak Shafran\\nYu Zhang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08131\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Jun 2023 20:51:00 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Tongzhou Chen; Cyril Allauzen; Yinghui Huang; Daniel Park; David Rybach; W. Ronny Huang; Rodrigo Cabrera; Kartik Audhkhasi; Bhuvana Ramabhadran; Pedro J. Moreno; Michael Riley', display:{Lore:['[{"text": "arXiv:2306.08133", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLarge-scale Language Model Rescoring on Long-form Data\\u00a7r\\n\\n\\u00a78\\u00a7oTongzhou Chen\\nCyril Allauzen\\nYinghui Huang\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08133\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2023 - 2023 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 5 Sep 2023 20:50:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted in ICASSP 2023\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Hejung Yang; Hong-Goo Kang', display:{Lore:['[{"text": "arXiv:2306.08406", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFeature Normalization for Fine-tuning Self-Supervised Models in Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oHejung Yang\\nHong-Goo Kang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08406\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Jun 2023 10:03:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023 accepted\\u00a7r"}']}
{title:'Yoon et al. (§72023§r)', author: 'Ji Won Yoon; Seok Min Kim; Nam Soo Kim', display:{Lore:['[{"text": "arXiv:2306.08463", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMCR-Data2vec 2.0: Improving Self-supervised Speech Pre-training via Model-level Consistency Regularization\\u00a7r\\n\\n\\u00a78\\u00a7oJi Won Yoon\\nSeok Min Kim\\nNam Soo Kim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08463\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Jun 2023 12:17:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023\\u00a7r"}']}
{title:'Fejgin et al. (§72023§r)', author: 'Daniel Fejgin; Wiebke Middelberg; Simon Doclo', display:{Lore:['[{"text": "arXiv:2306.08484", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBRUDEX Database: Binaural Room Impulse Responses with Uniformly Distributed External Microphones\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Fejgin\\nWiebke Middelberg\\nSimon Doclo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08484\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Jun 2023 12:59:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ITG 2023\\u00a7r"}']}
{title:'Diaz-Guerra et al. (§72023§r)', author: 'David Diaz-Guerra; Archontis Politis; Antonio Miguel; Jose R. Beltran; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2306.08510", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPermutation Invariant Recurrent Neural Networks for Sound Source Tracking Applications\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Diaz-Guerra\\nArchontis Politis\\nAntonio Miguel\\nJose R. Beltran\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08510\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.61782/fa.2023.1132\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Jun 2023 13:53:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at Forum Acusticum 2023\\u00a7r"}']}
{title:'Dietzen et al. (§72023§r)', author: 'Thomas Dietzen; Enzo De Sena; Toon van Waterschoot', display:{Lore:['[{"text": "arXiv:2306.08514", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Complexity Steered Response Power Mapping based on Low-Rank and Sparse Interpolation\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Dietzen\\nEnzo De Sena\\nToon van Waterschoot\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08514\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Jun 2023 14:00:44 GMT)\\u00a7r"}']}
{title:'Guo et al. (§72023§r)', author: 'Zilu Guo; Jun Du; Chin-Hui Lee; Yu Gao; Wenbin Zhang', display:{Lore:['[{"text": "arXiv:2306.08527", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVariance-Preserving-Based Interpolation Diffusion Models for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oZilu Guo\\nJun Du\\nChin-Hui Lee\\nYu Gao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08527\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 17 Sep 2023 13:27:18 GMT)\\u00a7r"}']}
{title:'Dhawan et al. (§72023§r)', author: 'Kunal Dhawan; Dima Rekesh; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2306.08753", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnified model for code-switching speech recognition and language identification based on a concatenated tokenizer\\u00a7r\\n\\n\\u00a78\\u00a7oKunal Dhawan\\nDima Rekesh\\nBoris Ginsburg\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08753\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 16 Sep 2023 05:32:12 GMT)\\u00a7r"}']}
{title:'Kojima et al. (§72023§r)', author: 'Takaaki Kojima; Kazuyuki Arikawa; Shoichi Koyama; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2306.08855", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultichannel Active Noise Control with Exterior Radiation Suppression Based on Riemannian Optimization\\u00a7r\\n\\n\\u00a78\\u00a7oTakaaki Kojima\\nKazuyuki Arikawa\\nShoichi Koyama\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08855\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Jun 2023 04:48:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to European Signal Processing Conference (EUSIPCO) 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Jiarui Wang; Prasanga Samarasinghe; Thushara Abhayapala; Jihui Aimee Zhang', display:{Lore:['[{"text": "arXiv:2306.09135", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTime-Domain Wideband Image Source Method for Spherical Microphone Arrays\\u00a7r\\n\\n\\u00a78\\u00a7oJiarui Wang\\nPrasanga Samarasinghe\\nThushara Abhayapala\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.09135\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 9 Aug 2023 08:27:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in the IEEE 25th International Workshop on Multimedia Signal Processing (IEEEMMSP 2023)\\u00a7r"}']}
{title:'Paturi et al. (§72023§r)', author: 'Rohit Paturi; Sundararajan Srinivasan; Xiang Li', display:{Lore:['[{"text": "arXiv:2306.09313", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLexical Speaker Error Correction: Leveraging Language Models for Speaker Diarization Error Correction\\u00a7r\\n\\n\\u00a78\\u00a7oRohit Paturi\\nSundararajan Srinivasan\\nXiang Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.09313\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Jun 2023 17:47:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2023\\u00a7r"}']}
{title:'Sun et al. (§72023§r)', author: 'Haiyang Sun; Fulin Zhang; Zheng Lian; Yingying Guo; Shilei Zhang', display:{Lore:['[{"text": "arXiv:2306.09361", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMFAS: Emotion Recognition through Multiple Perspectives Fusion Architecture Search Emulating Human Cognition\\u00a7r\\n\\n\\u00a78\\u00a7oHaiyang Sun\\nFulin Zhang\\nZheng Lian\\nYingying Guo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.09361\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 25 Dec 2023 01:57:40 GMT)\\u00a7r"}']}
{title:'Sasindran et al. (§72023§r)', author: 'Zitha Sasindran; Harsha Yelchuri; Pooja Rao; T. V. Prabhakar', display:{Lore:['[{"text": "arXiv:2306.09384", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMobileASR: A resource-aware on-device learning framework for user voice personalization applications on mobile phones\\u00a7r\\n\\n\\u00a78\\u00a7oZitha Sasindran\\nHarsha Yelchuri\\nPooja Rao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.09384\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 10 Nov 2023 04:50:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in AIMLSystems 2023\\u00a7r"}']}
{title:'Mehta et al. (§72023§r)', author: 'Shivam Mehta; Siyang Wang; Simon Alexanderson; Jonas Beskow; Éva Székely; Gustav Eje Henter', display:{Lore:['[{"text": "arXiv:2306.09417", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiff-TTSG: Denoising probabilistic integrated speech and gesture synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oShivam Mehta\\nSiyang Wang\\nSimon Alexanderson\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.09417\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/SSW.2023-24\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 9 Aug 2023 12:41:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 2 figures, presented at the ISCA Speech Synthesis Workshop (SSW) 2023\\u00a7r"}']}
{title:'Shivakumar et al. (§72023§r)', author: 'Prashanth Gurunath Shivakumar; Jari Kolehmainen; Yile Gu; Ankur Gandhe; Ariya Rastrow; Ivan Bulyko', display:{Lore:['[{"text": "arXiv:2306.09452", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDistillation Strategies for Discriminative Speech Recognition Rescoring\\u00a7r\\n\\n\\u00a78\\u00a7oPrashanth Gurunath Shivakumar\\nJari Kolehmainen\\nYile Gu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.09452\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Jun 2023 19:15:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2023\\u00a7r"}']}
{title:'Chung et al. (§72023§r)', author: 'Woo-Jin Chung; Doyeon Kim; Soo-Whan Chung; Hong-Goo Kang', display:{Lore:['[{"text": "arXiv:2306.09640", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMF-PAM: Accurate Pitch Estimation through Periodicity Analysis and Multi-level Feature Fusion\\u00a7r\\n\\n\\u00a78\\u00a7oWoo-Jin Chung\\nDoyeon Kim\\nSoo-Whan Chung\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.09640\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 16 Jun 2023 06:01:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at INTERSPEECH 2023\\u00a7r"}']}
{title:'Meyer et al. (§72023§r)', author: 'Luke Meyer; Laura Rachman; Gloria Araiza-Illan; Etienne Gaudrain; Deniz Başkent', display:{Lore:['[{"text": "arXiv:2306.09714", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUse of a humanoid robot for auditory psychophysical testing\\u00a7r\\n\\n\\u00a78\\u00a7oLuke Meyer\\nLaura Rachman\\nGloria Araiza-Illan\\nEtienne Gaudrain\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.09714\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1371/journal.pone.0294328\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPLOS ONE, 18(12), e0294328. 2023\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 15 Dec 2023 10:25:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o27 pages (single spaced), 7 figures, 82 references, published by Plos ONE\\u00a7r"}']}
{title:'Kang et al. (§72023§r)', author: 'Minsung Kang; Sangshin Oh; Hyeongi Moon; Kyungyun Lee; Ben Sangbae Chon', display:{Lore:['[{"text": "arXiv:2306.09807", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFALL-E: A Foley Sound Synthesis Model and Strategies\\u00a7r\\n\\n\\u00a78\\u00a7oMinsung Kang\\nSangshin Oh\\nHyeongi Moon\\nKyungyun Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.09807\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 10 Aug 2023 05:13:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Kakouros et al. (§72023§r)', author: 'Sofoklis Kakouros; Juraj Šimko; Martti Vainio; Antti Suni', display:{Lore:['[{"text": "arXiv:2306.09814", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating the Utility of Surprisal from Large Language Models for Speech Synthesis Prosody\\u00a7r\\n\\n\\u00a78\\u00a7oSofoklis Kakouros\\nJuraj \\u0160imko\\nMartti Vainio\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.09814\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 16 Jun 2023 12:49:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at SSW 2023\\u00a7r"}']}
{title:'Xie et al. (§72023§r)', author: 'Huang Xie; Khazar Khorrami; Okko Räsänen; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2306.09820", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCrowdsourcing and Evaluating Text-Based Audio Retrieval Relevances\\u00a7r\\n\\n\\u00a78\\u00a7oHuang Xie\\nKhazar Khorrami\\nOkko R\\u00e4s\\u00e4nen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.09820\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 15 Aug 2023 11:43:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at DCASE 2023 Workshop\\u00a7r"}']}
{title:'Çaylı et al. (§72023§r)', author: 'Özkan Çaylı; Xubo Liu; Volkan Kılıç; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2306.09947", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKnowledge Distillation for Efficient Audio-Visual Video Captioning\\u00a7r\\n\\n\\u00a78\\u00a7o\\u00d6zkan \\u00c7ayl\\u0131\\nXubo Liu\\nVolkan K\\u0131l\\u0131\\u00e7\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.09947\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 16 Jun 2023 16:28:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oEuropean Signal Processing Conference (EUSIPCO 2023)\\u00a7r"}']}
{title:'Zhao et al. (§72023§r)', author: 'Zhuoran Zhao; Jinbin Bai; Delong Chen; Debang Wang; Yubo Pan', display:{Lore:['[{"text": "arXiv:2306.10065", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTaming Diffusion Models for Music-driven Conducting Motion Generation\\u00a7r\\n\\n\\u00a78\\u00a7oZhuoran Zhao\\nJinbin Bai\\nDelong Chen\\nDebang Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10065\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 13 Nov 2023 08:44:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by AAAI 2023 Summer Symposium with Best Paper Award\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Hanxue Zhang; Zeyu Xie; Xuenan Xu; Mengyue Wu; Kai Yu', display:{Lore:['[{"text": "arXiv:2306.10090", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Audio Caption Fluency with Automatic Error Correction\\u00a7r\\n\\n\\u00a78\\u00a7oHanxue Zhang\\nZeyu Xie\\nXuenan Xu\\nMengyue Wu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10090\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 16 Jun 2023 13:37:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by NCMMSC 2022\\u00a7r"}']}
{title:'Oliveira et al. (§72023§r)', author: 'Frederico S. Oliveira; Edresson Casanova; Arnaldo Cândido Júnior; Anderson S. Soares; Arlindo R. Galvão Filho', display:{Lore:['[{"text": "arXiv:2306.10097", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCML-TTS A Multilingual Dataset for Speech Synthesis in Low-Resource Languages\\u00a7r\\n\\n\\u00a78\\u00a7oFrederico S. Oliveira\\nEdresson Casanova\\nArnaldo C\\u00e2ndido J\\u00fanior\\nAnderson S. Soares\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10097\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 16 Jun 2023 17:17:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 5 figures, Accepted at the 25th International Conference on Text, Speech and Dialogue (TSD 2022)\\u00a7r"}']}
{title:'Lakshminarayana et al. (§72023§r)', author: 'Kishor Kayyar Lakshminarayana; Christian Dittmar; Nicola Pia; Emanuël Habets', display:{Lore:['[{"text": "arXiv:2306.10152", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Resource Text-to-Speech Using Specific Data and Noise Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oKishor Kayyar Lakshminarayana\\nChristian Dittmar\\nNicola Pia\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10152\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 16 Jun 2023 19:42:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at EUSIPCO-2023, Helsinki\\u00a7r"}']}
{title:'Yadav et al. (§72023§r)', author: 'Manuj Yadav; Densil Cabrera; James Love; Jungsoo Kim; Jonothan Holmes; Hugo Caldwell; Richard de Dear', display:{Lore:['[{"text": "arXiv:2306.10268", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReliability and repeatability of ISO 3382-3 metrics based on repeated acoustic measurements in open-plan offices\\u00a7r\\n\\n\\u00a78\\u00a7oManuj Yadav\\nDensil Cabrera\\nJames Love\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10268\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.apacoust.2019.02.010\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 17 Jun 2023 06:10:52 GMT)\\u00a7r"}']}
{title:'Yadav et al. (§72023§r)', author: 'Manuj Yadav; Densil Cabrera', display:{Lore:['[{"text": "arXiv:2306.10269", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTwo simultaneous talkers distract more than one in simulated multi-talker environments, regardless of overall sound levels typical of open-plan offices\\u00a7r\\n\\n\\u00a78\\u00a7oManuj Yadav\\nDensil Cabrera\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10269\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.apacoust.2018.12.007\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 17 Jun 2023 06:14:21 GMT)\\u00a7r"}']}
{title:'Yadav et al. (§72023§r)', author: 'Manuj Yadav; Densil Cabrera', display:{Lore:['[{"text": "arXiv:2306.10271", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutophonic Loudness of Singers in Simulated Room Acoustic Environments\\u00a7r\\n\\n\\u00a78\\u00a7oManuj Yadav\\nDensil Cabrera\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10271\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.jvoice.2016.09.016\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 17 Jun 2023 06:34:05 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Lingwen Liu; Yuxuan Feng; Haitao Fu; Yajie Yang; Xin Pan; Chenlei Jin', display:{Lore:['[{"text": "arXiv:2306.10499", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChannel-Spatial-Based Few-Shot Bird Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oLingwen Liu\\nYuxuan Feng\\nHaitao Fu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10499\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 25 Jun 2023 08:45:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2023 Asia Pacific Signal and Information ProcessingAssociation Annual Summit and Conference\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Zhichao Wang; Yuanzhe Chen; Lei Xie; Qiao Tian; Yuping Wang', display:{Lore:['[{"text": "arXiv:2306.10521", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oZhichao Wang\\nYuanzhe Chen\\nLei Xie\\nQiao Tian\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10521\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 21 Aug 2023 02:21:06 GMT)\\u00a7r"}']}
{title:'Raj et al. (§72023§r)', author: 'Desh Raj; Daniel Povey; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:2306.10559", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSURT 2.0: Advances in Transducer-based Multi-talker Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDesh Raj\\nDaniel Povey\\nSanjeev Khudanpur\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10559\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 19 Sep 2023 13:44:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 7 figures. To appear in IEEE TASLP. Project webpage: https://sites.google.com/view/surt2\\u00a7r"}']}
{title:'Hu et al. (§72023§r)', author: 'Yuchen Hu; Ruizhe Li; Chen Chen; Chengwei Qin; Qiushi Zhu; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2306.10563", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHearing Lips in Noise: Universal Viseme-Phoneme Mapping and Transfer for Robust Audio-Visual Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYuchen Hu\\nRuizhe Li\\nChen Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10563\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 18 Jun 2023 13:53:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o19 pages, 9 figures, Accepted by ACL 2023\\u00a7r"}']}
{title:'Hu et al. (§72023§r)', author: 'Yuchen Hu; Chen Chen; Ruizhe Li; Heqing Zou; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2306.10567", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMIR-GAN: Refining Frame-Level Modality-Invariant Representations with Adversarial Network for Audio-Visual Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYuchen Hu\\nChen Chen\\nRuizhe Li\\nHeqing Zou\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10567\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 18 Jun 2023 14:02:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages, 5 figures, Accepted by ACL 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Helin Wang; Thomas Thebaud; Jesus Villalba; Myra Sydnor; Becky Lammers; Najim Dehak; Laureano Moro-Velazquez', display:{Lore:['[{"text": "arXiv:2306.10588", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDuTa-VC: A Duration-aware Typical-to-atypical Voice Conversion Approach with Diffusion Probabilistic Model\\u00a7r\\n\\n\\u00a78\\u00a7oHelin Wang\\nThomas Thebaud\\nJesus Villalba\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10588\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 18 Jun 2023 15:55:16 GMT)\\u00a7r"}']}
{title:'Eeckt et al. (§72023§r)', author: 'Steven Vander Eeckt; Hugo Van hamme', display:{Lore:['[{"text": "arXiv:2306.10860", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRehearsal-Free Online Continual Learning for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSteven Vander Eeckt\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.10860\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Jun 2023 11:24:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2023. 5 pages\\u00a7r"}']}
{title:'Abbas et al. (§72023§r)', author: 'Ammar Abbas; Sri Karlapati; Bastian Schnell; Penny Karanasou; Marcel Granero Moya; Amith Nagaraj; Ayman Boustati; Nicole Peinelt; Alexis Moinet; Thomas Drugman', display:{Lore:['[{"text": "arXiv:2306.11327", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7leCat: An End-to-End Model for Multi-Speaker TTS     Many-to-Many Fine-Grained Prosody Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oAmmar Abbas\\nSri Karlapati\\nBastian Schnell\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.11327\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Jun 2023 06:50:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to be published in the Proceedings of InterSpeech 2023\\u00a7r"}']}
{title:'Ji et al. (§72023§r)', author: 'Junwei Ji; Dongyuan Shi; Woon-Seng Gan; Xiaoyi Shen; Zhengding Luo', display:{Lore:['[{"text": "arXiv:2306.11408", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Computation-efficient Online Secondary Path Modeling Technique for Modified FXLMS Algorithm\\u00a7r\\n\\n\\u00a78\\u00a7oJunwei Ji\\nDongyuan Shi\\nWoon-Seng Gan\\nXiaoyi Shen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.11408\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Jun 2023 09:32:13 GMT)\\u00a7r"}']}
{title:'Min et al. (§72023§r)', author: 'Deokki Min; Hyeonuk Nam; Yong-Hwa Park', display:{Lore:['[{"text": "arXiv:2306.11427", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAuditory Neural Response Inspired Sound Event Detection Based on Spectro-temporal Receptive Field\\u00a7r\\n\\n\\u00a78\\u00a7oDeokki Min\\nHyeonuk Nam\\nYong-Hwa Park\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.11427\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Jun 2023 10:15:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to DCASE 2023 Workshop\\u00a7r"}']}
{title:'Pezzoli et al. (§72023§r)', author: 'Mirco Pezzoli; Fabio Antonacci; Augusto Sarti', display:{Lore:['[{"text": "arXiv:2306.11509", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImplicit neural representation with physics-informed neural networks for the reconstruction of the early part of room impulse responses\\u00a7r\\n\\n\\u00a78\\u00a7oMirco Pezzoli\\nFabio Antonacci\\nAugusto Sarti\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.11509\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.61782/fa.2023.1182\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Jun 2023 13:01:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at Forum Acusticum 2023\\u00a7r"}']}
{title:'Swiatkowski et al. (§72023§r)', author: 'Jakub Swiatkowski; Duo Wang; Mikolaj Babianski; Patrick Lumban Tobing; Ravichander Vipperla; Vincent Pollet', display:{Lore:['[{"text": "arXiv:2306.11658", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-lingual Prosody Transfer for Expressive Machine Dubbing\\u00a7r\\n\\n\\u00a78\\u00a7oJakub Swiatkowski\\nDuo Wang\\nMikolaj Babianski\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.11658\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Jun 2023 16:28:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH\\u00a7r"}']}
{title:'Swiatkowski et al. (§72023§r)', author: 'Jakub Swiatkowski; Duo Wang; Mikolaj Babianski; Giuseppe Coccia; Patrick Lumban Tobing; Ravichander Vipperla; Viacheslav Klimkov; Vincent Pollet', display:{Lore:['[{"text": "arXiv:2306.11662", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExpressive Machine Dubbing Through Phrase-level Cross-lingual Prosody Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oJakub Swiatkowski\\nDuo Wang\\nMikolaj Babianski\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.11662\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 21 Jun 2023 15:37:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2023\\u00a7r"}']}
{title:'and et al. (§72023§r)', author: 'Paul Primus and; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2306.11764", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Frequency-Wise Normalizations for Better Recording Device Generalization in Audio Spectrogram Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oPaul Primus and\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.11764\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Jun 2023 09:52:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oEUSIPCO 2023\\u00a7r"}']}
{title:'Agrawal et al. (§72023§r)', author: 'Aakriti Agrawal; Milind Rao; Anit Kumar Sahu; Gopinath Chennupati; Andreas Stolcke', display:{Lore:['[{"text": "arXiv:2306.12012", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning When to Trust Which Teacher for Weakly Supervised ASR\\u00a7r\\n\\n\\u00a78\\u00a7oAakriti Agrawal\\nMilind Rao\\nAnit Kumar Sahu\\nGopinath Chennupati\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.12012\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-2205\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech, Aug. 2023, pp. 381-385\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Jun 2023 04:23:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH 2023\\u00a7r"}']}
{title:'Rao et al. (§72023§r)', author: 'Milind Rao; Gopinath Chennupati; Gautam Tiwari; Anit Kumar Sahu; Anirudh Raju; Ariya Rastrow; Jasha Droppo', display:{Lore:['[{"text": "arXiv:2306.12015", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFederated Self-Learning with Weak Supervision for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMilind Rao\\nGopinath Chennupati\\nGautam Tiwari\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.12015\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Jun 2023 04:41:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of ICASSP 2023\\u00a7r"}']}
{title:'Zhou et al. (§72023§r)', author: 'Mohan Zhou; Yalong Bai; Wei Zhang; Ting Yao; Tiejun Zhao; Tao Mei', display:{Lore:['[{"text": "arXiv:2306.12020", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisual-Aware Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oMohan Zhou\\nYalong Bai\\nWei Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.12020\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10095084\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2023 - 2023 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP) 2023, 1-5\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Jun 2023 05:11:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted as oraland top 3\\u00a7r"}']}
{title:'Lemercier et al. (§72023§r)', author: 'Jean-Marie Lemercier; Simon Welker; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2306.12286", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffusion Posterior Sampling for Informed Single-Channel Dereverberation\\u00a7r\\n\\n\\u00a78\\u00a7oJean-Marie Lemercier\\nSimon Welker\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.12286\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Jun 2023 14:14:05 GMT)\\u00a7r"}']}
{title:'Tasnim et al. (§72023§r)', author: 'Mashrura Tasnim; Malikeh Ehghaghi; Brian Diep; Jekaterina Novikova', display:{Lore:['[{"text": "arXiv:2306.12443", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDEPAC: a Corpus for Depression and Anxiety Detection from Speech\\u00a7r\\n\\n\\u00a78\\u00a7oMashrura Tasnim\\nMalikeh Ehghaghi\\nBrian Diep\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.12443\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Jun 2023 12:21:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the Eighth Workshop on Computational Linguistics and Clinical Psychology (CLPsych) at NAACL 2022\\u00a7r"}']}
{title:'Ehghaghi et al. (§72023§r)', author: 'Malikeh Ehghaghi; Marija Stanojevic; Ali Akram; Jekaterina Novikova', display:{Lore:['[{"text": "arXiv:2306.12444", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFactors Affecting the Performance of Automated Speaker Verification in Alzheimer\'s Disease Clinical Trials\\u00a7r\\n\\n\\u00a78\\u00a7oMalikeh Ehghaghi\\nMarija Stanojevic\\nAli Akram\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.12444\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Jun 2023 12:24:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the 5th Clinical Natural Language Processing Workshop (ClinicalNLP) at ACL 2023\\u00a7r"}']}
{title:'Mishra et al. (§72023§r)', author: 'Jagabandhu Mishra; Amartya Chowdhury; S. R. Mahadeva Prasanna', display:{Lore:['[{"text": "arXiv:2306.12913", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImplicit spoken language diarization\\u00a7r\\n\\n\\u00a78\\u00a7oJagabandhu Mishra\\nAmartya Chowdhury\\nS. R. Mahadeva Prasanna\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.12913\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Jun 2023 14:29:53 GMT)\\u00a7r"}']}
{title:'Cui et al. (§72023§r)', author: 'Mingyu Cui; Jiawen Kang; Jiajun Deng; Xi Yin; Yutao Xie; Xie Chen; Xunying Liu', display:{Lore:['[{"text": "arXiv:2306.13307", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Effective and Compact Contextual Representation for Conformer Transducer Speech Recognition Systems\\u00a7r\\n\\n\\u00a78\\u00a7oMingyu Cui\\nJiawen Kang\\nJiajun Deng\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.13307\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 26 Jun 2023 02:48:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Cornell et al. (§72023§r)', author: 'Samuele Cornell; Matthew Wiesner; Shinji Watanabe; Desh Raj; Xuankai Chang; Paola Garcia; Matthew Maciejewski; Yoshiki Masuyama; Zhong-Qiu Wang; Stefano Squartini; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:2306.13734", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios\\u00a7r\\n\\n\\u00a78\\u00a7oSamuele Cornell\\nMatthew Wiesner\\nShinji Watanabe\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.13734\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 14 Jul 2023 09:45:21 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Jie Wang; Zhicong Chen; Haodong Zhou; Lin Li; Qingyang Hong', display:{Lore:['[{"text": "arXiv:2306.14530", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCommunity Detection Graph Convolutional Network for Overlap-Aware Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oJie Wang\\nZhicong Chen\\nHaodong Zhou\\nLin Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.14530\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10095143\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2023-2023 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Jun 2023 09:08:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2023\\u00a7r"}']}
{title:'Deng et al. (§72023§r)', author: 'Jiajun Deng; Guinan Li; Xurong Xie; Zengrui Jin; Mingyu Cui; Tianzi Wang; Shujie Hu; Mengzhe Geng; Xunying Liu', display:{Lore:['[{"text": "arXiv:2306.14608", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFactorised Speaker-environment Adaptive Training of Conformer Speech Recognition Systems\\u00a7r\\n\\n\\u00a78\\u00a7oJiajun Deng\\nGuinan Li\\nXurong Xie\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.14608\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Jun 2023 11:32:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Shuai Wang; Chengdong Liang; Xu Xiang; Bing Han; Zhengyang Chen; Hongji Wang; Wen Ding', display:{Lore:['[{"text": "arXiv:2306.15161", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWespeaker baselines for VoxSRC2023\\u00a7r\\n\\n\\u00a78\\u00a7oShuai Wang\\nChengdong Liang\\nXu Xiang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.15161\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 28 Jun 2023 11:49:49 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Tianzi Wang; Shoukang Hu; Jiajun Deng; Zengrui Jin; Mengzhe Geng; Yi Wang; Helen Meng; Xunying Liu', display:{Lore:['[{"text": "arXiv:2306.15265", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHyper-parameter Adaptation of Conformer ASR Systems for Elderly and Dysarthric Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oTianzi Wang\\nShoukang Hu\\nJiajun Deng\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.15265\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Jun 2023 07:49:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 3 tables, accepted by Interspeech2023\\u00a7r"}']}
{title:'Cong et al. (§72023§r)', author: 'Yahuan Cong; Haoyu Zhang; Haopeng Lin; Shichao Liu; Chunfeng Wang; Yi Ren; Xiang Yin; Zejun Ma', display:{Lore:['[{"text": "arXiv:2306.15304", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerTTS: Pronunciation Disentanglement for Timbre and Style Generalization in Cross-Lingual Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oYahuan Cong\\nHaoyu Zhang\\nHaopeng Lin\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.15304\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Jun 2023 08:47:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Ebbers et al. (§72023§r)', author: 'Janek Ebbers; Reinhold Haeb-Umbach; Romain Serizel', display:{Lore:['[{"text": "arXiv:2306.15440", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPost-Processing Independent Evaluation of Sound Event Detection Systems\\u00a7r\\n\\n\\u00a78\\u00a7oJanek Ebbers\\nReinhold Haeb-Umbach\\nRomain Serizel\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.15440\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Jun 2023 12:54:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to DCASE Workshop 2023\\u00a7r"}']}
{title:'Nigmatulina et al. (§72023§r)', author: 'Iuliia Nigmatulina; Srikanth Madikeri; Esaú Villatoro-Tello; Petr Motliček; Juan Zuluaga-Gomez; Karthik Pandia; Aravind Ganapathiraju', display:{Lore:['[{"text": "arXiv:2306.15685", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImplementing contextual biasing in GPU decoder for online ASR\\u00a7r\\n\\n\\u00a78\\u00a7oIuliia Nigmatulina\\nSrikanth Madikeri\\nEsa\\u00fa Villatoro-Tello\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.15685\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Jun 2023 08:59:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023\\u00a7r"}']}
{title:'Yu et al. (§72023§r)', author: 'Zhongzhi Yu; Yang Zhang; Kaizhi Qian; Yonggan Fu; Yingyan Lin', display:{Lore:['[{"text": "arXiv:2306.15686", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMaster-ASR: Achieving Multilingual Scalability and Low-Resource Adaptation in ASR with Modular Learning\\u00a7r\\n\\n\\u00a78\\u00a7oZhongzhi Yu\\nYang Zhang\\nKaizhi Qian\\nYonggan Fu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.15686\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Jun 2023 16:23:00 GMT)\\u00a7r"}']}
{title:'Le et al. (§72023§r)', author: 'Matthew Le; Apoorv Vyas; Bowen Shi; Brian Karrer; Leda Sari; Rashel Moritz; Mary Williamson; Vimal Manohar; Yossi Adi; Jay Mahadeokar; Wei-Ning Hsu', display:{Lore:['[{"text": "arXiv:2306.15687", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoicebox: Text-Guided Multilingual Universal Speech Generation at Scale\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew Le\\nApoorv Vyas\\nBowen Shi\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.15687\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 19 Oct 2023 13:23:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to NeurIPS 2023\\u00a7r"}']}
{title:'Gu et al. (§72023§r)', author: 'Yile Gu; Prashanth Gurunath Shivakumar; Jari Kolehmainen; Ankur Gandhe; Ariya Rastrow; Ivan Bulyko', display:{Lore:['[{"text": "arXiv:2306.15815", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScaling Laws for Discriminative Speech Recognition Rescoring Models\\u00a7r\\n\\n\\u00a78\\u00a7oYile Gu\\nPrashanth Gurunath Shivakumar\\nJari Kolehmainen\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.15815\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Jun 2023 22:19:44 GMT)\\u00a7r"}']}
{title:'Gitman et al. (§72023§r)', author: 'Igor Gitman; Vitaly Lavrukhin; Aleksandr Laptev; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2306.15824", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConfidence-based Ensembles of End-to-End Speech Recognition Models\\u00a7r\\n\\n\\u00a78\\u00a7oIgor Gitman\\nVitaly Lavrukhin\\nAleksandr Laptev\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.15824\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1281\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Jun 2023 23:13:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proc. INTERSPEECH 2023, August 20-24, 2023, Dublin, Ireland\\u00a7r"}']}
{title:'Nespoli et al. (§72023§r)', author: 'Francesco Nespoli; Daniel Barreda; Joerg Bitzer; Patrick A. Naylor', display:{Lore:['[{"text": "arXiv:2306.16069", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTwo-Stage Voice Anonymization for Enhanced Privacy\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesco Nespoli\\nDaniel Barreda\\nJoerg Bitzer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.16069\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Jun 2023 10:08:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to INTERSPEECH\\u00a7r"}']}
{title:'Nespoli et al. (§72023§r)', author: 'Francesco Nespoli; Jule Pohlhausen; Patrick A. Naylor; Joerg Bitzer', display:{Lore:['[{"text": "arXiv:2306.16071", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLong-term Conversation Analysis: Exploring Utility and Privacy\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesco Nespoli\\nJule Pohlhausen\\nPatrick A. Naylor\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.16071\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Jun 2023 10:10:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ITG Conference on Speech Communication, 2023\\u00a7r"}']}
{title:'Ewert et al. (§72023§r)', author: 'Stephan D. Ewert; Nico Gößling; Oliver Buttler; Steven van de Par; Hongmei Hu', display:{Lore:['[{"text": "arXiv:2306.16696", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComputationally-efficient and perceptually-motivated rendering of diffuse reflections in room acoustics simulation\\u00a7r\\n\\n\\u00a78\\u00a7oStephan D. Ewert\\nNico G\\u00f6\\u00dfling\\nOliver Buttler\\nSteven van de Par\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.16696\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 29 Jun 2023 05:39:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to Forum Acusticum 2023 for publication\\u00a7r"}']}
{title:'Gaznepoglu et al. (§72023§r)', author: 'Ünal Ege Gaznepoglu; Nils Peters', display:{Lore:['[{"text": "arXiv:2306.16860", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning-based F0 Synthesis for Speaker Anonymization\\u00a7r\\n\\n\\u00a78\\u00a7o\\u00dcnal Ege Gaznepoglu\\nNils Peters\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.16860\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 29 Jun 2023 11:12:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, 6 tables, accepted to EUSIPCO 2023\\u00a7r"}']}
{title:'Lu et al. (§72023§r)', author: 'Junchen Lu; Berrak Sisman; Mingyang Zhang; Haizhou Li', display:{Lore:['[{"text": "arXiv:2306.17005", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHigh-Quality Automatic Voice Over with Accurate Alignment: Supervision through Self-Supervised Discrete Speech Units\\u00a7r\\n\\n\\u00a78\\u00a7oJunchen Lu\\nBerrak Sisman\\nMingyang Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.17005\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 29 Jun 2023 15:02:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to INTERSPEECH 2023\\u00a7r"}']}
{title:'Fichna et al. (§72023§r)', author: 'Stefan Fichna; Steven van de Par; Stephan D. Ewert', display:{Lore:['[{"text": "arXiv:2306.17012", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluation of Virtual Acoustic Environments with Different Acoustic Level of Detail\\u00a7r\\n\\n\\u00a78\\u00a7oStefan Fichna\\nSteven van de Par\\nStephan D. Ewert\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.17012\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 10 Aug 2023 14:39:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to the I3DA 2023 International Conference on Immersive and 3D Audio for possible publication. Revised version after review\\u00a7r"}']}
{title:'Duret et al. (§72023§r)', author: 'Jarod Duret; Titouan Parcollet; Yannick Estève', display:{Lore:['[{"text": "arXiv:2306.17199", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Multilingual Expressive Speech Representation for Prosody Prediction without Parallel Data\\u00a7r\\n\\n\\u00a78\\u00a7oJarod Duret\\nTitouan Parcollet\\nYannick Est\\u00e8ve\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.17199\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nSpeech Synthesis Workshop (SSW), Aug 2023, Grenoble, France\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 29 Jun 2023 08:06:54 GMT)\\u00a7r"}']}
{title:'Yu et al. (§72023§r)', author: 'Chin-Yun Yu; György Fazekas', display:{Lore:['[{"text": "arXiv:2306.17252", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSinging Voice Synthesis Using Differentiable LPC and Glottal-Flow-Inspired Wavetables\\u00a7r\\n\\n\\u00a78\\u00a7oChin-Yun Yu\\nGy\\u00f6rgy Fazekas\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.17252\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 13 Jul 2023 02:38:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 4 figures. Accepted at ISMIR 2023\\u00a7r"}']}
{title:'Guo et al. (§72023§r)', author: 'Ning Guo; Tomohiro Nakatani; Shoko Araki; Takehiro Moriya', display:{Lore:['[{"text": "arXiv:2306.17317", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModified Parametric Multichannel Wiener Filter \\nfor Low-latency Enhancement of Speech Mixtures with Unknown Number of Speakers\\u00a7r\\n\\n\\u00a78\\u00a7oNing Guo\\nTomohiro Nakatani\\nShoko Araki\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.17317\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 29 Jun 2023 21:56:25 GMT)\\u00a7r"}']}
{title:'van Bemmel et al. (§72023§r)', author: 'Loes van Bemmel; Zhuoran Liu; Nik Vaessen; Martha Larson', display:{Lore:['[{"text": "arXiv:2306.17700", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.CY\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBeyond Neural-on-Neural Approaches to Speaker Gender Protection\\u00a7r\\n\\n\\u00a78\\u00a7oLoes van Bemmel\\nZhuoran Liu\\nNik Vaessen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.17700\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10096668\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2023 IEEE International Conference on Acoustics, Speech, and\\n  Signal Processing (ICASSP 2023)\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Jun 2023 14:26:49 GMT)\\u00a7r"}']}
{title:'Diatlova et al. (§72023§r)', author: 'Daria Diatlova; Vitaly Shutov', display:{Lore:['[{"text": "arXiv:2307.00024", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmoSpeech: Guiding FastSpeech2 Towards Emotional Text to Speech\\u00a7r\\n\\n\\u00a78\\u00a7oDaria Diatlova\\nVitaly Shutov\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.00024\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Jun 2023 19:34:16 GMT)\\u00a7r"}']}
{title:'Peri et al. (§72023§r)', author: 'Raghuveer Peri; Seyed Omid Sadjadi; Daniel Garcia-Romero', display:{Lore:['[{"text": "arXiv:2307.00169", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoxWatch: An open-set speaker recognition benchmark on VoxCeleb\\u00a7r\\n\\n\\u00a78\\u00a7oRaghuveer Peri\\nSeyed Omid Sadjadi\\nDaniel Garcia-Romero\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.00169\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 30 Jun 2023 23:11:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages\\u00a7r"}']}
{title:'Soman et al. (§72023§r)', author: 'Akshara Soman; Vidhi Sinha; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:2307.00366", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing the EEG Speech Match Mismatch Tasks With Word Boundaries\\u00a7r\\n\\n\\u00a78\\u00a7oAkshara Soman\\nVidhi Sinha\\nSriram Ganapathy\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.00366\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 1 Jul 2023 15:31:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, 4 tables, accepted to Interspeech2023 conference\\u00a7r"}']}
{title:'Guo et al. (§72023§r)', author: 'Houjian Guo; Chaoran Liu; Carlos Toshinori Ishi; Hiroshi Ishiguro', display:{Lore:['[{"text": "arXiv:2307.00393", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing joint training speaker encoder with consistency loss to achieve cross-lingual voice conversion and expressive voice conversion\\u00a7r\\n\\n\\u00a78\\u00a7oHoujian Guo\\nChaoran Liu\\nCarlos Toshinori Ishi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.00393\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 1 Jul 2023 17:44:18 GMT)\\u00a7r"}']}
{title:'Tits et al. (§72023§r)', author: 'Noé Tits; Zoé Broisson', display:{Lore:['[{"text": "arXiv:2307.02051", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFlowchase: a Mobile Application for Pronunciation Training\\u00a7r\\n\\n\\u00a78\\u00a7oNo\\u00e9 Tits\\nZo\\u00e9 Broisson\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.02051\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 5 Jul 2023 06:32:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023 - SLaTE workshop2023 (Speech and Language Technology in Education)\\u00a7r"}']}
{title:'Jacobs et al. (§72023§r)', author: 'Christiaan Jacobs; Herman Kamper', display:{Lore:['[{"text": "arXiv:2307.02083", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging multilingual transfer for unsupervised semantic acoustic word embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oChristiaan Jacobs\\nHerman Kamper\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.02083\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 5 Jul 2023 07:46:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEESPL\\u00a7r"}']}
{title:'Ackermann et al. (§72023§r)', author: 'David Ackermann; Fabian Brinkmann; Stefan Weinzierl', display:{Lore:['[{"text": "arXiv:2307.02110", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Database with Directivities of Musical Instruments\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Ackermann\\nFabian Brinkmann\\nStefan Weinzierl\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.02110\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 5 Jul 2023 08:37:23 GMT)\\u00a7r"}']}
{title:'Chouchane et al. (§72023§r)', author: 'Oubaïda Chouchane; Michele Panariello; Oualid Zari; Ismet Kerenciler; Imen Chihaoui; Massimiliano Todisco; Melek Önen', display:{Lore:['[{"text": "arXiv:2307.02135", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDifferentially Private Adversarial Auto-Encoder to Protect Gender in Voice Biometrics\\u00a7r\\n\\n\\u00a78\\u00a7oOuba\\u00efda Chouchane\\nMichele Panariello\\nOualid Zari\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.02135\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 5 Jul 2023 09:24:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 Pages, 3 figures\\u00a7r"}']}
{title:'Berthommier (§72023§r)', author: 'Frédéric Berthommier', display:{Lore:['[{"text": "arXiv:2307.02299", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhy can big.bi be changed to bi.gbi? A mathematical model of syllabification and articulatory synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oFr\\u00e9d\\u00e9ric Berthommier\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.02299\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 5 Jul 2023 13:58:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, submitted to INTERSPEECH 2023\\u00a7r"}']}
{title:'Miao et al. (§72023§r)', author: 'Haoran Miao; Gaofeng Cheng; Pengyuan Zhang; Yonghong Yan', display:{Lore:['[{"text": "arXiv:2307.02351", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOnline Hybrid CTC/Attention End-to-End Automatic Speech Recognition Architecture\\u00a7r\\n\\n\\u00a78\\u00a7oHaoran Miao\\nGaofeng Cheng\\nPengyuan Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.02351\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2020.2987752\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  Volume 28, 2020, Pages 1452 - 1465\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 5 Jul 2023 15:10:49 GMT)\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Peter Wu; Tingle Li; Yijing Lu; Yubin Zhang; Jiachen Lian; Alan W Black; Louis Goldstein; Shinji Watanabe; Gopala K. Anumanchipalli', display:{Lore:['[{"text": "arXiv:2307.02471", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Speech Synthesis from MRI-Based Articulatory Representations\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Wu\\nTingle Li\\nYijing Lu\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.02471\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 5 Jul 2023 17:45:36 GMT)\\u00a7r"}']}
{title:'Cai et al. (§72023§r)', author: 'Hongmin Cai; Xiaoke Huang; Zhengliang Liu; Wenxiong Liao; Haixing Dai; Zihao Wu; Dajiang Zhu; Hui Ren; Quanzheng Li; Tianming Liu; Xiang Li', display:{Lore:['[{"text": "arXiv:2307.02514", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Multimodal Approaches for Alzheimer\'s Disease Detection Using Patient Speech Transcript and Audio Data\\u00a7r\\n\\n\\u00a78\\u00a7oHongmin Cai\\nXiaoke Huang\\nZhengliang Liu\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.02514\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 5 Jul 2023 12:40:11 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Guinan Li; Jiajun Deng; Mengzhe Geng; Zengrui Jin; Tianzi Wang; Shujie Hu; Mingyu Cui; Helen Meng; Xunying Liu', display:{Lore:['[{"text": "arXiv:2307.02909", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-visual End-to-end Multi-channel Speech Separation, Dereverberation and Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGuinan Li\\nJiajun Deng\\nMengzhe Geng\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.02909\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 6 Jul 2023 10:50:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE/ACM Transactions on Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Deng et al. (§72023§r)', author: 'Keqi Deng; Philip C. Woodland', display:{Lore:['[{"text": "arXiv:2307.03088", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLabel-Synchronous Neural Transducer for End-to-End ASR\\u00a7r\\n\\n\\u00a78\\u00a7oKeqi Deng\\nPhilip C. Woodland\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.03088\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 11 Oct 2023 10:58:53 GMT)\\u00a7r"}']}
{title:'Zarazaga et al. (§72023§r)', author: 'Pablo Pérez Zarazaga; Zofia Malisz', display:{Lore:['[{"text": "arXiv:2307.03168", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRecovering implicit pitch contours from formants in whispered speech\\u00a7r\\n\\n\\u00a78\\u00a7oPablo P\\u00e9rez Zarazaga\\nZofia Malisz\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.03168\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 6 Jul 2023 17:49:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 2 tables, Accepted at ICPhS 2023\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Jian Wu; Yashesh Gaur; Zhuo Chen; Long Zhou; Yimeng Zhu; Tianrui Wang; Jinyu Li; Shujie Liu; Bo Ren; Linquan Liu; Yu Wu', display:{Lore:['[{"text": "arXiv:2307.03917", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn decoder-only architecture for speech-to-text and large language model integration\\u00a7r\\n\\n\\u00a78\\u00a7oJian Wu\\nYashesh Gaur\\nZhuo Chen\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.03917\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 2 Oct 2023 06:57:19 GMT)\\u00a7r"}']}
{title:'Ting et al. (§72023§r)', author: 'Wen-Yuan Ting; Syu-Siang Wang; Yu Tsao; Borching Su', display:{Lore:['[{"text": "arXiv:2307.04179", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIANS: Intelligibility-aware Null-steering Beamforming for Dual-Microphone Arrays\\u00a7r\\n\\n\\u00a78\\u00a7oWen-Yuan Ting\\nSyu-Siang Wang\\nYu Tsao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.04179\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 9 Jul 2023 14:04:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint submitted to IEEE MLSP 2023\\u00a7r"}']}
{title:'Oh et al. (§72023§r)', author: 'Sangshin Oh; Minsung Kang; Hyeongi Moon; Keunwoo Choi; Ben Sangbae Chon', display:{Lore:['[{"text": "arXiv:2307.04292", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Demand-Driven Perspective on Generative Audio AI\\u00a7r\\n\\n\\u00a78\\u00a7oSangshin Oh\\nMinsung Kang\\nHyeongi Moon\\nKeunwoo Choi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.04292\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 Jul 2023 00:58:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 7 figures\\u00a7r"}']}
{title:'Fejgin et al. (§72023§r)', author: 'Daniel Fejgin; Simon Doclo', display:{Lore:['[{"text": "arXiv:2307.04460", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploiting an External Microphone for Binaural RTF-Vector-Based Direction of Arrival Estimation for Multiple Speakers\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Fejgin\\nSimon Doclo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.04460\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 10 Jul 2023 10:13:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper accepted for Forum Acusticum 2023\\u00a7r"}']}
{title:'Chiang et al. (§72023§r)', author: 'Hsin-Tien Chiang; Kuo-Hsuan Hung; Szu-Wei Fu; Heng-Cheng Kuo; Ming-Hsueh Tsai; Yu Tsao', display:{Lore:['[{"text": "arXiv:2307.04517", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStudy on the Correlation between Objective Evaluations and Subjective Speech Quality and Intelligibility\\u00a7r\\n\\n\\u00a78\\u00a7oHsin-Tien Chiang\\nKuo-Hsuan Hung\\nSzu-Wei Fu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.04517\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 10 Oct 2023 04:27:51 GMT)\\u00a7r"}']}
{title:'Comanducci et al. (§72023§r)', author: 'Luca Comanducci; Fabio Antonacci; Augusto Sarti', display:{Lore:['[{"text": "arXiv:2307.04586", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTimbre transfer using image-to-image denoising diffusion implicit models\\u00a7r\\n\\n\\u00a78\\u00a7oLuca Comanducci\\nFabio Antonacci\\nAugusto Sarti\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.04586\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 28 Jul 2023 06:24:06 GMT)\\u00a7r"}']}
{title:'Thienpondt et al. (§72023§r)', author: 'Jenthe Thienpondt; Caroline M. Speksnijder; Kris Demuynck', display:{Lore:['[{"text": "arXiv:2307.04744", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBehavioral Analysis of Pathological Speaker Embeddings of Patients During Oncological Treatment of Oral Cancer\\u00a7r\\n\\n\\u00a78\\u00a7oJenthe Thienpondt\\nCaroline M. Speksnijder\\nKris Demuynck\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.04744\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 12 Aug 2023 09:28:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oproceedings of INTERSPEECH 2023\\u00a7r"}']}
{title:'Kafentzis et al. (§72023§r)', author: 'George P. Kafentzis; Stephane Tetsing; Joe Brew; Lola Jover; Mindaugas Galvosas; Carlos Chaccour; Peter M. Small', display:{Lore:['[{"text": "arXiv:2307.04842", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPredicting Tuberculosis from Real-World Cough Audio Recordings and Metadata\\u00a7r\\n\\n\\u00a78\\u00a7oGeorge P. Kafentzis\\nStephane Tetsing\\nJoe Brew\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.04842\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 12 Jul 2023 16:03:46 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Siyang Wang; Gustav Eje Henter; Joakim Gustafson; Éva Székely', display:{Lore:['[{"text": "arXiv:2307.05132", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Use of Self-Supervised Speech Representations in Spontaneous Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oSiyang Wang\\nGustav Eje Henter\\nJoakim Gustafson\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.05132\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Jul 2023 09:22:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 2 figures. 12th ISCA Speech Synthesis Workshop (SSW) 2023\\u00a7r"}']}
{title:'Lam et al. (§72023§r)', author: 'Bhan Lam; Kelvin Chee Quan Lim; Kenneth Ooi; Zhen-Ting Ong; Dongyuan Shi; Woon-Seng Gan', display:{Lore:['[{"text": "arXiv:2307.05533", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnti-noise window: Subjective perception of active noise reduction and effect of informational masking\\u00a7r\\n\\n\\u00a78\\u00a7oBhan Lam\\nKelvin Chee Quan Lim\\nKenneth Ooi\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.05533\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.scs.2023.104763\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nSustain. Cities Soc., 104763, 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 8 Jul 2023 07:39:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted manuscript submitted toSustainable Cities and Society\\u00a7r"}']}
{title:'Sharma et al. (§72023§r)', author: 'Aayush Kumar Sharma; Vineet Bhavikatti; Amogh Nidawani; Dr. Siddappaji; Sanath P; Dr Geetishree Mishra', display:{Lore:['[{"text": "arXiv:2307.05637", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Diarization and ASR with GMM\\u00a7r\\n\\n\\u00a78\\u00a7oAayush Kumar Sharma\\nVineet Bhavikatti\\nAmogh Nidawani\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.05637\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 11 Jul 2023 09:25:39 GMT)\\u00a7r"}']}
{title:'van Niekerk et al. (§72023§r)', author: 'Benjamin van Niekerk; Marc-André Carbonneau; Herman Kamper', display:{Lore:['[{"text": "arXiv:2307.06040", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRhythm Modeling for Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oBenjamin van Niekerk\\nMarc-Andr\\u00e9 Carbonneau\\nHerman Kamper\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.06040\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 12 Jul 2023 09:35:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, 4 tables, submitted to IEEE Signal Processing Letters\\u00a7r"}']}
{title:'Ghani et al. (§72023§r)', author: 'Burooj Ghani; Tom Denton; Stefan Kahl; Holger Klinck', display:{Lore:['[{"text": "arXiv:2307.06292", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGlobal birdsong embeddings enable superior transfer learning for bioacoustic classification\\u00a7r\\n\\n\\u00a78\\u00a7oBurooj Ghani\\nTom Denton\\nStefan Kahl\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.06292\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1038/s41598-023-49989-z\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 17 Nov 2023 08:27:34 GMT)\\u00a7r"}']}
{title:'Büthe et al. (§72023§r)', author: 'Jan Büthe; Jean-Marc Valin; Ahmed Mustafa', display:{Lore:['[{"text": "arXiv:2307.06610", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLACE: A light-weight, causal model for enhancing coded speech through adaptive convolutions\\u00a7r\\n\\n\\u00a78\\u00a7oJan B\\u00fcthe\\nJean-Marc Valin\\nAhmed Mustafa\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.06610\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 13 Jul 2023 08:17:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted at WASPAA 2023\\u00a7r"}']}
{title:'Delgado et al. (§72023§r)', author: 'Pablo M. Delgado; Jürgen Herre', display:{Lore:['[{"text": "arXiv:2307.06656", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Improved Metric of Informational Masking for Perceptual Audio Quality Measurement\\u00a7r\\n\\n\\u00a78\\u00a7oPablo M. Delgado\\nJ\\u00fcrgen Herre\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.06656\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 13 Jul 2023 09:59:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted Publication for WASPAA 2023 - IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, Mohonk Mountain House,New Paltz, NY, USA, Oct 22-25, 2023\\u00a7r"}']}
{title:'Kolehmainen et al. (§72023§r)', author: 'Jari Kolehmainen; Yile Gu; Aditya Gourav; Prashanth Gurunath Shivakumar; Ankur Gandhe; Ariya Rastrow; Ivan Bulyko', display:{Lore:['[{"text": "arXiv:2307.06832", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPersonalization for BERT-based Discriminative Speech Recognition Rescoring\\u00a7r\\n\\n\\u00a78\\u00a7oJari Kolehmainen\\nYile Gu\\nAditya Gourav\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.06832\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 13 Jul 2023 15:54:32 GMT)\\u00a7r"}']}
{title:'Joly et al. (§72023§r)', author: 'Arnaud Joly; Marco Nicolis; Ekaterina Peterova; Alessandro Lombardi; Ammar Abbas; Arent van Korlaar; Aman Hussain; Parul Sharma; Alexis Moinet; Mateusz Lajszczak; Penny Karanasou; Antonio Bonafonte; Thomas Drugman; Elena Sokolova', display:{Lore:['[{"text": "arXiv:2307.07062", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lControllable Emphasis with zero data for text-to-speech\\u00a7r\\n\\n\\u00a78\\u00a7oArnaud Joly\\nMarco Nicolis\\nEkaterina Peterova\\n+ 10 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.07062\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 13 Jul 2023 21:06:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn proceeding of 12th Speech Synthesis Workshop (SSW) 2023\\u00a7r"}']}
{title:'Cao et al. (§72023§r)', author: 'Faxian Cao; Yongqiang Cheng; Adil Mehmood Khan; Zhijing Yang; S. M. Ahsan Kazmiand Yingxiu Chang', display:{Lore:['[{"text": "arXiv:2307.07096", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow Rank Properties for Estimating Microphones Start Time and Sources Emission Time\\u00a7r\\n\\n\\u00a78\\u00a7oFaxian Cao\\nYongqiang Cheng\\nAdil Mehmood Khan\\nZhijing Yang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.07096\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 22 Jul 2023 03:56:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages for main content;9 pages for proof of proposed low rank properties; 13 figures\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Jiarui Wang; Thushara Abhayapala; Jihui Aimee Zhang; Prasanga Samarasinghe', display:{Lore:['[{"text": "arXiv:2307.07200", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReproducing the Acoustic Velocity Vectors in a Spherical Listening Region\\u00a7r\\n\\n\\u00a78\\u00a7oJiarui Wang\\nThushara Abhayapala\\nJihui Aimee Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.07200\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 20 Oct 2023 04:29:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEESignal Processing Letters\\u00a7r"}']}
{title:'Krishna et al. (§72023§r)', author: 'Varun Krishna; Tarun Sai; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:2307.07325", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRepresentation Learning With Hidden Unit Clustering For Low Resource Speech Applications\\u00a7r\\n\\n\\u00a78\\u00a7oVarun Krishna\\nTarun Sai\\nSriram Ganapathy\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.07325\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 14 Jul 2023 13:02:10 GMT)\\u00a7r"}']}
{title:'Lai et al. (§72023§r)', author: 'Richard Lee Lai; Jen-Cheng Hou; Mandar Gogate; Kia Dashtipour; Amir Hussain; Yu Tsao', display:{Lore:['[{"text": "arXiv:2307.07748", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-Visual Speech Enhancement Using Self-supervised Learning to Improve Speech Intelligibility in Cochlear Implant Simulations\\u00a7r\\n\\n\\u00a78\\u00a7oRichard Lee Lai\\nJen-Cheng Hou\\nMandar Gogate\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.07748\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 15 Jul 2023 09:05:57 GMT)\\u00a7r"}']}
{title:'Singh et al. (§72023§r)', author: 'Abhayjeet Singh; Arjun Singh Mehta; Ashish Khuraishi K S; Deekshitha G; Gauri Date; Jai Nanavati; Jesuraja Bandekar; Karnalius Basumatary; Karthika P; Sandhya Badiger; Sathvik Udupa; Saurabh Kumar; Savitha; Prasanta Kumar Ghosh; Prashanthi V; Priyanka Pai; Raoul Nanavati; Rohan Saxena; Sai Praneeth Reddy Mora; Srinivasa Raghavan', display:{Lore:['[{"text": "arXiv:2307.07948", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModel Adaptation for ASR in low-resource Indian Languages\\u00a7r\\n\\n\\u00a78\\u00a7oAbhayjeet Singh\\nArjun Singh Mehta\\nAshish Khuraishi K S\\n+ 16 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.07948\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 16 Jul 2023 05:25:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oASRU Special session overview paper\\u00a7r"}']}
{title:'Hu et al. (§72023§r)', author: 'Yuchen Hu; Chen Chen; Ruizhe Li; Qiushi Zhu; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2307.08029", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoise-aware Speech Enhancement using Diffusion Probabilistic Model\\u00a7r\\n\\n\\u00a78\\u00a7oYuchen Hu\\nChen Chen\\nRuizhe Li\\nQiushi Zhu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.08029\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 16 Jul 2023 12:46:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures\\u00a7r"}']}
{title:'Han et al. (§72023§r)', author: 'Bing Han; Zhengyang Chen; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2307.08205", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Binary Classification Loss For Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oBing Han\\nZhengyang Chen\\nYanmin Qian\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.08205\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Jul 2023 02:48:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Ling et al. (§72023§r)', author: 'Shaoshi Ling; Yuxuan Hu; Shuangbei Qian; Guoli Ye; Yao Qian; Yifan Gong; Ed Lin; Michael Zeng', display:{Lore:['[{"text": "arXiv:2307.08234", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdapting Large Language Model with Speech for Fully Formatted End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oShaoshi Ling\\nYuxuan Hu\\nShuangbei Qian\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.08234\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 3 Aug 2023 02:23:04 GMT)\\u00a7r"}']}
{title:'Huang et al. (§72023§r)', author: 'Siwei Huang; Jianfeng Chen; Jisheng Bai; Yafei Jia; Dongzhe Zhang', display:{Lore:['[{"text": "arXiv:2307.08239", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDynamic Kernel Convolution Network with Scene-dedicate Training for Sound Event Localization and Detection\\u00a7r\\n\\n\\u00a78\\u00a7oSiwei Huang\\nJianfeng Chen\\nJisheng Bai\\nYafei Jia\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.08239\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Jul 2023 04:41:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 6 figures\\u00a7r"}']}
{title:'Panariello et al. (§72023§r)', author: 'Michele Panariello; Massimiliano Todisco; Nicholas Evans', display:{Lore:['[{"text": "arXiv:2307.08403", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVocoder drift compensation by x-vector alignment in speaker anonymisation\\u00a7r\\n\\n\\u00a78\\u00a7oMichele Panariello\\nMassimiliano Todisco\\nNicholas Evans\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.08403\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Jul 2023 11:38:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the ISCA SPSC Symposium 2023\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Shilong Wu; Jun Du; Maokui He; Shutong Niu; Hang Chen; Haitao Tang; Chin-Hui Lee', display:{Lore:['[{"text": "arXiv:2307.08688", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-supervised multi-channel speaker diarization with cross-channel attention\\u00a7r\\n\\n\\u00a78\\u00a7oShilong Wu\\nJun Du\\nMaokui He\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.08688\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Jul 2023 17:48:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages,3 figures\\u00a7r"}']}
{title:'Marmor et al. (§72023§r)', author: 'Yanir Marmor; Kinneret Misgav; Yair Lifshitz', display:{Lore:['[{"text": "arXiv:2307.08720", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7livrit.ai: A Comprehensive Dataset of Hebrew Speech for AI Research and Development\\u00a7r\\n\\n\\u00a78\\u00a7oYanir Marmor\\nKinneret Misgav\\nYair Lifshitz\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.08720\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Jul 2023 04:19:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 1 table and 3 figures\\u00a7r"}']}
{title:'Westhausen et al. (§72023§r)', author: 'Nils L. Westhausen; Bernd T. Meyer', display:{Lore:['[{"text": "arXiv:2307.08858", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow bit rate binaural link for improved ultra low-latency low-complexity multichannel speech enhancement in Hearing Aids\\u00a7r\\n\\n\\u00a78\\u00a7oNils L. Westhausen\\nBernd T. Meyer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.08858\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 17 Jul 2023 21:33:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at WASPAA 2023\\u00a7r"}']}
{title:'Szwajcowski (§72023§r)', author: 'Adam Szwajcowski', display:{Lore:['[{"text": "arXiv:2307.09352", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient representation of head-related transfer functions in continuous space-frequency domains\\u00a7r\\n\\n\\u00a78\\u00a7oAdam Szwajcowski\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.09352\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.jsv.2023.117850\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJournal of Sound and Vibration, vol. 563, paper no. 117850, 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 Jul 2023 15:38:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o33 pages, 9 figures, preprint of published paper submitted for green open access to fulfill funding institution mandate\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yinghao Aaron Li; Cong Han; Nima Mesgarani', display:{Lore:['[{"text": "arXiv:2307.09435", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSLMGAN: Exploiting Speech Language Model Representations for Unsupervised Zero-Shot Voice Conversion in GANs\\u00a7r\\n\\n\\u00a78\\u00a7oYinghao Aaron Li\\nCong Han\\nNima Mesgarani\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.09435\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 18 Jul 2023 17:09:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWASPAA 2023\\u00a7r"}']}
{title:'Lin et al. (§72023§r)', author: 'Jingru Lin; Xianghu Yue; Junyi Ao; Haizhou Li', display:{Lore:['[{"text": "arXiv:2307.09871", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Acoustic Word Embedding Learning via Correspondence Transformer Encoder\\u00a7r\\n\\n\\u00a78\\u00a7oJingru Lin\\nXianghu Yue\\nJunyi Ao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.09871\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 19 Jul 2023 10:03:08 GMT)\\u00a7r"}']}
{title:'Stan et al. (§72023§r)', author: "Adriana Stan; Johannah O'Mahony", display:{Lore:['[{"text": "arXiv:2307.09898", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn analysis on the effects of speaker embedding choice in non auto-regressive TTS\\u00a7r\\n\\n\\u00a78\\u00a7oAdriana Stan\\nJohannah O\'Mahony\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.09898\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 19 Jul 2023 10:57:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at ISCA Speech Synthesis Workshop 2023\\u00a7r"}']}
{title:'K. et al. (§72023§r)', author: 'Vrindha M. K.; Geethu V.; Anurenjan P. R.; Deepak S.; Sreeni K. G.', display:{Lore:['[{"text": "arXiv:2307.10005", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAlzheimer\'s Disease Detection from Spontaneous Speech and Text: A review\\u00a7r\\n\\n\\u00a78\\u00a7oVrindha M. K.\\nGeethu V.\\nAnurenjan P. R.\\nDeepak S.\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.10005\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 19 Jul 2023 14:42:37 GMT)\\u00a7r"}']}
{title:'Liao et al. (§72023§r)', author: 'Feng-Ting Liao; Yung-Chieh Chan; Yi-Chang Chen; Chan-Jan Hsu; Da-shan Shiu', display:{Lore:['[{"text": "arXiv:2307.10274", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning\\u00a7r\\n\\n\\u00a78\\u00a7oFeng-Ting Liao\\nYung-Chieh Chan\\nYi-Chang Chen\\nChan-Jan Hsu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.10274\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 6 Oct 2023 03:41:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oF-T Liao and Y-C Chan contributed equally; paper accepted to ASRU2023; code and model weights available in https://github.com/mtkresearch/clairaudience\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Wonbin Kim; Hyun-seo Shin; Ju-ho Kim; Jungwoo Heo; Chan-yeong Lim; Ha-Jin Yu', display:{Lore:['[{"text": "arXiv:2307.10628", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPAS: Partial Additive Speech Data Augmentation Method for Noise Robust Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oWonbin Kim\\nHyun-seo Shin\\nJu-ho Kim\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.10628\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 20 Jul 2023 06:50:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 1 table, accepted to CKAIA2023 as a conference paper\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Changhong Wang; Gaël Richard; Brian McFee', display:{Lore:['[{"text": "arXiv:2307.10834", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransfer Learning and Bias Correction with Pre-trained Audio Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oChanghong Wang\\nGa\\u00ebl Richard\\nBrian McFee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.10834\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 20 Jul 2023 12:53:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages, 3 figures, accepted to the conference of the International Society for Music Information Retrieval (ISMIR 2023)\\u00a7r"}']}
{title:'van Dalen (§72023§r)', author: 'Rogier van Dalen', display:{Lore:['[{"text": "arXiv:2307.10975", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGlobally Normalising the Transducer for Streaming Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oRogier van Dalen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.10975\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 20 Jul 2023 16:04:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages plus referencesand appendices\\u00a7r"}']}
{title:'Porjazovski et al. (§72023§r)', author: 'Dejan Porjazovski; Tamás Grósz; Mikko Kurimo', display:{Lore:['[{"text": "arXiv:2307.11450", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTopic Identification For Spontaneous Speech: Enriching Audio Features With Embedded Linguistic Information\\u00a7r\\n\\n\\u00a78\\u00a7oDejan Porjazovski\\nTam\\u00e1s Gr\\u00f3sz\\nMikko Kurimo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.11450\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 Jul 2023 09:30:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to EUSIPCO 2023\\u00a7r"}']}
{title:'Fathullah et al. (§72023§r)', author: 'Yassir Fathullah; Chunyang Wu; Egor Lakomkin; Junteng Jia; Yuan Shangguan; Ke Li; Jinxi Guo; Wenhan Xiong; Jay Mahadeokar; Ozlem Kalinli; Christian Fuegen; Mike Seltzer', display:{Lore:['[{"text": "arXiv:2307.11795", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrompting Large Language Models with Speech Recognition Abilities\\u00a7r\\n\\n\\u00a78\\u00a7oYassir Fathullah\\nChunyang Wu\\nEgor Lakomkin\\n+ 8 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.11795\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 21 Jul 2023 08:39:15 GMT)\\u00a7r"}']}
{title:'Koo et al. (§72023§r)', author: 'Junghyun Koo; Yunkee Chae; Chang-Bin Jeon; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2307.12576", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-refining of Pseudo Labels for Music Source Separation with Noisy Labeled Data\\u00a7r\\n\\n\\u00a78\\u00a7oJunghyun Koo\\nYunkee Chae\\nChang-Bin Jeon\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.12576\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jul 2023 07:47:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o24th International Society for Music Information Retrieval Conference (ISMIR 2023)\\u00a7r"}']}
{title:'Tsunoo et al. (§72023§r)', author: 'Emiru Tsunoo; Hayato Futami; Yosuke Kashiwagi; Siddhant Arora; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2307.12767", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntegration of Frame- and Label-synchronous Beam Search for Streaming Encoder-decoder Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oEmiru Tsunoo\\nHayato Futami\\nYosuke Kashiwagi\\nSiddhant Arora\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.12767\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jul 2023 13:12:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for Interspeech 2023\\u00a7r"}']}
{title:'Yakura et al. (§72023§r)', author: 'Hiromu Yakura; Masataka Goto', display:{Lore:['[{"text": "arXiv:2307.13005", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIteraTTA: An interface for exploring both text prompts and audio priors in generating music with text-to-audio models\\u00a7r\\n\\n\\u00a78\\u00a7oHiromu Yakura\\nMasataka Goto\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.13005\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jul 2023 11:00:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the 24th International Society for Music Information Retrieval Conference (ISMIR 2023)\\u00a7r"}']}
{title:'Jain et al. (§72023§r)', author: 'Rishabh Jain; Andrei Barcovschi; Mariam Yiwere; Peter Corcoran; Horia Cucu', display:{Lore:['[{"text": "arXiv:2307.13008", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptation of Whisper models to child speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oRishabh Jain\\nAndrei Barcovschi\\nMariam Yiwere\\nPeter Corcoran\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.13008\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 24 Jul 2023 12:54:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in Interspeech 2023\\u00a7r"}']}
{title:'Jalal et al. (§72023§r)', author: 'Md Asif Jalal; Pablo Peso Parada; Jisi Zhang; Karthikeyan Saravanan; Mete Ozay; Myoungji Han; Jung In Lee; Seokyeong Jung', display:{Lore:['[{"text": "arXiv:2307.13343", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn-Device Speaker Anonymization of Acoustic Embeddings for ASR based onFlexible Location Gradient Reversal Layer\\u00a7r\\n\\n\\u00a78\\u00a7oMd Asif Jalal\\nPablo Peso Parada\\nJisi Zhang\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.13343\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Jul 2023 08:57:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH 2023\\u00a7r"}']}
{title:'Han et al. (§72023§r)', author: 'Chang Han; Xinmeng Xu; Weiping Tu; Yuhong Yang; Yajie Liu', display:{Lore:['[{"text": "arXiv:2307.13888", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring the Interactions between Target Positive and Negative Information for Acoustic Echo Cancellation\\u00a7r\\n\\n\\u00a78\\u00a7oChang Han\\nXinmeng Xu\\nWeiping Tu\\nYuhong Yang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.13888\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Jul 2023 01:28:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2023\\u00a7r"}']}
{title:'Kimura et al. (§72023§r)', author: 'Keisuke Kimura; Shoichi Koyama; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2307.13941", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerceptual Quality Enhancement of Sound Field Synthesis Based on Combination of Pressure and Amplitude Matching\\u00a7r\\n\\n\\u00a78\\u00a7oKeisuke Kimura\\nShoichi Koyama\\nHiroshi Saruwatari\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.13941\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Jul 2023 03:39:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEWorkshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Xingyu Chen; Fei Ma; Amy Bastine; Prasanga Samarasinghe; Huiyuan Sun', display:{Lore:['[{"text": "arXiv:2307.14013", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Field Estimation around a Rigid Sphere with Physics-informed Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oXingyu Chen\\nFei Ma\\nAmy Bastine\\nPrasanga Samarasinghe\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.14013\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Jul 2023 07:52:29 GMT)\\u00a7r"}']}
{title:'Pan et al. (§72023§r)', author: 'Zexu Pan; Marvin Borsdorf; Siqi Cai; Tanja Schultz; Haizhou Li', display:{Lore:['[{"text": "arXiv:2307.14303", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeuroHeed: Neuro-Steered Speaker Extraction using EEG Signals\\u00a7r\\n\\n\\u00a78\\u00a7oZexu Pan\\nMarvin Borsdorf\\nSiqi Cai\\nTanja Schultz\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.14303\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Jul 2023 17:07:27 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Soowon Kim; Young-Eun Lee; Seo-Hyun Lee; Seong-Whan Lee', display:{Lore:['[{"text": "arXiv:2307.14389", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiff-E: Diffusion-based Learning for Decoding Imagined Speech EEG\\u00a7r\\n\\n\\u00a78\\u00a7oSoowon Kim\\nYoung-Eun Lee\\nSeo-Hyun Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.14389\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 26 Jul 2023 07:12:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023\\u00a7r"}']}
{title:'Close et al. (§72023§r)', author: 'George Close; Thomas Hain; Stefan Goetze', display:{Lore:['[{"text": "arXiv:2307.14502", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Effect of Spoken Language on Speech Enhancement using Self-Supervised Speech Representation Loss Functions\\u00a7r\\n\\n\\u00a78\\u00a7oGeorge Close\\nThomas Hain\\nStefan Goetze\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.14502\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 20 Oct 2023 08:55:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at WASPAA 2023\\u00a7r"}']}
{title:'Wen et al. (§72023§r)', author: 'Yutong Wen; You Zhang; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2307.14547", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMitigating Cross-Database Differences for Learning Unified HRTF Representation\\u00a7r\\n\\n\\u00a78\\u00a7oYutong Wen\\nYou Zhang\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.14547\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Jul 2023 00:06:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, accepted by IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)2023\\u00a7r"}']}
{title:'Ma et al. (§72023§r)', author: 'Fei Ma; Thushara D. Abhayapala; Prasanga N. Samarasinghe; Xingyu Chen', display:{Lore:['[{"text": "arXiv:2307.14650", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatial Upsampling of Head-Related Transfer Functions Using a Physics-Informed Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oFei Ma\\nThushara D. Abhayapala\\nPrasanga N. Samarasinghe\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.14650\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 10 Dec 2023 08:57:49 GMT)\\u00a7r"}']}
{title:'Berghi et al. (§72023§r)', author: 'Davide Berghi; Philip J. B. Jackson', display:{Lore:['[{"text": "arXiv:2307.14739", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Inputs for Active Speaker Detection and Localization via Microphone Array\\u00a7r\\n\\n\\u00a78\\u00a7oDavide Berghi\\nPhilip J. B. Jackson\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.14739\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Jul 2023 09:52:14 GMT)\\u00a7r"}']}
{title:'Sulun et al. (§72023§r)', author: 'Serkan Sulun; Pedro Oliveira; Paula Viana', display:{Lore:['[{"text": "arXiv:2307.14783", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmotion4MIDI: a Lyrics-based Emotion-Labeled Symbolic Music Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oSerkan Sulun\\nPedro Oliveira\\nPaula Viana\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.14783\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Jul 2023 11:24:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to 22nd EPIA Conference on Artificial Intelligence (2023)\\u00a7r"}']}
{title:'Yan et al. (§72023§r)', author: 'Jiuqi Yan; Yingxian Chen; W. W. T. Fok', display:{Lore:['[{"text": "arXiv:2307.15101", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetection of Children Abuse by Voice and Audio Classification by Short-Time Fourier Transform Machine Learning implemented on Nvidia Edge GPU device\\u00a7r\\n\\n\\u00a78\\u00a7oJiuqi Yan\\nYingxian Chen\\nW. W. T. Fok\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.15101\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 27 Jul 2023 16:48:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 7 figures, PRAI 2023\\u00a7r"}']}
{title:'Xu et al. (§72023§r)', author: 'Xinmeng Xu; Weiping Tu; Yuhong Yang', display:{Lore:['[{"text": "arXiv:2307.15251", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPCNN: A Lightweight Parallel Conformer Neural Network for Efficient Monaural Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oXinmeng Xu\\nWeiping Tu\\nYuhong Yang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.15251\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Jul 2023 01:34:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2023\\u00a7r"}']}
{title:'Aironi et al. (§72023§r)', author: 'Carlo Aironi; Samuele Cornell; Luca Serafini; Stefano Squartini', display:{Lore:['[{"text": "arXiv:2307.15611", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Time-Frequency Generative Adversarial based method for Audio Packet Loss Concealment\\u00a7r\\n\\n\\u00a78\\u00a7oCarlo Aironi\\nSamuele Cornell\\nLuca Serafini\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.15611\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Jul 2023 15:13:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at EUSIPCO - 31st European Signal Processing Conference, 2023\\u00a7r"}']}
{title:'Seidel et al. (§72023§r)', author: 'Ernst Seidel; Pejman Mowlaee; Tim Fingscheidt', display:{Lore:['[{"text": "arXiv:2307.15630", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Acoustic Echo Suppression with Condition-Aware Training\\u00a7r\\n\\n\\u00a78\\u00a7oErnst Seidel\\nPejman Mowlaee\\nTim Fingscheidt\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.15630\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 28 Jul 2023 15:40:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, accepted to WASPAA 2023\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Xinfa Zhu; Yi Lei; Tao Li; Yongmao Zhang; Hongbin Zhou; Heng Lu; Lei Xie', display:{Lore:['[{"text": "arXiv:2307.15951", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMETTS: Multilingual Emotional Text-to-Speech by Cross-speaker and Cross-lingual Emotion Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oXinfa Zhu\\nYi Lei\\nTao Li\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.15951\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 29 Jul 2023 10:46:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 3 figures\\u00a7r"}']}
{title:'Sun et al. (§72023§r)', author: 'Eric Sun; Jinyu Li; Jian Xue; Yifan Gong', display:{Lore:['[{"text": "arXiv:2307.16332", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPre-training End-to-end ASR Models with Augmented Speech Samples Queried by Text\\u00a7r\\n\\n\\u00a78\\u00a7oEric Sun\\nJinyu Li\\nJian Xue\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.16332\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 30 Jul 2023 22:36:22 GMT)\\u00a7r"}']}
{title:'Lahiri et al. (§72023§r)', author: 'Rimita Lahiri; Tiantian Feng; Rajat Hebbar; Catherine Lord; So Hyun Kim; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2307.16398", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Self Supervised Speech Embeddings for Child-Adult Classification in Interactions involving Children with Autism\\u00a7r\\n\\n\\u00a78\\u00a7oRimita Lahiri\\nTiantian Feng\\nRajat Hebbar\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.16398\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Jul 2023 04:23:44 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Taejun Kim; Juhan Nam', display:{Lore:['[{"text": "arXiv:2307.16425", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAll-In-One Metrical And Functional Structure Analysis With Neighborhood Attentions on Demixed Audio\\u00a7r\\n\\n\\u00a78\\u00a7oTaejun Kim\\nJuhan Nam\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.16425\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Jul 2023 06:20:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper has been accepted for publication at the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2023\\u00a7r"}']}
{title:'Furnon et al. (§72023§r)', author: 'Nicolas Furnon; Romain Serizel; Slim Essid; Irina Illina', display:{Lore:['[{"text": "arXiv:2307.16582", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSAMbA: Speech enhancement with Asynchronous ad-hoc Microphone Arrays\\u00a7r\\n\\n\\u00a78\\u00a7oNicolas Furnon\\nRomain Serizel\\nSlim Essid\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.16582\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Jul 2023 11:38:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to INTERSPEECH 2022\\u00a7r"}']}
{title:'Ribeiro et al. (§72023§r)', author: 'Manuel Sam Ribeiro; Giulia Comini; Jaime Lorenzo-Trueba', display:{Lore:['[{"text": "arXiv:2307.16643", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving grapheme-to-phoneme conversion by learning pronunciations from speech recordings\\u00a7r\\n\\n\\u00a78\\u00a7oManuel Sam Ribeiro\\nGiulia Comini\\nJaime Lorenzo-Trueba\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.16643\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Jul 2023 13:25:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 4 tables. Interspeech 2023\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Guangyan Zhang; Thomas Merritt; Manuel Sam Ribeiro; Biel Tura-Vecino; Kayoko Yanagisawa; Kamil Pokora; Abdelhamid Ezzerg; Sebastian Cygert; Ammar Abbas; Piotr Bilinski; Roberto Barra-Chicote; Daniel Korzekwa; Jaime Lorenzo-Trueba', display:{Lore:['[{"text": "arXiv:2307.16679", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech\\u00a7r\\n\\n\\u00a78\\u00a7oGuangyan Zhang\\nThomas Merritt\\nManuel Sam Ribeiro\\n+ 9 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.16679\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Jul 2023 13:57:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 5 tables. Interspeech 2023\\u00a7r"}']}
{title:'Bruschi et al. (§72023§r)', author: 'Valeria Bruschi; Michela Cantarini; Luca Serafini; Stefano Nobili; Stefania Cecchi; Stefano Squartini', display:{Lore:['[{"text": "arXiv:2307.16809", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn enhanced system for the detection and active cancellation of snoring signals\\u00a7r\\n\\n\\u00a78\\u00a7oValeria Bruschi\\nMichela Cantarini\\nLuca Serafini\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.16809\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Jul 2023 16:28:16 GMT)\\u00a7r"}']}
{title:'Tang (§72023§r)', author: 'Qingming Tang', display:{Lore:['[{"text": "arXiv:2308.00129", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech representation learning: Learning bidirectional encoders with single-view, multi-view, and multi-task methods\\u00a7r\\n\\n\\u00a78\\u00a7oQingming Tang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.00129\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 25 Jul 2023 20:38:55 GMT)\\u00a7r"}']}
{title:'Monesi et al. (§72023§r)', author: 'Mohammad Jalilpour Monesi; Jonas Vanthornhout; Hugo Van hamme; Tom Francart', display:{Lore:['[{"text": "arXiv:2308.00161", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe role of vowel and consonant onsets in neural tracking of natural speech\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Jalilpour Monesi\\nJonas Vanthornhout\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.00161\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 31 Jul 2023 21:26:48 GMT)\\u00a7r"}']}
{title:'Ma et al. (§72023§r)', author: 'Fei Ma; Thushara D. Abhayapala; Prasanga N. Samarasinghe', display:{Lore:['[{"text": "arXiv:2308.00242", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCircumvent spherical Bessel function nulls for open sphere microphone arrays with physics informed neural network\\u00a7r\\n\\n\\u00a78\\u00a7oFei Ma\\nThushara D. Abhayapala\\nPrasanga N. Samarasinghe\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.00242\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Aug 2023 02:50:32 GMT)\\u00a7r"}']}
{title:'Karakonstantis et al. (§72023§r)', author: 'Xenofon Karakonstantis; Efren Fernandez-Grande', display:{Lore:['[{"text": "arXiv:2308.00426", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerative adversarial networks with physical sound field priors\\u00a7r\\n\\n\\u00a78\\u00a7oXenofon Karakonstantis\\nEfren Fernandez-Grande\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.00426\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Aug 2023 10:11:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 11 figures, submitted to the Journal of the Acoustical Society of America\\u00a7r"}']}
{title:'Elshahawy et al. (§72023§r)', author: 'Yousseif Elshahawy; Yassine El Kheir; Shammur Absar Chowdhury; Ahmed Ali', display:{Lore:['[{"text": "arXiv:2308.02503", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMyVoice: Arabic Speech Resource Collaboration Platform\\u00a7r\\n\\n\\u00a78\\u00a7oYousseif Elshahawy\\nYassine El Kheir\\nShammur Absar Chowdhury\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.02503\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 23 Jul 2023 07:13:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2 pages, accepted at InterSpeech23 Show and Tell Session\\u00a7r"}']}
{title:'Zhou et al. (§72023§r)', author: 'Jiuyang Zhou; Hong Zhu; Xingping Wang', display:{Lore:['[{"text": "arXiv:2308.02531", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChoir Transformer: Generating Polyphonic Music with Relative Attention on Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oJiuyang Zhou\\nHong Zhu\\nXingping Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.02531\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 1 Aug 2023 06:44:15 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Yafeng Chen; Siqi Zheng; Hui Wang; Luyao Cheng; Qian Chen; Shiliang Zhang', display:{Lore:['[{"text": "arXiv:2308.02774", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Distillation Network with Ensemble Prototypes: Learning Robust Speaker Representations without Supervision\\u00a7r\\n\\n\\u00a78\\u00a7oYafeng Chen\\nSiqi Zheng\\nHui Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.02774\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 12 Sep 2023 06:03:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2211.04168\\u00a7r"}']}
{title:'Kadiri et al. (§72023§r)', author: 'Sudarsana Reddy Kadiri; Farhad Javanmardi; Paavo Alku', display:{Lore:['[{"text": "arXiv:2308.03226", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigation of Self-supervised Pre-trained Models for Classification of Voice Quality from Speech and Neck Surface Accelerometer Signals\\u00a7r\\n\\n\\u00a78\\u00a7oSudarsana Reddy Kadiri\\nFarhad Javanmardi\\nPaavo Alku\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.03226\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.csl.2023.101550\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 6 Aug 2023 23:16:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Computer Speech Language\\u00a7r"}']}
{title:'Biswas et al. (§72023§r)', author: 'Arijit Biswas; Harald Mundt', display:{Lore:['[{"text": "arXiv:2308.03437", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudioVMAF: Audio Quality Prediction with VMAF\\u00a7r\\n\\n\\u00a78\\u00a7oArijit Biswas\\nHarald Mundt\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.03437\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Aug 2023 09:37:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to 155th Audio Engineering Society (AES), New York, NY, USA,October 2023\\u00a7r"}']}
{title:'Shi et al. (§72023§r)', author: 'Dongyuan Shi; Woon-Seng Gan; Bhan Lam; Shulin Wen; Xiaoyi Shen', display:{Lore:['[{"text": "arXiv:2308.03684", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lActive Noise Control based on the Momentum Multichannel Normalized Filtered-x Least Mean Square Algorithm\\u00a7r\\n\\n\\u00a78\\u00a7oDongyuan Shi\\nWoon-Seng Gan\\nBhan Lam\\nShulin Wen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.03684\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Aug 2023 15:59:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oConference: INTER-NOISE and NOISE-CON Congress and Conference Proceedings 2020 At Korea Volume: 261\\u00a7r"}']}
{title:'Kamo et al. (§72023§r)', author: 'Naoyuki Kamo; Marc Delcroix; Tomohiro Nakatani', display:{Lore:['[{"text": "arXiv:2308.03987", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTarget Speech Extraction with Conditional Diffusion Model\\u00a7r\\n\\n\\u00a78\\u00a7oNaoyuki Kamo\\nMarc Delcroix\\nTomohiro Nakatani\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.03987\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 17 Aug 2023 02:22:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, Interspeech2023\\u00a7r"}']}
{title:'Kuhlmann et al. (§72023§r)', author: 'Michael Kuhlmann; Adrian Meise; Fritz Seebauer; Petra Wagner; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2308.04225", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating Speaker Embedding Disentanglement on Natural Read Speech\\u00a7r\\n\\n\\u00a78\\u00a7oMichael Kuhlmann\\nAdrian Meise\\nFritz Seebauer\\nPetra Wagner\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.04225\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Aug 2023 12:43:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published at 15th ITG conference on speech communication\\u00a7r"}']}
{title:'Primus et al. (§72023§r)', author: 'Paul Primus; Khaled Koutini; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2308.04258", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdvancing Natural-Language Based Audio Retrieval with PaSST and Large Audio-Caption Data Sets\\u00a7r\\n\\n\\u00a78\\u00a7oPaul Primus\\nKhaled Koutini\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.04258\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Aug 2023 13:46:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to DCASE Workshop 2023\\u00a7r"}']}
{title:'Vieting et al. (§72023§r)', author: 'Peter Vieting; Ralf Schlüter; Hermann Ney', display:{Lore:['[{"text": "arXiv:2308.04286", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparative Analysis of the wav2vec 2.0 Feature Extractor\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Vieting\\nRalf Schl\\u00fcter\\nHermann Ney\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.04286\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 8 Aug 2023 14:29:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ITG 2023\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Xubo Liu; Qiuqiang Kong; Yan Zhao; Haohe Liu; Yi Yuan; Yuzhuo Liu; Rui Xia; Yuxuan Wang; Mark D. Plumbley; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2308.05037", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSeparate Anything You Describe\\u00a7r\\n\\n\\u00a78\\u00a7oXubo Liu\\nQiuqiang Kong\\nYan Zhao\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.05037\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 27 Oct 2023 15:34:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCode, benchmark and pre-trained models: https://github.com/Audio-AGI/AudioSep\\u00a7r"}']}
{title:'Zeitler et al. (§72023§r)', author: 'Johannes Zeitler; Simon Deniffel; Michael Krause; Meinard Müller', display:{Lore:['[{"text": "arXiv:2308.05429", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStabilizing Training with Soft Dynamic Time Warping: A Case Study for Pitch Class Estimation with Weakly Aligned Targets\\u00a7r\\n\\n\\u00a78\\u00a7oJohannes Zeitler\\nSimon Deniffel\\nMichael Krause\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.05429\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 10 Aug 2023 08:42:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ISMIR 2023, Milano, Italy\\u00a7r"}']}
{title:'Soleymanpour et al. (§72023§r)', author: 'Mohammad Soleymanpour; Mahmoud Al Ismail; Fahimeh Bahmaninezhad; Kshitiz Kumar; Jian Wu', display:{Lore:['[{"text": "arXiv:2308.06327", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBilingual Streaming ASR with Grapheme units and Auxiliary Monolingual Loss\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Soleymanpour\\nMahmoud Al Ismail\\nFahimeh Bahmaninezhad\\nKshitiz Kumar\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.06327\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 11 Aug 2023 18:06:33 GMT)\\u00a7r"}']}
{title:'Lai et al. (§72023§r)', author: 'Wenqiang Lai; Qihan Yang; Ye Mao; Endong Sun; Jiangnan Ye', display:{Lore:['[{"text": "arXiv:2308.06533", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKnowledge Distilled Ensemble Model for sEMG-based Silent Speech Interface\\u00a7r\\n\\n\\u00a78\\u00a7oWenqiang Lai\\nQihan Yang\\nYe Mao\\nEndong Sun\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.06533\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 7 Aug 2023 03:52:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 5 figures\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Han Zhu; Dongji Gao; Gaofeng Cheng; Daniel Povey; Pengyuan Zhang; Yonghong Yan', display:{Lore:['[{"text": "arXiv:2308.06547", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAlternative Pseudo-Labeling for Semi-Supervised Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHan Zhu\\nDongji Gao\\nGaofeng Cheng\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.06547\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 12 Aug 2023 12:13:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE/ACM Transactionson Audio, Speech and Language Processing (TASLP), 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Xiaofei Wang; Manthan Thakker; Zhuo Chen; Naoyuki Kanda; Sefik Emre Eskimez; Sanyuan Chen; Min Tang; Shujie Liu; Jinyu Li; Takuya Yoshioka', display:{Lore:['[{"text": "arXiv:2308.06873", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeechX: Neural Codec Language Model as a Versatile Speech Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oXiaofei Wang\\nManthan Thakker\\nZhuo Chen\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.06873\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Aug 2023 01:01:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSee https://aka.ms/speechx for demo samples\\u00a7r"}']}
{title:'Lin et al. (§72023§r)', author: 'Yuke Lin; Xiaoyi Qin; Guoqing Zhao; Ming Cheng; Ning Jiang; Haiyang Wu; Ming Li', display:{Lore:['[{"text": "arXiv:2308.07056", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoxBlink: A Large Scale Speaker Verification Dataset on Camera\\u00a7r\\n\\n\\u00a78\\u00a7oYuke Lin\\nXiaoyi Qin\\nGuoqing Zhao\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.07056\\u00a7r\\n\\nVersion:\\u00a77v7 (Wed, 13 Dec 2023 02:24:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted By ICASSP2024\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Wen Wu; Chao Zhang; Philip C. Woodland', display:{Lore:['[{"text": "arXiv:2308.07145", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntegrating Emotion Recognition with Speech Recognition and Speaker Diarisation for Conversations\\u00a7r\\n\\n\\u00a78\\u00a7oWen Wu\\nChao Zhang\\nPhilip C. Woodland\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.07145\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-293\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Aug 2023 13:50:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023\\u00a7r"}']}
{title:'Araujo-Simon (§72023§r)', author: 'Jake Araujo-Simon', display:{Lore:['[{"text": "arXiv:2308.07229", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7acs.SY\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a7eeess.SY\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCompositional nonlinear audio signal processing with Volterra series\\u00a7r\\n\\n\\u00a78\\u00a7oJake Araujo-Simon\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.07229\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 14 Sep 2023 19:59:51 GMT)\\u00a7r"}']}
{title:'Pandey et al. (§72023§r)', author: 'Ruchi Pandey; Santosh Nannuru', display:{Lore:['[{"text": "arXiv:2308.07265", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLocalization of DOA trajectories \\u2013 Beyond the grid\\u00a7r\\n\\n\\u00a78\\u00a7oRuchi Pandey\\nSantosh Nannuru\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.07265\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 14 Aug 2023 16:50:00 GMT)\\u00a7r"}']}
{title:'Cheng et al. (§72023§r)', author: 'Ming Cheng; Weiqing Wang; Xiaoyi Qin; Yuke Lin; Ning Jiang; Guoqing Zhao; Ming Li', display:{Lore:['[{"text": "arXiv:2308.07595", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe DKU-MSXF Diarization System for the VoxCeleb Speaker Recognition Challenge 2023\\u00a7r\\n\\n\\u00a78\\u00a7oMing Cheng\\nWeiqing Wang\\nXiaoyi Qin\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.07595\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 17 Aug 2023 01:54:46 GMT)\\u00a7r"}']}
{title:'Lam et al. (§72023§r)', author: 'Bhan Lam; Zhen-Ting Ong; Kenneth Ooi; Wen-Hui Ong; Trevor Wong; Karn N. Watcharasupat; Woon-Seng Gan', display:{Lore:['[{"text": "arXiv:2308.07767", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPreliminary investigation of the short-term in situ performance of an automatic masker selection system\\u00a7r\\n\\n\\u00a78\\u00a7oBhan Lam\\nZhen-Ting Ong\\nKenneth Ooi\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.07767\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Aug 2023 13:37:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7opaper submitted to the 52nd International Congressand Exposition on Noise Control Engineering held in Chiba, Greater Tokyo, Japan, on 20-23 August 2023 (Inter-Noise 2023)\\u00a7r"}']}
{title:'Park et al. (§72023§r)', author: 'Dongkeon Park; Ji Won Kim; Kang Ryeol Kim; Do Hyun Lee; Hong Kook Kim', display:{Lore:['[{"text": "arXiv:2308.07788", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGIST-AiTeR Speaker Diarization System for VoxCeleb Speaker Recognition Challenge (VoxSRC) 2023\\u00a7r\\n\\n\\u00a78\\u00a7oDongkeon Park\\nJi Won Kim\\nKang Ryeol Kim\\nDo Hyun Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.07788\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 25 Aug 2023 06:07:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oVoxSRC 2023 Track4\\u00a7r"}']}
{title:'Yusuf et al. (§72023§r)', author: 'Bolaji Yusuf; Jan Cernocky; Murat Saraclar', display:{Lore:['[{"text": "arXiv:2308.08027", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Open Vocabulary Keyword Search With Multilingual Neural Representations\\u00a7r\\n\\n\\u00a78\\u00a7oBolaji Yusuf\\nJan Cernocky\\nMurat Saraclar\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.08027\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3301239\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nin IEEE/ACM Transactions on Audio, Speech, and Language\\n  Processing, vol. 31, pp. 3070-3080, 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 15 Aug 2023 20:33:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE/ACM Transactionson Audio, Speech and Language Processing (TASLP), 2023\\u00a7r"}']}
{title:'Torgashov et al. (§72023§r)', author: 'Nikita Torgashov; Rostislav Makarov; Ivan Yakovlev; Pavel Malov; Andrei Balykin; Anton Okhotnikov', display:{Lore:['[{"text": "arXiv:2308.08294", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe ID R   D VoxCeleb Speaker Recognition Challenge 2023 System Description\\u00a7r\\n\\n\\u00a78\\u00a7oNikita Torgashov\\nRostislav Makarov\\nIvan Yakovlev\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.08294\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 20 Aug 2023 09:08:32 GMT)\\u00a7r"}']}
{title:'Braun et al. (§72023§r)', author: 'Franziska Braun; Sebastian P. Bayerl; Paula A. Pérez-Toro; Florian Hönig; Hartmut Lehfeld; Thomas Hillemacher; Elmar Nöth; Tobias Bocklet; Korbinian Riedhammer', display:{Lore:['[{"text": "arXiv:2308.08306", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lClassifying Dementia in the Presence of Depression: A Cross-Corpus Study\\u00a7r\\n\\n\\u00a78\\u00a7oFranziska Braun\\nSebastian P. Bayerl\\nPaula A. P\\u00e9rez-Toro\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.08306\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 16 Aug 2023 12:09:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at INTERSPEECH 2023\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Ze Li; Yuke Lin; Xiaoyi Qin; Ning Jiang; Guoqing Zhao; Ming Li', display:{Lore:['[{"text": "arXiv:2308.08766", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe DKU-MSXF Speaker Verification System for the VoxCeleb Speaker Recognition Challenge 2023\\u00a7r\\n\\n\\u00a78\\u00a7oZe Li\\nYuke Lin\\nXiaoyi Qin\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.08766\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Aug 2023 03:49:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2210.05092\\u00a7r"}']}
{title:'He et al. (§72023§r)', author: 'Liang He; Ruida Li; Mengqi Niu', display:{Lore:['[{"text": "arXiv:2308.08767", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGraph Neural Network Backend for Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oLiang He\\nRuida Li\\nMengqi Niu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.08767\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Aug 2023 03:50:37 GMT)\\u00a7r"}']}
{title:'Hu et al. (§72023§r)', author: 'Jinbo Hu; Yin Cao; Ming Wu; Feiran Yang; Ziying Yu; Wenwu Wang; Mark D. Plumbley; Jun Yang', display:{Lore:['[{"text": "arXiv:2308.08847", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMETA-SELD: Meta-Learning for Fast Adaptation to the new environment in Sound Event Localization and Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJinbo Hu\\nYin Cao\\nMing Wu\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.08847\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Aug 2023 08:10:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to DCASE 2023 Workshop\\u00a7r"}']}
{title:'Tevissen et al. (§72023§r)', author: 'Yannis Tevissen; Dan Istrate; Vincent Zalc; Jérôme Boudy; Gérard Chollet; Frédéric Petitpont; Sami Boutamine', display:{Lore:['[{"text": "arXiv:2308.08985", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHome monitoring for frailty detection through sound and speaker diarization analysis\\u00a7r\\n\\n\\u00a78\\u00a7oYannis Tevissen\\nDan Istrate\\nVincent Zalc\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.08985\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Aug 2023 13:47:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oJETSAN, Jun 2023, Aubervilliers Paris, France\\u00a7r"}']}
{title:'Kadiri et al. (§72023§r)', author: 'Sudarsana Reddy Kadiri; Manila Kodali; Paavo Alku', display:{Lore:['[{"text": "arXiv:2308.09042", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSeverity Classification of Parkinson\'s Disease from Speech using Single Frequency Filtering-based Features\\u00a7r\\n\\n\\u00a78\\u00a7oSudarsana Reddy Kadiri\\nManila Kodali\\nPaavo Alku\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.09042\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-2531\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Aug 2023 15:22:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Alku et al. (§72023§r)', author: 'Paavo Alku; Sudarsana Reddy Kadiri; Dhananjaya Gowda', display:{Lore:['[{"text": "arXiv:2308.09051", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRefining a Deep Learning-based Formant Tracker using Linear Prediction Methods\\u00a7r\\n\\n\\u00a78\\u00a7oPaavo Alku\\nSudarsana Reddy Kadiri\\nDhananjaya Gowda\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.09051\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.csl.2023.101515\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 17 Aug 2023 15:32:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oComputer Speech and Language, Vol. 81, Article 101515, June 2023\\u00a7r"}']}
{title:'Jiang et al. (§72023§r)', author: 'Guanxin Jiang; Lars Villemoes; Arijit Biswas', display:{Lore:['[{"text": "arXiv:2308.09493", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerative Machine Listener\\u00a7r\\n\\n\\u00a78\\u00a7oGuanxin Jiang\\nLars Villemoes\\nArijit Biswas\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.09493\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 18 Aug 2023 12:07:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to 155th Audio Engineering Society (AES) Convention, New York, NY, USA, October 2023\\u00a7r"}']}
{title:'Su et al. (§72023§r)', author: 'Tung-Cheng Su; Yung-Chuan Chang; Yi-Wen Liu', display:{Lore:['[{"text": "arXiv:2308.10021", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEffects of Convolutional Autoencoder Bottleneck Width on StarGAN-based Singing Technique Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oTung-Cheng Su\\nYung-Chuan Chang\\nYi-Wen Liu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.10021\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 19 Aug 2023 14:13:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe original edition of this paper will be published in the CMMR 2023 Proceedings. This ArXivpublication is a copy\\u00a7r"}']}
{title:'Cai et al. (§72023§r)', author: 'Zexin Cai; Weiqing Wang; Yikang Wang; Ming Li', display:{Lore:['[{"text": "arXiv:2308.10281", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe DKU-DUKEECE System for the Manipulation Region Location Task of ADD 2023\\u00a7r\\n\\n\\u00a78\\u00a7oZexin Cai\\nWeiqing Wang\\nYikang Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.10281\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 20 Aug 2023 14:29:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe DKU-DukeECE system description to Task 2 ofAudio Deepfake Detection Challenge (ADD 2023)\\u00a7r"}']}
{title:'Chiu et al. (§72023§r)', author: 'Ching-Yu Chiu; Meinard Müller; Matthew E. P. Davies; Alvin Wen-Yu Su; Yi-Hsuan Yang', display:{Lore:['[{"text": "arXiv:2308.10355", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLocal Periodicity-Based Beat Tracking for Expressive Classical Piano Music\\u00a7r\\n\\n\\u00a78\\u00a7oChing-Yu Chiu\\nMeinard M\\u00fcller\\nMatthew E. P. Davies\\nAlvin Wen-Yu Su\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.10355\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 20 Aug 2023 20:12:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEE/ACM Transactionson Audio, Speech, and Language Processing (July 2023)\\u00a7r"}']}
{title:'Xue et al. (§72023§r)', author: 'Heyang Xue; Shuai Guo; Pengcheng Zhu; Mengxiao Bi', display:{Lore:['[{"text": "arXiv:2308.10428", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-GradSpeech: Towards Diffusion-based Multi-Speaker Text-to-speech Using Consistent Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oHeyang Xue\\nShuai Guo\\nPengcheng Zhu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.10428\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 31 Aug 2023 07:37:39 GMT)\\u00a7r"}']}
{title:'Mishra et al. (§72023§r)', author: 'Jagabandhu Mishra; S. R. Mahadeva Prasanna', display:{Lore:['[{"text": "arXiv:2308.10470", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImplicit Self-supervised Language Representation for Spoken Language Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oJagabandhu Mishra\\nS. R. Mahadeva Prasanna\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.10470\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 21 Aug 2023 05:11:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPlanning to Submit in IEEE-JSTSP\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Hangting Chen; Jianwei Yu; Yi Luo; Rongzhi Gu; Weihua Li; Zhuocheng Lu; Chao Weng', display:{Lore:['[{"text": "arXiv:2308.11053", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUltra Dual-Path Compression For Joint Echo Cancellation And Noise Suppression\\u00a7r\\n\\n\\u00a78\\u00a7oHangting Chen\\nJianwei Yu\\nYi Luo\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.11053\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-2302\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 10 Oct 2023 06:46:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of INTERSPEECH\\u00a7r"}']}
{title:'Gaznepoglu et al. (§72023§r)', author: 'Ünal Ege Gaznepoglu; Nils Peters', display:{Lore:['[{"text": "arXiv:2308.11337", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluation of the Speech Resynthesis Capabilities of the VoicePrivacy Challenge Baseline B1\\u00a7r\\n\\n\\u00a78\\u00a7o\\u00dcnal Ege Gaznepoglu\\nNils Peters\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.11337\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 22 Aug 2023 10:32:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to 3rd Symposium on Security and Privacy of Speech Communication\\u00a7r"}']}
{title:'Takeuchi et al. (§72023§r)', author: 'Daiki Takeuchi; Yasunori Ohishi; Daisuke Niizumi; Noboru Harada; Kunio Kashino', display:{Lore:['[{"text": "arXiv:2308.11923", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement\\u00a7r\\n\\n\\u00a78\\u00a7oDaiki Takeuchi\\nYasunori Ohishi\\nDaisuke Niizumi\\nNoboru Harada\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.11923\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 23 Aug 2023 05:13:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to DCASE2023 Workshop\\u00a7r"}']}
{title:'Hou et al. (§72023§r)', author: 'Yuanbo Hou; Siyang Song; Cheng Luo; Andrew Mitchell; Qiaoqiao Ren; Weicheng Xie; Jian Kang; Wenwu Wang; Dick Botteldooren', display:{Lore:['[{"text": "arXiv:2308.11980", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oYuanbo Hou\\nSiyang Song\\nCheng Luo\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.11980\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 23 Aug 2023 08:05:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023, Code and models: https://github.com/Yuanbo2020/HGRL\\u00a7r"}']}
{title:'Tamm et al. (§72023§r)', author: 'Bastiaan Tamm; Rik Vandenberghe; Hugo Van hamme', display:{Lore:['[{"text": "arXiv:2308.12077", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalysis of XLS-R for Speech Quality Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oBastiaan Tamm\\nRik Vandenberghe\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.12077\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 23 Aug 2023 11:52:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, submitted to WASPAA 2023\\u00a7r"}']}
{title:'Zheng et al. (§72023§r)', author: 'Yu Zheng; Yajun Zhang; Chuanying Niu; Yibin Zhan; Yanhua Long; Dongxing Xu', display:{Lore:['[{"text": "arXiv:2308.12526", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUNISOUND System for VoxCeleb Speaker Recognition Challenge 2023\\u00a7r\\n\\n\\u00a78\\u00a7oYu Zheng\\nYajun Zhang\\nChuanying Niu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.12526\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Aug 2023 03:30:38 GMT)\\u00a7r"}']}
{title:'Jo et al. (§72023§r)', author: 'Byeongho Jo; Seungkwon Beack', display:{Lore:['[{"text": "arXiv:2308.12566", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHybrid noise shaping for audio coding using perfectly overlapped window\\u00a7r\\n\\n\\u00a78\\u00a7oByeongho Jo\\nSeungkwon Beack\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.12566\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 24 Aug 2023 05:12:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to WASPAA (IEEE Workshopon Applications of Signal Processing to Audio and Acoustics) 2023\\u00a7r"}']}
{title:'Deng et al. (§72023§r)', author: 'Keqi Deng; Philip C. Woodland', display:{Lore:['[{"text": "arXiv:2308.13345", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDecoupled Structure for Improved Adaptability of End-to-End Models\\u00a7r\\n\\n\\u00a78\\u00a7oKeqi Deng\\nPhilip C. Woodland\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.13345\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 25 Aug 2023 12:31:12 GMT)\\u00a7r"}']}
{title:'Chouchane et al. (§72023§r)', author: 'Oubaida Chouchane; Michele Panariello; Chiara Galdi; Massimiliano Todisco; Nicholas Evans', display:{Lore:['[{"text": "arXiv:2308.14049", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFairness and Privacy in Voice Biometrics:A Study of Gender Influences Using wav2vec 2.0\\u00a7r\\n\\n\\u00a78\\u00a7oOubaida Chouchane\\nMichele Panariello\\nChiara Galdi\\nMassimiliano Todisco\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.14049\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 27 Aug 2023 09:04:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o7 pages\\u00a7r"}']}
{title:'Ji et al. (§72023§r)', author: 'Shengpeng Ji; Jialong Zuo; Minghui Fang; Ziyue Jiang; Feiyang Chen; Xinyu Duan; Baoxing Huai; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2308.14430", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTextrolSpeech: A Text Style Control Speech Corpus With Codec Language Text-to-Speech Models\\u00a7r\\n\\n\\u00a78\\u00a7oShengpeng Ji\\nJialong Zuo\\nMinghui Fang\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.14430\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP48485.2024.10445879\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2024 IEEE International Conference on Acoustics, Speech and Signal\\n  Processing (ICASSP)\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Aug 2023 09:06:32 GMT)\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Qiushi Zhu; Yu Gu; Rilin Chen; Chao Weng; Yuchen Hu; Lirong Dai; Jie Zhang', display:{Lore:['[{"text": "arXiv:2308.14553", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRep2wav: Noise Robust text-to-speech Using self-supervised representations\\u00a7r\\n\\n\\u00a78\\u00a7oQiushi Zhu\\nYu Gu\\nRilin Chen\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.14553\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 4 Sep 2023 02:24:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages,2 figures\\u00a7r"}']}
{title:'Tuna et al. (§72023§r)', author: 'Cagdas Tuna; Altan Akat; H. Nazim Bicer; Andreas Walther; Emanuël A. P. Habets', display:{Lore:['[{"text": "arXiv:2308.14611", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData-driven 3D Room Geometry Inference with a Linear Loudspeaker Array and a Single Microphone\\u00a7r\\n\\n\\u00a78\\u00a7oCagdas Tuna\\nAltan Akat\\nH. Nazim Bicer\\nAndreas Walther\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.14611\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Aug 2023 14:28:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in Forum Acusticum 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Ruoyu Wang; Maokui He; Jun Du; Hengshun Zhou; Shutong Niu; Hang Chen; Yanyan Yue; Gaobin Yang; Shilong Wu; Lei Sun; Yanhui Tu; Haitao Tang; Shuangqing Qian; Tian Gao; Mengzhi Wang; Genshun Wan; Jia Pan; Jianqing Gao; Chin-Hui Lee', display:{Lore:['[{"text": "arXiv:2308.14638", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe USTC-NERCSLIP Systems for the CHiME-7 DASR Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oRuoyu Wang\\nMaokui He\\nJun Du\\n+ 15 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.14638\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 11 Oct 2023 03:04:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by 2023 CHiME Workshop, Oral\\u00a7r"}']}
{title:'Su et al. (§72023§r)', author: 'Jia-Jyu Su; Pang-Chen Liao; Yen-Ting Lin; Wu-Hao Li; Guan-Ting Liou; Cheng-Che Kao; Wei-Cheng Chen; Jen-Chieh Chiang; Wen-Yang Chang; Pin-Han Lin; Chen-Yu Chiang', display:{Lore:['[{"text": "arXiv:2308.14763", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoiceBank-2023: A Multi-Speaker Mandarin Speech Corpus for Constructing Personalized TTS Systems for the Speech Impaired\\u00a7r\\n\\n\\u00a78\\u00a7oJia-Jyu Su\\nPang-Chen Liao\\nYen-Ting Lin\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.14763\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 27 Aug 2023 07:35:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to 26thInternational Conference of the ORIENTAL-COCOSDA\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Hongxu Zhu; Siqi Cai; Yidi Jiang; Qiquan Zhang; Haizhou Li', display:{Lore:['[{"text": "arXiv:2308.14774", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEEG-Derived Voice Signature for Attended Speaker Detection\\u00a7r\\n\\n\\u00a78\\u00a7oHongxu Zhu\\nSiqi Cai\\nYidi Jiang\\nQiquan Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.14774\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Aug 2023 10:39:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 2 figures\\u00a7r"}']}
{title:'Zheng et al. (§72023§r)', author: 'Zhisheng Zheng; Ziyang Ma; Yu Wang; Xie Chen', display:{Lore:['[{"text": "arXiv:2308.14814", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Active Learning: Optimizing Labeling Cost-Effectiveness for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhisheng Zheng\\nZiyang Ma\\nYu Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.14814\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 28 Aug 2023 18:04:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures. Accepted to Interspeech 2023\\u00a7r"}']}
{title:'Hu et al. (§72023§r)', author: 'Cheng-Hung Hu; Yusuke Yasuda; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2308.15203", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPreference-based training framework for automatic speech quality assessment using deep neural network\\u00a7r\\n\\n\\u00a78\\u00a7oCheng-Hung Hu\\nYusuke Yasuda\\nTomoki Toda\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.15203\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-589\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Aug 2023 10:40:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2023, oral\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Dongheon Lee; Jung-Woo Choi', display:{Lore:['[{"text": "arXiv:2308.15777", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeFTAN-II: Efficient Multichannel Speech Enhancement with Subgroup Processing\\u00a7r\\n\\n\\u00a78\\u00a7oDongheon Lee\\nJung-Woo Choi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.15777\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 30 Aug 2023 06:08:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 6 figures, submitted to IEEE/ACM Trans. Audio, Speech, Lang. Process\\u00a7r"}']}
{title:'Veaux et al. (§72023§r)', author: 'Christophe Veaux; Ranniery Maia; Spyridoula Papandreou', display:{Lore:['[{"text": "arXiv:2308.15945", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe DeepZen Speech Synthesis System for Blizzard Challenge 2023\\u00a7r\\n\\n\\u00a78\\u00a7oChristophe Veaux\\nRanniery Maia\\nSpyridoula Papandreou\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.15945\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 1 Sep 2023 09:47:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oBlizzard Challenge 2023\\u00a7r"}']}
{title:'Ozyurt et al. (§72023§r)', author: 'Fatih Ozyurt; Jafar Majidpour; Tarik A. Rashid; Amir Majidpour; Canan Koc', display:{Lore:['[{"text": "arXiv:2308.16203", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.NE\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Transfer Learning Techniques for Detecting Auditory Brainstem Response\\u00a7r\\n\\n\\u00a78\\u00a7oFatih Ozyurt\\nJafar Majidpour\\nTarik A. Rashid\\nAmir Majidpour\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.16203\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.apacoust.2023.109604\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nApplied Acoustics, 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 29 Aug 2023 10:40:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o19\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Xuechen Wang; Shiwan Zhao; Yong Qin', display:{Lore:['[{"text": "arXiv:2308.16485", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSupervised Contrastive Learning with Nearest Neighbor Search for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oXuechen Wang\\nShiwan Zhao\\nYong Qin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.16485\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-842\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nINTERSPEECH 2023, 1913-1917\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Aug 2023 06:45:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by lnterspeech 2023, poster\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Hui Wang; Shiwan Zhao; Xiguang Zheng; Yong Qin', display:{Lore:['[{"text": "arXiv:2308.16488", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRAMP: Retrieval-Augmented MOS Prediction via Confidence-based Dynamic Weighting\\u00a7r\\n\\n\\u00a78\\u00a7oHui Wang\\nShiwan Zhao\\nXiguang Zheng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.16488\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-851\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nINTERSPEECH 2023, 1095-1099\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Aug 2023 06:48:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2023, oral\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Yong-Hyeok Lee; Namhyun Cho', display:{Lore:['[{"text": "arXiv:2308.16511", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhonMatchNet: Phoneme-Guided Zero-Shot Keyword Spotting for User-Defined Keywords\\u00a7r\\n\\n\\u00a78\\u00a7oYong-Hyeok Lee\\nNamhyun Cho\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.16511\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-597\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. INTERSPEECH 2023, 3964-3968\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Aug 2023 07:48:24 GMT)\\u00a7r"}']}
{title:'Gowda et al. (§72023§r)', author: 'Dhananjaya Gowda; Sudarsana Reddy Kadiri; Brad Story; Paavo Alku', display:{Lore:['[{"text": "arXiv:2308.16540", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTime-Varying Quasi-Closed-Phase Analysis for Accurate Formant Tracking in Speech Signals\\u00a7r\\n\\n\\u00a78\\u00a7oDhananjaya Gowda\\nSudarsana Reddy Kadiri\\nBrad Story\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.16540\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2020.3000037\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  Vol. 28, pp. 1901-1914, 2020\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Aug 2023 08:30:20 GMT)\\u00a7r"}']}
{title:'Gu et al. (§72023§r)', author: 'Rongzhi Gu; Yi Luo', display:{Lore:['[{"text": "arXiv:2308.16892", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReZero: Region-customizable Sound Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oRongzhi Gu\\nYi Luo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.16892\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Aug 2023 17:53:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 11 figures\\u00a7r"}']}
{title:'Huang et al. (§72023§r)', author: 'Zhichao Huang; Chutong Meng; Tom Ko', display:{Lore:['[{"text": "arXiv:2309.00169", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRepCodec: A Speech Representation Codec for Speech Tokenization\\u00a7r\\n\\n\\u00a78\\u00a7oZhichao Huang\\nChutong Meng\\nTom Ko\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.00169\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Aug 2023 23:26:10 GMT)\\u00a7r"}']}
{title:'Qi et al. (§72023§r)', author: 'Xin Qi; Xiaopeng Wang; Zhiyong Wang; Wang Liu; Mingming Ding; Shuchen Shi', display:{Lore:['[{"text": "arXiv:2309.00223", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe FruitShell French synthesis system at the Blizzard 2023 Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oXin Qi\\nXiaopeng Wang\\nZhiyong Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.00223\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Sep 2023 02:56:20 GMT)\\u00a7r"}']}
{title:'Saijo et al. (§72023§r)', author: 'Kohei Saijo; Tetsuji Ogawa', display:{Lore:['[{"text": "arXiv:2309.00376", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRemixing-based Unsupervised Source Separation from Scratch\\u00a7r\\n\\n\\u00a78\\u00a7oKohei Saijo\\nTetsuji Ogawa\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.00376\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Sep 2023 10:23:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech2023, 5pages, 2figures, 2tables\\u00a7r"}']}
{title:'Qiang et al. (§72023§r)', author: 'Chunyu Qiang; Hao Li; Yixin Tian; Ruibo Fu; Tao Wang; Longbiao Wang; Jianwu Dang', display:{Lore:['[{"text": "arXiv:2309.00424", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Speech Representation From Contrastive Token-Acoustic Pretraining\\u00a7r\\n\\n\\u00a78\\u00a7oChunyu Qiang\\nHao Li\\nYixin Tian\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.00424\\u00a7r\\n\\nVersion:\\u00a77v5 (Mon, 18 Dec 2023 12:49:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Seunghan Yang; Byeonggeun Kim; Kyuhong Shim; Simyung Chang', display:{Lore:['[{"text": "arXiv:2309.00647", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Small Footprint Few-shot Keyword Spotting with Supervision on Auxiliary Data\\u00a7r\\n\\n\\u00a78\\u00a7oSeunghan Yang\\nByeonggeun Kim\\nKyuhong Shim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.00647\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 31 Aug 2023 07:29:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Zhichao Wang; Xinsheng Wang; Qicong Xie; Tao Li; Lei Xie; Qiao Tian; Yuping Wang', display:{Lore:['[{"text": "arXiv:2309.01142", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMSM-VC: High-fidelity Source Style Transfer for Non-Parallel Voice Conversion by Multi-scale Style Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oZhichao Wang\\nXinsheng Wang\\nQicong Xie\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.01142\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 3 Sep 2023 11:33:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work was submitted on April 10, 2022 and accepted on August 29, 2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Yu-Wen Chen; Julia Hirschberg; Yu Tsao', display:{Lore:['[{"text": "arXiv:2309.01164", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoise robust speech emotion recognition with signal-to-noise ratio adapting speech enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oYu-Wen Chen\\nJulia Hirschberg\\nYu Tsao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.01164\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 3 Sep 2023 13:00:04 GMT)\\u00a7r"}']}
{title:'Nustede et al. (§72023§r)', author: 'Eike J. Nustede; Jörn Anemüller', display:{Lore:['[{"text": "arXiv:2309.01535", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSingle-Channel Speech Enhancement with Deep Complex U-Networks and Probabilistic Latent Space Models\\u00a7r\\n\\n\\u00a78\\u00a7oEike J. Nustede\\nJ\\u00f6rn Anem\\u00fcller\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.01535\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10096208\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2023 - 2023 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-5\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Sep 2023 11:30:32 GMT)\\u00a7r"}']}
{title:'Riou et al. (§72023§r)', author: 'Alain Riou; Stefan Lattner; Gaëtan Hadjeres; Geoffroy Peeters', display:{Lore:['[{"text": "arXiv:2309.02265", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPESTO: Pitch Estimation with Self-supervised Transposition-equivariant Objective\\u00a7r\\n\\n\\u00a78\\u00a7oAlain Riou\\nStefan Lattner\\nGa\\u00ebtan Hadjeres\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.02265\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Sep 2023 14:20:08 GMT)\\u00a7r"}']}
{title:'Leng et al. (§72023§r)', author: 'Yichong Leng; Zhifang Guo; Kai Shen; Xu Tan; Zeqian Ju; Yanqing Liu; Yufei Liu; Dongchao Yang; Leying Zhang; Kaitao Song; Lei He; Xiang-Yang Li; Sheng Zhao; Tao Qin; Jiang Bian', display:{Lore:['[{"text": "arXiv:2309.02285", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPromptTTS 2: Describing and Generating Voices with Text Prompt\\u00a7r\\n\\n\\u00a78\\u00a7oYichong Leng\\nZhifang Guo\\nKai Shen\\n+ 11 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.02285\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 12 Oct 2023 03:05:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDemo page: https://speechresearch.github.io/prompttts2\\u00a7r"}']}
{title:'Schilk et al. (§72023§r)', author: 'Philipp Schilk; Niccolò Polvani; Andrea Ronco; Milos Cernak; Michele Magno', display:{Lore:['[{"text": "arXiv:2309.02393", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIn-Ear-Voice: Towards Milli-Watt Audio Enhancement With Bone-Conduction Microphones for In-Ear Sensing Platforms\\u00a7r\\n\\n\\u00a78\\u00a7oPhilipp Schilk\\nNiccol\\u00f2 Polvani\\nAndrea Ronco\\nMilos Cernak\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.02393\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3576842.3582365\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Sep 2023 17:04:09 GMT)\\u00a7r"}']}
{title:'Tran et al. (§72023§r)', author: 'Minh Tran; Yufeng Yin; Mohammad Soleymani', display:{Lore:['[{"text": "arXiv:2309.02418", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPersonalized Adaptation with Pre-trained Speech Encoders for Continuous Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMinh Tran\\nYufeng Yin\\nMohammad Soleymani\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.02418\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Sep 2023 17:50:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Xu et al. (§72023§r)', author: 'Ziyi Xu; Marvin Sach; Jan Pirklbauer; Tim Fingscheidt', display:{Lore:['[{"text": "arXiv:2309.02432", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmploying Real Training Data for Deep Noise Suppression\\u00a7r\\n\\n\\u00a78\\u00a7oZiyi Xu\\nMarvin Sach\\nJan Pirklbauer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.02432\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Sep 2023 17:58:58 GMT)\\u00a7r"}']}
{title:'Eugenio (§72023§r)', author: 'Paul Myles Eugenio', display:{Lore:['[{"text": "arXiv:2309.02466", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMinimal Effective Theory for Phonotactic Memory: Capturing Local Correlations due to Errors in Speech\\u00a7r\\n\\n\\u00a78\\u00a7oPaul Myles Eugenio\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.02466\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Sep 2023 22:11:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages; 7 figs\\u00a7r"}']}
{title:'Watcharasupat et al. (§72023§r)', author: 'Karn N. Watcharasupat; Chih-Wei Wu; Yiwei Ding; Iroro Orife; Aaron J. Hipple; Phillip A. Williams; Scott Kramer; Alexander Lerch; William Wolcott', display:{Lore:['[{"text": "arXiv:2309.02539", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Generalized Bandsplit Neural Network for Cinematic Audio Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oKarn N. Watcharasupat\\nChih-Wei Wu\\nYiwei Ding\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.02539\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/OJSP.2023.3339428\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 1 Dec 2023 22:43:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to the IEEE Open Journal of Signal Processing (ICASSP 2024 Track)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Huan Zhang; Emmanouil Karystinaios; Simon Dixon; Gerhard Widmer; Carlos Eduardo Cancino-Chacón', display:{Lore:['[{"text": "arXiv:2309.02567", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSymbolic Music Representations for Classification Tasks: A Systematic Evaluation\\u00a7r\\n\\n\\u00a78\\u00a7oHuan Zhang\\nEmmanouil Karystinaios\\nSimon Dixon\\nGerhard Widmer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.02567\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 24th International Society for Music\\n  Information Retrieval Conference (ISMIR 2023), Milan, Italy\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 10 Sep 2023 12:36:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in the Proceedings of the 24th International Society for Music Information Retrieval Conference (ISMIR 2023), Milan, Italy\\u00a7r"}']}
{title:'Lim et al. (§72023§r)', author: 'Hyungseob Lim; Kyungguen Byun; Sunkuk Moon; Erik Visser', display:{Lore:['[{"text": "arXiv:2309.02730", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStylebook: Content-Dependent Speaking Style Modeling for Any-to-Any Voice Conversion using Only Speech Data\\u00a7r\\n\\n\\u00a78\\u00a7oHyungseob Lim\\nKyungguen Byun\\nSunkuk Moon\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.02730\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 15 Dec 2023 04:36:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 2 tables\\u00a7r"}']}
{title:'Xu et al. (§72023§r)', author: 'Zhihang Xu; Shaofei Zhang; Xi Wang; Jiajun Zhang; Wenning Wei; Lei He; Sheng Zhao', display:{Lore:['[{"text": "arXiv:2309.02743", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMuLanTTS: The Microsoft Speech Synthesis System for Blizzard Challenge 2023\\u00a7r\\n\\n\\u00a78\\u00a7oZhihang Xu\\nShaofei Zhang\\nXi Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.02743\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 12 Sep 2023 02:45:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages\\u00a7r"}']}
{title:'Cai et al. (§72023§r)', author: 'Danwei Cai; Ming Li', display:{Lore:['[{"text": "arXiv:2309.03019", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging ASR Pretrained Conformers for Speaker Verification through Transfer Learning and Knowledge Distillation\\u00a7r\\n\\n\\u00a78\\u00a7oDanwei Cai\\nMing Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.03019\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Sep 2023 14:02:50 GMT)\\u00a7r"}']}
{title:'Accolti et al. (§72023§r)', author: 'Ernesto Accolti; Lukas Aspöck; Manuj Yadav; Michael Vorländer', display:{Lore:['[{"text": "arXiv:2309.03149", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-time auralization for performers on virtual stages\\u00a7r\\n\\n\\u00a78\\u00a7oErnesto Accolti\\nLukas Asp\\u00f6ck\\nManuj Yadav\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.03149\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Sep 2023 16:44:50 GMT)\\u00a7r"}']}
{title:'Ick et al. (§72023§r)', author: 'Christopher Ick; Brian McFee', display:{Lore:['[{"text": "arXiv:2309.03337", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Geometrical Acoustic Simulations of Spatial Room Impulse Responses for Improved Sound Event Detection and Localization\\u00a7r\\n\\n\\u00a78\\u00a7oChristopher Ick\\nBrian McFee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.03337\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Sep 2023 19:34:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 3 tables, presented in theProceedings of the 8th Detection and Classification of Acoustic Scenes and Events 2023 Workshop (DCASE2023)\\u00a7r"}']}
{title:'Xu et al. (§72023§r)', author: 'Zeyu Xu; Adrian Herzog; Alexander Lodermeyer; Emanuël A. P. Habets; Albert G. Prinn', display:{Lore:['[{"text": "arXiv:2309.03486", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSimulating room transfer functions between transducers mounted on audio devices using a modified image source method\\u00a7r\\n\\n\\u00a78\\u00a7oZeyu Xu\\nAdrian Herzog\\nAlexander Lodermeyer\\nEmanu\\u00ebl A. P. Habets\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.03486\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Sep 2023 05:45:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe following article has been submitted to the Journal of the Acoustical Society of America (JASA). After it is published, it will be found at http://asa.scitation.org/journal/jas\\u00a7r"}']}
{title:'Bartolewska et al. (§72023§r)', author: 'Julitta Bartolewska; Stanisław Kacprzak; Konrad Kowalczyk', display:{Lore:['[{"text": "arXiv:2309.03684", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCausal Signal-Based DCCRN with Overlapped-Frame Prediction for Online Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oJulitta Bartolewska\\nStanis\\u0142aw Kacprzak\\nKonrad Kowalczyk\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.03684\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-2177\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. INTERSPEECH 2023, 4039-4043 (2023)\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Sep 2023 12:52:21 GMT)\\u00a7r"}']}
{title:'Bassi et al. (§72023§r)', author: 'Saksham Bassi; Giulio Duregon; Siddhartha Jalagam; David Roth', display:{Lore:['[{"text": "arXiv:2309.04516", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Speech Recognition and Disfluency Removal with Acoustic Language Model Pretraining\\u00a7r\\n\\n\\u00a78\\u00a7oSaksham Bassi\\nGiulio Duregon\\nSiddhartha Jalagam\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.04516\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Sep 2023 17:12:14 GMT)\\u00a7r"}']}
{title:'Bhati et al. (§72023§r)', author: 'Saurabhchand Bhati; Jesús Villalba; Laureano Moro-Velazquez; Thomas Thebaud; Najim Dehak', display:{Lore:['[{"text": "arXiv:2309.04628", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Pretrained Image-text Models for Improving Audio-Visual Learning\\u00a7r\\n\\n\\u00a78\\u00a7oSaurabhchand Bhati\\nJes\\u00fas Villalba\\nLaureano Moro-Velazquez\\nThomas Thebaud\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.04628\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Sep 2023 22:41:36 GMT)\\u00a7r"}']}
{title:'Grondin et al. (§72023§r)', author: 'François Grondin; Caleb Rascón', display:{Lore:['[{"text": "arXiv:2309.05057", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGray Jedi MVDR Post-filtering\\u00a7r\\n\\n\\u00a78\\u00a7oFran\\u00e7ois Grondin\\nCaleb Rasc\\u00f3n\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.05057\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 10 Sep 2023 15:45:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oo\\u0327p\\u0327y\\u0327\\u0157i\\u0327\\u0123\\u1e29\\u0163 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for "}','{"text": "advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Park et al. (§72023§r)', author: 'Tae Jin Park; Kunal Dhawan; Nithin Koluguri; Jagadeesh Balam', display:{Lore:['[{"text": "arXiv:2309.05248", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Speaker Diarization with Large Language Models: A Contextual Beam Search Approach\\u00a7r\\n\\n\\u00a78\\u00a7oTae Jin Park\\nKunal Dhawan\\nNithin Koluguri\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.05248\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 14 Sep 2023 01:08:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages 1 reference page, ICASSP format\\u00a7r"}']}
{title:'Oneata et al. (§72023§r)', author: 'Dan Oneata; Adriana Stan; Octavian Pascu; Elisabeta Oneata; Horia Cucu', display:{Lore:['[{"text": "arXiv:2309.05384", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards generalisable and calibrated synthetic speech detection with self-supervised representations\\u00a7r\\n\\n\\u00a78\\u00a7oDan Oneata\\nAdriana Stan\\nOctavian Pascu\\nElisabeta Oneata\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.05384\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Sep 2023 11:11:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Zhong et al. (§72023§r)', author: 'Jinzuomu Zhong; Yang Li; Hui Huang; Jie Liu; Zhiba Su; Jing Guo; Benlai Tang; Fengjie Zhu', display:{Lore:['[{"text": "arXiv:2309.05423", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Modal Automatic Prosody Annotation with Contrastive Pretraining of SSWP\\u00a7r\\n\\n\\u00a78\\u00a7oJinzuomu Zhong\\nYang Li\\nHui Huang\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.05423\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Sep 2023 12:50:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Deichler et al. (§72023§r)', author: 'Anna Deichler; Shivam Mehta; Simon Alexanderson; Jonas Beskow', display:{Lore:['[{"text": "arXiv:2309.05455", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffusion-Based Co-Speech Gesture Generation Using Joint Text and Audio Representation\\u00a7r\\n\\n\\u00a78\\u00a7oAnna Deichler\\nShivam Mehta\\nSimon Alexanderson\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.05455\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3577190.3616117\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Sep 2023 13:51:06 GMT)\\u00a7r"}']}
{title:'Yamada et al. (§72023§r)', author: 'Yasunori Yamada; Kaoru Shinkawa; Masatomo Kobayashi; Miyuki Nemoto; Miho Ota; Kiyotaka Nemoto; Tetsuaki Arai', display:{Lore:['[{"text": "arXiv:2309.05777", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSmartwatch-derived Acoustic Markers for Deficits in Cognitively Relevant Everyday Functioning\\u00a7r\\n\\n\\u00a78\\u00a7oYasunori Yamada\\nKaoru Shinkawa\\nMasatomo Kobayashi\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.05777\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICDH60066.2023.00015\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2023 IEEE International Conference on Digital Health (ICDH)\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Sep 2023 19:12:09 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Xin Wang; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2309.06014", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCan large-scale vocoded spoofed data improve speech spoofing countermeasure with a self-supervised front end?\\u00a7r\\n\\n\\u00a78\\u00a7oXin Wang\\nJunichi Yamagishi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.06014\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 28 Dec 2023 03:31:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP 2024. code on github: https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts/tree/master/project/10-asvspoof-vocoded-trn-ssl\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Yong-Hyeok Lee; Namhyun Cho', display:{Lore:['[{"text": "arXiv:2309.06096", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7liPhonMatchNet: Zero-Shot User-Defined Keyword Spotting Using Implicit Acoustic Echo Cancellation\\u00a7r\\n\\n\\u00a78\\u00a7oYong-Hyeok Lee\\nNamhyun Cho\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.06096\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 14 Dec 2023 00:03:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Gonzalez et al. (§72023§r)', author: 'Philippe Gonzalez; Tommy Sonne Alstrøm; Tobias May', display:{Lore:['[{"text": "arXiv:2309.06183", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAssessing the Generalization Gap of Learning-Based Speech Enhancement Systems in Noisy and Reverberant Environments\\u00a7r\\n\\n\\u00a78\\u00a7oPhilippe Gonzalez\\nTommy Sonne Alstr\\u00f8m\\nTobias May\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.06183\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3318965\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 8 Nov 2023 08:09:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEE/ACM TASLP\\u00a7r"}']}
{title:'Moryossef (§72023§r)', author: 'Amit Moryossef', display:{Lore:['[{"text": "arXiv:2309.06572", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAddressing the Blind Spots in Spoken Language Processing\\u00a7r\\n\\n\\u00a78\\u00a7oAmit Moryossef\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.06572\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Sep 2023 10:29:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Matsuda et al. (§72023§r)', author: 'Ryo Matsuda; Makoto Otani', display:{Lore:['[{"text": "arXiv:2309.06661", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound field decomposition based on two-stage neural networks\\u00a7r\\n\\n\\u00a78\\u00a7oRyo Matsuda\\nMakoto Otani\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.06661\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Sep 2023 01:32:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o31 pages, 16 figures\\u00a7r"}']}
{title:'Hernandez-Olivan et al. (§72023§r)', author: 'Carlos Hernandez-Olivan; Koichi Saito; Naoki Murata; Chieh-Hsin Lai; Marco A. Martínez-Ramirez; Wei-Hsiang Liao; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2309.06934", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVRDMG: Vocal Restoration via Diffusion Posterior Sampling with Multiple Guidance\\u00a7r\\n\\n\\u00a78\\u00a7oCarlos Hernandez-Olivan\\nKoichi Saito\\nNaoki Murata\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.06934\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Sep 2023 13:14:01 GMT)\\u00a7r"}']}
{title:'Friedrichs et al. (§72023§r)', author: 'Daniel Friedrichs; Volker Dellwo', display:{Lore:['[{"text": "arXiv:2309.06946", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.PE\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReorganization of the auditory-perceptual space across the human vocal range\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Friedrichs\\nVolker Dellwo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.06946\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 20th International Congress of Phonetic\\n  Sciences (2023) 560-564\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Sep 2023 13:30:05 GMT)\\u00a7r"}']}
{title:'Peer et al. (§72023§r)', author: 'Tal Peer; Simon Welker; Johannes Kolhoff; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2309.07043", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Flexible Online Framework for Projection-Based STFT Phase Retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oTal Peer\\nSimon Welker\\nJohannes Kolhoff\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07043\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Sep 2023 15:55:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 24\\u00a7r"}']}
{title:'Ranjan et al. (§72023§r)', author: 'Anshul Ranjan; Kaushik Jegadeesan', display:{Lore:['[{"text": "arXiv:2309.07164", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHybrid ASR for Resource-Constrained Robots: HMM - Deep Learning Fusion\\u00a7r\\n\\n\\u00a78\\u00a7oAnshul Ranjan\\nKaushik Jegadeesan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07164\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Sep 2023 15:28:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published in IEEE Access, 9 pages, 14 figures, Received valuable support from CCBD PESU, for associated code, see https://github.com/AnshulRanjan2004/PyHMM\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Jialu Li; Mark Hasegawa-Johnson; Karrie Karahalios', display:{Lore:['[{"text": "arXiv:2309.07287", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Child Vocalization Classification in Multi-Channel Child-Adult Conversations Through Wav2vec2 Children ASR Features\\u00a7r\\n\\n\\u00a78\\u00a7oJialu Li\\nMark Hasegawa-Johnson\\nKarrie Karahalios\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07287\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Sep 2023 20:13:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Ling et al. (§72023§r)', author: 'Shaoshi Ling; Guoli Ye; Rui Zhao; Yifan Gong', display:{Lore:['[{"text": "arXiv:2309.07369", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHybrid Attention-based Encoder-decoder Model for Efficient Language Model Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oShaoshi Ling\\nGuoli Ye\\nRui Zhao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07369\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Sep 2023 01:07:36 GMT)\\u00a7r"}']}
{title:'Deshmukh et al. (§72023§r)', author: 'Soham Deshmukh; Benjamin Elizalde; Dimitra Emmanouilidou; Bhiksha Raj; Rita Singh; Huaming Wang', display:{Lore:['[{"text": "arXiv:2309.07372", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTraining Audio Captioning Models without Audio\\u00a7r\\n\\n\\u00a78\\u00a7oSoham Deshmukh\\nBenjamin Elizalde\\nDimitra Emmanouilidou\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07372\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Sep 2023 01:16:02 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Yifan Yang; Feiyu Shen; Chenpeng Du; Ziyang Ma; Kai Yu; Daniel Povey; Xie Chen', display:{Lore:['[{"text": "arXiv:2309.07377", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Universal Speech Discrete Tokens: A Case Study for ASR and TTS\\u00a7r\\n\\n\\u00a78\\u00a7oYifan Yang\\nFeiyu Shen\\nChenpeng Du\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07377\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Dec 2023 13:18:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ICASSP 2024\\u00a7r"}']}
{title:'Naderi et al. (§72023§r)', author: 'Babak Naderi; Ross Cutler; Nicolae-Catalin Ristea', display:{Lore:['[{"text": "arXiv:2309.07385", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-dimensional Speech Quality Assessment in Crowdsourcing\\u00a7r\\n\\n\\u00a78\\u00a7oBabak Naderi\\nRoss Cutler\\nNicolae-Catalin Ristea\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07385\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Sep 2023 02:04:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2303.06566\\u00a7r"}']}
{title:'Mishra et al. (§72023§r)', author: 'Ansh Mishra; Jia Qi Yip; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2309.07466", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCodec Data Augmentation for Time-domain Heart Sound Classification\\u00a7r\\n\\n\\u00a78\\u00a7oAnsh Mishra\\nJia Qi Yip\\nEng Siong Chng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07466\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Sep 2023 06:47:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICAICTA 2023\\u00a7r"}']}
{title:'Lan et al. (§72023§r)', author: 'Haiyan Lan; Qiaoxi Zhu; Jian Guan; Yuming Wei; Wenwu Wang', display:{Lore:['[{"text": "arXiv:2309.07498", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierarchical Metadata Information Constrained Self-Supervised Learning for Anomalous Sound Detection Under Domain Shift\\u00a7r\\n\\n\\u00a78\\u00a7oHaiyan Lan\\nQiaoxi Zhu\\nJian Guan\\nYuming Wei\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07498\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 18 Dec 2023 06:41:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at ICASSP 2024\\u00a7r"}']}
{title:'Ghosh et al. (§72023§r)', author: 'Suhita Ghosh; Arnab Das; Yamini Sinha; Ingo Siegert; Tim Polzehl; Sebastian Stober', display:{Lore:['[{"text": "arXiv:2309.07586", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmo-StarGAN: A Semi-Supervised Any-to-Many Non-Parallel Emotion-Preserving Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oSuhita Ghosh\\nArnab Das\\nYamini Sinha\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07586\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-191\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Sep 2023 10:40:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in Interspeech 2023\\u00a7r"}']}
{title:'Das et al. (§72023§r)', author: 'Arnab Das; Suhita Ghosh; Tim Polzehl; Sebastian Stober', display:{Lore:['[{"text": "arXiv:2309.07592", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStarGAN-VC++: Towards Emotion Preserving Voice Conversion Using Deep Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oArnab Das\\nSuhita Ghosh\\nTim Polzehl\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07592\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Sep 2023 10:52:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in 12th Speech Synthesis Workshop (SSW), Satellite event in Interspeech 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Peng Wang; Yifan Yang; Zheng Liang; Tian Tan; Shiliang Zhang; Xie Chen', display:{Lore:['[{"text": "arXiv:2309.07648", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIncorporating Class-based Language Model for Named Entity Recognition in Factorized Neural Transducer\\u00a7r\\n\\n\\u00a78\\u00a7oPeng Wang\\nYifan Yang\\nZheng Liang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07648\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Sep 2023 12:14:49 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Hangting Chen; Jianwei Yu; Chao Weng', display:{Lore:['[{"text": "arXiv:2309.07757", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComplexity Scaling for Speech Denoising\\u00a7r\\n\\n\\u00a78\\u00a7oHangting Chen\\nJianwei Yu\\nChao Weng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07757\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Sep 2023 14:45:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP2024\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Sipan Li; Songxiang Liu; Luwen Zhang; Xiang Li; Yanyao Bian; Chao Weng; Zhiyong Wu; Helen Meng', display:{Lore:['[{"text": "arXiv:2309.07803", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSnakeGAN: A Universal Vocoder Leveraging DDSP Prior Knowledge and Periodic Inductive Bias\\u00a7r\\n\\n\\u00a78\\u00a7oSipan Li\\nSongxiang Liu\\nLuwen Zhang\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07803\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Sep 2023 15:46:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICME 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Haotian Wang; Yuxuan Xi; Hang Chen; Jun Du; Yan Song; Qing Wang; Hengshun Zhou; Chenxi Wang; Jiefeng Ma; Pengfei Hu; Ya Jiang; Shi Cheng; Jie Zhang; Yuzhe Weng', display:{Lore:['[{"text": "arXiv:2309.07925", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierarchical Audio-Visual Information Fusion with Multi-label Joint Decoding for MER 2023\\u00a7r\\n\\n\\u00a78\\u00a7oHaotian Wang\\nYuxuan Xi\\nHang Chen\\n+ 10 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07925\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3581783.3612859\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nThe 31st ACM International Conference on Multimedia (MM\'23), 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Sep 2023 03:19:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Yunyi Liu; Craig Jin; David Gunawan', display:{Lore:['[{"text": "arXiv:2309.08060", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDDSP-SFX: Acoustically-guided sound effects generation with differentiable digital signal processing\\u00a7r\\n\\n\\u00a78\\u00a7oYunyi Liu\\nCraig Jin\\nDavid Gunawan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08060\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Sep 2023 23:13:48 GMT)\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Jian Wu; Naoyuki Kanda; Takuya Yoshioka; Rui Zhao; Zhuo Chen; Jinyu Li', display:{Lore:['[{"text": "arXiv:2309.08131", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lt-SOT FNT: Streaming Multi-talker ASR with Text-only Domain Adaptation Capability\\u00a7r\\n\\n\\u00a78\\u00a7oJian Wu\\nNaoyuki Kanda\\nTakuya Yoshioka\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08131\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 03:55:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, submitted to ICASSP2024\\u00a7r"}']}
{title:'Shimizu et al. (§72023§r)', author: 'Reo Shimizu; Ryuichi Yamamoto; Masaya Kawamura; Yuma Shirahata; Hironori Doi; Tatsuya Komatsu; Kentaro Tachibana', display:{Lore:['[{"text": "arXiv:2309.08140", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPromptTTS++: Controlling Speaker Identity in Prompt-Based Text-to-Speech Using Natural Language Descriptions\\u00a7r\\n\\n\\u00a78\\u00a7oReo Shimizu\\nRyuichi Yamamoto\\nMasaya Kawamura\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08140\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 27 Dec 2023 10:41:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Komatsu et al. (§72023§r)', author: 'Tatsuya Komatsu; Yusuke Fujita; Kazuya Takeda; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2309.08141", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Difference Learning for Audio Captioning\\u00a7r\\n\\n\\u00a78\\u00a7oTatsuya Komatsu\\nYusuke Fujita\\nKazuya Takeda\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08141\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 04:11:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP2024\\u00a7r"}']}
{title:'Shao et al. (§72023§r)', author: 'Nian Shao; Xian Li; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2309.08153", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFine-tune the pretrained ATST model for sound event detection\\u00a7r\\n\\n\\u00a78\\u00a7oNian Shao\\nXian Li\\nXiaofei Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08153\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 29 Dec 2023 06:31:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, camera-ready version for ICASSP 2024\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Pengyu Wang; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2309.08157", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRVAE-EM: Generative speech dereverberation based on recurrent variational auto-encoder and convolutive transfer function\\u00a7r\\n\\n\\u00a78\\u00a7oPengyu Wang\\nXiaofei Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08157\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 17 Oct 2023 12:28:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP2024\\u00a7r"}']}
{title:'Piotrowski et al. (§72023§r)', author: 'Dariusz Piotrowski; Renard Korzeniowski; Alessio Falai; Sebastian Cygert; Kamil Pokora; Georgi Tinchev; Ziyao Zhang; Kayoko Yanagisawa', display:{Lore:['[{"text": "arXiv:2309.08255", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-lingual Knowledge Distillation via Flow-based Voice Conversion for Robust Polyglot Text-To-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oDariusz Piotrowski\\nRenard Korzeniowski\\nAlessio Falai\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08255\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 09:03:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICONIP 2023\\u00a7r"}']}
{title:'Ghosh et al. (§72023§r)', author: 'Suhita Ghosh; Yamini Sinha; Ingo Siegert; Sebastian Stober', display:{Lore:['[{"text": "arXiv:2309.08263", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Voice Conversion for Dissimilar Speakers Using Perceptual Losses\\u00a7r\\n\\n\\u00a78\\u00a7oSuhita Ghosh\\nYamini Sinha\\nIngo Siegert\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08263\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 18 Sep 2023 08:18:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in The German Annual Conference on Acoustics 2023 (DAGA)\\u00a7r"}']}
{title:'Lu et al. (§72023§r)', author: 'Jingze Lu; Yuxiang Zhang; Wenchao Wang; Zengqiang Shang; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2309.08285", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOne-Class Knowledge Distillation for Spoofing Speech Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJingze Lu\\nYuxiang Zhang\\nWenchao Wang\\nZengqiang Shang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08285\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 09:59:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to icassp 2024\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Xingyu Chen; Fei Ma; Yile Zhang; Amy Bastine; Prasanga N. Samarasinghe', display:{Lore:['[{"text": "arXiv:2309.08290", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHead-Related Transfer Function Interpolation with a Spherical CNN\\u00a7r\\n\\n\\u00a78\\u00a7oXingyu Chen\\nFei Ma\\nYile Zhang\\nAmy Bastine\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08290\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 10:11:37 GMT)\\u00a7r"}']}
{title:'Ohlenbusch et al. (§72023§r)', author: 'Mattes Ohlenbusch; Christian Rollwage; Simon Doclo', display:{Lore:['[{"text": "arXiv:2309.08294", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech-dependent Modeling of Own Voice Transfer Characteristics for In-ear Microphones in Hearables\\u00a7r\\n\\n\\u00a78\\u00a7oMattes Ohlenbusch\\nChristian Rollwage\\nSimon Doclo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08294\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.61782/fa.2023.1030\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 10:19:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at Forum Acusticum 2023\\u00a7r"}']}
{title:'Gurvich et al. (§72023§r)', author: 'Ilya Gurvich; Ido Leichter; Dharmendar Reddy Palle; Yossi Asher; Alon Vinnikov; Igor Abramovski; Vishak Gopal; Ross Cutler; Eyal Krupka', display:{Lore:['[{"text": "arXiv:2309.08295", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Real-Time Active Speaker Detection System Integrating an Audio-Visual Signal with a Spatial Querying Mechanism\\u00a7r\\n\\n\\u00a78\\u00a7oIlya Gurvich\\nIdo Leichter\\nDharmendar Reddy Palle\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08295\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 10:20:16 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Ju-ho Kim; Jungwoo Heo; Hyun-seo Shin; Chan-yeong Lim; Ha-Jin Yu', display:{Lore:['[{"text": "arXiv:2309.08320", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiff-SV: A Unified Hierarchical Framework for Noise-Robust Speaker Verification Using Score-Based Diffusion Probabilistic Models\\u00a7r\\n\\n\\u00a78\\u00a7oJu-ho Kim\\nJungwoo Heo\\nHyun-seo Shin\\nChan-yeong Lim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08320\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Dec 2023 01:02:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, accepted for ICASSP 2024\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Shilong Wu; Chenxi Wang; Hang Chen; Yusheng Dai; Chenyue Zhang; Ruoyu Wang; Hongbo Lan; Jun Du; Chin-Hui Lee; Jingdong Chen; Shinji Watanabe; Sabato Marco Siniscalchi; Odette Scharenborg; Zhong-Qiu Wang; Jia Pan; Jianqing Gao', display:{Lore:['[{"text": "arXiv:2309.08348", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oShilong Wu\\nChenxi Wang\\nHang Chen\\n+ 12 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08348\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 12:15:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yiming Li; Xiangdong Wang; Hong Liu; Rui Tao; Long Yan; Kazushige Ouchi', display:{Lore:['[{"text": "arXiv:2309.08355", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemi-supervised Sound Event Detection with Local and Global Consistency Regularization\\u00a7r\\n\\n\\u00a78\\u00a7oYiming Li\\nXiangdong Wang\\nHong Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08355\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 12:29:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yiming Li; Xiangdong Wang; Hong Liu', display:{Lore:['[{"text": "arXiv:2309.08357", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-free Prompt Tuning for Language-Audio Models\\u00a7r\\n\\n\\u00a78\\u00a7oYiming Li\\nXiangdong Wang\\nHong Liu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08357\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 12:31:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Han et al. (§72023§r)', author: 'Jiangyu Han; Federico Landini; Johan Rohdin; Mireia Diez; Lukas Burget; Yuhang Cao; Heng Lu; Jan Cernocky', display:{Lore:['[{"text": "arXiv:2309.08377", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiaCorrect: Error Correction Back-end For Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oJiangyu Han\\nFederico Landini\\nJohan Rohdin\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08377\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 13:08:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Vieting et al. (§72023§r)', author: 'Peter Vieting; Simon Berger; Thilo von Neumann; Christoph Boeddeker; Ralf Schlüter; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2309.08454", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMixture Encoder Supporting Continuous Speech Separation for Meeting Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Vieting\\nSimon Berger\\nThilo von Neumann\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08454\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 14:57:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Huang et al. (§72023§r)', author: 'Yiling Huang; Weiran Wang; Guanlong Zhao; Hank Liao; Wei Xia; Quan Wang', display:{Lore:['[{"text": "arXiv:2309.08489", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Word-Level End-to-End Neural Speaker Diarization with Auxiliary Network\\u00a7r\\n\\n\\u00a78\\u00a7oYiling Huang\\nWeiran Wang\\nGuanlong Zhao\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08489\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 15:48:45 GMT)\\u00a7r"}']}
{title:'Navon et al. (§72023§r)', author: 'Aviv Navon; Aviv Shamsian; Neta Glazer; Gill Hetz; Joseph Keshet', display:{Lore:['[{"text": "arXiv:2309.08561", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOpen-vocabulary Keyword-spotting with Adaptive Instance Normalization\\u00a7r\\n\\n\\u00a78\\u00a7oAviv Navon\\nAviv Shamsian\\nNeta Glazer\\nGill Hetz\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08561\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Sep 2023 13:49:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder Review\\u00a7r"}']}
{title:'Lan et al. (§72023§r)', author: 'Gael Le Lan; Varun Nagaraja; Ernie Chang; David Kant; Zhaoheng Ni; Yangyang Shi; Forrest Iandola; Vikas Chandra', display:{Lore:['[{"text": "arXiv:2309.08804", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStack-and-Delay: a new codebook pattern for music generation\\u00a7r\\n\\n\\u00a78\\u00a7oGael Le Lan\\nVarun Nagaraja\\nErnie Chang\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08804\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Sep 2023 22:57:25 GMT)\\u00a7r"}']}
{title:'Yen et al. (§72023§r)', author: 'Hao Yen; Sabato Marco Siniscalchi; Chin-Hui Lee', display:{Lore:['[{"text": "arXiv:2309.08828", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBoosting End-to-End Multilingual Phoneme Recognition through Exploiting Universal Speech Attributes Constraints\\u00a7r\\n\\n\\u00a78\\u00a7oHao Yen\\nSabato Marco Siniscalchi\\nChin-Hui Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08828\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 16 Sep 2023 01:08:22 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Heming Wang; Meng Yu; Hao Zhang; Chunlei Zhang; Zhongweiyang Xu; Muqiao Yang; Yixuan Zhang; Dong Yu', display:{Lore:['[{"text": "arXiv:2309.09028", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnifying Robustness and Fidelity: A Comprehensive Study of Pretrained Generative Methods for Speech Enhancement in Adverse Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oHeming Wang\\nMeng Yu\\nHao Zhang\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09028\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 16 Sep 2023 15:42:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper in submission\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Gaobin Yang; Maokui He; Shutong Niu; Ruoyu Wang; Yanyan Yue; Shuangqing Qian; Shilong Wu; Jun Du; Chin-Hui Lee', display:{Lore:['[{"text": "arXiv:2309.09180", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Speaker Diarization Using Memory-Aware Multi-Speaker Embedding with Sequence-to-Sequence Architecture\\u00a7r\\n\\n\\u00a78\\u00a7oGaobin Yang\\nMaokui He\\nShutong Niu\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09180\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 26 Dec 2023 07:33:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Attia et al. (§72023§r)', author: 'Ahmed Adel Attia; Yashish M. Siriwardena; Carol Espy-Wilson', display:{Lore:['[{"text": "arXiv:2309.09220", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Speech Inversion Through Self-Supervised Embeddings and Enhanced Tract Variables\\u00a7r\\n\\n\\u00a78\\u00a7oAhmed Adel Attia\\nYashish M. Siriwardena\\nCarol Espy-Wilson\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09220\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 17 Sep 2023 09:18:04 GMT)\\u00a7r"}']}
{title:'Yao et al. (§72023§r)', author: 'Jixun Yao; Yuguang Yang; Yi Lei; Ziqian Ning; Yanni Hu; Yu Pan; Jingjing Yin; Hongbin Zhou; Heng Lu; Lei Xie', display:{Lore:['[{"text": "arXiv:2309.09262", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPromptVC: Flexible Stylistic Voice Conversion in Latent Space Driven by Natural Language Prompts\\u00a7r\\n\\n\\u00a78\\u00a7oJixun Yao\\nYuguang Yang\\nYi Lei\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09262\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 26 Dec 2023 07:48:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Song Li; Yongbin You; Xuezhi Wang; Ke Ding; Guanglu Wan', display:{Lore:['[{"text": "arXiv:2309.09443", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Multilingual Speech Recognition through Language Prompt Tuning and Frame-Level Language Adapter\\u00a7r\\n\\n\\u00a78\\u00a7oSong Li\\nYongbin You\\nXuezhi Wang\\nKe Ding\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09443\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 19 Sep 2023 06:19:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP2024\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yinghao Aaron Li; Cong Han; Xilin Jiang; Nima Mesgarani', display:{Lore:['[{"text": "arXiv:2309.09493", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHiFTNet: A Fast High-Quality Neural Vocoder with Harmonic-plus-Noise Filter and Inverse Short Time Fourier Transform\\u00a7r\\n\\n\\u00a78\\u00a7oYinghao Aaron Li\\nCong Han\\nXilin Jiang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09493\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Sep 2023 05:30:15 GMT)\\u00a7r"}']}
{title:'Zezario et al. (§72023§r)', author: 'Ryandhimas E. Zezario; Fei Chen; Chiou-Shann Fuh; Hsin-Min Wang; Yu Tsao', display:{Lore:['[{"text": "arXiv:2309.09548", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUtilizing Whisper to Enhance Multi-Branched Speech Intelligibility Prediction Model for Hearing Aids\\u00a7r\\n\\n\\u00a78\\u00a7oRyandhimas E. Zezario\\nFei Chen\\nChiou-Shann Fuh\\nHsin-Min Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09548\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Sep 2023 07:51:09 GMT)\\u00a7r"}']}
{title:'Bartolewska et al. (§72023§r)', author: 'Julitta Bartolewska; Stanisław Kacprzak; Konrad Kowalczyk', display:{Lore:['[{"text": "arXiv:2309.09630", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRefining DNN-based Mask Estimation using CGMM-based EM Algorithm for Multi-channel Noise Reduction\\u00a7r\\n\\n\\u00a78\\u00a7oJulitta Bartolewska\\nStanis\\u0142aw Kacprzak\\nKonrad Kowalczyk\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09630\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2022-10632\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. Interspeech 2022, 2923-2927 (2022)\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Sep 2023 10:05:41 GMT)\\u00a7r"}']}
{title:'Ghosh et al. (§72023§r)', author: 'Sreyan Ghosh; Sonal Kumar; Chandra Kiran Reddy Evuru; Ramani Duraiswami; Dinesh Manocha', display:{Lore:['[{"text": "arXiv:2309.09836", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRECAP: Retrieval-Augmented Audio Captioning\\u00a7r\\n\\n\\u00a78\\u00a7oSreyan Ghosh\\nSonal Kumar\\nChandra Kiran Reddy Evuru\\nRamani Duraiswami\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09836\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Sep 2023 14:53:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCode and data soon here: https://github.com/Sreyan88/RECAP\\u00a7r"}']}
{title:'de Oliveira et al. (§72023§r)', author: 'Danilo de Oliveira; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2309.09920", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDistilling HuBERT with LSTMs via Decoupled Knowledge Distillation\\u00a7r\\n\\n\\u00a78\\u00a7oDanilo de Oliveira\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09920\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Sep 2023 16:34:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Koluguri et al. (§72023§r)', author: 'Nithin Rao Koluguri; Samuel Kriman; Georgy Zelenfroind; Somshubra Majumdar; Dima Rekesh; Vahid Noroozi; Jagadeesh Balam; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2309.09950", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating End-to-End ASR Architectures for Long Form Audio Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oNithin Rao Koluguri\\nSamuel Kriman\\nGeorgy Zelenfroind\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09950\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Sep 2023 18:39:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPrePrint. Submitted to ICASSP 2024\\u00a7r"}']}
{title:'Garg et al. (§72023§r)', author: 'Shefali Garg; Zhouyuan Huo; Khe Chai Sim; Suzan Schwartz; Mason Chua; Alëna Aksënova; Tsendsuren Munkhdalai; Levi King; Darryl Wright; Zion Mengesha; Dongseong Hwang; Tara Sainath; Françoise Beaufays; Pedro Moreno Mengibar', display:{Lore:['[{"text": "arXiv:2309.09996", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Speech Recognition for African American English With Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oShefali Garg\\nZhouyuan Huo\\nKhe Chai Sim\\n+ 10 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09996\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 16 Sep 2023 19:57:45 GMT)\\u00a7r"}']}
{title:'Sun et al. (§72023§r)', author: 'Hanbo Sun; Jian Gao; Xiaomin Wu; Anjie Fang; Cheng Cao; Zheng Du', display:{Lore:['[{"text": "arXiv:2309.10089", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHTEC: Human Transcription Error Correction\\u00a7r\\n\\n\\u00a78\\u00a7oHanbo Sun\\nJian Gao\\nXiaomin Wu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10089\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Sep 2023 19:03:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 4 figures, 11 tables, AMLC 2023\\u00a7r"}']}
{title:'Do et al. (§72023§r)', author: 'Andrea Do; Oscar Brown; Zhengjie Wang; Nikhil Mathew; Zixin Liu; Jawwad Ahmed; Cheng Yu', display:{Lore:['[{"text": "arXiv:2309.10299", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing fine-tuning and min lookahead beam search to improve Whisper\\u00a7r\\n\\n\\u00a78\\u00a7oAndrea Do\\nOscar Brown\\nZhengjie Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10299\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Sep 2023 04:04:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, submitted to IEEE ICASSP 2024\\u00a7r"}']}
{title:'Zheng et al. (§72023§r)', author: 'Rui-Chen Zheng; Yang Ai; Zhen-Hua Ling', display:{Lore:['[{"text": "arXiv:2309.10455", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIncorporating Ultrasound Tongue Images for Audio-Visual Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oRui-Chen Zheng\\nYang Ai\\nZhen-Hua Ling\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10455\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 20 Nov 2023 06:27:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmmited to IEEE/ACM Transactionson Audio, Speech and Language Processing. arXiv admin note: text overlap with arXiv:2305.14933\\u00a7r"}']}
{title:'Higuchi et al. (§72023§r)', author: 'Yosuke Higuchi; Tetsuji Ogawa; Tetsunori Kobayashi', display:{Lore:['[{"text": "arXiv:2309.10524", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHarnessing the Zero-Shot Power of Instruction-Tuned Large Language Model in End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYosuke Higuchi\\nTetsuji Ogawa\\nTetsunori Kobayashi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10524\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Sep 2023 11:10:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP2024\\u00a7r"}']}
{title:'Mei et al. (§72023§r)', author: 'Xinhao Mei; Varun Nagaraja; Gael Le Lan; Zhaoheng Ni; Ernie Chang; Yangyang Shi; Vikas Chandra', display:{Lore:['[{"text": "arXiv:2309.10537", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFoleyGen: Visually-Guided Audio Generation\\u00a7r\\n\\n\\u00a78\\u00a7oXinhao Mei\\nVarun Nagaraja\\nGael Le Lan\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10537\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Sep 2023 11:33:43 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Yile Angela Zhang; Fei Ma; Thushara Abhayapala; Prasanga Samarasinghe; Amy Bastine', display:{Lore:['[{"text": "arXiv:2309.10605", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Active Noise Control System Based on Soundfield Interpolation Using a Physics-informed Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oYile Angela Zhang\\nFei Ma\\nThushara Abhayapala\\nPrasanga Samarasinghe\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10605\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Sep 2023 13:20:47 GMT)\\u00a7r"}']}
{title:'Su et al. (§72023§r)', author: 'Hsuan Su; Ting-Yao Hu; Hema Swetha Koppula; Raviteja Vemulapalli; Jen-Hao Rick Chang; Karren Yang; Gautam Varma Mantena; Oncel Tuzel', display:{Lore:['[{"text": "arXiv:2309.10707", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCorpus Synthesis for Zero-shot ASR domain Adaptation using Large Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oHsuan Su\\nTing-Yao Hu\\nHema Swetha Koppula\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10707\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Sep 2023 15:43:08 GMT)\\u00a7r"}']}
{title:'Ni et al. (§72023§r)', author: 'Zhaoheng Ni; Sravya Popuri; Ning Dong; Kohei Saijo; Xiaohui Zhang; Gael Le Lan; Yangyang Shi; Vikas Chandra; Changhan Wang', display:{Lore:['[{"text": "arXiv:2309.10795", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Speech Enhancement for Low-resource Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oZhaoheng Ni\\nSravya Popuri\\nNing Dong\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10795\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Sep 2023 17:42:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Lakomkin et al. (§72023§r)', author: 'Egor Lakomkin; Chunyang Wu; Yassir Fathullah; Ozlem Kalinli; Michael L. Seltzer; Christian Fuegen', display:{Lore:['[{"text": "arXiv:2309.10917", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Speech Recognition Contextualization with Large Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oEgor Lakomkin\\nChunyang Wu\\nYassir Fathullah\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10917\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Sep 2023 20:28:57 GMT)\\u00a7r"}']}
{title:'Puvvada et al. (§72023§r)', author: 'Krishna C. Puvvada; Nithin Rao Koluguri; Kunal Dhawan; Jagadeesh Balam; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2309.10922", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiscrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oKrishna C. Puvvada\\nNithin Rao Koluguri\\nKunal Dhawan\\nJagadeesh Balam\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10922\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Sep 2023 20:49:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint. Submitted to ICASSP 2024\\u00a7r"}']}
{title:'Atmaja et al. (§72023§r)', author: 'Bagus Tris Atmaja; Akira Sasou', display:{Lore:['[{"text": "arXiv:2309.11014", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnsembling Multilingual Pre-Trained Models for Predicting Multi-Label Regression Emotion Share from Speech\\u00a7r\\n\\n\\u00a78\\u00a7oBagus Tris Atmaja\\nAkira Sasou\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.11014\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Sep 2023 02:28:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 6 tables, accepted in APSIPA-ASC 2023\\u00a7r"}']}
{title:'Ahmed et al. (§72023§r)', author: 'Shafique Ahmed; Chia-Wei Chen; Wenze Ren; Chin-Jou Li; Ernie Chu; Jun-Cheng Chen; Amir Hussain; Hsin-Min Wang; Yu Tsao; Jen-Cheng Hou', display:{Lore:['[{"text": "arXiv:2309.11059", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Complex U-Net with Conformer for Audio-Visual Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oShafique Ahmed\\nChia-Wei Chen\\nWenze Ren\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.11059\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 8 Oct 2023 11:36:59 GMT)\\u00a7r"}']}
{title:'Dekel et al. (§72023§r)', author: 'Avihu Dekel; Slava Shechtman; Raul Fernandez; David Haws; Zvi Kons; Ron Hoory', display:{Lore:['[{"text": "arXiv:2309.11210", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeak While You Think: Streaming Speech Synthesis During Text Generation\\u00a7r\\n\\n\\u00a78\\u00a7oAvihu Dekel\\nSlava Shechtman\\nRaul Fernandez\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.11210\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Sep 2023 11:00:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review for ICASSP 2024\\u00a7r"}']}
{title:'Abdallah et al. (§72023§r)', author: 'Ahmed Amine Ben Abdallah; Ata Kabboudi; Amir Kanoun; Salah Zaiem', display:{Lore:['[{"text": "arXiv:2309.11327", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Data Collection and Unsupervised Learning for Code-switched Tunisian Arabic Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAhmed Amine Ben Abdallah\\nAta Kabboudi\\nAmir Kanoun\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.11327\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 25 Sep 2023 11:20:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, submitted to ICASSP 2024\\u00a7r"}']}
{title:'Shechtman et al. (§72023§r)', author: 'Slava Shechtman; Raul Fernandez', display:{Lore:['[{"text": "arXiv:2309.11487", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Neural TTS System with Parallel Prosody Transfer from Unseen Speakers\\u00a7r\\n\\n\\u00a78\\u00a7oSlava Shechtman\\nRaul Fernandez\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.11487\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1032\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. INTERSPEECH 2023, 4853-4857 (2023)\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Sep 2023 17:33:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at Interspeech 2023\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Shuai Wang; Qibing Bai; Qi Liu; Jianwei Yu; Zhengyang Chen; Bing Han; Yanmin Qian; Haizhou Li', display:{Lore:['[{"text": "arXiv:2309.11730", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging In-the-Wild Data for Effective Self-Supervised Pretraining in Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oShuai Wang\\nQibing Bai\\nQi Liu\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.11730\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 27 Sep 2023 01:05:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Wei Liu; Zhiyuan Peng; Tan Lee', display:{Lore:['[{"text": "arXiv:2309.11768", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCoMFLP: Correlation Measure based Fast Search on ASR Layer Pruning\\u00a7r\\n\\n\\u00a78\\u00a7oWei Liu\\nZhiyuan Peng\\nTan Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.11768\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Sep 2023 04:02:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Interspeech 2023\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Yuxiang Zhang; Zhuo Li; Jingze Lu; Hua Hua; Wenchao Wang; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2309.11827", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Impact of Silence on Speech Anti-Spoofing\\u00a7r\\n\\n\\u00a78\\u00a7oYuxiang Zhang\\nZhuo Li\\nJingze Lu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.11827\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Sep 2023 06:59:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages, 9 figures, 13 tables\\u00a7r"}']}
{title:'Bergsma et al. (§72023§r)', author: 'Boris Bergsma; Marta Brzezinska; Oleg V. Yazyev; Milos Cernak', display:{Lore:['[{"text": "arXiv:2309.11922", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCluster-based pruning techniques for audio data\\u00a7r\\n\\n\\u00a78\\u00a7oBoris Bergsma\\nMarta Brzezinska\\nOleg V. Yazyev\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.11922\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Sep 2023 09:33:41 GMT)\\u00a7r"}']}
{title:'Hiroe et al. (§72023§r)', author: 'Atsuo Hiroe; Katsutoshi Itoyama; Kazuhiro Nakadai', display:{Lore:['[{"text": "arXiv:2309.12065", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIs the Ideal Ratio Mask Really the Best? \\u2013 Exploring the Best Extraction Performance and Optimal Mask of Mask-based Beamformers\\u00a7r\\n\\n\\u00a78\\u00a7oAtsuo Hiroe\\nKatsutoshi Itoyama\\nKazuhiro Nakadai\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.12065\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Sep 2023 13:35:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in APSIPA 2023\\u00a7r"}']}
{title:'Borgstrom et al. (§72023§r)', author: 'Bengt J. Borgstrom; Michael S. Brandstein', display:{Lore:['[{"text": "arXiv:2309.12121", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Multiscale Autoencoder (MSAE) Framework for End-to-End Neural Network Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oBengt J. Borgstrom\\nMichael S. Brandstein\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.12121\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Sep 2023 14:41:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 9 figures\\u00a7r"}']}
{title:'Cutler et al. (§72023§r)', author: 'Ross Cutler; Ando Saabas; Tanel Parnamaa; Marju Purin; Evgenii Indenbom; Nicolae-Catalin Ristea; Jegor Gužvin; Hannes Gamper; Sebastian Braun; Robert Aichner', display:{Lore:['[{"text": "arXiv:2309.12553", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lICASSP 2023 Acoustic Echo Cancellation Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oRoss Cutler\\nAndo Saabas\\nTanel Parnamaa\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.12553\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Sep 2023 00:51:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2202.13290, arXiv:2009.04972\\u00a7r"}']}
{title:'Nakamura et al. (§72023§r)', author: 'Tomohiko Nakamura; Kohei Yatabe', display:{Lore:['[{"text": "arXiv:2309.12581", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSampling-Frequency-Independent Universal Sound Separation\\u00a7r\\n\\n\\u00a78\\u00a7oTomohiko Nakamura\\nKohei Yatabe\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.12581\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Sep 2023 02:16:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP2024\\u00a7r"}']}
{title:'Tawara et al. (§72023§r)', author: 'Naohiro Tawara; Marc Delcroix; Atsushi Ando; Atsunori Ogawa', display:{Lore:['[{"text": "arXiv:2309.12656", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNTT speaker diarization system for CHiME-7: multi-domain, multi-microphone End-to-end and vector clustering diarization\\u00a7r\\n\\n\\u00a78\\u00a7oNaohiro Tawara\\nMarc Delcroix\\nAtsushi Ando\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.12656\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Sep 2023 06:53:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, Submitted to ICASSP 2024\\u00a7r"}']}
{title:'Malard et al. (§72023§r)', author: 'Hugo Malard; Salah Zaiem; Robin Algayres', display:{Lore:['[{"text": "arXiv:2309.12712", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBig model only for hard audios: Sample dependent Whisper model selection for efficient inferences\\u00a7r\\n\\n\\u00a78\\u00a7oHugo Malard\\nSalah Zaiem\\nRobin Algayres\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.12712\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Sep 2023 08:50:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Tehrani et al. (§72023§r)', author: 'Amirali Soltani Tehrani; Niloufar Faridani; Ramin Toosi', display:{Lore:['[{"text": "arXiv:2309.12714", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Representations Improve Supervised Learning in Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAmirali Soltani Tehrani\\nNiloufar Faridani\\nRamin Toosi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.12714\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Sep 2023 08:54:06 GMT)\\u00a7r"}']}
{title:'Ullah et al. (§72023§r)', author: 'Asad Ullah; Alessandro Ragano; Andrew Hines', display:{Lore:['[{"text": "arXiv:2309.12763", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReduce, Reuse, Recycle: Is Perturbed Data better than Other Language augmentation for Low Resource Self-Supervised Speech Models\\u00a7r\\n\\n\\u00a78\\u00a7oAsad Ullah\\nAlessandro Ragano\\nAndrew Hines\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.12763\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Sep 2023 10:09:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, ICASSP24\\u00a7r"}']}
{title:'Gu et al. (§72023§r)', author: 'Yu Gu; Yianrao Bian; Guangzhi Lei; Chao Weng; Dan Su', display:{Lore:['[{"text": "arXiv:2309.12792", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDurIAN-E: Duration Informed Attention Network For Expressive Text-to-Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYu Gu\\nYianrao Bian\\nGuangzhi Lei\\nChao Weng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.12792\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Sep 2023 11:06:04 GMT)\\u00a7r"}']}
{title:'Guimarães et al. (§72023§r)', author: 'Heitor R. Guimarães; Arthur Pimentel; Anderson Avila; Tiago H. Falk', display:{Lore:['[{"text": "arXiv:2309.12914", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVIC-KD: Variance-Invariance-Covariance Knowledge Distillation to Make Keyword Spotting More Robust Against Adversarial Attacks\\u00a7r\\n\\n\\u00a78\\u00a7oHeitor R. Guimar\\u00e3es\\nArthur Pimentel\\nAnderson Avila\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.12914\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Sep 2023 15:03:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Weiran Wang; Rohit Prabhavalkar; Dongseong Hwang; Qiujia Li; Khe Chai Sim; Bo Li; James Qin; Xingyu Cai; Adam Stooke; Zhong Meng; CJ Zheng; Yanzhang He; Tara Sainath; Pedro Moreno Mengibar', display:{Lore:['[{"text": "arXiv:2309.12963", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMassive End-to-end Models for Short Search Queries\\u00a7r\\n\\n\\u00a78\\u00a7oWeiran Wang\\nRohit Prabhavalkar\\nDongseong Hwang\\n+ 10 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.12963\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Sep 2023 16:00:50 GMT)\\u00a7r"}']}
{title:'Carvalho et al. (§72023§r)', author: 'Carlos Carvalho; Alberto Abad', display:{Lore:['[{"text": "arXiv:2309.13029", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMemory-augmented conformer for improved end-to-end long-form ASR\\u00a7r\\n\\n\\u00a78\\u00a7oCarlos Carvalho\\nAlberto Abad\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13029\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-893\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. INTERSPEECH 2023, 2218--2222\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Sep 2023 17:44:58 GMT)\\u00a7r"}']}
{title:'Azam et al. (§72023§r)', author: 'Sheikh Shams Azam; Tatiana Likhomanenko; Martin Pelikan; Jan "Honza" Silovsky', display:{Lore:['[{"text": "arXiv:2309.13102", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.DC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImportance of Smoothness Induced by Optimizers in FL4ASR: Towards Understanding Federated Learning for End-to-End ASR\\u00a7r\\n\\n\\u00a78\\u00a7oSheikh Shams Azam\\nTatiana Likhomanenko\\nMartin Pelikan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13102\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Sep 2023 17:23:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) 2023\\u00a7r"}']}
{title:'Tu et al. (§72023§r)', author: 'Youzhi Tu; Man-Wai Mak; Jen-Tzung Chien', display:{Lore:['[{"text": "arXiv:2309.13253", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContrastive Speaker Embedding With Sequential Disentanglement\\u00a7r\\n\\n\\u00a78\\u00a7oYouzhi Tu\\nMan-Wai Mak\\nJen-Tzung Chien\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13253\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 23 Sep 2023 04:22:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Chunxi Wang; Maoshen Jia; Meiran Li; Changchun Bao; Wenyu Jin', display:{Lore:['[{"text": "arXiv:2309.13504", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention Is All You Need For Blind Room Volume Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oChunxi Wang\\nMaoshen Jia\\nMeiran Li\\nChangchun Bao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13504\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 27 Dec 2023 16:38:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, to be published in proceedings of ICASSP 2024\\u00a7r"}']}
{title:'Purushothaman et al. (§72023§r)', author: 'Anurenjan Purushothaman; Debottam Dutta; Rohit Kumar; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:2309.13537", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech enhancement with frequency domain auto-regressive modeling\\u00a7r\\n\\n\\u00a78\\u00a7oAnurenjan Purushothaman\\nDebottam Dutta\\nRohit Kumar\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13537\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3317570\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech and Language Processing\\n  2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 24 Sep 2023 03:25:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Jingyu Li; Tan Lee', display:{Lore:['[{"text": "arXiv:2309.13605", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Black-Box Speaker Verification Model Adaptation with Reprogramming and Backend Learning\\u00a7r\\n\\n\\u00a78\\u00a7oJingyu Li\\nTan Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13605\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 24 Sep 2023 10:50:31 GMT)\\u00a7r"}']}
{title:'Lu et al. (§72023§r)', author: 'Xugang Lu; Peng Shen; Yu Tsao; Hisashi Kawai', display:{Lore:['[{"text": "arXiv:2309.13650", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCross-modal Alignment with Optimal Transport for CTC-based ASR\\u00a7r\\n\\n\\u00a78\\u00a7oXugang Lu\\nPeng Shen\\nYu Tsao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13650\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 24 Sep 2023 14:34:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEASRU 2023\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Yeonghyeon Lee; Inmo Yeon; Juhan Nam; Joon Son Chung', display:{Lore:['[{"text": "arXiv:2309.13664", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoiceLDM: Text-to-Speech with Environmental Context\\u00a7r\\n\\n\\u00a78\\u00a7oYeonghyeon Lee\\nInmo Yeon\\nJuhan Nam\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13664\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 24 Sep 2023 15:20:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDemos and code are available at https://voiceldm.github.io\\u00a7r"}']}
{title:'Lai et al. (§72023§r)', author: 'Wei-Ting Lai; Lachlan Birnie; Thushara Abhayapala; Amy Bastine; Shaoheng Xu; Prasanga Samarasinghe', display:{Lore:['[{"text": "arXiv:2309.13819", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Two-Step Approach for Narrowband Source Localization in Reverberant Rooms\\u00a7r\\n\\n\\u00a78\\u00a7oWei-Ting Lai\\nLachlan Birnie\\nThushara Abhayapala\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13819\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Sep 2023 02:00:40 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Leying Zhang; Yao Qian; Linfeng Yu; Heming Wang; Xinkai Wang; Hemin Yang; Long Zhou; Shujie Liu; Yanmin Qian; Michael Zeng', display:{Lore:['[{"text": "arXiv:2309.13874", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffusion Conditional Expectation Model for Efficient and Robust Target Speech Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oLeying Zhang\\nYao Qian\\nLinfeng Yu\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13874\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Sep 2023 04:58:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Yu et al. (§72023§r)', author: 'Jianwei Yu; Hangting Chen; Yanyao Bian; Xiang Li; Yi Luo; Jinchuan Tian; Mengyang Liu; Jiayi Jiang; Shuai Wang', display:{Lore:['[{"text": "arXiv:2309.13905", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutoPrep: An Automatic Preprocessing Framework for In-the-Wild Speech Data\\u00a7r\\n\\n\\u00a78\\u00a7oJianwei Yu\\nHangting Chen\\nYanyao Bian\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13905\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Sep 2023 07:01:10 GMT)\\u00a7r"}']}
{title:'Liang et al. (§72023§r)', author: 'Di Liang; Nian Shao; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2309.13916", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrame-wise streaming end-to-end speaker diarization with non-autoregressive self-attention-based attractors\\u00a7r\\n\\n\\u00a78\\u00a7oDi Liang\\nNian Shao\\nXiaofei Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13916\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Sep 2023 07:33:54 GMT)\\u00a7r"}']}
{title:'Harju et al. (§72023§r)', author: 'Manu Harju; Annamaria Mesaros', display:{Lore:['[{"text": "arXiv:2309.13938", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluating Classification Systems Against Soft Labels with Fuzzy Precision and Recall\\u00a7r\\n\\n\\u00a78\\u00a7oManu Harju\\nAnnamaria Mesaros\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13938\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Sep 2023 08:16:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7opublished in DCASE 2023\\u00a7r"}']}
{title:'Yu et al. (§72023§r)', author: 'Wenyi Yu; Changli Tang; Guangzhi Sun; Xianzhao Chen; Tian Tan; Wei Li; Lu Lu; Zejun Ma; Chao Zhang', display:{Lore:['[{"text": "arXiv:2309.13963", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConnecting Speech Encoder and Large Language Model for ASR\\u00a7r\\n\\n\\u00a78\\u00a7oWenyi Yu\\nChangli Tang\\nGuangzhi Sun\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13963\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 26 Sep 2023 11:09:25 GMT)\\u00a7r"}']}
{title:'Poncelet et al. (§72023§r)', author: 'Jakob Poncelet; Hugo Van hamme', display:{Lore:['[{"text": "arXiv:2309.13994", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Accent Adaptation Through Masked Language Model Correction Of Discrete Self-Supervised Speech Units\\u00a7r\\n\\n\\u00a78\\u00a7oJakob Poncelet\\nHugo Van hamme\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13994\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Sep 2023 09:51:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP2024\\u00a7r"}']}
{title:'Kadiri et al. (§72023§r)', author: 'Sudarsana Reddy Kadiri; Paavo Alku', display:{Lore:['[{"text": "arXiv:2309.14080", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalysis and Detection of Pathological Voice using Glottal Source Features\\u00a7r\\n\\n\\u00a78\\u00a7oSudarsana Reddy Kadiri\\nPaavo Alku\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.14080\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/JSTSP.2019.2957988\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Journal of Selected Topics in Signal Processing, Vol. 14, No.\\n  2, pp. 367-379, February 2020\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 17 Oct 2023 13:36:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCopyright 2020 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising "}','{"text": "or promotional purposes, creating newcollective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Javanmardi et al. (§72023§r)', author: 'Farhad Javanmardi; Saska Tirronen; Manila Kodali; Sudarsana Reddy Kadiri; Paavo Alku', display:{Lore:['[{"text": "arXiv:2309.14107", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWav2vec-based Detection and Severity Level Classification of Dysarthria from Speech\\u00a7r\\n\\n\\u00a78\\u00a7oFarhad Javanmardi\\nSaska Tirronen\\nManila Kodali\\nSudarsana Reddy Kadiri\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.14107\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP49357.2023.10094857\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nin Proc. ICASSP, Rhodes Island, Greece, June 4-10, 2023\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 17 Oct 2023 13:38:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ocopyright 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising "}','{"text": "or promotional purposes, creating newcollective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Lin et al. (§72023§r)', author: 'Yuke Lin; Xiaoyi Qin; Ning Jiang; Guoqing Zhao; Ming Li', display:{Lore:['[{"text": "arXiv:2309.14109", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHaha-Pod: An Attempt for Laughter-based Non-Verbal Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oYuke Lin\\nXiaoyi Qin\\nNing Jiang\\nGuoqing Zhao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.14109\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 9 Oct 2023 07:18:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by ASRU 2023\\u00a7r"}']}
{title:'Lindsey et al. (§72023§r)', author: 'Mark Lindsey; Ankit Shah; Francis Kubala; Richard M. Stern', display:{Lore:['[{"text": "arXiv:2309.14460", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOnline Active Learning For Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oMark Lindsey\\nAnkit Shah\\nFrancis Kubala\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.14460\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Sep 2023 18:48:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024. Publication will belong to IEEE\\u00a7r"}']}
{title:'Pimentel et al. (§72023§r)', author: 'Arthur Pimentel; Heitor Guimarães; Anderson R. Avila; Mehdi Rezagholizadeh; Tiago H. Falk', display:{Lore:['[{"text": "arXiv:2309.14462", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Impact of Quantization and Pruning of Self-Supervised Speech Models for Downstream Speech Recognition Tasks \\"In-the-Wild\\u201d\\u00a7r\\n\\n\\u00a78\\u00a7oArthur Pimentel\\nHeitor Guimar\\u00e3es\\nAnderson R. Avila\\nMehdi Rezagholizadeh\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.14462\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Sep 2023 18:54:16 GMT)\\u00a7r"}']}
{title:'Heo et al. (§72023§r)', author: 'Hee-Soo Heo; KiHyun Nam; Bong-Jin Lee; Youngki Kwon; Minjae Lee; You Jin Kim; Joon Son Chung', display:{Lore:['[{"text": "arXiv:2309.14741", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRethinking Session Variability: Leveraging Session Embeddings for Session Robustness in Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oHee-Soo Heo\\nKiHyun Nam\\nBong-Jin Lee\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.14741\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Sep 2023 08:09:30 GMT)\\u00a7r"}']}
{title:'An et al. (§72023§r)', author: 'Keyu An; Shiliang Zhang', display:{Lore:['[{"text": "arXiv:2309.14758", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring RWKV for Memory Efficient and Low Latency Streaming ASR\\u00a7r\\n\\n\\u00a78\\u00a7oKeyu An\\nShiliang Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.14758\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Sep 2023 08:41:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Cámara et al. (§72023§r)', author: 'Mateo Cámara; Zhiyuan Xu; Yisu Zong; José Luis Blanco; Joshua D. Reiss', display:{Lore:['[{"text": "arXiv:2309.14761", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimization Techniques for a Physical Model of Human Vocalisation\\u00a7r\\n\\n\\u00a78\\u00a7oMateo C\\u00e1mara\\nZhiyuan Xu\\nYisu Zong\\nJos\\u00e9 Luis Blanco\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.14761\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Sep 2023 08:45:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to DAFx 2023\\u00a7r"}']}
{title:'Someki et al. (§72023§r)', author: 'Masao Someki; Nicholas Eng; Yosuke Higuchi; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2309.14922", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSegment-Level Vectorized Beam Search Based on Partially Autoregressive Inference\\u00a7r\\n\\n\\u00a78\\u00a7oMasao Someki\\nNicholas Eng\\nYosuke Higuchi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.14922\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ASRU57964.2023.10389796\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE Automatic Speech Recognition and Understanding Workshop 2023\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 1 Oct 2023 03:48:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ASRU 2023\\u00a7r"}']}
{title:'Takawale et al. (§72023§r)', author: 'Harshvardhan Takawale; Nirupam Roy', display:{Lore:['[{"text": "arXiv:2309.15064", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSimultaneously Learning Speaker\'s Direction and Head Orientation from Binaural Recordings\\u00a7r\\n\\n\\u00a78\\u00a7oHarshvardhan Takawale\\nNirupam Roy\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.15064\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Sep 2023 16:49:21 GMT)\\u00a7r"}']}
{title:'Wilkinghoff et al. (§72023§r)', author: 'Kevin Wilkinghoff; Frank Kurth', display:{Lore:['[{"text": "arXiv:2309.15643", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhy do Angular Margin Losses work well for Semi-Supervised Anomalous Sound Detection?\\u00a7r\\n\\n\\u00a78\\u00a7oKevin Wilkinghoff\\nFrank Kurth\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.15643\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3337153\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE/ACM Transactions on Audio, Speech, and Language Processing,\\n  vol. 32 (2024), p. 608-622\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 24 Nov 2023 06:20:25 GMT)\\u00a7r"}']}
{title:'Gao et al. (§72023§r)', author: 'Dongji Gao; Hainan Xu; Desh Raj; Leibny Paola Garcia Perera; Daniel Povey; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:2309.15796", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning from Flawed Data: Weakly Supervised Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDongji Gao\\nHainan Xu\\nDesh Raj\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.15796\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Sep 2023 12:58:40 GMT)\\u00a7r"}']}
{title:'Jiang et al. (§72023§r)', author: 'Xilin Jiang; Cong Han; Yinghao Aaron Li; Nima Mesgarani', display:{Lore:['[{"text": "arXiv:2309.15938", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Self-Supervised Contrastive Learning of Spatial Sound Event Representation\\u00a7r\\n\\n\\u00a78\\u00a7oXilin Jiang\\nCong Han\\nYinghao Aaron Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.15938\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Sep 2023 18:23:03 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Hao Zhang; Yixuan Zhang; Meng Yu; Dong Yu', display:{Lore:['[{"text": "arXiv:2309.16048", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdvancing Acoustic Howling Suppression through Recursive Training of Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oHao Zhang\\nYixuan Zhang\\nMeng Yu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.16048\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Sep 2023 22:02:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper in submission\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Yixuan Zhang; Hao Zhang; Meng Yu; Dong Yu', display:{Lore:['[{"text": "arXiv:2309.16049", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Network Augmented Kalman Filter for Robust Acoustic Howling Suppression\\u00a7r\\n\\n\\u00a78\\u00a7oYixuan Zhang\\nHao Zhang\\nMeng Yu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.16049\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Sep 2023 22:07:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPaper in submission\\u00a7r"}']}
{title:'Lu et al. (§72023§r)', author: 'Xugang Lu; Peng Shen; Yu Tsao; Hisashi Kawai', display:{Lore:['[{"text": "arXiv:2309.16093", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierarchical Cross-Modality Knowledge Transfer with Sinkhorn Attention for CTC-based ASR\\u00a7r\\n\\n\\u00a78\\u00a7oXugang Lu\\nPeng Shen\\nYu Tsao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.16093\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Sep 2023 01:31:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Lyu et al. (§72023§r)', author: 'Xiang Lyu; Yuhang Cao; Qing Wang; Jingjing Yin; Yuguang Yang; Pengpeng Zou; Yanni Hu; Heng Lu', display:{Lore:['[{"text": "arXiv:2309.16247", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPP-MeT: a Real-world Personalized Prompt based Meeting Transcription System\\u00a7r\\n\\n\\u00a78\\u00a7oXiang Lyu\\nYuhang Cao\\nQing Wang\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.16247\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Sep 2023 08:40:48 GMT)\\u00a7r"}']}
{title:'Çoban et al. (§72023§r)', author: 'Enis Berk Çoban; Megan Perra; Michael I. Mandel', display:{Lore:['[{"text": "arXiv:2309.16867", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards High Resolution Weather Monitoring with Sound Data\\u00a7r\\n\\n\\u00a78\\u00a7oEnis Berk \\u00c7oban\\nMegan Perra\\nMichael I. Mandel\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.16867\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Sep 2023 21:49:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, submitted to ICASSP 2024\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Hexin Liu; Leibny Paola Garcia; Xiangyu Zhang; Andy W. H. Khong; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:2309.16953", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Code-switching Speech Recognition with Interactive Language Biases\\u00a7r\\n\\n\\u00a78\\u00a7oHexin Liu\\nLeibny Paola Garcia\\nXiangyu Zhang\\nAndy W. H. Khong\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.16953\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Sep 2023 03:37:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEEICASSP 2024\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Yuxiang Zhang; Zhuo Li; Jingze Lu; Wenchao Wang; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2309.16954", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynthetic Speech Detection Based on Temporal Consistency and Distribution of Speaker Features\\u00a7r\\n\\n\\u00a78\\u00a7oYuxiang Zhang\\nZhuo Li\\nJingze Lu\\nWenchao Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.16954\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Sep 2023 03:50:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 4 tables\\u00a7r"}']}
{title:'Hsu et al. (§72023§r)', author: 'Po-chun Hsu; Ali Elkahky; Wei-Ning Hsu; Yossi Adi; Tu Anh Nguyen; Jade Copet; Emmanuel Dupoux; Hung-yi Lee; Abdelrahman Mohamed', display:{Lore:['[{"text": "arXiv:2309.17020", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLow-Resource Self-Supervised Learning with SSL-Enhanced TTS\\u00a7r\\n\\n\\u00a78\\u00a7oPo-chun Hsu\\nAli Elkahky\\nWei-Ning Hsu\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.17020\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Sep 2023 07:09:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint\\u00a7r"}']}
{title:'Antonova (§72023§r)', author: 'Alexandra Antonova', display:{Lore:['[{"text": "arXiv:2309.17267", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWiki-En-ASR-Adapt: Large-scale synthetic dataset for English ASR Customization\\u00a7r\\n\\n\\u00a78\\u00a7oAlexandra Antonova\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.17267\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Sep 2023 14:18:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEASRU 2023\\u00a7r"}']}
{title:'Yakovlev et al. (§72023§r)', author: 'Ivan Yakovlev; Mikhail Melnikov; Nikita Bukhal; Rostislav Makarov; Alexander Alenin; Nikita Torgashov; Anton Okhotnikov', display:{Lore:['[{"text": "arXiv:2309.17298", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLRPD: Large Replay Parallel Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oIvan Yakovlev\\nMikhail Melnikov\\nNikita Bukhal\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.17298\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9746527\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2022 - 2022 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP), 6612-6616\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Sep 2023 14:55:30 GMT)\\u00a7r"}']}
{title:'Lu et al. (§72023§r)', author: 'Austin Lu; Ethaniel Moore; Arya Nallanthighall; Kanad Sarkar; Manan Mittal; Ryan M. Corey; Paris Smaragdis; Andrew Singer', display:{Lore:['[{"text": "arXiv:2310.00587", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SY\\u00a7r, \\u00a7eeess.SY\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMechatronic Generation of Datasets for Acoustics Research\\u00a7r\\n\\n\\u00a78\\u00a7oAustin Lu\\nEthaniel Moore\\nArya Nallanthighall\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.00587\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/IWAENC53105.2022.9914771\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 1 Oct 2023 06:16:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 5 figures, IWAENC 2022\\u00a7r"}']}
{title:'Dey et al. (§72023§r)', author: 'Spandan Dey; Premjeet Singh; Goutam Saha', display:{Lore:['[{"text": "arXiv:2310.00602", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWavelet Scattering Transform for Improving Generalization in Low-Resourced Spoken Language Identification\\u00a7r\\n\\n\\u00a78\\u00a7oSpandan Dey\\nPremjeet Singh\\nGoutam Saha\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.00602\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 3 Oct 2023 08:00:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted and presented in INTERSPEECH 2023\\u00a7r"}']}
{title:'Gajecki et al. (§72023§r)', author: 'Tom Gajecki; Waldo Nogueira', display:{Lore:['[{"text": "arXiv:2310.01122", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Fused Deep Denoising Sound Coding Strategy for Bilateral Cochlear Implants\\u00a7r\\n\\n\\u00a78\\u00a7oTom Gajecki\\nWaldo Nogueira\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.01122\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 2 Oct 2023 11:54:49 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Tianchi Liu; Kong Aik Lee; Qiongqiong Wang; Haizhou Li', display:{Lore:['[{"text": "arXiv:2310.01128", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDisentangling Voice and Content with Self-Supervision for Speaker Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oTianchi Liu\\nKong Aik Lee\\nQiongqiong Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.01128\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 1 Nov 2023 16:27:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to NeurIPS 2023 (main track)\\u00a7r"}']}
{title:'Hung et al. (§72023§r)', author: 'Yun-Ning Hung; Ju-Chiang Wang; Minz Won; Duc Le', display:{Lore:['[{"text": "arXiv:2310.01353", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScaling Up Music Information Retrieval Training with Semi-Supervised Learning\\u00a7r\\n\\n\\u00a78\\u00a7oYun-Ning Hung\\nJu-Chiang Wang\\nMinz Won\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.01353\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 2 Oct 2023 17:16:47 GMT)\\u00a7r"}']}
{title:'Cornell et al. (§72023§r)', author: 'Samuele Cornell; Jee-weon Jung; Shinji Watanabe; Stefano Squartini', display:{Lore:['[{"text": "arXiv:2310.01688", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOne model to rule them all ? Towards End-to-End Joint Speaker Diarization and Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSamuele Cornell\\nJee-weon Jung\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.01688\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 2 Oct 2023 23:03:30 GMT)\\u00a7r"}']}
{title:'Yan et al. (§72023§r)', author: 'Bi-Cheng Yan; Hsin-Wei Wang; Yi-Cheng Wang; Jiun-Ting Li; Chi-Han Lin; Berlin Chen', display:{Lore:['[{"text": "arXiv:2310.01839", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPreserving Phonemic Distinctions for Ordinal Regression: A Novel Loss Function for Automatic Pronunciation Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oBi-Cheng Yan\\nHsin-Wei Wang\\nYi-Cheng Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.01839\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 4 Oct 2023 06:51:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ASRU 2023\\u00a7r"}']}
{title:'Xu et al. (§72023§r)', author: 'Anfeng Xu; Kevin Huang; Tiantian Feng; Helen Tager-Flusberg; Shrikanth Narayanan', display:{Lore:['[{"text": "arXiv:2310.01867", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-visual child-adult speaker classification in dyadic interactions\\u00a7r\\n\\n\\u00a78\\u00a7oAnfeng Xu\\nKevin Huang\\nTiantian Feng\\nHelen Tager-Flusberg\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.01867\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 9 Oct 2023 22:16:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIn review for ICASSP 2024,5 pages\\u00a7r"}']}
{title:'Feng et al. (§72023§r)', author: 'Yajing Feng; Laurence Devillers', display:{Lore:['[{"text": "arXiv:2310.02281", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Continuous Speech Emotion Recognition in Real-life Customer Service Call Center Conversations\\u00a7r\\n\\n\\u00a78\\u00a7oYajing Feng\\nLaurence Devillers\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.02281\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2023 11th International Conference on Affective Computing and\\n  Intelligent Interaction Workshops and Demos (ACIIW), Sep 2023, Boston (MA),\\n  United States\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 2 Oct 2023 11:53:48 GMT)\\u00a7r"}']}
{title:'Cooper et al. (§72023§r)', author: 'Erica Cooper; Wen-Chin Huang; Yu Tsao; Hsin-Min Wang; Tomoki Toda; Junichi Yamagishi', display:{Lore:['[{"text": "arXiv:2310.02640", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe VoiceMOS Challenge 2023: Zero-shot Subjective Speech Quality Prediction for Multiple Domains\\u00a7r\\n\\n\\u00a78\\u00a7oErica Cooper\\nWen-Chin Huang\\nYu Tsao\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.02640\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 7 Oct 2023 01:26:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ASRU 2023\\u00a7r"}']}
{title:'Klement et al. (§72023§r)', author: 'Dominik Klement; Mireia Diez; Federico Landini; Lukáš Burget; Anna Silnova; Marc Delcroix; Naohiro Tawara', display:{Lore:['[{"text": "arXiv:2310.02732", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiscriminative Training of VBx Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oDominik Klement\\nMireia Diez\\nFederico Landini\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.02732\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 4 Oct 2023 11:10:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Ning et al. (§72023§r)', author: 'Ziqian Ning; Yuepeng Jiang; Zhichao Wang; Bin Zhang; Lei Xie', display:{Lore:['[{"text": "arXiv:2310.02802", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVITS-Based Singing Voice Conversion Leveraging Whisper and multi-scale F0 Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oZiqian Ning\\nYuepeng Jiang\\nZhichao Wang\\nBin Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.02802\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 4 Oct 2023 13:25:04 GMT)\\u00a7r"}']}
{title:'Chang et al. (§72023§r)', author: 'Kai-Wei Chang; Ming-Hsin Chen; Yun-Ping Lin; Jing Neng Hsu; Paul Kuo-Ming Huang; Chien-yu Huang; Shang-Wen Li; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2310.02971", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech Model\\u00a7r\\n\\n\\u00a78\\u00a7oKai-Wei Chang\\nMing-Hsin Chen\\nYun-Ping Lin\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.02971\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 14 Nov 2023 21:15:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEASRU 2023\\u00a7r"}']}
{title:'Bous et al. (§72023§r)', author: 'Frederik Bous; Axel Roebel', display:{Lore:['[{"text": "arXiv:2310.03444", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVaSAB: The variable size adaptive information bottleneck for disentanglement on speech and singing voice\\u00a7r\\n\\n\\u00a78\\u00a7oFrederik Bous\\nAxel Roebel\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.03444\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 5 Oct 2023 10:30:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Rafaely et al. (§72023§r)', author: 'Boaz Rafaely; Koby Alhaiany', display:{Lore:['[{"text": "arXiv:2310.03688", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker localization using direct path dominance test based on sound field directivity\\u00a7r\\n\\n\\u00a78\\u00a7oBoaz Rafaely\\nKoby Alhaiany\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.03688\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.sigpro.2017.08.010\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nSignal Processing, vol. 143, pp. 42 - 47, 2018\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 5 Oct 2023 17:09:50 GMT)\\u00a7r"}']}
{title:'Hou et al. (§72023§r)', author: 'Yuanbo Hou; Siyang Song; Chuang Yu; Wenwu Wang; Dick Botteldooren', display:{Lore:['[{"text": "arXiv:2310.03889", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Event-Relational Graph Representation Learning for Acoustic Scene Classification\\u00a7r\\n\\n\\u00a78\\u00a7oYuanbo Hou\\nSiyang Song\\nChuang Yu\\nWenwu Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.03889\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2023.3319233\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 5 Oct 2023 20:48:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE Signal Processing Letters, doi: 10.1109/LSP.2023.3319233\\u00a7r"}']}
{title:'Shao (§72023§r)', author: 'Yiwen Shao', display:{Lore:['[{"text": "arXiv:2310.03901", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChallenges and Insights: Exploring 3D Spatial Features and Complex Networks on the MISP Dataset\\u00a7r\\n\\n\\u00a78\\u00a7oYiwen Shao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.03901\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 5 Oct 2023 21:16:54 GMT)\\u00a7r"}']}
{title:'Niwa et al. (§72023§r)', author: 'Shoko Niwa; Sayaka Shiota; Hitoshi Kiya', display:{Lore:['[{"text": "arXiv:2310.04035", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA privacy-preserving method using secret key for convolutional neural network-based speech classification\\u00a7r\\n\\n\\u00a78\\u00a7oShoko Niwa\\nSayaka Shiota\\nHitoshi Kiya\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.04035\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 Oct 2023 06:14:35 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in the 31st European Signal Processing Conference (EUSIPCO 2023)\\u00a7r"}']}
{title:'Rafaely (§72023§r)', author: 'Boaz Rafaely', display:{Lore:['[{"text": "arXiv:2310.04169", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatial sampling and beamforming for spherical microphone arrays\\u00a7r\\n\\n\\u00a78\\u00a7oBoaz Rafaely\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.04169\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/HSCMA.2008.4538673\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2008 Hands-Free Speech Communication and Microphone Arrays,\\n  Trento, Italy, 2008, pp. 5-8\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 Oct 2023 11:38:01 GMT)\\u00a7r"}']}
{title:'Rafaely (§72023§r)', author: 'Boaz Rafaely', display:{Lore:['[{"text": "arXiv:2310.04191", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZones of quiet in a broadband diffuse sound field\\u00a7r\\n\\n\\u00a78\\u00a7oBoaz Rafaely\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.04191\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/1.1377632\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJ. Acoust. Soc. Am., vol. 110, no. 1, pp. 296-302, July 2001\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 Oct 2023 12:11:36 GMT)\\u00a7r"}']}
{title:'Rafaely et al. (§72023§r)', author: 'Boaz Rafaely; Dima Khaykin', display:{Lore:['[{"text": "arXiv:2310.04202", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimal model-based beamforming and independent steering for spherical loudspeaker arrays\\u00a7r\\n\\n\\u00a78\\u00a7oBoaz Rafaely\\nDima Khaykin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.04202\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASL.2011.2116011\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nin IEEE Trans. Audio, Speech, and Lang. Proc., vol. 19, no. 7, pp.\\n  2234-2238, Sept. 2011\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 Oct 2023 12:40:02 GMT)\\u00a7r"}']}
{title:'Yu (§72023§r)', author: 'Guo Yu', display:{Lore:['[{"text": "arXiv:2310.04249", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalysis on the Influence of Synchronization Error on Fixed-filter Active Noise Control\\u00a7r\\n\\n\\u00a78\\u00a7oGuo Yu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.04249\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 Oct 2023 13:48:14 GMT)\\u00a7r"}']}
{title:'Macary et al. (§72023§r)', author: 'Manon Macary; Marie Tahon; Yannick Estève; Daniel Luzzati', display:{Lore:['[{"text": "arXiv:2310.04481", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic and linguistic representations for speech continuous emotion recognition in call center conversations\\u00a7r\\n\\n\\u00a78\\u00a7oManon Macary\\nMarie Tahon\\nYannick Est\\u00e8ve\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.04481\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 Oct 2023 10:22:51 GMT)\\u00a7r"}']}
{title:'Hai et al. (§72023§r)', author: 'Jiarui Hai; Helin Wang; Dongchao Yang; Karan Thakkar; Najim Dehak; Mounya Elhilali', display:{Lore:['[{"text": "arXiv:2310.04567", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDPM-TSE: A Diffusion Probabilistic Model for Target Sound Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oJiarui Hai\\nHelin Wang\\nDongchao Yang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.04567\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 10 Oct 2023 00:44:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Huang et al. (§72023§r)', author: 'Kaixun Huang; Ao Zhang; Binbin Zhang; Tianyi Xu; Xingchen Song; Lei Xie', display:{Lore:['[{"text": "arXiv:2310.04657", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpike-Triggered Contextual Biasing for End-to-End Mandarin Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oKaixun Huang\\nAo Zhang\\nBinbin Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.04657\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 7 Oct 2023 02:31:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ASRU2023\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Zihan Zhang; Jiayao Sun; Xianjun Xia; Ziqian Wang; Xiaopeng Yan; Yijian Xiao; Lei Xie', display:{Lore:['[{"text": "arXiv:2310.04715", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Exploration of Task-decoupling on Two-stage Neural Post Filter for Real-time Personalized Acoustic Echo Cancellation\\u00a7r\\n\\n\\u00a78\\u00a7oZihan Zhang\\nJiayao Sun\\nXianjun Xia\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.04715\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 7 Oct 2023 07:14:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to ASRU 2023\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Ze Li; Yuke Lin; Ning Jiang; Xiaoyi Qin; Guoqing Zhao; Haiying Wu; Ming Li', display:{Lore:['[{"text": "arXiv:2310.04760", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-objective Progressive Clustering for Semi-supervised Domain Adaptation in Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oZe Li\\nYuke Lin\\nNing Jiang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.04760\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 7 Oct 2023 09:46:07 GMT)\\u00a7r"}']}
{title:'Nguyen et al. (§72023§r)', author: 'Theodor Nguyen; Guangzhi Sun; Xianrui Zheng; Chao Zhang; Philip C Woodland', display:{Lore:['[{"text": "arXiv:2310.04791", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConditional Diffusion Model for Target Speaker Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oTheodor Nguyen\\nGuangzhi Sun\\nXianrui Zheng\\nChao Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.04791\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 7 Oct 2023 12:48:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, submitted to ICASSP 2024\\u00a7r"}']}
{title:'Yadav et al. (§72023§r)', author: 'Hemant Yadav; Erica Cooper; Junichi Yamagishi; Sunayana Sitaram; Rajiv Ratn Shah', display:{Lore:['[{"text": "arXiv:2310.05078", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPartial Rank Similarity Minimization Method for Quality MOS Prediction of Unseen Speech Synthesis Systems in Zero-Shot and Semi-supervised setting\\u00a7r\\n\\n\\u00a78\\u00a7oHemant Yadav\\nErica Cooper\\nJunichi Yamagishi\\nSunayana Sitaram\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.05078\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 8 Oct 2023 09:01:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ASRU 2023\\u00a7r"}']}
{title:'Yamamoto et al. (§72023§r)', author: 'Ryuichi Yamamoto; Reo Yoneyama; Lester Phillip Violeta; Wen-Chin Huang; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2310.05203", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparative Study of Voice Conversion Models with Large-Scale Speech and Singing Data: The T13 Systems for the Singing Voice Conversion Challenge 2023\\u00a7r\\n\\n\\u00a78\\u00a7oRyuichi Yamamoto\\nReo Yoneyama\\nLester Phillip Violeta\\nWen-Chin Huang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.05203\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 8 Oct 2023 15:30:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ASRU 2023\\u00a7r"}']}
{title:'Lapidot et al. (§72023§r)', author: 'Itshak Lapidot; Jean-Francois Bonastre', display:{Lore:['[{"text": "arXiv:2310.05534", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThech. Report: Genuinization of Speech waveform PMF for speaker detection spoofing and countermeasures\\u00a7r\\n\\n\\u00a78\\u00a7oItshak Lapidot\\nJean-Francois Bonastre\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.05534\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 9 Oct 2023 08:56:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o17 pages, 11 figures\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Junkang Yang; Hongqing Liu; Lu Gan; Yi Zhou', display:{Lore:['[{"text": "arXiv:2310.05629", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSuper Denoise Net: Speech Super Resolution with Noise Cancellation in Low Sampling Rate Noisy Environments\\u00a7r\\n\\n\\u00a78\\u00a7oJunkang Yang\\nHongqing Liu\\nLu Gan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.05629\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 10 Oct 2023 02:14:18 GMT)\\u00a7r"}']}
{title:'Dabike et al. (§72023§r)', author: 'Gerardo Roa Dabike; Scott Bannister; Jennifer Firth; Simone Graetzer; Rebecca Vos; Michael A. Akeroyd; Jon Barker; Trevor J. Cox; Bruno Fazenda; Alinka Greasley; William Whitmer', display:{Lore:['[{"text": "arXiv:2310.05799", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe First Cadenza Signal Processing Challenge: Improving Music for Those With a Hearing Loss\\u00a7r\\n\\n\\u00a78\\u00a7oGerardo Roa Dabike\\nScott Bannister\\nJennifer Firth\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.05799\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 9 Oct 2023 15:36:15 GMT)\\u00a7r"}']}
{title:'Sun et al. (§72023§r)', author: 'Guangzhi Sun; Wenyi Yu; Changli Tang; Xianzhao Chen; Tian Tan; Wei Li; Lu Lu; Zejun Ma; Chao Zhang', display:{Lore:['[{"text": "arXiv:2310.05863", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFine-grained Audio-Visual Joint Representations for Multimodal Large Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oGuangzhi Sun\\nWenyi Yu\\nChangli Tang\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.05863\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 10 Oct 2023 05:30:49 GMT)\\u00a7r"}']}
{title:'Shivakumar et al. (§72023§r)', author: 'Prashanth Gurunath Shivakumar; Jari Kolehmainen; Yile Gu; Ankur Gandhe; Ariya Rastrow; Ivan Bulyko', display:{Lore:['[{"text": "arXiv:2310.06248", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiscriminative Speech Recognition Rescoring with Pre-trained Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oPrashanth Gurunath Shivakumar\\nJari Kolehmainen\\nYile Gu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.06248\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Oct 2023 01:52:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oASRU 2023\\u00a7r"}']}
{title:'Teixeira et al. (§72023§r)', author: 'Francisco Teixeira; Alberto Abad; Bhiksha Raj; Isabel Trancoso', display:{Lore:['[{"text": "arXiv:2310.06652", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrivacy-oriented manipulation of speaker representations\\u00a7r\\n\\n\\u00a78\\u00a7oFrancisco Teixeira\\nAlberto Abad\\nBhiksha Raj\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.06652\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 10 Oct 2023 14:18:57 GMT)\\u00a7r"}']}
{title:'Hao et al. (§72023§r)', author: 'Xiang Hao; Jibin Wu; Jianwei Yu; Chenglin Xu; Kay Chen Tan', display:{Lore:['[{"text": "arXiv:2310.07284", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTyping to Listen at the Cocktail Party: Text-Guided Target Speaker Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oXiang Hao\\nJibin Wu\\nJianwei Yu\\nChenglin Xu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.07284\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 15 Oct 2023 03:58:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review, https://github.com/haoxiangsnr/llm-tse\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Yuewei Zhang; Huanbin Zou; Jie Zhu', display:{Lore:['[{"text": "arXiv:2310.07295", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVSANet: Real-time Speech Enhancement Based on Voice Activity Detection and Causal Spatial Attention\\u00a7r\\n\\n\\u00a78\\u00a7oYuewei Zhang\\nHuanbin Zou\\nJie Zhu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.07295\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 1 Nov 2023 09:18:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ASRU 2023\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Yuewei Zhang; Huanbin Zou; Jie Zhu', display:{Lore:['[{"text": "arXiv:2310.07316", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMagnitude-and-phase-aware Speech Enhancement with Parallel Sequence Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oYuewei Zhang\\nHuanbin Zou\\nJie Zhu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.07316\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 11 Oct 2023 09:03:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ASRU 2023\\u00a7r"}']}
{title:'Schlecht et al. (§72023§r)', author: 'Sebastian J. Schlecht; Karolina Prawda; Rudolf Rabenstein; Maximilian Schäfer', display:{Lore:['[{"text": "arXiv:2310.07363", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDamping Density of an Absorptive Shoebox Room Derived from the Image-Source Method\\u00a7r\\n\\n\\u00a78\\u00a7oSebastian J. Schlecht\\nKarolina Prawda\\nRudolf Rabenstein\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.07363\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 11 Oct 2023 10:25:35 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Kyuyeon Kim; Junsik Jung; Woo Jae Kim; Sung-Eui Yoon', display:{Lore:['[{"text": "arXiv:2310.07663", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Video Inpainting Guided by Audio-Visual Self-Supervision\\u00a7r\\n\\n\\u00a78\\u00a7oKyuyeon Kim\\nJunsik Jung\\nWoo Jae Kim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.07663\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP43922.2022.9747073\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 11 Oct 2023 17:03:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2022\\u00a7r"}']}
{title:'Baas et al. (§72023§r)', author: 'Matthew Baas; Herman Kamper', display:{Lore:['[{"text": "arXiv:2310.08104", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Conversion for Stuttered Speech, Instruments, Unseen Languages and Textually Described Voices\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew Baas\\nHerman Kamper\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.08104\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 12 Oct 2023 08:00:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 1 figure, 5 tables. Accepted at SACAIR 2023\\u00a7r"}']}
{title:'Park et al. (§72023§r)', author: 'Chanho Park; Chengsong Lu; Mingjie Chen; Thomas Hain', display:{Lore:['[{"text": "arXiv:2310.08225", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFast Word Error Rate Estimation Using Self-Supervised Representations For Speech And Text\\u00a7r\\n\\n\\u00a78\\u00a7oChanho Park\\nChengsong Lu\\nMingjie Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.08225\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 12 Oct 2023 11:17:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Saijo et al. (§72023§r)', author: 'Kohei Saijo; Wangyou Zhang; Zhong-Qiu Wang; Shinji Watanabe; Tetsunori Kobayashi; Tetsuji Ogawa', display:{Lore:['[{"text": "arXiv:2310.08277", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Single Speech Enhancement Model Unifying Dereverberation, Denoising, Speaker Counting, Separation, and Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oKohei Saijo\\nWangyou Zhang\\nZhong-Qiu Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.08277\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 12 Oct 2023 12:28:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 4 figures, 2 tables, accepted by ASRU2023\\u00a7r"}']}
{title:'Onu et al. (§72023§r)', author: 'Charles C. Onu; Samantha Latremouille; Arsenii Gorin; Junhao Wang; Innocent Udeogu; Uchenna Ekwochi; Peter O. Ubuane; Omolara A. Kehinde; Muhammad A. Salisu; Datonye Briggs; Yoshua Bengio; Doina Precup', display:{Lore:['[{"text": "arXiv:2310.08338", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA cry for help: Early detection of brain injury in newborns\\u00a7r\\n\\n\\u00a78\\u00a7oCharles C. Onu\\nSamantha Latremouille\\nArsenii Gorin\\n+ 8 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.08338\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 3 Nov 2023 18:12:26 GMT)\\u00a7r"}']}
{title:'Morrison et al. (§72023§r)', author: 'Max Morrison; Pranav Pawar; Nathan Pruyne; Jennifer Cole; Bryan Pardo', display:{Lore:['[{"text": "arXiv:2310.08464", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCrowdsourced and Automatic Speech Prominence Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oMax Morrison\\nPranav Pawar\\nNathan Pruyne\\nJennifer Cole\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.08464\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 22 Dec 2023 21:55:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished as a conference paper at ICASSP 2024\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Weiqing Wang; Ming Li', display:{Lore:['[{"text": "arXiv:2310.08696", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end Online Speaker Diarization with Target Speaker Tracking\\u00a7r\\n\\n\\u00a78\\u00a7oWeiqing Wang\\nMing Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.08696\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 12 Oct 2023 20:02:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEE/ACM Transactionson Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Bandekar et al. (§72023§r)', author: 'Jesuraj Bandekar; Sathvik Udupa; Abhayjeet Singh; Anjali Jayakumar; Deekshitha G; Sandhya Badiger; Saurabh Kumar; Pooja VH; Prasanta Kumar Ghosh', display:{Lore:['[{"text": "arXiv:2310.08846", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaking rate attention-based duration prediction for speed control TTS\\u00a7r\\n\\n\\u00a78\\u00a7oJesuraj Bandekar\\nSathvik Udupa\\nAbhayjeet Singh\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.08846\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 13 Oct 2023 04:13:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oo\\u0327p\\u0327y\\u0327\\u0157i\\u0327\\u0123\\u1e29\\u0163 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for "}','{"text": "advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Ali et al. (§72023§r)', author: 'Hashim Ali; Dhimant Khuttan; Rafi Ud Daula Refat; Hafiz Malik', display:{Lore:['[{"text": "arXiv:2310.09404", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProtecting Voice-Controlled Devices against LASER Injection Attacks\\u00a7r\\n\\n\\u00a78\\u00a7oHashim Ali\\nDhimant Khuttan\\nRafi Ud Daula Refat\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.09404\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 13 Oct 2023 21:09:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 7 figures\\u00a7r"}']}
{title:'Patel et al. (§72023§r)', author: 'Kashyap Patel; Anton Kovalyov; Issa Panahi', display:{Lore:['[{"text": "arXiv:2310.10026", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-time Speech Enhancement and Separation with a Unified Deep Neural Network for Single/Dual Talker Scenarios\\u00a7r\\n\\n\\u00a78\\u00a7oKashyap Patel\\nAnton Kovalyov\\nIssa Panahi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.10026\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 16 Oct 2023 03:02:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 Pages, Accepted at IEEE Asilomar\\u00a7r"}']}
{title:'Porjazovski et al. (§72023§r)', author: 'Dejan Porjazovski; Yaroslav Getman; Tamás Grósz; Mikko Kurimo', display:{Lore:['[{"text": "arXiv:2310.10179", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdvancing Audio Emotion and Intent Recognition with Large Pre-Trained Models and Bayesian Inference\\u00a7r\\n\\n\\u00a78\\u00a7oDejan Porjazovski\\nYaroslav Getman\\nTam\\u00e1s Gr\\u00f3sz\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.10179\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1145/3581783.3612848\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 16 Oct 2023 08:40:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ACMM 2023\\u00a7r"}']}
{title:'Bralios et al. (§72023§r)', author: 'Dimitrios Bralios; Gordon Wichern; François G. Germain; Zexu Pan; Sameer Khurana; Chiori Hori; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2310.10604", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeneration or Replication: Auscultating Audio Latent Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oDimitrios Bralios\\nGordon Wichern\\nFran\\u00e7ois G. Germain\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.10604\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 16 Oct 2023 17:31:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Ghorbani et al. (§72023§r)', author: 'Shahram Ghorbani; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:2310.11004", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdvanced accent/dialect identification and accentedness assessment with multi-embedding models and automatic speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oShahram Ghorbani\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.11004\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 17 Oct 2023 05:13:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to The Journal of the Acoustical Society of America\\u00a7r"}']}
{title:'Ogawa et al. (§72023§r)', author: 'Atsunori Ogawa; Takafumi Moriya; Naoyuki Kamo; Naohiro Tawara; Marc Delcroix', display:{Lore:['[{"text": "arXiv:2310.11010", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIterative Shallow Fusion of Backward Language Model for End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAtsunori Ogawa\\nTakafumi Moriya\\nNaoyuki Kamo\\nNaohiro Tawara\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.11010\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 17 Oct 2023 05:44:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2023\\u00a7r"}']}
{title:'Sunder et al. (§72023§r)', author: 'Vishal Sunder; Beulah Karrolla; Eric Fosler-Lussier', display:{Lore:['[{"text": "arXiv:2310.11486", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End real time tracking of children\'s reading with pointer network\\u00a7r\\n\\n\\u00a78\\u00a7oVishal Sunder\\nBeulah Karrolla\\nEric Fosler-Lussier\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.11486\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 17 Oct 2023 16:12:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures\\u00a7r"}']}
{title:'Huang et al. (§72023§r)', author: 'Wei Huang; Fan Gao; Junting Wang; Hao Zhang', display:{Lore:['[{"text": "arXiv:2310.11708", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExperimental Results of Underwater Sound Speed Profile Inversion by Few-shot Multi-task Learning\\u00a7r\\n\\n\\u00a78\\u00a7oWei Huang\\nFan Gao\\nJunting Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.11708\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 18 Oct 2023 04:53:01 GMT)\\u00a7r"}']}
{title:'Lu et al. (§72023§r)', author: 'Jingze Lu; Yuxiang Zhang; Wenchao Wang; Zengqiang Shang; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2310.12014", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Spoofing Speech Detection Using Rhythm Information\\u00a7r\\n\\n\\u00a78\\u00a7oJingze Lu\\nYuxiang Zhang\\nWenchao Wang\\nZengqiang Shang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.12014\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 25 Nov 2023 13:48:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oFive pages, two figures\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Yuanyuan Wang; Yang Zhang; Zhiyong Wu; Zhihan Yang; Tao Wei; Kun Zou; Helen Meng', display:{Lore:['[{"text": "arXiv:2310.12111", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDASA: Difficulty-Aware Semantic Augmentation for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oYuanyuan Wang\\nYang Zhang\\nZhiyong Wu\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.12111\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 18 Oct 2023 17:07:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2023\\u00a7r"}']}
{title:'Park et al. (§72023§r)', author: 'Tae Jin Park; He Huang; Coleman Hooper; Nithin Koluguri; Kunal Dhawan; Ante Jukic; Jagadeesh Balam; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2310.12371", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProperty-Aware Multi-Speaker Data Simulation: A Probabilistic Modelling Technique for Synthetic Data Generation\\u00a7r\\n\\n\\u00a78\\u00a7oTae Jin Park\\nHe Huang\\nColeman Hooper\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.12371\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nCHiME-7 Workshop 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 18 Oct 2023 22:46:20 GMT)\\u00a7r"}']}
{title:'Park et al. (§72023§r)', author: 'Tae Jin Park; He Huang; Ante Jukic; Kunal Dhawan; Krishna C. Puvvada; Nithin Koluguri; Nikolay Karpov; Aleksandr Laptev; Jagadeesh Balam; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2310.12378", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe CHiME-7 Challenge: System Description and Performance of NeMo Team\'s DASR System\\u00a7r\\n\\n\\u00a78\\u00a7oTae Jin Park\\nHe Huang\\nAnte Jukic\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.12378\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nCHiME-7 Workshop 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 18 Oct 2023 23:10:46 GMT)\\u00a7r"}']}
{title:'Hsu et al. (§72023§r)', author: 'Ming-Hao Hsu; Kai-Wei Chang; Shang-Wen Li; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2310.12477", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Exploration of In-Context Learning for Speech Language Model\\u00a7r\\n\\n\\u00a78\\u00a7oMing-Hao Hsu\\nKai-Wei Chang\\nShang-Wen Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.12477\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 19 Oct 2023 05:31:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe first two authors contributed equally\\u00a7r"}']}
{title:'Rautenberg et al. (§72023§r)', author: 'Frederik Rautenberg; Michael Kuhlmann; Jana Wiechmann; Fritz Seebauer; Petra Wagner; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2310.12599", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Feature Importance and Interpretability of Speaker Representations\\u00a7r\\n\\n\\u00a78\\u00a7oFrederik Rautenberg\\nMichael Kuhlmann\\nJana Wiechmann\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.12599\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 19 Oct 2023 09:10:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at theITG conference on Speech Communication 2023\\u00a7r"}']}
{title:'Chang et al. (§72023§r)', author: 'Hsinyu Chang; Yicheng Hsu; Mingsian R. Bai', display:{Lore:['[{"text": "arXiv:2310.12837", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Beamforming for Speech Enhancement and Speaker Localization with an Array Response-Aware Loss Function\\u00a7r\\n\\n\\u00a78\\u00a7oHsinyu Chang\\nYicheng Hsu\\nMingsian R. Bai\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.12837\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 22 Oct 2023 11:59:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages\\u00a7r"}']}
{title:'Soltau et al. (§72023§r)', author: 'Hagen Soltau; Izhak Shafran; Alex Ottenwess; Joseph R. JR Duffy; Rene L. Utianski; Leland R. Barnard; John L. Stricker; Daniela Wiepert; David T. Jones; Hugo Botha', display:{Lore:['[{"text": "arXiv:2310.13010", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetecting Speech Abnormalities with a Perceiver-based Sequence Classifier that Leverages a Universal Speech Model\\u00a7r\\n\\n\\u00a78\\u00a7oHagen Soltau\\nIzhak Shafran\\nAlex Ottenwess\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.13010\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. ASRU, 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 16 Oct 2023 21:07:12 GMT)\\u00a7r"}']}
{title:'Gao et al. (§72023§r)', author: 'Yingying Gao; Shilei Zhang; Zihao Cui; Yanhan Xu; Chao Deng; Junlan Feng', display:{Lore:['[{"text": "arXiv:2310.13418", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenDistiller: Distilling Pre-trained Language Models based on Generative Models\\u00a7r\\n\\n\\u00a78\\u00a7oYingying Gao\\nShilei Zhang\\nZihao Cui\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.13418\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 20 Oct 2023 10:56:01 GMT)\\u00a7r"}']}
{title:'Thuillier et al. (§72023§r)', author: 'Etienne Thuillier; Craig Jin; Vesa Välimäki', display:{Lore:['[{"text": "arXiv:2310.13430", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHRTF Interpolation using a Spherical Neural Process Meta-Learner\\u00a7r\\n\\n\\u00a78\\u00a7oEtienne Thuillier\\nCraig Jin\\nVesa V\\u00e4lim\\u00e4ki\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.13430\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 20 Oct 2023 11:41:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages. 11 figures. Submitted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing (T-ASL)\\u00a7r"}']}
{title:'Lu et al. (§72023§r)', author: 'Xugang Lu; Peng Shen; Yu Tsao; Hisashi Kawai', display:{Lore:['[{"text": "arXiv:2310.13471", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural domain alignment for spoken language recognition based on optimal transport\\u00a7r\\n\\n\\u00a78\\u00a7oXugang Lu\\nPeng Shen\\nYu Tsao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.13471\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 20 Oct 2023 13:12:35 GMT)\\u00a7r"}']}
{title:'Tao et al. (§72023§r)', author: 'Dehua Tao; Tan Lee; Harold Chui; Sarah Luk', display:{Lore:['[{"text": "arXiv:2310.14178", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModeling Intrapersonal and Interpersonal Influences for Automatic Estimation of Therapist Empathy in Counseling Conversation\\u00a7r\\n\\n\\u00a78\\u00a7oDehua Tao\\nTan Lee\\nHarold Chui\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.14178\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 22 Oct 2023 05:00:04 GMT)\\u00a7r"}']}
{title:'Tao et al. (§72023§r)', author: 'Dehua Tao; Tan Lee; Harold Chui; Sarah Luk', display:{Lore:['[{"text": "arXiv:2310.14181", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Study on Prosodic Entrainment in Relation to Therapist Empathy in Counseling Conversation\\u00a7r\\n\\n\\u00a78\\u00a7oDehua Tao\\nTan Lee\\nHarold Chui\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.14181\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 22 Oct 2023 05:09:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Bai et al. (§72023§r)', author: 'Yibo Bai; Xiao-Lei Zhang', display:{Lore:['[{"text": "arXiv:2310.14270", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffusion-Based Adversarial Purification for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oYibo Bai\\nXiao-Lei Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.14270\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 24 Oct 2023 13:07:58 GMT)\\u00a7r"}']}
{title:'Hasanabadi (§72023§r)', author: 'Mohammad Reza Hasanabadi', display:{Lore:['[{"text": "arXiv:2310.14300", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMFCC-GAN Codec: A New AI-based Audio Coding\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Reza Hasanabadi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.14300\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 22 Oct 2023 13:44:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ABU Technical Review journal 2023/3\\u00a7r"}']}
{title:'Hasanabadi (§72023§r)', author: 'Mohammad Reza Hasanabadi', display:{Lore:['[{"text": "arXiv:2310.14301", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn overview of text-to-speech systems and media applications\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Reza Hasanabadi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.14301\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 22 Oct 2023 13:52:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ABU Technical Review journal 2023/6\\u00a7r"}']}
{title:'Joo et al. (§72023§r)', author: 'Seongho Joo; Hyukhun Koh; Kyomin Jung', display:{Lore:['[{"text": "arXiv:2310.14663", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDPP-TTS: Diversifying prosodic features of speech via determinantal point processes\\u00a7r\\n\\n\\u00a78\\u00a7oSeongho Joo\\nHyukhun Koh\\nKyomin Jung\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.14663\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 23 Oct 2023 07:59:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oEMNLP 2023\\u00a7r"}']}
{title:'Kelley et al. (§72023§r)', author: 'Matthew C. Kelley; Scott James Perry; Benjamin V. Tucker', display:{Lore:['[{"text": "arXiv:2310.15425", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Mason-Alberta Phonetic Segmenter: A forced alignment system based on deep neural networks and interpolation\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew C. Kelley\\nScott James Perry\\nBenjamin V. Tucker\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.15425\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 24 Oct 2023 00:43:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted for publication\\u00a7r"}']}
{title:'Cámara et al. (§72023§r)', author: 'Mateo Cámara; José Luis Blanco', display:{Lore:['[{"text": "arXiv:2310.15663", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFOLEY-VAE: Generaci\\u00f3n de efectos de audio para cine con inteligencia artificial\\u00a7r\\n\\n\\u00a78\\u00a7oMateo C\\u00e1mara\\nJos\\u00e9 Luis Blanco\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.15663\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 24 Oct 2023 09:21:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, in Spanish, Tecniac\\u00fastica\\u00a7r"}']}
{title:'Marcos et al. (§72023§r)', author: 'Fernando Marcos; Rodrigo Tamaki; Mateo Cámara; Virginia Yagüe; José Luis Blanco', display:{Lore:['[{"text": "arXiv:2310.16140", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIA Para el Mantenimiento Predictivo en Canteras: Modelado\\u00a7r\\n\\n\\u00a78\\u00a7oFernando Marcos\\nRodrigo Tamaki\\nMateo C\\u00e1mara\\nVirginia Yag\\u00fce\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.16140\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 24 Oct 2023 19:27:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, in Spanish language, 5 figures. Presented in Tecniacustica 2023 conference (Cuenca, Spain)\\u00a7r"}']}
{title:'Gode et al. (§72023§r)', author: 'Henri Gode; Simon Doclo', display:{Lore:['[{"text": "arXiv:2310.16327", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCovariance Blocking and Whitening Method for Successive Relative Transfer Function Vector Estimation in Multi-Speaker Scenarios\\u00a7r\\n\\n\\u00a78\\u00a7oHenri Gode\\nSimon Doclo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.16327\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 25 Oct 2023 03:19:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), New Paltz, NY, USA, Oct 22-25, 2023\\u00a7r"}']}
{title:'Huang et al. (§72023§r)', author: 'Zili Huang; Yiwen Shao; Shi-Xiong Zhang; Dong Yu', display:{Lore:['[{"text": "arXiv:2310.16367", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUniX-Encoder: A Universal X-Channel Speech Encoder for Ad-Hoc Microphone Array Speech Processing\\u00a7r\\n\\n\\u00a78\\u00a7oZili Huang\\nYiwen Shao\\nShi-Xiong Zhang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.16367\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 25 Oct 2023 05:12:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Hasanabadi (§72023§r)', author: 'Mohammad Reza Hasanabadi', display:{Lore:['[{"text": "arXiv:2310.16481", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Novel Approach for Object Based Audio Broadcasting\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Reza Hasanabadi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.16481\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 25 Oct 2023 09:05:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ABU Technical Review Journal 2020/9\\u00a7r"}']}
{title:'Hanschke et al. (§72023§r)', author: 'Jan-Hendrik Hanschke; Daniel Arteaga; Giulio Cengarle; Joshua Lando; Mark R. P. Thomas; Alan Seefeldt', display:{Lore:['[{"text": "arXiv:2310.17004", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Panning on Non-Equidistant Loudspeakers with Direct Sound Level Compensation\\u00a7r\\n\\n\\u00a78\\u00a7oJan-Hendrik Hanschke\\nDaniel Arteaga\\nGiulio Cengarle\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.17004\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the Audio Engineering Society Convention 155, New\\n  York, paper 10669 (October 2023).\\n  https://www.aes.org/e-lib/inst/browse.cfm?elib=22250\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 27 Oct 2023 07:53:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages. Accepted for presentation in AES Convention 155 (2023)\\u00a7r"}']}
{title:'Poh et al. (§72023§r)', author: 'Yang Yi Poh; Ethan Grooby; Kenneth Tan; Lindsay Zhou; Arrabella King; Ashwin Ramanathan; Atul Malhotra; Mehrtash Harandi; Faezeh Marzbanrad', display:{Lore:['[{"text": "arXiv:2310.17116", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-time Neonatal Chest Sound Separation using Deep Learning\\u00a7r\\n\\n\\u00a78\\u00a7oYang Yi Poh\\nEthan Grooby\\nKenneth Tan\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.17116\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 26 Oct 2023 03:05:40 GMT)\\u00a7r"}']}
{title:'Gul et al. (§72023§r)', author: 'Sania Gul; Muhammad Salman Khan; Muhammad Fazeel', display:{Lore:['[{"text": "arXiv:2310.17142", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSingle channel speech enhancement by colored spectrograms\\u00a7r\\n\\n\\u00a78\\u00a7oSania Gul\\nMuhammad Salman Khan\\nMuhammad Fazeel\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.17142\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 26 Oct 2023 04:29:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages, 6 figures, 5 tables\\u00a7r"}']}
{title:'Tran et al. (§72023§r)', author: 'Minh Tran; Mohammad Soleymani', display:{Lore:['[{"text": "arXiv:2310.17194", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrivacy-preserving Representation Learning for Speech Understanding\\u00a7r\\n\\n\\u00a78\\u00a7oMinh Tran\\nMohammad Soleymani\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.17194\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 26 Oct 2023 07:20:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023\\u00a7r"}']}
{title:'Ulloa (§72023§r)', author: 'Diego Saldaña Ulloa', display:{Lore:['[{"text": "arXiv:2310.17655", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Recommendation Based on Audio Fingerprint\\u00a7r\\n\\n\\u00a78\\u00a7oDiego Salda\\u00f1a Ulloa\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.17655\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 6 Oct 2023 03:49:13 GMT)\\u00a7r"}']}
{title:'Suda (§72023§r)', author: 'Chandra Suda', display:{Lore:['[{"text": "arXiv:2310.17675", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEarly Detection of Tuberculosis with Machine Learning Cough Audio Analysis: Towards More Accessible Global Triaging Usage\\u00a7r\\n\\n\\u00a78\\u00a7oChandra Suda\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.17675\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 25 Oct 2023 23:22:20 GMT)\\u00a7r"}']}
{title:'Hu et al. (§72023§r)', author: 'Yi Hu; Kai Ye; Hyeonjin Kim; Ning Lu', display:{Lore:['[{"text": "arXiv:2310.17742", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBERT-PIN: A BERT-based Framework for Recovering Missing Data Segments in Time-series Load Profiles\\u00a7r\\n\\n\\u00a78\\u00a7oYi Hu\\nKai Ye\\nHyeonjin Kim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.17742\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 26 Oct 2023 19:30:31 GMT)\\u00a7r"}']}
{title:'Hwang et al. (§72023§r)', author: 'Jeff Hwang; Moto Hira; Caroline Chen; Xiaohui Zhang; Zhaoheng Ni; Guangzhi Sun; Pingchuan Ma; Ruizhe Huang; Vineel Pratap; Yuekai Zhang; Anurag Kumar; Chin-Yun Yu; Chuang Zhu; Chunxi Liu; Jacob Kahn; Mirco Ravanelli; Peng Sun; Shinji Watanabe; Yangyang Shi; Yumeng Tao; Robin Scheibler; Samuele Cornell; Sean Kim; Stavros Petridis', display:{Lore:['[{"text": "arXiv:2310.17864", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTorchAudio 2.1: Advancing speech recognition, self-supervised learning, and audio processing components for PyTorch\\u00a7r\\n\\n\\u00a78\\u00a7oJeff Hwang\\nMoto Hira\\nCaroline Chen\\n+ 20 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.17864\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 27 Oct 2023 03:00:51 GMT)\\u00a7r"}']}
{title:'Middelberg et al. (§72023§r)', author: 'Wiebke Middelberg; Henri Gode; Simon Doclo', display:{Lore:['[{"text": "arXiv:2310.18199", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRelative Transfer Function Vector Estimation for Acoustic Sensor Networks Exploiting Covariance Matrix Structure\\u00a7r\\n\\n\\u00a78\\u00a7oWiebke Middelberg\\nHenri Gode\\nSimon Doclo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.18199\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 27 Oct 2023 15:18:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), New Paltz NY, USA, Oct. 2023\\u00a7r"}']}
{title:'Xie et al. (§72023§r)', author: 'Jiamin Xie; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:2310.18450", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMixRep: Hidden Representation Mixup for Low-Resource Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJiamin Xie\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.18450\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1216\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 27 Oct 2023 19:48:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to Interspeech 2023\\u00a7r"}']}
{title:'Hirvonen et al. (§72023§r)', author: 'Toni Hirvonen; Mahmoud Namazi', display:{Lore:['[{"text": "arXiv:2310.18461", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproved Lossless Coding for Storage and Transmission of Multichannel Immersive Audio\\u00a7r\\n\\n\\u00a78\\u00a7oToni Hirvonen\\nMahmoud Namazi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.18461\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 27 Oct 2023 20:14:00 GMT)\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Suyeon Lee; Chaeyoung Jung; Youngjoon Jang; Jaehun Kim; Joon Son Chung', display:{Lore:['[{"text": "arXiv:2310.19581", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSeeing Through the Conversation: Audio-Visual Speech Separation based on Diffusion Model\\u00a7r\\n\\n\\u00a78\\u00a7oSuyeon Lee\\nChaeyoung Jung\\nYoungjoon Jang\\nJaehun Kim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.19581\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 Oct 2023 14:39:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProject page with demo: https://mm.kaist.ac.kr/projects/avdiffuss/\\u00a7r"}']}
{title:'Pan et al. (§72023§r)', author: 'Zexu Pan; Gordon Wichern; Yoshiki Masuyama; Francois G. Germain; Sameer Khurana; Chiori Hori; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2310.19644", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScenario-Aware Audio-Visual TF-GridNet for Target Speech Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oZexu Pan\\nGordon Wichern\\nYoshiki Masuyama\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.19644\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 30 Oct 2023 15:32:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ASRU 2023\\u00a7r"}']}
{title:'Tu et al. (§72023§r)', author: 'Zehai Tu; Ning Ma; Jon Barker', display:{Lore:['[{"text": "arXiv:2310.19817", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntelligibility prediction with a pretrained noise-robust automatic speech recognition model\\u00a7r\\n\\n\\u00a78\\u00a7oZehai Tu\\nNing Ma\\nJon Barker\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.19817\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 20 Oct 2023 15:45:54 GMT)\\u00a7r"}']}
{title:'Maymon et al. (§72023§r)', author: 'Yanir Maymon; Israel Nelken; Boaz Rafaely', display:{Lore:['[{"text": "arXiv:2310.20238", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStudy of speaker localization with binaural microphone array incorporating auditory filters and lateral angle estimation\\u00a7r\\n\\n\\u00a78\\u00a7oYanir Maymon\\nIsrael Nelken\\nBoaz Rafaely\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.20238\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.apacoust.2023.109632\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nApplied Acoustics, Volume 213,2023, 109632\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 31 Oct 2023 07:43:12 GMT)\\u00a7r"}']}
{title:'Shao et al. (§72023§r)', author: 'Yiwen Shao; Shi-Xiong Zhang; Dong Yu', display:{Lore:['[{"text": "arXiv:2311.00146", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRIR-SF: Room Impulse Response Based Spatial Feature for Multi-channel Multi-talker ASR\\u00a7r\\n\\n\\u00a78\\u00a7oYiwen Shao\\nShi-Xiong Zhang\\nDong Yu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.00146\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 31 Oct 2023 20:42:08 GMT)\\u00a7r"}']}
{title:'Chung et al. (§72023§r)', author: 'Woo-Jin Chung; Miseul Kim; Hong-Goo Kang', display:{Lore:['[{"text": "arXiv:2311.00364", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a75physics.bio-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lC2C: Cough to COVID-19 Detection in BHI 2023 Data Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oWoo-Jin Chung\\nMiseul Kim\\nHong-Goo Kang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.00364\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Nov 2023 08:28:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o1st place winning paper from the BHI 2023 Data Challenge Competition: Sensor Informatics\\u00a7r"}']}
{title:'Stânea et al. (§72023§r)', author: 'Adrian Bogdan Stânea; Vlad Striletchi; Cosmin Striletchi; Adriana Stan', display:{Lore:['[{"text": "arXiv:2311.00394", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn analysis of large speech models-based representations for speech emotion recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAdrian Bogdan St\\u00e2nea\\nVlad Striletchi\\nCosmin Striletchi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.00394\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Nov 2023 09:40:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at SPED2023 - IEEE 12th Conference on Speech Technology and Human-Computer Dialogue, october 2023,Bucharest, Romania\\u00a7r"}']}
{title:'Brooks-Park et al. (§72023§r)', author: 'James Brooks-Park; Steven van de Par', display:{Lore:['[{"text": "arXiv:2311.00624", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReverberant sound field equalisation for an enhanced stereo playback experience\\u00a7r\\n\\n\\u00a78\\u00a7oJames Brooks-Park\\nSteven van de Par\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.00624\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Nov 2023 16:22:22 GMT)\\u00a7r"}']}
{title:'Romana et al. (§72023§r)', author: 'Amrit Romana; Kazuhito Koishida; Emily Mower Provost', display:{Lore:['[{"text": "arXiv:2311.00867", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Disfluency Detection from Untranscribed Speech\\u00a7r\\n\\n\\u00a78\\u00a7oAmrit Romana\\nKazuhito Koishida\\nEmily Mower Provost\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.00867\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 1 Nov 2023 21:36:39 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Hanglei Zhang; Yiwei Guo; Sen Liu; Xie Chen; Kai Yu', display:{Lore:['[{"text": "arXiv:2311.01260", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExpressive TTS Driven by Natural Language Prompts Using Few Human Annotations\\u00a7r\\n\\n\\u00a78\\u00a7oHanglei Zhang\\nYiwei Guo\\nSen Liu\\nXie Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.01260\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 Nov 2023 14:20:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages,3 figures, submitted to ICASSP 2024\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Minchan Kim; Myeonghun Jeong; Byoung Jin Choi; Dongjune Lee; Nam Soo Kim', display:{Lore:['[{"text": "arXiv:2311.02898", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oMinchan Kim\\nMyeonghun Jeong\\nByoung Jin Choi\\nDongjune Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.02898\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 8 Nov 2023 05:52:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ASRU2023\\u00a7r"}']}
{title:'Brima et al. (§72023§r)', author: 'Yusuf Brima; Ulf Krumnack; Simone Pika; Gunther Heidemann', display:{Lore:['[{"text": "arXiv:2311.03389", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Disentangled Speech Representations\\u00a7r\\n\\n\\u00a78\\u00a7oYusuf Brima\\nUlf Krumnack\\nSimone Pika\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.03389\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 4 Nov 2023 04:54:17 GMT)\\u00a7r"}']}
{title:'Labrador et al. (§72023§r)', author: 'Beltrán Labrador; Pai Zhu; Guanlong Zhao; Angelo Scorza Scarpati; Quan Wang; Alicia Lozano-Diez; Alex Park; Ignacio López Moreno', display:{Lore:['[{"text": "arXiv:2311.03419", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPersonalizing Keyword Spotting with Speaker Information\\u00a7r\\n\\n\\u00a78\\u00a7oBeltr\\u00e1n Labrador\\nPai Zhu\\nGuanlong Zhao\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.03419\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 Nov 2023 12:16:06 GMT)\\u00a7r"}']}
{title:'Jayaram et al. (§72023§r)', author: 'Vivek Jayaram; Ira Kemelmacher-Shlizerman; Steven M. Seitz', display:{Lore:['[{"text": "arXiv:2311.03560", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHRTF Estimation in the Wild\\u00a7r\\n\\n\\u00a78\\u00a7oVivek Jayaram\\nIra Kemelmacher-Shlizerman\\nSteven M. Seitz\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.03560\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 Nov 2023 22:01:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 Pages. Presented at UIST \'23\\u00a7r"}']}
{title:'Ruiying et al. (§72023§r)', author: 'Zhu Ruiying; Shen Meng', display:{Lore:['[{"text": "arXiv:2311.04122", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFine-tuning convergence model in Bengali speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhu Ruiying\\nShen Meng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.04122\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Nov 2023 16:47:19 GMT)\\u00a7r"}']}
{title:'Nercessian et al. (§72023§r)', author: 'Shahan Nercessian; Johannes Imort', display:{Lore:['[{"text": "arXiv:2311.04339", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInstrumentGen: Generating Sample-Based Musical Instruments From Text\\u00a7r\\n\\n\\u00a78\\u00a7oShahan Nercessian\\nJohannes Imort\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.04339\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 Nov 2023 20:45:59 GMT)\\u00a7r"}']}
{title:'Lin et al. (§72023§r)', author: 'Jingru Lin; Meng Ge; Wupeng Wang; Haizhou Li; Mengling Feng', display:{Lore:['[{"text": "arXiv:2311.04526", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelective HuBERT: Self-Supervised Pre-Training for Target Speaker in Clean and Mixture Speech\\u00a7r\\n\\n\\u00a78\\u00a7oJingru Lin\\nMeng Ge\\nWupeng Wang\\nHaizhou Li\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.04526\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 8 Nov 2023 08:28:25 GMT)\\u00a7r"}']}
{title:'Choi et al. (§72023§r)', author: 'Ha-Yeong Choi; Sang-Hoon Lee; Seong-Whan Lee', display:{Lore:['[{"text": "arXiv:2311.04693", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiff-HierVC: Diffusion-based Hierarchical Voice Conversion with Robust Pitch Generation and Masked Prior for Zero-shot Speaker Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oHa-Yeong Choi\\nSang-Hoon Lee\\nSeong-Whan Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.04693\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 8 Nov 2023 14:02:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023 (Oral)\\u00a7r"}']}
{title:'Singla et al. (§72023§r)', author: 'Karan Singla; Shahab Jalalvand; Yeon-Jun Kim; Antonio Moreno Daniel; Srinivas Bangalore; Andrej Ljolje; Ben Stern', display:{Lore:['[{"text": "arXiv:2311.04753", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l1SPU: 1-step Speech Processing Unit\\u00a7r\\n\\n\\u00a78\\u00a7oKaran Singla\\nShahab Jalalvand\\nYeon-Jun Kim\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.04753\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 10 Dec 2023 08:45:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at International Conference on Natural Language Processing 2023\\u00a7r"}']}
{title:'Galvez et al. (§72023§r)', author: 'Daniel Galvez; Tim Kaldewey', display:{Lore:['[{"text": "arXiv:2311.04996", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGPU-Accelerated WFST Beam Search Decoder for CTC-based Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Galvez\\nTim Kaldewey\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.04996\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 8 Nov 2023 19:57:10 GMT)\\u00a7r"}']}
{title:'Lin et al. (§72023§r)', author: 'Zhaofeng Lin; Tanvina Patel; Odette Scharenborg', display:{Lore:['[{"text": "arXiv:2311.05179", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Whispered Speech Recognition Performance using Pseudo-whispered based Data Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oZhaofeng Lin\\nTanvina Patel\\nOdette Scharenborg\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.05179\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ASRU57964.2023.10389801\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Nov 2023 07:31:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ASRU 2023\\u00a7r"}']}
{title:'Liang et al. (§72023§r)', author: 'Zining Liang; Wen Zhang; Thushara D. Abhayapala', display:{Lore:['[{"text": "arXiv:2311.05188", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound field reconstruction using neural processes with dynamic kernels\\u00a7r\\n\\n\\u00a78\\u00a7oZining Liang\\nWen Zhang\\nThushara D. Abhayapala\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.05188\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 9 Nov 2023 07:59:03 GMT)\\u00a7r"}']}
{title:'Shahin et al. (§72023§r)', author: 'Mostafa Shahin; Julien Epps; Beena Ahmed', display:{Lore:['[{"text": "arXiv:2311.07037", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhonological Level wav2vec2-based Mispronunciation Detection and Diagnosis Method\\u00a7r\\n\\n\\u00a78\\u00a7oMostafa Shahin\\nJulien Epps\\nBeena Ahmed\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.07037\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Nov 2023 02:41:41 GMT)\\u00a7r"}']}
{title:'Yu et al. (§72023§r)', author: 'Chin-Yun Yu; Emilian Postolache; Emanuele Rodolà; György Fazekas', display:{Lore:['[{"text": "arXiv:2311.07345", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero-Shot Duet Singing Voices Separation with Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oChin-Yun Yu\\nEmilian Postolache\\nEmanuele Rodol\\u00e0\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.07345\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 Nov 2023 14:01:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 1 figure. Published at Sound Demixing Workshop 2023\\u00a7r"}']}
{title:'Chu et al. (§72023§r)', author: 'Yunfei Chu; Jin Xu; Xiaohuan Zhou; Qian Yang; Shiliang Zhang; Zhijie Yan; Chang Zhou; Jingren Zhou', display:{Lore:['[{"text": "arXiv:2311.07919", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lQwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oYunfei Chu\\nJin Xu\\nXiaohuan Zhou\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.07919\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 21 Dec 2023 10:20:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe code, checkpoints and demo are released at https://github.com/QwenLM/Qwen-Audio\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Haici Yang; Inseon Jang; Minje Kim', display:{Lore:['[{"text": "arXiv:2311.08330", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerative De-Quantization for Neural Speech Codec via Latent Diffusion\\u00a7r\\n\\n\\u00a78\\u00a7oHaici Yang\\nInseon Jang\\nMinje Kim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.08330\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 15 Nov 2023 15:23:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Salewski et al. (§72023§r)', author: 'Leonard Salewski; Stefan Fauth; A. Sophia Koepke; Zeynep Akata', display:{Lore:['[{"text": "arXiv:2311.08396", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero-shot audio captioning with audio-language model guidance and audio context keywords\\u00a7r\\n\\n\\u00a78\\u00a7oLeonard Salewski\\nStefan Fauth\\nA. Sophia Koepke\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.08396\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Nov 2023 18:55:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oNeurIPS 2023 - Machine Learning for Audio Workshop (Oral)\\u00a7r"}']}
{title:'Taherian et al. (§72023§r)', author: 'Hassan Taherian; DeLiang Wang', display:{Lore:['[{"text": "arXiv:2311.08630", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-channel Conversational Speaker Separation via Neural Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oHassan Taherian\\nDeLiang Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.08630\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Nov 2023 01:09:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 4 figures\\u00a7r"}']}
{title:'Chiang et al. (§72023§r)', author: 'Hsin-Tien Chiang; Szu-Wei Fu; Hsin-Min Wang; Yu Tsao; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:2311.08878", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-objective Non-intrusive Hearing-aid Speech Assessment Model\\u00a7r\\n\\n\\u00a78\\u00a7oHsin-Tien Chiang\\nSzu-Wei Fu\\nHsin-Min Wang\\nYu Tsao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.08878\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Nov 2023 11:32:50 GMT)\\u00a7r"}']}
{title:'Hou et al. (§72023§r)', author: 'Yuanbo Hou; Qiaoqiao Ren; Huizhong Zhang; Andrew Mitchell; Francesco Aletta; Jian Kang; Dick Botteldooren', display:{Lore:['[{"text": "arXiv:2311.09030", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAI-based soundscape analysis: Jointly identifying sound sources and predicting annoyance\\u00a7r\\n\\n\\u00a78\\u00a7oYuanbo Hou\\nQiaoqiao Ren\\nHuizhong Zhang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.09030\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/10.0022408\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nThe Journal of the Acoustical Society of America, 154, 3145 (2023)\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 Nov 2023 15:23:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe Journal of the Acoustical Society of America, 154 (5), 3145\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Helin Wang; Venkatesh Ravichandran; Milind Rao; Becky Lammers; Myra Sydnor; Nicholas Maragakis; Ankur A. Butala; Jayne Zhang; Lora Clawson; Victoria Chovaz; Laureano Moro-Velazquez', display:{Lore:['[{"text": "arXiv:2311.10149", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving fairness for spoken language understanding in atypical speech with Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oHelin Wang\\nVenkatesh Ravichandran\\nMilind Rao\\n+ 7 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.10149\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 Nov 2023 19:09:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at SyntheticData4ML 2023 Oral\\u00a7r"}']}
{title:'Qi et al. (§72023§r)', author: 'Zili Qi; Xinhui Hu; Wangjin Zhou; Sheng Li; Hao Wu; Jian Lu; Xinkang Xu', display:{Lore:['[{"text": "arXiv:2311.10656", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLE-SSL-MOS: Self-Supervised Learning MOS Prediction with Listener Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oZili Qi\\nXinhui Hu\\nWangjin Zhou\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.10656\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 17 Nov 2023 17:20:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted in IEEE-ASRU2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Xiaojiao Chen; Sheng Li; Jiyi Li; Hao Huang; Yang Cao; Liang He', display:{Lore:['[{"text": "arXiv:2311.10664", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReprogramming Self-supervised Learning-based Speech Representations for Speaker Anonymization\\u00a7r\\n\\n\\u00a78\\u00a7oXiaojiao Chen\\nSheng Li\\nJiyi Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.10664\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 17 Nov 2023 17:35:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted in ACM Multimedia Asia2023\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Xiaojiao Chen; Sheng Li; Jiyi Li; Hao Huang; Yang Cao; Liang He', display:{Lore:['[{"text": "arXiv:2311.10689", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGhostVec: A New Threat to Speaker Privacy of End-to-End Speech Recognition System\\u00a7r\\n\\n\\u00a78\\u00a7oXiaojiao Chen\\nSheng Li\\nJiyi Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.10689\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 17 Nov 2023 18:20:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted in ACM Multimedia Asia 2023\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Yi Zhu; Mahsa Abdollahi; Ségolène Maucourt; Nico Coallier; Heitor R. Guimarães; Pierre Giovenazzo; Tiago H. Falk', display:{Lore:['[{"text": "arXiv:2311.10876", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMSPB: a longitudinal multi-sensor dataset with phenotypic trait measurements from honey bees\\u00a7r\\n\\n\\u00a78\\u00a7oYi Zhu\\nMahsa Abdollahi\\nS\\u00e9gol\\u00e8ne Maucourt\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.10876\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 17 Nov 2023 21:35:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review; project webpage: https://zhu00121.github.io/MSPB-webpage/\\u00a7r"}']}
{title:'Deng et al. (§72023§r)', author: 'Keqi Deng; Philip C. Woodland', display:{Lore:['[{"text": "arXiv:2311.11353", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLabel-Synchronous Neural Transducer for Adaptable Online E2E Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oKeqi Deng\\nPhilip C. Woodland\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.11353\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 19 Nov 2023 15:31:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\\u00a7r"}']}
{title:'Du et al. (§72023§r)', author: 'Hui-Peng Du; Ye-Xin Lu; Yang Ai; Zhen-Hua Ling', display:{Lore:['[{"text": "arXiv:2311.11545", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAPNet2: High-quality and High-efficiency Neural Vocoder with Direct Prediction of Amplitude and Phase Spectra\\u00a7r\\n\\n\\u00a78\\u00a7oHui-Peng Du\\nYe-Xin Lu\\nYang Ai\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.11545\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Nov 2023 05:37:03 GMT)\\u00a7r"}']}
{title:'Segawa et al. (§72023§r)', author: 'Hanako Segawa; Tsubasa Ochiai; Marc Delcroix; Tomohiro Nakatani; Rintaro Ikeshita; Shoko Araki; Takeshi Yamada; Shoji Makino', display:{Lore:['[{"text": "arXiv:2311.11595", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural network-based virtual microphone estimation with virtual microphone and beamformer-level multi-task loss\\u00a7r\\n\\n\\u00a78\\u00a7oHanako Segawa\\nTsubasa Ochiai\\nMarc Delcroix\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.11595\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Nov 2023 08:18:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 1 table\\u00a7r"}']}
{title:'Iwamoto et al. (§72023§r)', author: 'Kazuma Iwamoto; Tsubasa Ochiai; Marc Delcroix; Rintaro Ikeshita; Hiroshi Sato; Shoko Araki; Shigeru Katagiri', display:{Lore:['[{"text": "arXiv:2311.11599", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHow does end-to-end speech recognition training impact speech enhancement artifacts?\\u00a7r\\n\\n\\u00a78\\u00a7oKazuma Iwamoto\\nTsubasa Ochiai\\nMarc Delcroix\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.11599\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 Nov 2023 08:23:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, 1 table\\u00a7r"}']}
{title:'Zhao et al. (§72023§r)', author: 'Sipei Zhao; Guoqiang Zhang; Eva Cheng; Ian S. Burnett', display:{Lore:['[{"text": "arXiv:2311.12427", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Distributed Algorithm for Personal Sound Zones Systems\\u00a7r\\n\\n\\u00a78\\u00a7oSipei Zhao\\nGuoqiang Zhang\\nEva Cheng\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.12427\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Nov 2023 08:39:42 GMT)\\u00a7r"}']}
{title:'Isik et al. (§72023§r)', author: 'Murat Isik; Hiruna Vishwamith; Kayode Inadagbo; I. Can Dikmen', display:{Lore:['[{"text": "arXiv:2311.12449", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHPCNeuroNet: Advancing Neuromorphic Audio Signal Processing with Transformer-Enhanced Spiking Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oMurat Isik\\nHiruna Vishwamith\\nKayode Inadagbo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.12449\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Nov 2023 09:01:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEETransactions on Signal Processing\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Jun-You Wang; Chon-In Leong; Yu-Chen Lin; Li Su; Jyh-Shing Roger Jang', display:{Lore:['[{"text": "arXiv:2311.12488", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdapting pretrained speech model for Mandarin lyrics transcription and alignment\\u00a7r\\n\\n\\u00a78\\u00a7oJun-You Wang\\nChon-In Leong\\nYu-Chen Lin\\nLi Su\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.12488\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Nov 2023 09:59:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ASRU 2023\\u00a7r"}']}
{title:'Hsu et al. (§72023§r)', author: 'Yicheng Hsu; Mingsian R. Bai', display:{Lore:['[{"text": "arXiv:2311.12706", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning-based Array Configuration-Independent Binaural Audio Telepresence with Scalable Signal Enhancement and Ambience Preservation\\u00a7r\\n\\n\\u00a78\\u00a7oYicheng Hsu\\nMingsian R. Bai\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.12706\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Nov 2023 16:19:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 11 figures\\u00a7r"}']}
{title:'Wei et al. (§72023§r)', author: 'Wenqing Wei; Zhengdong Yang; Yuan Gao; Jiyi Li; Chenhui Chu; Shogo Okada; Sheng Li', display:{Lore:['[{"text": "arXiv:2311.13043", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFedCPC: An Effective Federated Contrastive Learning Method for Privacy Preserving Early-Stage Alzheimer\'s Speech Detection\\u00a7r\\n\\n\\u00a78\\u00a7oWenqing Wei\\nZhengdong Yang\\nYuan Gao\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.13043\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 Nov 2023 23:08:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted in IEEE-ASRU2023\\u00a7r"}']}
{title:'Yu et al. (§72023§r)', author: 'Meng Yu; Dong Yu', display:{Lore:['[{"text": "arXiv:2311.13075", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Audio Zooming: Beamwidth-Controllable Neural Beamformer\\u00a7r\\n\\n\\u00a78\\u00a7oMeng Yu\\nDong Yu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.13075\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 Nov 2023 00:38:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 5 figures\\u00a7r"}']}
{title:'Berger et al. (§72023§r)', author: 'Ami Berger; Vladimir Tourbabin; Jacob Donley; Zamir Ben-Hur; Boaz Rafaely', display:{Lore:['[{"text": "arXiv:2311.13390", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerformance Analysis Of Binaural Signal Matching (BSM) in the Time-Frequency Domain\\u00a7r\\n\\n\\u00a78\\u00a7oAmi Berger\\nVladimir Tourbabin\\nJacob Donley\\nZamir Ben-Hur\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.13390\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nin Proceedings of the 24th International Congress on Acoustics\\n  (ICA 2022), ABS-0302, 2022\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 23 Nov 2023 08:04:29 GMT)\\u00a7r"}']}
{title:'Pirard (§72023§r)', author: 'Ludovic Pirard', display:{Lore:['[{"text": "arXiv:2311.13397", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatial Audio and Individualized HRTFs using a Convolutional Neural Network (CNN)\\u00a7r\\n\\n\\u00a78\\u00a7oLudovic Pirard\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.13397\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 Nov 2023 13:52:51 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Jie Zhang; Qing-Tian Xu; Zhen-Hua Ling', display:{Lore:['[{"text": "arXiv:2311.13436", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSparsity-Driven EEG Channel Selection for Brain-Assisted Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oJie Zhang\\nQing-Tian Xu\\nZhen-Hua Ling\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.13436\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 23 Nov 2023 04:04:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: text overlapwith arXiv:2305.09994\\u00a7r"}']}
{title:'Tang et al. (§72023§r)', author: 'Duowei Tang; Peter Kuppens; Luc Geurts; Toon van Waterschoot', display:{Lore:['[{"text": "arXiv:2311.13678", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-end Transfer Learning for Speaker-independent Cross-language Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDuowei Tang\\nPeter Kuppens\\nLuc Geurts\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.13678\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 Nov 2023 20:11:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 6 figures, 4 tables\\u00a7r"}']}
{title:'Cífka et al. (§72023§r)', author: 'Ondřej Cífka; Constantinos Dimitriou; Cheng-i Wang; Hendrik Schreiber; Luke Miner; Fabian-Robert Stöter', display:{Lore:['[{"text": "arXiv:2311.13987", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJam-ALT: A Formatting-Aware Lyrics Transcription Benchmark\\u00a7r\\n\\n\\u00a78\\u00a7oOnd\\u0159ej C\\u00edfka\\nConstantinos Dimitriou\\nCheng-i Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.13987\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 Nov 2023 13:13:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages (3 pages main content); website: https://audioshake.github.io/jam-alt/; data: https://huggingface.co/datasets/audioshake/jam-alt; code: https://github.com/audioshake/alt-eval/\\u00a7r"}']}
{title:'Yin et al. (§72023§r)', author: 'Han Yin; Jisheng Bai; Mou Wang; Dongyuan Shi; Woon-Seng Gan; Jianfeng Chen', display:{Lore:['[{"text": "arXiv:2311.14068", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInteractive Dual-Conformer with Scene-Inspired Mask for Soft Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oHan Yin\\nJisheng Bai\\nMou Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.14068\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 7 Dec 2023 14:37:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oto be improved (unfinished)\\u00a7r"}']}
{title:'Grossi et al. (§72023§r)', author: 'Alessandra Grossi; Francesca Gasparini', display:{Lore:['[{"text": "arXiv:2311.14483", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSER_AMPEL: a multi-source dataset for speech emotion recognition of Italian older adults\\u00a7r\\n\\n\\u00a78\\u00a7oAlessandra Grossi\\nFrancesca Gasparini\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.14483\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 14 Dec 2023 15:39:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 1 Figure, 7 Tables, submitted to ForItAAL 2023 (12 Forum Italiano Ambient Assisted Living)\\u00a7r"}']}
{title:'Gburrek et al. (§72023§r)', author: 'Tobias Gburrek; Joerg Schmalenstroeer; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2311.15597", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatial Diarization for Meeting Transcription with Ad-Hoc Acoustic Sensor Networks\\u00a7r\\n\\n\\u00a78\\u00a7oTobias Gburrek\\nJoerg Schmalenstroeer\\nReinhold Haeb-Umbach\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.15597\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Nov 2023 07:46:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Asilomar Conference on Signals, Systems, andComputers 2023\\u00a7r"}']}
{title:'Tang et al. (§72023§r)', author: 'Chenyu Tang; Muzi Xu; Wentian Yi; Zibo Zhang; Edoardo Occhipinti; Chaoqun Dong; Dafydd Ravenscroft; Sung-Min Jung; Sanghyo Lee; Shuo Gao; Jong Min Kim; Luigi G. Occhipinti', display:{Lore:['[{"text": "arXiv:2311.15683", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUltrasensitive Textile Strain Sensors Redefine Wearable Silent Speech Interfaces with High Machine Learning Efficiency\\u00a7r\\n\\n\\u00a78\\u00a7oChenyu Tang\\nMuzi Xu\\nWentian Yi\\n+ 8 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.15683\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1038/s41528-024-00315-1\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nnpj Flexible Electronics (2024)\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 7 Dec 2023 09:16:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 figures in the article; 11 figures and 4 tables in supplementary information\\u00a7r"}']}
{title:'Leschanowsky et al. (§72023§r)', author: 'Anna Leschanowsky; Ünal Ege Gaznepoglu; Nils Peters', display:{Lore:['[{"text": "arXiv:2311.15804", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CY\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Anonymization for All \\u2013 Bias Evaluation of the Voice Privacy Challenge Baseline System\\u00a7r\\n\\n\\u00a78\\u00a7oAnna Leschanowsky\\n\\u00dcnal Ege Gaznepoglu\\nNils Peters\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.15804\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 Nov 2023 13:26:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Chi-Chang Lee; Hong-Wei Chen; Chu-Song Chen; Hsin-Min Wang; Tsung-Te Liu; Yu Tsao', display:{Lore:['[{"text": "arXiv:2311.16604", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLC4SV: A Denoising Framework Learning to Compensate for Unseen Speaker Verification Models\\u00a7r\\n\\n\\u00a78\\u00a7oChi-Chang Lee\\nHong-Wei Chen\\nChu-Song Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.16604\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Nov 2023 08:44:04 GMT)\\u00a7r"}']}
{title:'Berebi et al. (§72023§r)', author: 'Or Berebi; Zamir Ben-Hur; David Lou Alon; Boaz Rafaely', display:{Lore:['[{"text": "arXiv:2311.16702", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7liMagLS: Interaural Level Difference with Magnitude Least-Squares Loss for Optimized First-Order Head-Related Transfer Function\\u00a7r\\n\\n\\u00a78\\u00a7oOr Berebi\\nZamir Ben-Hur\\nDavid Lou Alon\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.16702\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.61782/fa.2023.0678\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Nov 2023 11:25:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o3 pages, 2 figures, ForumAcusticum 2023\\u00a7r"}']}
{title:'Mitchell et al. (§72023§r)', author: 'Daniel A. Mitchell; Boaz Rafaely', display:{Lore:['[{"text": "arXiv:2311.16927", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStudy of speaker localization under dynamic and reverberant environments\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel A. Mitchell\\nBoaz Rafaely\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.16927\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nin Proceedings of the 24rd International Congress on Acoustics\\n  (ICA 2022), no. ABS-0359, Oct 2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 28 Nov 2023 16:31:36 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Rongxiang Wang; Felix Xiaozhu Lin', display:{Lore:['[{"text": "arXiv:2311.17065", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Deep Speech Understanding at the Edge\\u00a7r\\n\\n\\u00a78\\u00a7oRongxiang Wang\\nFelix Xiaozhu Lin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.17065\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 4 Dec 2023 15:37:57 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Yuhang Yang; Yizhou Peng; Xionghu Zhong; Hao Huang; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2311.17382", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdapting OpenAI\'s Whisper for Speech Recognition on Code-Switch Mandarin-English SEAME and ASRU2019 Datasets\\u00a7r\\n\\n\\u00a78\\u00a7oYuhang Yang\\nYizhou Peng\\nXionghu Zhong\\nHao Huang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.17382\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 29 Nov 2023 06:16:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 3 figures, 4 tables\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Young-Eun Lee; Seo-Hyun Lee; Soowon Kim; Jung-Sun Lee; Deok-Seon Kim; Seong-Whan Lee', display:{Lore:['[{"text": "arXiv:2311.17923", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhanced Generative Adversarial Networks for Unseen Word Generation from EEG Signals\\u00a7r\\n\\n\\u00a78\\u00a7oYoung-Eun Lee\\nSeo-Hyun Lee\\nSoowon Kim\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.17923\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 Nov 2023 00:20:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures\\u00a7r"}']}
{title:'Liu et al. (§72023§r)', author: 'Yuzhuo Liu; Xubo Liu; Yan Zhao; Yuanyuan Wang; Rui Xia; Pingchuan Tain; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2311.18399", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Prompt Tuning for Universal Sound Separation\\u00a7r\\n\\n\\u00a78\\u00a7oYuzhuo Liu\\nXubo Liu\\nYan Zhao\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.18399\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Nov 2023 09:49:22 GMT)\\u00a7r"}']}
{title:'Hafezi et al. (§72023§r)', author: 'Sina Hafezi; Alastair H. Moore; Pierre H. Guiraud; Patrick A. Naylor; Jacob Donley; Vladimir Tourbabin; Thomas Lunner', display:{Lore:['[{"text": "arXiv:2311.18689", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSubspace Hybrid MVDR Beamforming for Augmented Hearing\\u00a7r\\n\\n\\u00a78\\u00a7oSina Hafezi\\nAlastair H. Moore\\nPierre H. Guiraud\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.18689\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Nov 2023 16:34:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages, 10 figures, submitted for IEEE/ACMTransactions on Audio, Speech, and Language Processing on 23-Nov-2023\\u00a7r"}']}
{title:'Roth et al. (§72023§r)', author: 'Jonas Roth; Domenic Keller; Oscar Castañeda; Christoph Studer', display:{Lore:['[{"text": "arXiv:2311.18774", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SY\\u00a7r, \\u00a7eeess.SY\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Aliasing-Free Hybrid Digital-Analog Polyphonic Synthesizer\\u00a7r\\n\\n\\u00a78\\u00a7oJonas Roth\\nDomenic Keller\\nOscar Casta\\u00f1eda\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.18774\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Nov 2023 18:20:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at DAFx23\\u00a7r"}']}
{title:'Srinivasagan et al. (§72023§r)', author: 'Gokul Srinivasagan; Michael Deisher; Munir Georges', display:{Lore:['[{"text": "arXiv:2312.00174", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCompression of end-to-end non-autoregressive image-to-speech system for low-resourced devices\\u00a7r\\n\\n\\u00a78\\u00a7oGokul Srinivasagan\\nMichael Deisher\\nMunir Georges\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.00174\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Nov 2023 20:13:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 2 tables, presented at the 15th ITG Conference on Speech Communications, September 2023, Aachen\\u00a7r"}']}
{title:'Onu et al. (§72023§r)', author: 'Charles C. Onu; Hemanth K. Sheetha; Arsenii Gorin; Doina Precup', display:{Lore:['[{"text": "arXiv:2312.00231", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning domain-invariant classifiers for infant cry sounds\\u00a7r\\n\\n\\u00a78\\u00a7oCharles C. Onu\\nHemanth K. Sheetha\\nArsenii Gorin\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.00231\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Nov 2023 22:27:57 GMT)\\u00a7r"}']}
{title:'Liang et al. (§72023§r)', author: 'Jinhua Liang; Xubo Liu; Wenwu Wang; Mark D. Plumbley; Huy Phan; Emmanouil Benetos', display:{Lore:['[{"text": "arXiv:2312.00249", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities\\u00a7r\\n\\n\\u00a78\\u00a7oJinhua Liang\\nXubo Liu\\nWenwu Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.00249\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 Nov 2023 23:43:59 GMT)\\u00a7r"}']}
{title:'Singh et al. (§72023§r)', author: 'Abhayjeet Singh; Charu Shah; Rajashri Varadaraj; Sonakshi Chauhan; Prasanta Kumar Ghosh', display:{Lore:['[{"text": "arXiv:2312.00698", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSPIRE-SIES: A Spontaneous Indian English Speech Corpus\\u00a7r\\n\\n\\u00a78\\u00a7oAbhayjeet Singh\\nCharu Shah\\nRajashri Varadaraj\\nSonakshi Chauhan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.00698\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Dec 2023 16:29:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 7 plots, 3 tables, Accepted at O-COCOSDA2023\\u00a7r"}']}
{title:'Strauss et al. (§72023§r)', author: 'Martin Strauss; Nicola Pia; Nagashree K. S. Rao; Bernd Edler', display:{Lore:['[{"text": "arXiv:2312.01744", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSEFGAN: Harvesting the Power of Normalizing Flows and GANs for Efficient High-Quality Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oMartin Strauss\\nNicola Pia\\nNagashree K. S. Rao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.01744\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/WASPAA58266.2023.10248144\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Dec 2023 09:10:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint. Accepted to IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)2023\\u00a7r"}']}
{title:'Müller et al. (§72023§r)', author: 'Kaspar Müller; Bilgesu Çakmak; Paul Didier; Simon Doclo; Jan Østergaard; Tobias Wolff', display:{Lore:['[{"text": "arXiv:2312.01808", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHead Orientation Estimation with Distributed Microphones Using Speech Radiation Patterns\\u00a7r\\n\\n\\u00a78\\u00a7oKaspar M\\u00fcller\\nBilgesu \\u00c7akmak\\nPaul Didier\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.01808\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Dec 2023 11:14:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, submitted to 57th Asilomar Conference on Signals, Systems, and Computers, Pacific Grove, CA, USA, 2023\\u00a7r"}']}
{title:'Müller et al. (§72023§r)', author: 'Kaspar Müller; Franz Zotter', display:{Lore:['[{"text": "arXiv:2312.02581", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAuralization based on multi-perspective ambisonic room impulse responses\\u00a7r\\n\\n\\u00a78\\u00a7oKaspar M\\u00fcller\\nFranz Zotter\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.02581\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1051/aacus/2020024\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nActa Acustica, Volume 4, Number 6, Article Number 25, 2020\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Dec 2023 08:57:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages, published in ActaAcustica (Open Access), datasets are available via https://paperswithcode.com/dataset/cube-b-format-ambisonic-rir-dataset and https://paperswithcode.com/dataset/variable-perspective-arir-rendering-listening\\u00a7"}','{"text": "r"}']}
{title:'Yang et al. (§72023§r)', author: 'Ziye Yang; Mengfei Zhang; Jie Chen', display:{Lore:['[{"text": "arXiv:2312.03034", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDistributed Speech Dereverberation Using Weighted Prediction Error\\u00a7r\\n\\n\\u00a78\\u00a7oZiye Yang\\nMengfei Zhang\\nJie Chen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.03034\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Dec 2023 13:22:58 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Yixuan Zhang; Heming Wang; DeLiang Wang', display:{Lore:['[{"text": "arXiv:2312.03129", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Laryngograph Data for Robust Voicing Detection in Speech\\u00a7r\\n\\n\\u00a78\\u00a7oYixuan Zhang\\nHeming Wang\\nDeLiang Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.03129\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Dec 2023 20:57:00 GMT)\\u00a7r"}']}
{title:'Li et al. (§72023§r)', author: 'Yanxiong Li; Zhongjie Jiang; Qisheng Huang; Wenchang Cao; Jialong Li', display:{Lore:['[{"text": "arXiv:2312.03324", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLightweight Speaker Verification Using Transformation Module with Feature Partition and Fusion\\u00a7r\\n\\n\\u00a78\\u00a7oYanxiong Li\\nZhongjie Jiang\\nQisheng Huang\\nWenchang Cao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.03324\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Dec 2023 07:25:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 5 figures, 6 tables; accepted for publication in IEEE-ACM TASLP\\u00a7r"}']}
{title:'Hono et al. (§72023§r)', author: 'Yukiya Hono; Koh Mitsuda; Tianyu Zhao; Kentaro Mitsui; Toshiaki Wakatsuki; Kei Sawada', display:{Lore:['[{"text": "arXiv:2312.03668", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Integration of Pre-Trained Speech and Language Models for End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYukiya Hono\\nKoh Mitsuda\\nTianyu Zhao\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.03668\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 6 Dec 2023 18:34:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 2 figures, 3 tables, The model is available at https://huggingface.co/rinna/nue-asr\\u00a7r"}']}
{title:'Zhao et al. (§72023§r)', author: 'Huan Zhao; Li Zhang; Yue Li; Yannan Wang; Hongji Wang; Wei Rao; Qing Wang; Lei Xie', display:{Lore:['[{"text": "arXiv:2312.04131", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Training or Not: An Exploration of Pre-trained Speech Models in Audio-Visual Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oHuan Zhao\\nLi Zhang\\nYue Li\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.04131\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Dec 2023 08:40:37 GMT)\\u00a7r"}']}
{title:'Landini et al. (§72023§r)', author: 'Federico Landini; Mireia Diez; Themos Stafylakis; Lukáš Burget', display:{Lore:['[{"text": "arXiv:2312.04324", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiaPer: End-to-End Neural Diarization with Perceiver-Based Attractors\\u00a7r\\n\\n\\u00a78\\u00a7oFederico Landini\\nMireia Diez\\nThemos Stafylakis\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.04324\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 22 Dec 2023 13:28:18 GMT)\\u00a7r"}']}
{title:'Gonzalez et al. (§72023§r)', author: 'Philippe Gonzalez; Zheng-Hua Tan; Jan Østergaard; Jesper Jensen; Tommy Sonne Alstrøm; Tobias May', display:{Lore:['[{"text": "arXiv:2312.04370", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating the Design Space of Diffusion Models for Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oPhilippe Gonzalez\\nZheng-Hua Tan\\nJan \\u00d8stergaard\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.04370\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Dec 2023 15:40:55 GMT)\\u00a7r"}']}
{title:'Westhausen et al. (§72023§r)', author: 'Nils L. Westhausen; Bernd T. Meyer', display:{Lore:['[{"text": "arXiv:2312.05173", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBinaural multichannel blind speaker separation with a causal low-latency and low-complexity approach\\u00a7r\\n\\n\\u00a78\\u00a7oNils L. Westhausen\\nBernd T. Meyer\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.05173\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Dec 2023 16:53:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at IEEE ICASSP 2024 OJSP track\\u00a7r"}']}
{title:'Mun et al. (§72023§r)', author: 'Sung Hwan Mun; Min Hyun Han; Canyeong Moon; Nam Soo Kim', display:{Lore:['[{"text": "arXiv:2312.06065", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEEND-DEMUX: End-to-End Neural Speaker Diarization via Demultiplexed Speaker Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oSung Hwan Mun\\nMin Hyun Han\\nCanyeong Moon\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.06065\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Dec 2023 02:14:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEESignal Processing Letters\\u00a7r"}']}
{title:'Derington et al. (§72023§r)', author: 'Anna Derington; Hagen Wierstorf; Ali Özkil; Florian Eyben; Felix Burkhardt; Björn W. Schuller', display:{Lore:['[{"text": "arXiv:2312.06270", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTesting Speech Emotion Recognition Machine Learning Models\\u00a7r\\n\\n\\u00a78\\u00a7oAnna Derington\\nHagen Wierstorf\\nAli \\u00d6zkil\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.06270\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Dec 2023 10:15:35 GMT)\\u00a7r"}']}
{title:'Santos et al. (§72023§r)', author: 'Orlem Lima dos Santos; Karen Rosero; Roberto de Alencar Lotufo', display:{Lore:['[{"text": "arXiv:2312.06907", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lw2v-SELD: A Sound Event Localization and Detection Framework for Self-Supervised Spatial Audio Pre-Training\\u00a7r\\n\\n\\u00a78\\u00a7oOrlem Lima dos Santos\\nKaren Rosero\\nRoberto de Alencar Lotufo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.06907\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 30 Dec 2023 04:46:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o17 pages, 5 figures\\u00a7r"}']}
{title:'Pan et al. (§72023§r)', author: 'Zexu Pan; Gordon Wichern; Francois G. Germain; Sameer Khurana; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2312.07513", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeuroHeed+: Improving Neuro-steered Speaker Extraction with Joint Auditory Attention Detection\\u00a7r\\n\\n\\u00a78\\u00a7oZexu Pan\\nGordon Wichern\\nFrancois G. Germain\\nSameer Khurana\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.07513\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Dec 2023 18:45:35 GMT)\\u00a7r"}']}
{title:'Shetu et al. (§72023§r)', author: 'Shrishti Saha Shetu; Soumitro Chakrabarty; Oliver Thiergart; Edwin Mabande', display:{Lore:['[{"text": "arXiv:2312.08132", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUltra Low Complexity Deep Learning Based Noise Suppression\\u00a7r\\n\\n\\u00a78\\u00a7oShrishti Saha Shetu\\nSoumitro Chakrabarty\\nOliver Thiergart\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.08132\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Dec 2023 13:34:15 GMT)\\u00a7r"}']}
{title:'Grekov et al. (§72023§r)', author: 'A. N. Grekov; N. A. Grekov; E. N. Sychev', display:{Lore:['[{"text": "arXiv:2312.08496", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMetrological support of acoustic measuring installations mid-frequency devices\\u00a7r\\n\\n\\u00a78\\u00a7oA. N. Grekov\\nN. A. Grekov\\nE. N. Sychev\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.08496\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.33075/2220-5861-2020-2-117-126\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nEnvironmental control systems. 2023. Issue. 2 (40). pp. 117-126\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Dec 2023 20:16:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 1 figure\\u00a7r"}']}
{title:'Heo et al. (§72023§r)', author: 'Hyun-Jun Heo; Ui-Hyeop Shin; Ran Lee; YoungJu Cheon; Hyung-Min Park', display:{Lore:['[{"text": "arXiv:2312.08603", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeXt-TDNN: Modernizing Multi-Scale Temporal Convolution Backbone for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oHyun-Jun Heo\\nUi-Hyeop Shin\\nRan Lee\\nYoungJu Cheon\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.08603\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 15 Dec 2023 01:51:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Lu et al. (§72023§r)', author: 'Kunxing Lu; Xianrui Wang; Tetsuya Ueda; Shoji Makino; Jingdong Chen', display:{Lore:['[{"text": "arXiv:2312.08610", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA computationally efficient semi-blind source separation based approach for nonlinear echo cancellation based on an element-wise iterative source steering\\u00a7r\\n\\n\\u00a78\\u00a7oKunxing Lu\\nXianrui Wang\\nTetsuya Ueda\\nShoji Makino\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.08610\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Dec 2023 02:23:38 GMT)\\u00a7r"}']}
{title:'Wu et al. (§72023§r)', author: 'Haibin Wu; Heng-Cheng Kuo; Yu Tsao; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2312.08622", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScalable Ensemble-based Detection Method against Adversarial Attacks for speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oHaibin Wu\\nHeng-Cheng Kuo\\nYu Tsao\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.08622\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Dec 2023 03:04:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to 2024 ICASSP\\u00a7r"}']}
{title:'Jin et al. (§72023§r)', author: 'Zengrui Jin; Xurong Xie; Tianzi Wang; Mengzhe Geng; Jiajun Deng; Guinan Li; Shujie Hu; Xunying Liu', display:{Lore:['[{"text": "arXiv:2312.08641", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Automatic Data Augmentation for Disordered Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZengrui Jin\\nXurong Xie\\nTianzi Wang\\n+ 4 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.08641\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Dec 2023 03:49:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at IEEE ICASSP 2024\\u00a7r"}']}
{title:'Ohlenbusch et al. (§72023§r)', author: 'Mattes Ohlenbusch; Christian Rollwage; Simon Doclo', display:{Lore:['[{"text": "arXiv:2312.08908", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Microphone Noise Data Augmentation for DNN-based Own Voice Reconstruction for Hearables in Noisy Environments\\u00a7r\\n\\n\\u00a78\\u00a7oMattes Ohlenbusch\\nChristian Rollwage\\nSimon Doclo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.08908\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP48485.2024.10447066\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Dec 2023 13:15:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2024 (c) 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for "}','{"text": "advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Ting Zhu; Shufei Duan; Huizhi Liang; Wei Zhang', display:{Lore:['[{"text": "arXiv:2312.08998", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDesign, construction and evaluation of emotional multimodal pathological speech database\\u00a7r\\n\\n\\u00a78\\u00a7oTing Zhu\\nShufei Duan\\nHuizhi Liang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.08998\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Dec 2023 14:43:31 GMT)\\u00a7r"}']}
{title:'Berghi et al. (§72023§r)', author: 'Davide Berghi; Peipei Wu; Jinzheng Zhao; Wenwu Wang; Philip J. B. Jackson', display:{Lore:['[{"text": "arXiv:2312.09034", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFusion of Audio and Visual Embeddings for Sound Event Localization and Detection\\u00a7r\\n\\n\\u00a78\\u00a7oDavide Berghi\\nPeipei Wu\\nJinzheng Zhao\\nWenwu Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.09034\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Dec 2023 15:34:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\\u00a7r"}']}
{title:'Deng et al. (§72023§r)', author: 'Keqi Deng; Philip C. Woodland', display:{Lore:['[{"text": "arXiv:2312.09100", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFastInject: Injecting Unpaired Text Data into CTC-based ASR training\\u00a7r\\n\\n\\u00a78\\u00a7oKeqi Deng\\nPhilip C. Woodland\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.09100\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Dec 2023 16:38:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2024\\u00a7r"}']}
{title:'Lee et al. (§72023§r)', author: 'Sunghwa Lee; Younghoon Shin; Myungjong Kim; Jiwon Seo', display:{Lore:['[{"text": "arXiv:2312.09572", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIR-UWB Radar-Based Contactless Silent Speech Recognition of Vowels, Consonants, Words, and Phrases\\u00a7r\\n\\n\\u00a78\\u00a7oSunghwa Lee\\nYounghoon Shin\\nMyungjong Kim\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.09572\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ACCESS.2023.3344177\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Dec 2023 07:04:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEEAccess\\u00a7r"}']}
{title:'Wilkinghoff (§72023§r)', author: 'Kevin Wilkinghoff', display:{Lore:['[{"text": "arXiv:2312.09578", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Learning for Anomalous Sound Detection\\u00a7r\\n\\n\\u00a78\\u00a7oKevin Wilkinghoff\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.09578\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Dec 2023 07:16:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for presentation at IEEE ICASSP 2024\\u00a7r"}']}
{title:'Xiang et al. (§72023§r)', author: 'Yang Xiang; Jingguang Tian; Xinhui Hu; Xinkang Xu; ZhaoHui Yin', display:{Lore:['[{"text": "arXiv:2312.09620", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Deep Representation Learning-based Speech Enhancement Method Using Complex Convolution Recurrent Variational Autoencoder\\u00a7r\\n\\n\\u00a78\\u00a7oYang Xiang\\nJingguang Tian\\nXinhui Hu\\nXinkang Xu\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.09620\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Dec 2023 09:03:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Frost et al. (§72023§r)', author: 'Geoffrey Frost; Emily Morris; Joshua Jansen van Vüren; Thomas Niesler', display:{Lore:['[{"text": "arXiv:2312.09645", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFine-Tuned Self-Supervised Speech Representations for Language Diarization in Multilingual Code-Switched Speech\\u00a7r\\n\\n\\u00a78\\u00a7oGeoffrey Frost\\nEmily Morris\\nJoshua Jansen van V\\u00fcren\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.09645\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-031-22321-1_17\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Dec 2023 09:40:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPresented at SACAIR 2022\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Ao Zhang; Pan Zhou; Kaixun Huang; Yong Zou; Ming Liu; Lei Xie', display:{Lore:['[{"text": "arXiv:2312.09760", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lU2-KWS: Unified Two-pass Open-vocabulary Keyword Spotting with Keyword Bias\\u00a7r\\n\\n\\u00a78\\u00a7oAo Zhang\\nPan Zhou\\nKaixun Huang\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.09760\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Dec 2023 13:00:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ASRU2023\\u00a7r"}']}
{title:'Thornton et al. (§72023§r)', author: 'Mike Thornton; Danilo Mandic; Tobias Reichenbach', display:{Lore:['[{"text": "arXiv:2312.09768", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDecoding Envelope and Frequency-Following EEG Responses to Continuous Speech Using Deep Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oMike Thornton\\nDanilo Mandic\\nTobias Reichenbach\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.09768\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Dec 2023 13:16:24 GMT)\\u00a7r"}']}
{title:'Hou et al. (§72023§r)', author: 'Yuanbo Hou; Qiaoqiao Ren; Siyang Song; Yuxin Song; Wenwu Wang; Dick Botteldooren', display:{Lore:['[{"text": "arXiv:2312.09952", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-level graph learning for audio event classification and human-perceived annoyance rating prediction\\u00a7r\\n\\n\\u00a78\\u00a7oYuanbo Hou\\nQiaoqiao Ren\\nSiyang Song\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.09952\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Dec 2023 17:02:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Chang et al. (§72023§r)', author: 'Oscar Chang; Dongseong Hwang; Olivier Siohan', display:{Lore:['[{"text": "arXiv:2312.10087", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRevisiting the Entropy Semiring for Neural Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oOscar Chang\\nDongseong Hwang\\nOlivier Siohan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.10087\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 19 Dec 2023 01:42:19 GMT)\\u00a7r"}']}
{title:'Chang et al. (§72023§r)', author: 'Oscar Chang; Otavio Braga; Hank Liao; Dmitriy Serdyuk; Olivier Siohan', display:{Lore:['[{"text": "arXiv:2312.10088", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Robustness to Missing Video for Audiovisual Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oOscar Chang\\nOtavio Braga\\nHank Liao\\nDmitriy Serdyuk\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.10088\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 19 Dec 2023 01:44:13 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Yuzhu Wang; Archontis Politis; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2312.10756", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention-Driven Multichannel Speech Enhancement in Moving Sound Source Scenarios\\u00a7r\\n\\n\\u00a78\\u00a7oYuzhu Wang\\nArchontis Politis\\nTuomas Virtanen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.10756\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 17 Dec 2023 16:12:35 GMT)\\u00a7r"}']}
{title:'Cao et al. (§72023§r)', author: 'Rui Cao; Tianrui Wang; Meng Ge; Longbiao Wang; Jianwu Dang', display:{Lore:['[{"text": "arXiv:2312.11201", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Refining Underlying Information Framework for Monaural Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oRui Cao\\nTianrui Wang\\nMeng Ge\\nLongbiao Wang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.11201\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 24 Dec 2023 12:00:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Meyer et al. (§72023§r)', author: 'Luke Meyer; Gloria Araiza-Illan; Laura Rachman; Etienne Gaudrain; Deniz Baskent', display:{Lore:['[{"text": "arXiv:2312.12262", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluating Speech-in-Speech Perception via a Humanoid Robot\\u00a7r\\n\\n\\u00a78\\u00a7oLuke Meyer\\nGloria Araiza-Illan\\nLaura Rachman\\nEtienne Gaudrain\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.12262\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3389/fnins.2024.1293120\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Dec 2023 15:46:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o25 pages (single spaced), 6 figures (at the end of the manuscript), 88 references, under revision with Frontiers\\u00a7r"}']}
{title:'Meng et al. (§72023§r)', author: 'Lingjun Meng; Jozef Coldenhoff; Paul Kendrick; Tijana Stojkovic; Andrew Harper; Kiril Ratmanski; Milos Cernak', display:{Lore:['[{"text": "arXiv:2312.12415", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn real-time multi-stage speech enhancement systems\\u00a7r\\n\\n\\u00a78\\u00a7oLingjun Meng\\nJozef Coldenhoff\\nPaul Kendrick\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.12415\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Dec 2023 18:47:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear at ICASSP 2024\\u00a7r"}']}
{title:'Ogawa et al. (§72023§r)', author: 'Atsunori Ogawa; Naohiro Tawara; Marc Delcroix; Shoko Araki', display:{Lore:['[{"text": "arXiv:2312.12764", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLattice Rescoring Based on Large Ensemble of Complementary Neural Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oAtsunori Ogawa\\nNaohiro Tawara\\nMarc Delcroix\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.12764\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Dec 2023 04:52:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022\\u00a7r"}']}
{title:'Seth et al. (§72023§r)', author: 'Ashish Seth; Sreyan Ghosh; S. Umesh; Dinesh Manocha', display:{Lore:['[{"text": "arXiv:2312.12783", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStable Distillation: Regularizing Continued Pre-training for Low-Resource Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAshish Seth\\nSreyan Ghosh\\nS. Umesh\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.12783\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Dec 2023 06:02:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024. Code: https://github.com/cs20s030/stable_distillation\\u00a7r"}']}
{title:'Lian et al. (§72023§r)', author: 'Jiachen Lian; Carly Feng; Naasir Farooqi; Steve Li; Anshul Kashyap; Cheol Jun Cho; Peter Wu; Robbie Netzorg; Tingle Li; Gopala Krishna Anumanchipalli', display:{Lore:['[{"text": "arXiv:2312.12810", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnconstrained Dysfluency Modeling for Dysfluent Speech Transcription and Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJiachen Lian\\nCarly Feng\\nNaasir Farooqi\\n+ 6 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.12810\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Dec 2023 07:20:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2023 ASRU\\u00a7r"}']}
{title:'Shul et al. (§72023§r)', author: 'Yusun Shul; Jung-Woo Choi', display:{Lore:['[{"text": "arXiv:2312.12821", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCST-former: Transformer with Channel-Spectro-Temporal Attention for Sound Event Localization and Detection\\u00a7r\\n\\n\\u00a78\\u00a7oYusun Shul\\nJung-Woo Choi\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.12821\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Dec 2023 07:49:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Seth et al. (§72023§r)', author: 'Ashish Seth; Sreyan Ghosh; S. Umesh; Dinesh Manocha', display:{Lore:['[{"text": "arXiv:2312.13026", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFusDom: Combining In-Domain and Out-of-Domain Knowledge for Continuous Self-Supervised Learning\\u00a7r\\n\\n\\u00a78\\u00a7oAshish Seth\\nSreyan Ghosh\\nS. Umesh\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.13026\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Dec 2023 13:50:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2024. Code: https://github.com/cs20s030/fusdom\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Miseul Kim; Zhenyu Piao; Jihyun Lee; Hong-Goo Kang', display:{Lore:['[{"text": "arXiv:2312.13600", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBrainTalker: Low-Resource Brain-to-Speech Synthesis with Transfer Learning using Wav2Vec 2.0\\u00a7r\\n\\n\\u00a78\\u00a7oMiseul Kim\\nZhenyu Piao\\nJihyun Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.13600\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Dec 2023 06:10:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages. Accepted to BHI 2023\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Miseul Kim; Zhenyu Piao; Jihyun Lee; Hong-Goo Kang', display:{Lore:['[{"text": "arXiv:2312.13603", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStyle Modeling for Multi-Speaker Articulation-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oMiseul Kim\\nZhenyu Piao\\nJihyun Lee\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.13603\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Dec 2023 06:28:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, Accepted to ICASSP 2023\\u00a7r"}']}
{title:'Kim et al. (§72023§r)', author: 'Miseul Kim; Minh Tri Ho; Hong-Goo Kang', display:{Lore:['[{"text": "arXiv:2312.13615", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervised Complex Network for Machine Sound Anomaly Detection\\u00a7r\\n\\n\\u00a78\\u00a7oMiseul Kim\\nMinh Tri Ho\\nHong-Goo Kang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.13615\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Dec 2023 07:04:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in EUSIPCO 2021\\u00a7r"}']}
{title:'Hadadi et al. (§72023§r)', author: 'Yogev Hadadi; Vladimir Tourbabin; Paul Calamia; Boaz Rafaely', display:{Lore:['[{"text": "arXiv:2312.13707", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBlind Localization of Room Reflections with Application to Spatial Audio\\u00a7r\\n\\n\\u00a78\\u00a7oYogev Hadadi\\nVladimir Tourbabin\\nPaul Calamia\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.13707\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nin 2023 Immersive and 3D Audio: from Architecture to Automotive\\n  (I3DA 2023), Bologna, Italy, September 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Dec 2023 10:18:25 GMT)\\u00a7r"}']}
{title:'Berghi et al. (§72023§r)', author: 'Davide Berghi; Philip J. B. Jackson', display:{Lore:['[{"text": "arXiv:2312.14021", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging Visual Supervision for Array-based Active Speaker Detection and Localization\\u00a7r\\n\\n\\u00a78\\u00a7oDavide Berghi\\nPhilip J. B. Jackson\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.14021\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3346643\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Dec 2023 16:53:04 GMT)\\u00a7r"}']}
{title:'Yeiser et al. (§72023§r)', author: 'Aaron J. Yeiser; Emma F. Wawrzynek; John Z. Zhang; Lukas Graf; Christopher I. McHugh; Ioannis Kymissis; Elizabeth S. Olson; Jeffrey H. Lang; Hideko Heidi Nakajima', display:{Lore:['[{"text": "arXiv:2312.14339", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a75physics.med-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe UmboMic: A PVDF Cantilever Microphone\\u00a7r\\n\\n\\u00a78\\u00a7oAaron J. Yeiser\\nEmma F. Wawrzynek\\nJohn Z. Zhang\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.14339\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Dec 2023 00:03:59 GMT)\\u00a7r"}']}
{title:'Moliner et al. (§72023§r)', author: 'Eloi Moliner; Leonardo Fierro; Alec Wright; Matti Hämäläinen; Vesa Välimäki', display:{Lore:['[{"text": "arXiv:2312.14586", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoise Morphing for Audio Time Stretching\\u00a7r\\n\\n\\u00a78\\u00a7oEloi Moliner\\nLeonardo Fierro\\nAlec Wright\\nMatti H\\u00e4m\\u00e4l\\u00e4inen\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.14586\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Dec 2023 10:23:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to IEEESignal Processing Letters\\u00a7r"}']}
{title:'Ogawa et al. (§72023§r)', author: 'Atsunori Ogawa; Naohiro Tawara; Takatomo Kano; Marc Delcroix', display:{Lore:['[{"text": "arXiv:2312.14609", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBLSTM-Based Confidence Estimation for End-to-End Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAtsunori Ogawa\\nNaohiro Tawara\\nTakatomo Kano\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.14609\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Dec 2023 11:12:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2021\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'John Z. Zhang; Lukas Graf; Annesya Banerjee; Aaron Yeiser; Christopher I. McHugh; Ioannis Kymissis; Jeffrey H. Lang; Elizabeth S. Olson; Hideko Heidi Nakajima', display:{Lore:['[{"text": "arXiv:2312.14844", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a75physics.med-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Implantable Piezofilm Middle Ear Microphone: Performance in Human Cadaveric Temporal Bones\\u00a7r\\n\\n\\u00a78\\u00a7oJohn Z. Zhang\\nLukas Graf\\nAnnesya Banerjee\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.14844\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s10162-024-00927-4\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Dec 2023 17:18:24 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72023§r)', author: 'Yuanyuan Wang; Hangting Chen; Dongchao Yang; Jianwei Yu; Chao Weng; Zhiyong Wu; Helen Meng', display:{Lore:['[{"text": "arXiv:2312.15463", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConsistent and Relevant: Rethink the Query Embedding in General Sound Separation\\u00a7r\\n\\n\\u00a78\\u00a7oYuanyuan Wang\\nHangting Chen\\nDongchao Yang\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.15463\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 24 Dec 2023 11:47:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Zhang et al. (§72023§r)', author: 'Yuanyuan Zhang; Aaricia Herygers; Tanvina Patel; Zhengjun Yue; Odette Scharenborg', display:{Lore:['[{"text": "arXiv:2312.15499", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring data augmentation in bias mitigation against non-native-accented speech\\u00a7r\\n\\n\\u00a78\\u00a7oYuanyuan Zhang\\nAaricia Herygers\\nTanvina Patel\\nZhengjun Yue\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.15499\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 24 Dec 2023 14:58:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ASRU 2023\\u00a7r"}']}
{title:'Ge et al. (§72023§r)', author: 'Meng Ge; Yizhou Peng; Yidi Jiang; Jingru Lin; Junyi Ao; Mehmet Sinan Yildirim; Shuai Wang; Haizhou Li; Mengling Feng', display:{Lore:['[{"text": "arXiv:2312.16002", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe NUS-HLT System for ICASSP2024 ICMC-ASR Grand Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oMeng Ge\\nYizhou Peng\\nYidi Jiang\\n+ 5 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.16002\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Dec 2023 11:11:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTechnical Report. 2 pages. For ICMC-ASR-2023 Challenge\\u00a7r"}']}
{title:'Hu et al. (§72023§r)', author: 'Jinbo Hu; Yin Cao; Ming Wu; Qiuqiang Kong; Feiran Yang; Mark D. Plumbley; Jun Yang', display:{Lore:['[{"text": "arXiv:2312.16422", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelective-Memory Meta-Learning with Environment Representations for Sound Event Localization and Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJinbo Hu\\nYin Cao\\nMing Wu\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.16422\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Dec 2023 06:02:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 11 figures\\u00a7r"}']}
{title:'Hiroe (§72023§r)', author: 'Atsuo Hiroe', display:{Lore:['[{"text": "arXiv:2312.16449", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOnline Similarity-and-Independence-Aware Beamformer for Low-latency Target Sound Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oAtsuo Hiroe\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.16449\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Dec 2023 07:27:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEEOpen Journal of Signal Processing\\u00a7r"}']}
{title:'McKnight et al. (§72023§r)', author: 'Simon W. McKnight; Aidan O. T. Hogg; Vincent W. Neo; Patrick A. Naylor', display:{Lore:['[{"text": "arXiv:2312.16763", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUncertainty Quantification in Machine Learning for Joint Speaker Diarization and Identification\\u00a7r\\n\\n\\u00a78\\u00a7oSimon W. McKnight\\nAidan O. T. Hogg\\nVincent W. Neo\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.16763\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 30 Dec 2023 18:24:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 7 figures\\u00a7r"}']}
{title:'Tan (§72023§r)', author: 'Johann Kay Ann Tan', display:{Lore:['[{"text": "arXiv:2312.16884", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBinaural recording methods with analysis on inter-aural time, level, and phase differences\\u00a7r\\n\\n\\u00a78\\u00a7oJohann Kay Ann Tan\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.16884\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Dec 2023 08:18:04 GMT)\\u00a7r"}']}
{title:'Chang et al. (§72023§r)', author: 'Oscar Chang; Dung N. Tran; Kazuhito Koishida', display:{Lore:['[{"text": "arXiv:2312.17255", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSingle-channel speech enhancement using learnable loss mixup\\u00a7r\\n\\n\\u00a78\\u00a7oOscar Chang\\nDung N. Tran\\nKazuhito Koishida\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.17255\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Dec 2023 00:25:55 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72023§r)', author: 'Yun Chen; Lingxiao Yang; Qi Chen; Jian-Huang Lai; Xiaohua Xie', display:{Lore:['[{"text": "arXiv:2312.17508", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention-based Interactive Disentangling Network for Instance-level Emotional Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oYun Chen\\nLingxiao Yang\\nQi Chen\\nJian-Huang Lai\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.17508\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-39\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Dec 2023 08:06:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by INTERSPEECH 2023\\u00a7r"}']}
{title:'Torcoli et al. (§72023§r)', author: 'Matteo Torcoli; Chih-Wei Wu; Sascha Dick; Phillip A. Williams; Mhd Modar Halimeh; William Wolcott; Emanuel A. P. Habets', display:{Lore:['[{"text": "arXiv:2401.00197", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lODAQ: Open Dataset of Audio Quality\\u00a7r\\n\\n\\u00a78\\u00a7oMatteo Torcoli\\nChih-Wei Wu\\nSascha Dick\\n+ 3 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.00197\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 30 Dec 2023 10:32:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted paper. IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), Seoul, Korea, April 2024\\u00a7r"}']}
{title:'Zhu et al. (§72023§r)', author: 'Ting Zhu; Shufei Duan; Camille Dingam; Huizhi Liang; Wei Zhang', display:{Lore:['[{"text": "arXiv:2401.00225", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing dysarthria speech feature representation with empirical mode decomposition and Walsh-Hadamard transform\\u00a7r\\n\\n\\u00a78\\u00a7oTing Zhu\\nShufei Duan\\nCamille Dingam\\nHuizhi Liang\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.00225\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 30 Dec 2023 13:25:26 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72023§r)', author: 'Chih-Kai Yang; Kuan-Po Huang; Ke-Han Lu; Chun-Yi Kuan; Chi-Yuan Hsiao; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2401.00273", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInvestigating Zero-Shot Generalizability on Mandarin-English Code-Switched ASR and Speech-to-text Translation of Recent Foundation Models with Self-Supervision and Weak Supervision\\u00a7r\\n\\n\\u00a78\\u00a7oChih-Kai Yang\\nKuan-Po Huang\\nKe-Han Lu\\n+ 2 others\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.00273\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 30 Dec 2023 16:15:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024 Self-supervision in Audio, Speech and Beyond workshop\\u00a7r"}']}
{title:'Gubnitsky et al. (§72023§r)', author: 'Guy Gubnitsky; Roee Diamant', display:{Lore:['[{"text": "arXiv:2401.00900", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetecting the presence of sperm whales echolocation clicks in noisy environments\\u00a7r\\n\\n\\u00a78\\u00a7oGuy Gubnitsky\\nRoee Diamant\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.00900\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 31 Dec 2023 13:24:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages and 10 figures\\u00a7r"}']}
{title:'Kahsu et al. (§72023§r)', author: 'Ataklti Kahsu; Solomon Teferra', display:{Lore:['[{"text": "arXiv:2402.04254", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLarge Vocabulary Spontaneous Speech Recognition for Tigrigna\\u00a7r\\n\\n\\u00a78\\u00a7oAtaklti Kahsu\\nSolomon Teferra\\u00a7r\\n\\n\\u00a772023\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.04254\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 15 Oct 2023 13:07:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 1 figures\\u00a7r"}']}
