{title:'Wang et al. (§72024§r)', author: 'Quan Wang; Ignacio Lopez Moreno', display:{Lore:['[{"text": "arXiv:2007.12069", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.DC\\u00a7r, \\u00a7acs.NI\\u00a7r, \\u00a7acs.SE\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVersion Control of Speaker Recognition Systems\\u00a7r\\n\\n\\u00a78\\u00a7oQuan Wang\\nIgnacio Lopez Moreno\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2007.12069\\u00a7r\\n\\nVersion:\\u00a77v6 (Wed, 10 Apr 2024 23:49:04 GMT)\\u00a7r"}']}
{title:'Weng et al. (§72024§r)', author: 'Zhenzi Weng; Zhijin Qin; Geoffrey Ye Li', display:{Lore:['[{"text": "arXiv:2107.11190", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemantic Communications for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZhenzi Weng\\nZhijin Qin\\nGeoffrey Ye Li\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2107.11190\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 27 Apr 2024 18:04:57 GMT)\\u00a7r"}']}
{title:'Khorrami et al. (§72024§r)', author: 'Khazar Khorrami; Okko Räsänen', display:{Lore:['[{"text": "arXiv:2109.14200", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCan phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? \\u2013 A computational investigation\\u00a7r\\n\\n\\u00a78\\u00a7oKhazar Khorrami\\nOkko R\\u00e4s\\u00e4nen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2109.14200\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.34842/w3vw-s845\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nLanguage Development Research, 1(1), 123-191 (2021)\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Mar 2024 21:01:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oFinal manuscript published inLanguage Development Research under CC BY-NC-SA 4.0. Pre-print redistributed through arXiv with permission. Replaces corrupted PsyArXiv pre-print repository at https://psyarxiv.com/37zna\\u00a7r"}']}
{title:'Ge et al. (§72024§r)', author: 'Wanying Ge; Jose Patino; Massimiliano Todisco; Nicholas Evans', display:{Lore:['[{"text": "arXiv:2110.03309", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExplaining deep learning models for spoofing and deepfake detection with SHapley Additive exPlanations\\u00a7r\\n\\n\\u00a78\\u00a7oWanying Ge\\nJose Patino\\nMassimiliano Todisco\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2110.03309\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 26 Apr 2024 09:00:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2022\\u00a7r"}']}
{title:'Cheddad et al. (§72024§r)', author: 'Zohra Adila Cheddad; Abbas Cheddad', display:{Lore:['[{"text": "arXiv:2111.10891", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lActive Restoration of Lost Audio Signals Using Machine Learning and Latent Information\\u00a7r\\n\\n\\u00a78\\u00a7oZohra Adila Cheddad\\nAbbas Cheddad\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2111.10891\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/978-3-031-47721-8_1\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nLecture Notes in Networks and Systems, vol 822, 2024, Springer,\\n  Cham\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 18 Jan 2024 22:43:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 Pages, 2 Tables, 8 Figures\\u00a7r"}']}
{title:'Heo et al. (§72024§r)', author: 'Hee-Soo Heo; Jee-weon Jung; Jingu Kang; Youngki Kwon; You Jin Kim; Bong-Jin Lee; Joon Son Chung', display:{Lore:['[{"text": "arXiv:2203.14525", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCurriculum learning for self-supervised speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oHee-Soo Heo\\nJee-weon Jung\\nJingu Kang\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2203.14525\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 14 Feb 2024 04:51:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023. 5 pages, 3 figures, 4 tables\\u00a7r"}']}
{title:'Della Libera et al. (§72024§r)', author: 'Luca Della Libera; Cem Subakan; Mirco Ravanelli; Samuele Cornell; Frédéric Lepoutre; François Grondin', display:{Lore:['[{"text": "arXiv:2206.09507", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lResource-Efficient Separation Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oLuca Della Libera\\nCem Subakan\\nMirco Ravanelli\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2206.09507\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 15 Jan 2024 17:35:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Kevin Liu; Julien DeMori; Kobi Abayomi', display:{Lore:['[{"text": "arXiv:2209.07548", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a72math.OC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOpen Set Recognition For Music Genre Classification\\u00a7r\\n\\n\\u00a78\\u00a7oKevin Liu\\nJulien DeMori\\nKobi Abayomi\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.07548\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 14 May 2024 16:20:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 5 figures, 4 tables\\u00a7r"}']}
{title:'Chen et al. (§72024§r)', author: 'Meiying Chen; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2209.11866", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lControlVC: Zero-Shot Voice Conversion with Time-Varying Controls on Pitch and Speed\\u00a7r\\n\\n\\u00a78\\u00a7oMeiying Chen\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2209.11866\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1788\\u00a7r\\n\\nVersion:\\u00a77v5 (Thu, 11 Jan 2024 19:45:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAudio samples: https://bit.ly/3PsrKLJ;Code: https://github.com/MelissaChen15/control-vc\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Shupei Liu; Linfeng Feng; Yijun Gong; Chengdong Liang; Chen Zhang; Xiao-Lei Zhang; Xuelong Li', display:{Lore:['[{"text": "arXiv:2210.10265", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Learning Based Stage-wise Two-dimensional Speaker Localization with Large Ad-hoc Microphone Arrays\\u00a7r\\n\\n\\u00a78\\u00a7oShupei Liu\\nLinfeng Feng\\nYijun Gong\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.10265\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 1 Apr 2024 12:51:16 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Quan Wang; Yiling Huang; Han Lu; Guanlong Zhao; Ignacio Lopez Moreno', display:{Lore:['[{"text": "arXiv:2210.13690", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHighly Efficient Real-Time Streaming and Fully On-Device Speaker Diarization with Multi-Stage Clustering\\u00a7r\\n\\n\\u00a78\\u00a7oQuan Wang\\nYiling Huang\\nHan Lu\\nGuanlong Zhao\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2210.13690\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 8 Jan 2024 17:05:51 GMT)\\u00a7r"}']}
{title:'Xu et al. (§72024§r)', author: 'Hainan Xu; Fei Jia; Somshubra Majumdar; Shinji Watanabe; Boris Ginsburg', display:{Lore:['[{"text": "arXiv:2211.03541", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-blank Transducers for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHainan Xu\\nFei Jia\\nSomshubra Majumdar\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2211.03541\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2023\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 11 Apr 2024 22:58:21 GMT)\\u00a7r"}']}
{title:'Lemercier et al. (§72024§r)', author: 'Jean-Marie Lemercier; Julius Richter; Simon Welker; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2212.11851", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStoRM: A Diffusion-based Stochastic Regeneration Model for Speech Enhancement and Dereverberation\\u00a7r\\n\\n\\u00a78\\u00a7oJean-Marie Lemercier\\nJulius Richter\\nSimon Welker\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2212.11851\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2023.3294692\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 12 Mar 2024 15:31:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in IEEE/ACM Transactionson Audio, Speech and Language Processing, 2023\\u00a7r"}']}
{title:'Cohen et al. (§72024§r)', author: 'Idan Cohen; Ofir Lindenbaum; Sharon Gannot', display:{Lore:['[{"text": "arXiv:2301.00448", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised Acoustic Scene Mapping Based on Acoustic Features and Dimensionality Reduction\\u00a7r\\n\\n\\u00a78\\u00a7oIdan Cohen\\nOfir Lindenbaum\\nSharon Gannot\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2301.00448\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 12 Mar 2024 18:48:40 GMT)\\u00a7r"}']}
{title:'Ratnarajah et al. (§72024§r)', author: 'Anton Ratnarajah; Dinesh Manocha', display:{Lore:['[{"text": "arXiv:2302.02809", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lListen2Scene: Interactive material-aware binaural sound propagation for reconstructed 3D scenes\\u00a7r\\n\\n\\u00a78\\u00a7oAnton Ratnarajah\\nDinesh Manocha\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.02809\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 1 Feb 2024 13:51:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEVR 2024. Project page: https://anton-jeran.github.io/Listen2Scene/\\u00a7r"}']}
{title:'Lian et al. (§72024§r)', author: 'Jiachen Lian; Alexei Baevski; Wei-Ning Hsu; Michael Auli', display:{Lore:['[{"text": "arXiv:2302.06419", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAV-data2vec: Self-supervised Learning of Audio-Visual Speech Representations with Contextualized Target Representations\\u00a7r\\n\\n\\u00a78\\u00a7oJiachen Lian\\nAlexei Baevski\\nWei-Ning Hsu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2302.06419\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 21 Jan 2024 07:41:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2023 ASRU\\u00a7r"}']}
{title:'Boeddeker et al. (§72024§r)', author: 'Christoph Boeddeker; Aswin Shanmugam Subramanian; Gordon Wichern; Reinhold Haeb-Umbach; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2303.03849", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTS-SEP: Joint Diarization and Separation Conditioned on Estimated Speaker Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oChristoph Boeddeker\\nAswin Shanmugam Subramanian\\nGordon Wichern\\nReinhold Haeb-Umbach\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.03849\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 1 Jan 2024 14:33:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEE/ACM TASLP\\u00a7r"}']}
{title:'Iliescu et al. (§72024§r)', author: 'Dan Andrei Iliescu; Devang Savita Ram Mohan; Tian Huey Teh; Zack Hodari', display:{Lore:['[{"text": "arXiv:2303.09446", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lControllable Prosody Generation With Partial Inputs\\u00a7r\\n\\n\\u00a78\\u00a7oDan Andrei Iliescu\\nDevang Savita Ram Mohan\\nTian Huey Teh\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.09446\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 Apr 2024 01:33:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Morrone et al. (§72024§r)', author: 'Giovanni Morrone; Samuele Cornell; Luca Serafini; Enrico Zovato; Alessio Brutti; Stefano Squartini', display:{Lore:['[{"text": "arXiv:2303.12002", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd-to-End Integration of Speech Separation and Voice Activity Detection for Low-Latency Diarization of Telephone Conversations\\u00a7r\\n\\n\\u00a78\\u00a7oGiovanni Morrone\\nSamuele Cornell\\nLuca Serafini\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2303.12002\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.specom.2024.103081\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nSpeech Communication 161 (2024) 103081\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 22 May 2024 07:42:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages, 7 figures\\u00a7r"}']}
{title:'Hu et al. (§72024§r)', author: 'Yuchen Hu; Chen Chen; Qiushi Zhu; Eng Siong Chng', display:{Lore:['[{"text": "arXiv:2304.04974", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWav2code: Restore Clean Speech Representations via Codebook Lookup for Noise-Robust ASR\\u00a7r\\n\\n\\u00a78\\u00a7oYuchen Hu\\nChen Chen\\nQiushi Zhu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.04974\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 18 Apr 2024 06:39:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 7 figures, IEEE/ACM Transactionson Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Dutta et al. (§72024§r)', author: 'Soumya Dutta; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:2304.06910", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHCAM \\u2013 Hierarchical Cross Attention Model for Multi-modal Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSoumya Dutta\\nSriram Ganapathy\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2304.06910\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 Jan 2024 11:45:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 6 figures\\u00a7r"}']}
{title:'Di Carlo et al. (§72024§r)', author: 'Diego Di Carlo; Aditya Arie Nugraha; Mathieu Fontaine; Mathieu Fontaine; Kazuyoshi Yoshii', display:{Lore:['[{"text": "arXiv:2305.04447", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Steerer: Novel Steering Vector Synthesis with a Causal Neural Field over Frequency and Source Positions\\u00a7r\\n\\n\\u00a78\\u00a7oDiego Di Carlo\\nAditya Arie Nugraha\\nMathieu Fontaine\\nMathieu Fontaine\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.04447\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 1 Mar 2024 16:50:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCamera ready version for HSCMA 24 atICASSP 24\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Zhichao Wang; Liumeng Xue; Qiuqiang Kong; Lei Xie; Yuanzhe Chen; Qiao Tian; Yuping Wang', display:{Lore:['[{"text": "arXiv:2305.07204", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-level Temporal-channel Speaker Retrieval for Zero-shot Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oZhichao Wang\\nLiumeng Xue\\nQiuqiang Kong\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.07204\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 19 May 2024 02:26:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to TASLP\\u00a7r"}']}
{title:'Gong et al. (§72024§r)', author: 'Yuan Gong; Hongyin Luo; Alexander H. Liu; Leonid Karlinsky; James Glass', display:{Lore:['[{"text": "arXiv:2305.10790", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lListen, Think, and Understand\\u00a7r\\n\\n\\u00a78\\u00a7oYuan Gong\\nHongyin Luo\\nAlexander H. Liu\\nLeonid Karlinsky\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.10790\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 19 Feb 2024 23:51:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICLR2024. Code, dataset, and models are available at https://github.com/YuanGongND/ltu. The interactive demois at https://huggingface.co/spaces/yuangongfdu/ltu\\u00a7r"}']}
{title:'Deshmukh et al. (§72024§r)', author: 'Soham Deshmukh; Benjamin Elizalde; Rita Singh; Huaming Wang', display:{Lore:['[{"text": "arXiv:2305.11834", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPengi: An Audio Language Model for Audio Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oSoham Deshmukh\\nBenjamin Elizalde\\nRita Singh\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.11834\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 19 Jan 2024 01:42:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at NeurIPS 2023. The manuscript is updated with additional experiments suggestedby reviewers\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Huadai Liu; Rongjie Huang; Xuan Lin; Wenqiang Xu; Maozong Zheng; Hong Chen; Jinzheng He; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2305.12708", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lViT-TTS: Visual Text-to-Speech with Scalable Diffusion Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oHuadai Liu\\nRongjie Huang\\nXuan Lin\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12708\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 21 Apr 2024 07:17:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by EMNLP 2023\\u00a7r"}']}
{title:'He et al. (§72024§r)', author: 'Jianfeng He; Julian Salazar; Kaisheng Yao; Haoqi Li; Jinglun Cai', display:{Lore:['[{"text": "arXiv:2305.12793", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero-Shot End-to-End Spoken Language Understanding via Cross-Modal Selective Self-Training\\u00a7r\\n\\n\\u00a78\\u00a7oJianfeng He\\nJulian Salazar\\nKaisheng Yao\\nHaoqi Li\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.12793\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 3 Feb 2024 03:24:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 pages, 7 figures\\u00a7r"}']}
{title:'Likhomanenko et al. (§72024§r)', author: 'Tatiana Likhomanenko; Loren Lugosch; Ronan Collobert', display:{Lore:['[{"text": "arXiv:2305.13330", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnsupervised ASR via Cross-Lingual Pseudo-Labeling\\u00a7r\\n\\n\\u00a78\\u00a7oTatiana Likhomanenko\\nLoren Lugosch\\nRonan Collobert\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2305.13330\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 16 Feb 2024 16:20:18 GMT)\\u00a7r"}']}
{title:'Xie et al. (§72024§r)', author: 'Yuan Xie; Jiawei Ren; Ji Xu', display:{Lore:['[{"text": "arXiv:2306.01002", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaptive ship-radiated noise recognition with learnable fine-grained wavelet transform\\u00a7r\\n\\n\\u00a78\\u00a7oYuan Xie\\nJiawei Ren\\nJi Xu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01002\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.oceaneng.2022.112626\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nOcean Engineering 265 (2022): 112626\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 19 Feb 2024 09:28:15 GMT)\\u00a7r"}']}
{title:'Moliner et al. (§72024§r)', author: 'Eloi Moliner; Filip Elvander; Vesa Välimäki', display:{Lore:['[{"text": "arXiv:2306.01433", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBlind Audio Bandwidth Extension: A Diffusion-Based Zero-Shot Approach\\u00a7r\\n\\n\\u00a78\\u00a7oEloi Moliner\\nFilip Elvander\\nVesa V\\u00e4lim\\u00e4ki\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.01433\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 30 Jan 2024 15:40:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEE/ACM Transactionson Audio, Speech and Language Processing\\u00a7r"}']}
{title:'Yemini et al. (§72024§r)', author: 'Yochai Yemini; Aviv Shamsian; Lior Bracha; Sharon Gannot; Ethan Fetaya', display:{Lore:['[{"text": "arXiv:2306.03258", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLipVoicer: Generating Speech from Silent Videos Guided by Lip Reading\\u00a7r\\n\\n\\u00a78\\u00a7oYochai Yemini\\nAviv Shamsian\\nLior Bracha\\nSharon Gannot\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.03258\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 28 Mar 2024 09:35:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICLR 2024\\u00a7r"}']}
{title:'Hogg et al. (§72024§r)', author: 'Aidan O. T. Hogg; Mads Jenkins; He Liu; Isaac Squires; Samuel J. Cooper; Lorenzo Picinali', display:{Lore:['[{"text": "arXiv:2306.05812", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHRTF upsampling with a generative adversarial network using a gnomonic equiangular projection\\u00a7r\\n\\n\\u00a78\\u00a7oAidan O. T. Hogg\\nMads Jenkins\\nHe Liu\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.05812\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 27 Feb 2024 13:40:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 9 figures, Preprint (Accepted to IEEE/ACM Transactionson Audio, Speech, and Language Processing on the 15 Feb 2024)\\u00a7r"}']}
{title:'Shin et al. (§72024§r)', author: 'Ui-Hyeop Shin; Hyung-Min Park', display:{Lore:['[{"text": "arXiv:2306.07562", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStatistical Beamformer Exploiting Non-stationarity and Sparsity with Spatially Constrained ICA for Robust Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oUi-Hyeop Shin\\nHyung-Min Park\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.07562\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 5 Jan 2024 05:20:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by TASLP\\u00a7r"}']}
{title:'Huybrechts et al. (§72024§r)', author: 'Goeric Huybrechts; Srikanth Ronanki; Xilai Li; Hadis Nosrati; Sravan Bodapati; Katrin Kirchhoff', display:{Lore:['[{"text": "arXiv:2306.08175", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDCTX-Conformer: Dynamic context carry-over for low latency unified streaming and non-streaming Conformer ASR\\u00a7r\\n\\n\\u00a78\\u00a7oGoeric Huybrechts\\nSrikanth Ronanki\\nXilai Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.08175\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 1 Mar 2024 21:25:16 GMT)\\u00a7r"}']}
{title:'Nortje et al. (§72024§r)', author: 'Leanne Nortje; Dan Oneata; Herman Kamper', display:{Lore:['[{"text": "arXiv:2306.11371", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVisually grounded few-shot word learning in low-resource settings\\u00a7r\\n\\n\\u00a78\\u00a7oLeanne Nortje\\nDan Oneata\\nHerman Kamper\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.11371\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 18 Apr 2024 17:36:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to TASLP. arXiv admin note:substantial text overlap with arXiv:2305.15937\\u00a7r"}']}
{title:'Lemercier et al. (§72024§r)', author: 'Jean-Marie Lemercier; Joachim Thiemann; Raphael Koning; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2306.12867", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWind Noise Reduction with a Diffusion-based Stochastic Regeneration Model\\u00a7r\\n\\n\\u00a78\\u00a7oJean-Marie Lemercier\\nJoachim Thiemann\\nRaphael Koning\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2306.12867\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 Jan 2024 08:46:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to VDE 15th ITG conference onSpeech Communication\\u00a7r"}']}
{title:'Baas et al. (§72024§r)', author: 'Matthew Baas; Herman Kamper', display:{Lore:['[{"text": "arXiv:2307.01673", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDisentanglement in a GAN for Unconditional Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew Baas\\nHerman Kamper\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.01673\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 Jan 2024 13:44:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 5 tables, 4 figures. Accepted to IEEE TASLP. arXiv admin note: substantial text overlap with arXiv:2210.05271\\u00a7r"}']}
{title:'Farhadipour et al. (§72024§r)', author: 'Aref Farhadipour; Hadi Veisi', display:{Lore:['[{"text": "arXiv:2307.03296", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGammatonegram Representation for End-to-End Dysarthric Speech Processing Tasks: Speech Recognition, Speaker Identification, and Intelligibility Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oAref Farhadipour\\nHadi Veisi\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.03296\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s42044-024-00175-y\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Mar 2024 19:08:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages, 8 figures. Iran JComput Sci (2024)\\u00a7r"}']}
{title:'Moussa et al. (§72024§r)', author: 'Denise Moussa; Germans Hirsch; Sebastian Wankerl; Christian Riess', display:{Lore:['[{"text": "arXiv:2307.05641", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPoint to the Hidden: Exposing Speech Audio Splicing via Signal Pointer Nets\\u00a7r\\n\\n\\u00a78\\u00a7oDenise Moussa\\nGermans Hirsch\\nSebastian Wankerl\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.05641\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-996\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. INTERSPEECH 2023, 5057-5061 (2023)\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 3 May 2024 15:26:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7opublished at Interspeech 2023 - Code: https://faui1-gitlab.cs.fau.de/mmsec/exposing-speech-audio-splicing-via-signal-pointer-nets\\u00a7r"}']}
{title:'Jiang et al. (§72024§r)', author: 'Ziyue Jiang; Jinglin Liu; Yi Ren; Jinzheng He; Zhenhui Ye; Shengpeng Ji; Qian Yang; Chen Zhang; Pengfei Wei; Chunfeng Wang; Xiang Yin; Zejun Ma; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2307.07218", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMega-TTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oZiyue Jiang\\nJinglin Liu\\nYi Ren\\n+ 9 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2307.07218\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 10 Apr 2024 10:05:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICLR 2024\\u00a7r"}']}
{title:'Fabbro et al. (§72024§r)', author: 'Giorgio Fabbro; Stefan Uhlich; Chieh-Hsin Lai; Woosung Choi; Marco Martínez-Ramírez; Weihsiang Liao; Igor Gadelha; Geraldo Ramos; Eddie Hsu; Hugo Rodrigues; Fabian-Robert Stöter; Alexandre Défossez; Yi Luo; Jianwei Yu; Dipam Chakraborty; Sharada Mohanty; Roman Solovyev; Alexander Stempkovskiy; Tatiana Habruseva; Nabarun Goswami; Tatsuya Harada; Minseok Kim; Jun Hyung Lee; Yuanliang Dong; Xinran Zhang; Jiafeng Liu; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2308.06979", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Sound Demixing Challenge 2023 x2013 Music Demixing Track\\u00a7r\\n\\n\\u00a78\\u00a7oGiorgio Fabbro\\nStefan Uhlich\\nChieh-Hsin Lai\\n+ 23 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.06979\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.5334/tismir.171\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nTransactions of the International Society for Music Information\\n  Retrieval, 7(1), pp.63-84, 2024\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 19 Apr 2024 09:16:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in Transactions of the International Society for Music Information Retrieval (https://transactions.ismir.net/articles/10.5334/tismir.171)\\u00a7r"}']}
{title:'Uhlich et al. (§72024§r)', author: 'Stefan Uhlich; Giorgio Fabbro; Masato Hirano; Shusuke Takahashi; Gordon Wichern; Jonathan Le Roux; Dipam Chakraborty; Sharada Mohanty; Kai Li; Yi Luo; Jianwei Yu; Rongzhi Gu; Roman Solovyev; Alexander Stempkovskiy; Tatiana Habruseva; Mikhail Sukhovei; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2308.06981", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Sound Demixing Challenge 2023 x2013 Cinematic Demixing Track\\u00a7r\\n\\n\\u00a78\\u00a7oStefan Uhlich\\nGiorgio Fabbro\\nMasato Hirano\\n+ 13 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.06981\\u00a7r\\n\\nVersion:\\u00a77v4 (Thu, 18 Apr 2024 06:46:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for Transactions of the International Society for Music Information Retrieval\\u00a7r"}']}
{title:'Lu et al. (§72024§r)', author: 'Ye-Xin Lu; Yang Ai; Zhen-Hua Ling', display:{Lore:['[{"text": "arXiv:2308.08926", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExplicit Estimation of Magnitude and Phase Spectra in Parallel for High-Quality Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oYe-Xin Lu\\nYang Ai\\nZhen-Hua Ling\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.08926\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 1 Apr 2024 08:31:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmmited to IEEE Transactions on Audio, Speech and Language Processing\\u00a7r"}']}
{title:'Zezario et al. (§72024§r)', author: 'Ryandhimas E. Zezario; Bo-Ren Brian Bai; Chiou-Shann Fuh; Hsin-Min Wang; Yu Tsao', display:{Lore:['[{"text": "arXiv:2308.09262", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Task Pseudo-Label Learning for Non-Intrusive Speech Quality Assessment Model\\u00a7r\\n\\n\\u00a78\\u00a7oRyandhimas E. Zezario\\nBo-Ren Brian Bai\\nChiou-Shann Fuh\\nHsin-Min Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.09262\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 13 Mar 2024 13:15:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEICASSP 2024\\u00a7r"}']}
{title:'Kamath et al. (§72024§r)', author: 'Purnima Kamath; Chitralekha Gupta; Lonce Wyse; Suranga Nanayakkara', display:{Lore:['[{"text": "arXiv:2308.11859", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExample-Based Framework for Perceptually Guided Audio Texture Generation\\u00a7r\\n\\n\\u00a78\\u00a7oPurnima Kamath\\nChitralekha Gupta\\nLonce Wyse\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.11859\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2024.3393741\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 14 Apr 2024 10:14:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at IEEE Transactions on Audio, Speech and Language Processing\\u00a7r"}']}
{title:'Nzeyimana (§72024§r)', author: 'Antoine Nzeyimana', display:{Lore:['[{"text": "arXiv:2308.11863", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods\\u00a7r\\n\\n\\u00a78\\u00a7oAntoine Nzeyimana\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.11863\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 2 Mar 2024 07:14:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 2 figures, 5 tables\\u00a7r"}']}
{title:'Zaiem et al. (§72024§r)', author: 'Salah Zaiem; Youcef Kemiche; Titouan Parcollet; Slim Essid; Mirco Ravanelli', display:{Lore:['[{"text": "arXiv:2308.14456", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Self-Supervised Representations Benchmarking: a Case for Larger Probing Heads\\u00a7r\\n\\n\\u00a78\\u00a7oSalah Zaiem\\nYoucef Kemiche\\nTitouan Parcollet\\nSlim Essid\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.14456\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 21 Feb 2024 16:57:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o18 Pages\\u00a7r"}']}
{title:'Kim et al. (§72024§r)', author: 'Ji-Hoon Kim; Jaehun Kim; Joon Son Chung', display:{Lore:['[{"text": "arXiv:2308.15256", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLet There Be Sound: Reconstructing High Quality Speech from Silent Videos\\u00a7r\\n\\n\\u00a78\\u00a7oJi-Hoon Kim\\nJaehun Kim\\nJoon Son Chung\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2308.15256\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 4 Jan 2024 11:10:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to AAAI2024\\u00a7r"}']}
{title:'Maharana et al. (§72024§r)', author: 'Sarthak Kumar Maharana; Krishna Kamal Adidam; Shoumik Nandi; Ajitesh Srivastava', display:{Lore:['[{"text": "arXiv:2309.01108", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic-to-articulatory inversion for dysarthric speech: Are pre-trained self-supervised representations favorable?\\u00a7r\\n\\n\\u00a78\\u00a7oSarthak Kumar Maharana\\nKrishna Kamal Adidam\\nShoumik Nandi\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.01108\\u00a7r\\n\\nVersion:\\u00a77v4 (Fri, 9 Feb 2024 23:01:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEICASSP Workshops 2024\\u00a7r"}']}
{title:'Yeon et al. (§72024§r)', author: 'Inmo Yeon; Jung-Woo Choi', display:{Lore:['[{"text": "arXiv:2309.01513", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRGI-Net: 3D Room Geometry Inference from Room Impulse Responses in the Absence of First-order Echoes\\u00a7r\\n\\n\\u00a78\\u00a7oInmo Yeon\\nJung-Woo Choi\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.01513\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 1 May 2024 04:58:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 3 tables\\u00a7r"}']}
{title:'Veillon et al. (§72024§r)', author: 'Clément Le Moine Veillon; Victor Rosi; Pablo Arias Sarah; Léane Salais; Nicolas Obin', display:{Lore:['[{"text": "arXiv:2309.02592", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBWSNet: Automatic Perceptual Assessment of Audio Signals\\u00a7r\\n\\n\\u00a78\\u00a7oCl\\u00e9ment Le Moine Veillon\\nVictor Rosi\\nPablo Arias Sarah\\nL\\u00e9ane Salais\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.02592\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 21 Jan 2024 12:14:15 GMT)\\u00a7r"}']}
{title:'Mehta et al. (§72024§r)', author: 'Shivam Mehta; Ruibo Tu; Jonas Beskow; Éva Székely; Gustav Eje Henter', display:{Lore:['[{"text": "arXiv:2309.03199", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMatcha-TTS: A fast TTS architecture with conditional flow matching\\u00a7r\\n\\n\\u00a78\\u00a7oShivam Mehta\\nRuibo Tu\\nJonas Beskow\\n\\u00c9va Sz\\u00e9kely\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.03199\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 Jan 2024 21:02:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures. Final version, accepted to IEEE ICASSP 2024\\u00a7r"}']}
{title:'Gan et al. (§72024§r)', author: 'Chong-Xin Gan; Man-Wai Mak; Weiwei Lin; Jen-Tzung Chien', display:{Lore:['[{"text": "arXiv:2309.04265", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAsymmetric Clean Segments-Guided Self-Supervised Learning for Robust Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oChong-Xin Gan\\nMan-Wai Mak\\nWeiwei Lin\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.04265\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 11 Mar 2024 12:48:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, accepted by ICASSP 2024\\u00a7r"}']}
{title:'Guo et al. (§72024§r)', author: 'Yiwei Guo; Chenpeng Du; Ziyang Ma; Xie Chen; Kai Yu', display:{Lore:['[{"text": "arXiv:2309.05027", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching\\u00a7r\\n\\n\\u00a78\\u00a7oYiwei Guo\\nChenpeng Du\\nZiyang Ma\\nXie Chen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.05027\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 Jan 2024 07:38:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 figure, 5 pages, accepted to ICASSP 2024\\u00a7r"}']}
{title:'Seshadri et al. (§72024§r)', author: 'Pavan Seshadri; Chaeyeon Han; Bon-Woo Koo; Noah Posner; Subhrajit Guhathakurta; Alexander Lerch', display:{Lore:['[{"text": "arXiv:2309.06531", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lASPED: An Audio Dataset for Detecting Pedestrians\\u00a7r\\n\\n\\u00a78\\u00a7oPavan Seshadri\\nChaeyeon Han\\nBon-Woo Koo\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.06531\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 17 Jan 2024 02:46:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4+1 pages, ICASSP 2024\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Siyin Wang; Chao-Han Huck Yang; Ji Wu; Chao Zhang', display:{Lore:['[{"text": "arXiv:2309.07081", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCan Whisper perform speech-based in-context learning?\\u00a7r\\n\\n\\u00a78\\u00a7oSiyin Wang\\nChao-Han Huck Yang\\nJi Wu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07081\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 20 Mar 2024 03:04:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Yang et al. (§72024§r)', author: 'Xiaoyu Yang; Wei Kang; Zengwei Yao; Yifan Yang; Liyong Guo; Fangjun Kuang; Long Lin; Daniel Povey', display:{Lore:['[{"text": "arXiv:2309.07414", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPromptASR for contextualized ASR with controllable style\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoyu Yang\\nWei Kang\\nZengwei Yao\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07414\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 24 Jan 2024 08:29:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProc. ICASSP 2024\\u00a7r"}']}
{title:'Prabhu et al. (§72024§r)', author: 'Navin Raj Prabhu; Bunlong Lay; Simon Welker; Nale Lehmann-Willenbrock; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2309.07828", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEMOCONV-DIFF: Diffusion-based Speech Emotion Conversion for Non-parallel and In-the-wild Data\\u00a7r\\n\\n\\u00a78\\u00a7oNavin Raj Prabhu\\nBunlong Lay\\nSimon Welker\\nNale Lehmann-Willenbrock\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07828\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Jan 2024 15:27:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Attia et al. (§72024§r)', author: 'Ahmed Adel Attia; Jing Liu; Wei Ai; Dorottya Demszky; Carol Espy-Wilson', display:{Lore:['[{"text": "arXiv:2309.07927", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKid-Whisper: Towards Bridging the Performance Gap in Automatic Speech Recognition for Children VS. Adults\\u00a7r\\n\\n\\u00a78\\u00a7oAhmed Adel Attia\\nJing Liu\\nWei Ai\\nDorottya Demszky\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07927\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 15 May 2024 07:05:32 GMT)\\u00a7r"}']}
{title:'Maiti et al. (§72024§r)', author: 'Soumi Maiti; Yifan Peng; Shukjae Choi; Jee-weon Jung; Xuankai Chang; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2309.07937", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks\\u00a7r\\n\\n\\u00a78\\u00a7oSoumi Maiti\\nYifan Peng\\nShukjae Choi\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.07937\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 24 Jan 2024 15:36:31 GMT)\\u00a7r"}']}
{title:'Aris et al. (§72024§r)', author: 'William Aris; François Grondin', display:{Lore:['[{"text": "arXiv:2309.08005", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Face Detection with Audio-Based Region Proposals for Human-Robot Interactions\\u00a7r\\n\\n\\u00a78\\u00a7oWilliam Aris\\nFran\\u00e7ois Grondin\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08005\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 15 Mar 2024 20:21:36 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72024§r)', author: 'Mu Yang; Naoyuki Kanda; Xiaofei Wang; Junkun Chen; Peidong Wang; Jian Xue; Jinyu Li; Takuya Yoshioka', display:{Lore:['[{"text": "arXiv:2309.08007", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiariST: Streaming Speech Translation with Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oMu Yang\\nNaoyuki Kanda\\nXiaofei Wang\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08007\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 22 Jan 2024 23:05:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Zhao et al. (§72024§r)', author: 'Guanlong Zhao; Yongqiang Wang; Jason Pelecanos; Yu Zhang; Hank Liao; Yiling Huang; Han Lu; Quan Wang', display:{Lore:['[{"text": "arXiv:2309.08023", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUSM-SCD: Multilingual Speaker Change Detection Based on Large Pretrained Foundation Models\\u00a7r\\n\\n\\u00a78\\u00a7oGuanlong Zhao\\nYongqiang Wang\\nJason Pelecanos\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08023\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 6 Jan 2024 05:27:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, 4 tables\\u00a7r"}']}
{title:'Chou et al. (§72024§r)', author: 'Ju-Chieh Chou; Chung-Ming Chien; Karen Livescu', display:{Lore:['[{"text": "arXiv:2309.08030", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised Features for Audio-Visual Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oJu-Chieh Chou\\nChung-Ming Chien\\nKaren Livescu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08030\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 8 Apr 2024 22:23:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oextended version for the accepted paper at ICASSP 2024\\u00a7r"}']}
{title:'Kang et al. (§72024§r)', author: 'Wei Kang; Xiaoyu Yang; Zengwei Yao; Fangjun Kuang; Yifan Yang; Liyong Guo; Long Lin; Daniel Povey', display:{Lore:['[{"text": "arXiv:2309.08105", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLibriheavy: a 50,000 hours ASR corpus with punctuation casing and context\\u00a7r\\n\\n\\u00a78\\u00a7oWei Kang\\nXiaoyu Yang\\nZengwei Yao\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08105\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 15 Jan 2024 01:58:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Yuxiang Zhang; Jingze Lu; Zengqiang Shang; Wenchao Wang; Pengyuan Zhang', display:{Lore:['[{"text": "arXiv:2309.08279", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Short Utterance Anti-Spoofing with AASIST2\\u00a7r\\n\\n\\u00a78\\u00a7oYuxiang Zhang\\nJingze Lu\\nZengqiang Shang\\nWenchao Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08279\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 4 Jan 2024 03:24:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, accepted by ICASSP\\u00a7r"}']}
{title:'Zeineldeen et al. (§72024§r)', author: 'Mohammad Zeineldeen; Albert Zeyer; Ralf Schlüter; Hermann Ney', display:{Lore:['[{"text": "arXiv:2309.08436", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChunked Attention-based Encoder-Decoder Model for Streaming Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Zeineldeen\\nAlbert Zeyer\\nRalf Schl\\u00fcter\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08436\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 17 Jan 2024 11:28:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2024\\u00a7r"}']}
{title:'Chen et al. (§72024§r)', author: 'Junyu Chen; Susmitha Vekkot; Pancham Shukla', display:{Lore:['[{"text": "arXiv:2309.08684", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusic Source Separation Based on a Lightweight Deep Learning Framework (DTTNET: DUAL-PATH TFC-TDF UNET)\\u00a7r\\n\\n\\u00a78\\u00a7oJunyu Chen\\nSusmitha Vekkot\\nPancham Shukla\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08684\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP48485.2024.10448020\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 19 Mar 2024 15:26:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP 2024. Additional experiments can be found in the published version on IEEE Xplore\\u00a7r"}']}
{title:'Deng et al. (§72024§r)', author: 'Zihao Deng; Yinghao Ma; Yudong Liu; Rongchen Guo; Ge Zhang; Wenhu Chen; Wenhao Huang; Emmanouil Benetos', display:{Lore:['[{"text": "arXiv:2309.08730", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response\\u00a7r\\n\\n\\u00a78\\u00a7oZihao Deng\\nYinghao Ma\\nYudong Liu\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08730\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2024 Annual Conference of the North American Chapter of the\\n  Association for Computational Linguistics\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 2 Apr 2024 13:35:59 GMT)\\u00a7r"}']}
{title:'Tsunoo et al. (§72024§r)', author: 'Emiru Tsunoo; Hayato Futami; Yosuke Kashiwagi; Siddhant Arora; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2309.08876", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDecoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation\\u00a7r\\n\\n\\u00a78\\u00a7oEmiru Tsunoo\\nHayato Futami\\nYosuke Kashiwagi\\nSiddhant Arora\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.08876\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 Jan 2024 09:02:23 GMT)\\u00a7r"}']}
{title:'Guo et al. (§72024§r)', author: 'Zilu Guo; Jun Du; CHin-Hui Lee', display:{Lore:['[{"text": "arXiv:2309.09270", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContinuous Modeling of the Denoising Process for Speech Enhancement Based on Deep Learning\\u00a7r\\n\\n\\u00a78\\u00a7oZilu Guo\\nJun Du\\nCHin-Hui Lee\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09270\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 7 Jan 2024 15:52:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWe found the results are got from some wrong experimental settings. We needs new experiments\\u00a7r"}']}
{title:'Huang et al. (§72024§r)', author: 'Chien-yu Huang; Ke-Han Lu; Shih-Heng Wang; Chi-Yuan Hsiao; Chun-Yi Kuan; Haibin Wu; Siddhant Arora; Kai-Wei Chang; Jiatong Shi; Yifan Peng; Roshan Sharma; Shinji Watanabe; Bhiksha Ramakrishnan; Shady Shehata; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2309.09510", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech\\u00a7r\\n\\n\\u00a78\\u00a7oChien-yu Huang\\nKe-Han Lu\\nShih-Heng Wang\\n+ 11 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09510\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 22 Mar 2024 15:25:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in the proceedings of ICASSP 2024\\u00a7r"}']}
{title:'Wright et al. (§72024§r)', author: 'George August Wright; Umberto Cappellazzo; Salah Zaiem; Desh Raj; Lucas Ondel Yang; Daniele Falavigna; Mohamed Nabih Ali; Alessio Brutti', display:{Lore:['[{"text": "arXiv:2309.09546", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTraining dynamic models using early exits for automatic speech recognition on resource-constrained devices\\u00a7r\\n\\n\\u00a78\\u00a7oGeorge August Wright\\nUmberto Cappellazzo\\nSalah Zaiem\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09546\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 22 Feb 2024 15:10:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the ICASSP Workshop Self-supervision in Audio, Speech and Beyond 2024\\u00a7r"}']}
{title:'Lay et al. (§72024§r)', author: 'Bunlong Lay; Jean-Marie Lemercier; Julius Richter; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2309.09677", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSingle and Few-step Diffusion for Generative Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oBunlong Lay\\nJean-Marie Lemercier\\nJulius Richter\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.09677\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 15 Jan 2024 14:17:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ocopyright 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising "}','{"text": "or promotional purposes, creating newcollective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Tseng et al. (§72024§r)', author: 'Yuan Tseng; Layne Berry; Yi-Ting Chen; I-Hsiang Chiu; Hsuan-Hao Lin; Max Liu; Puyuan Peng; Yi-Jen Shih; Hung-Yu Wang; Haibin Wu; Po-Yao Huang; Chun-Mao Lai; Shang-Wen Li; David Harwath; Yu Tsao; Shinji Watanabe; Abdelrahman Mohamed; Chi-Luen Feng; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2309.10787", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models\\u00a7r\\n\\n\\u00a78\\u00a7oYuan Tseng\\nLayne Berry\\nYi-Ting Chen\\n+ 15 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.10787\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 19 Mar 2024 08:51:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024; EvaluationCode: https://github.com/roger-tseng/av-superb Submission Platform: https://av.superbbenchmark.org\\u00a7r"}']}
{title:'Fuglsig et al. (§72024§r)', author: 'Andreas J. Fuglsig; Jesper Jensen; Zheng-Hua Tan; Lars S. Bertelsen; Jens Christian Lindof; Jan Østergaard', display:{Lore:['[{"text": "arXiv:2309.11243", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Minimum Processing Beamforming and Near-end Listening Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oAndreas J. Fuglsig\\nJesper Jensen\\nZheng-Hua Tan\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.11243\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 5 Feb 2024 13:41:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at IEEEICASSP 2024 Workshop on Hands-free Speech Communication and Microphone Arrays (HSCMA) 2024\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Wei Liu; Ying Qin; Zhiyuan Peng; Tan Lee', display:{Lore:['[{"text": "arXiv:2309.11756", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSparsely Shared LoRA on Whisper for Child Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oWei Liu\\nYing Qin\\nZhiyuan Peng\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.11756\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Jan 2024 04:29:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Coldenhoff et al. (§72024§r)', author: 'Jozef Coldenhoff; Andrew Harper; Paul Kendrick; Tijana Stojkovic; Milos Cernak', display:{Lore:['[{"text": "arXiv:2309.11976", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Channel MOSRA: Mean Opinion Score and Room Acoustics Estimation Using Simulated Data and a Teacher Model\\u00a7r\\n\\n\\u00a78\\u00a7oJozef Coldenhoff\\nAndrew Harper\\nPaul Kendrick\\nTijana Stojkovic\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.11976\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 13 Mar 2024 08:32:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2024\\u00a7r"}']}
{title:'Yip et al. (§72024§r)', author: 'Jia Qi Yip; Shengkui Zhao; Yukun Ma; Chongjia Ni; Chong Zhang; Hao Wang; Trung Hieu Nguyen; Kun Zhou; Dianwen Ng; Eng Siong Chng; Bin Ma', display:{Lore:['[{"text": "arXiv:2309.12608", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSPGM: Prioritizing Local Features for enhanced speech separation performance\\u00a7r\\n\\n\\u00a78\\u00a7oJia Qi Yip\\nShengkui Zhao\\nYukun Ma\\n+ 7 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.12608\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 11 Mar 2024 02:44:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper was accepted by ICASSP 2024\\u00a7r"}']}
{title:'Zezario et al. (§72024§r)', author: 'Ryandhimas E. Zezario; Yu-Wen Chen; Szu-Wei Fu; Yu Tsao; Hsin-Min Wang; Chiou-Shann Fuh', display:{Lore:['[{"text": "arXiv:2309.12766", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Study on Incorporating Whisper for Robust Speech Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oRyandhimas E. Zezario\\nYu-Wen Chen\\nSzu-Wei Fu\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.12766\\u00a7r\\n\\nVersion:\\u00a77v4 (Mon, 29 Apr 2024 06:47:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEICME 2024\\u00a7r"}']}
{title:'Xie et al. (§72024§r)', author: 'Jiamin Xie; Ke Li; Jinxi Guo; Andros Tjandra; Yuan Shangguan; Leda Sari; Chunyang Wu; Junteng Jia; Jay Mahadeokar; Ozlem Kalinli', display:{Lore:['[{"text": "arXiv:2309.13018", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model\\u00a7r\\n\\n\\u00a78\\u00a7oJiamin Xie\\nKe Li\\nJinxi Guo\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.13018\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 11 Jan 2024 19:15:32 GMT)\\u00a7r"}']}
{title:'Zhou et al. (§72024§r)', author: 'Huali Zhou; Yueqian Lin; Yao Shi; Peng Sun; Ming Li', display:{Lore:['[{"text": "arXiv:2309.14089", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBiSinger: Bilingual Singing Voice Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oHuali Zhou\\nYueqian Lin\\nYao Shi\\nPeng Sun\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.14089\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 9 Jan 2024 07:04:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ASRU2023\\u00a7r"}']}
{title:'Panariello et al. (§72024§r)', author: 'Michele Panariello; Francesco Nespoli; Massimiliano Todisco; Nicholas Evans', display:{Lore:['[{"text": "arXiv:2309.14129", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker anonymization using neural audio codec language models\\u00a7r\\n\\n\\u00a78\\u00a7oMichele Panariello\\nFrancesco Nespoli\\nMassimiliano Todisco\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.14129\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 12 Jan 2024 15:05:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2024\\u00a7r"}']}
{title:'Kuan et al. (§72024§r)', author: 'Chun-Yi Kuan; Chen An Li; Tsu-Yuan Hsu; Tse-Yang Lin; Ho-Lam Chung; Kai-Wei Chang; Shuo-yiin Chang; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2309.14324", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards General-Purpose Text-Instruction-Guided Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oChun-Yi Kuan\\nChen An Li\\nTsu-Yuan Hsu\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.14324\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 Jan 2024 13:53:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ASRU 2023\\u00a7r"}']}
{title:'Subramani et al. (§72024§r)', author: 'Krishna Subramani; Jean-Marc Valin; Jan Buethe; Paris Smaragdis; Mike Goodwin', display:{Lore:['[{"text": "arXiv:2309.14507", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoise-Robust DSP-Assisted Neural Pitch Estimation with Very Low Complexity\\u00a7r\\n\\n\\u00a78\\u00a7oKrishna Subramani\\nJean-Marc Valin\\nJan Buethe\\nParis Smaragdis\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.14507\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 Jan 2024 05:10:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024, 5 pages\\u00a7r"}']}
{title:'Büthe et al. (§72024§r)', author: 'Jan Büthe; Ahmed Mustafa; Jean-Marc Valin; Karim Helwani; Michael M. Goodwin', display:{Lore:['[{"text": "arXiv:2309.14521", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoLACE: Improving Low-Complexity Speech Codec Enhancement Through Adaptive Temporal Shaping\\u00a7r\\n\\n\\u00a78\\u00a7oJan B\\u00fcthe\\nAhmed Mustafa\\nJean-Marc Valin\\nKarim Helwani\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.14521\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 12 Jan 2024 10:40:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ofinal version, accepted at ICASSP 2024\\u00a7r"}']}
{title:'Juvela et al. (§72024§r)', author: 'Lauri Juvela; Xin Wang', display:{Lore:['[{"text": "arXiv:2309.15224", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCollaborative Watermarking for Adversarial Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oLauri Juvela\\nXin Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.15224\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 2 Jan 2024 09:32:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Ning et al. (§72024§r)', author: 'Ziqian Ning; Yuepeng Jiang; Pengcheng Zhu; Shuai Wang; Jixun Yao; Lei Xie; Mengxiao Bi', display:{Lore:['[{"text": "arXiv:2309.15496", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDualVC 2: Dynamic Masked Convolution for Unified Streaming and Non-Streaming Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oZiqian Ning\\nYuepeng Jiang\\nPengcheng Zhu\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.15496\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 18 Jan 2024 09:59:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2024\\u00a7r"}']}
{title:'Cwitkowitz et al. (§72024§r)', author: 'Frank Cwitkowitz; Kin Wai Cheuk; Woosung Choi; Marco A. Martínez-Ramírez; Keisuke Toyama; Wei-Hsiang Liao; Yuki Mitsufuji', display:{Lore:['[{"text": "arXiv:2309.15717", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTimbre-Trap: A Low-Resource Framework for Instrument-Agnostic Music Transcription\\u00a7r\\n\\n\\u00a78\\u00a7oFrank Cwitkowitz\\nKin Wai Cheuk\\nWoosung Choi\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.15717\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 24 Jan 2024 13:43:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Higuchi et al. (§72024§r)', author: 'Takuya Higuchi; Avamarie Brueggeman; Masood Delfarah; Stephen Shum', display:{Lore:['[{"text": "arXiv:2309.16036", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultichannel Voice Trigger Detection Based on Transform-average-concatenate\\u00a7r\\n\\n\\u00a78\\u00a7oTakuya Higuchi\\nAvamarie Brueggeman\\nMasood Delfarah\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.16036\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 14 Feb 2024 00:28:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at HSCMA 2024\\u00a7r"}']}
{title:'Brueggeman et al. (§72024§r)', author: 'Avamarie Brueggeman; Takuya Higuchi; Masood Delfarah; Stephen Shum; Vineet Garg', display:{Lore:['[{"text": "arXiv:2309.16060", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDoes Single-channel Speech Enhancement Improve Keyword Spotting Accuracy? A Case Study\\u00a7r\\n\\n\\u00a78\\u00a7oAvamarie Brueggeman\\nTakuya Higuchi\\nMasood Delfarah\\nStephen Shum\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.16060\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 21 Feb 2024 23:57:50 GMT)\\u00a7r"}']}
{title:'von Neumann et al. (§72024§r)', author: 'Thilo von Neumann; Christoph Boeddeker; Tobias Cord-Landwehr; Marc Delcroix; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2309.16482", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMeeting Recognition with Continuous Speech Separation and Transcription-Supported Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oThilo von Neumann\\nChristoph Boeddeker\\nTobias Cord-Landwehr\\nMarc Delcroix\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.16482\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 6 May 2024 07:09:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at HSCMA Sattelite Workshop at ICASSP 2024\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Wangyou Zhang; Kohei Saijo; Zhong-Qiu Wang; Shinji Watanabe; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2309.17384", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lToward Universal Speech Enhancement for Diverse Input Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oWangyou Zhang\\nKohei Saijo\\nZhong-Qiu Wang\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2309.17384\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 16 Feb 2024 04:44:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 3 figures, 5 tables, published in ASRU 2023 (corrected the results of noisy speech on CHiME-4 (Simu) in Table 4)\\u00a7r"}']}
{title:'Cappellazzo et al. (§72024§r)', author: 'Umberto Cappellazzo; Enrico Fini; Muqiao Yang; Daniele Falavigna; Alessio Brutti; Bhiksha Raj', display:{Lore:['[{"text": "arXiv:2310.02699", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContinual Contrastive Spoken Language Understanding\\u00a7r\\n\\n\\u00a78\\u00a7oUmberto Cappellazzo\\nEnrico Fini\\nMuqiao Yang\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.02699\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 29 May 2024 15:43:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ACL Findings 2024\\u00a7r"}']}
{title:'Huang et al. (§72024§r)', author: 'Kuan-Po Huang; Chih-Kai Yang; Yu-Kuan Fu; Ewan Dunbar; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2310.03018", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero Resource Code-switched Speech Benchmark Using Speech Utterance Pairs For Multiple Spoken Languages\\u00a7r\\n\\n\\u00a78\\u00a7oKuan-Po Huang\\nChih-Kai Yang\\nYu-Kuan Fu\\nEwan Dunbar\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.03018\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 18 Mar 2024 07:57:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024 (v2)\\u00a7r"}']}
{title:'Ronchini et al. (§72024§r)', author: 'Francesca Ronchini; Romain Serizel', display:{Lore:['[{"text": "arXiv:2310.03455", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerformance and energy balance: a comprehensive study of state-of-the-art sound event detection systems\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesca Ronchini\\nRomain Serizel\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.03455\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 Jan 2024 21:35:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Dabike et al. (§72024§r)', author: 'Gerardo Roa Dabike; Michael A. Akeroyd; Scott Bannister; Jon Barker; Trevor J. Cox; Bruno Fazenda; Jennifer Firth; Simone Graetzer; Alinka Greasley; Rebecca R. Vos; William M. Whitmer', display:{Lore:['[{"text": "arXiv:2310.03480", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe ICASSP SP Cadenza Challenge: Music Demixing/Remixing for Hearing Aids\\u00a7r\\n\\n\\u00a78\\u00a7oGerardo Roa Dabike\\nMichael A. Akeroyd\\nScott Bannister\\n+ 7 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.03480\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 29 Jan 2024 11:48:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2-page paper for ICASSP 2024 SP Grand Challenge\\u00a7r"}']}
{title:'Bae et al. (§72024§r)', author: 'Jae-Sung Bae; Joun Yeop Lee; Ji-Hyun Lee; Seongkyu Mun; Taehwa Kang; Hoon-Young Cho; Chanwoo Kim', display:{Lore:['[{"text": "arXiv:2310.03538", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLatent Filling: Latent Space Data Augmentation for Zero-shot Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oJae-Sung Bae\\nJoun Yeop Lee\\nJi-Hyun Lee\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.03538\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 22 Jan 2024 12:21:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Mehta et al. (§72024§r)', author: 'Shivam Mehta; Ruibo Tu; Simon Alexanderson; Jonas Beskow; Éva Székely; Gustav Eje Henter', display:{Lore:['[{"text": "arXiv:2310.05181", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnified speech and gesture synthesis using flow matching\\u00a7r\\n\\n\\u00a78\\u00a7oShivam Mehta\\nRuibo Tu\\nSimon Alexanderson\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.05181\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 Jan 2024 21:23:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure. Final version, accepted to IEEE ICASSP 2024\\u00a7r"}']}
{title:'Ohlenbusch et al. (§72024§r)', author: 'Mattes Ohlenbusch; Christian Rollwage; Simon Doclo', display:{Lore:['[{"text": "arXiv:2310.06554", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModeling of Speech-dependent Own Voice Transfer Characteristics for Hearables with In-ear Microphones\\u00a7r\\n\\n\\u00a78\\u00a7oMattes Ohlenbusch\\nChristian Rollwage\\nSimon Doclo\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.06554\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 22 Mar 2024 14:27:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o20 pages, 11 figures; Extended version of arXiv:2309.08294 (moredetailed description of the problem, additional models considered, more systematic evaluation conducted on a different, larger dataset) -> Updated version "}','{"text": "(20th march 2024): major changes after internal review; in submission\\u00a7r"}']}
{title:'Manocha et al. (§72024§r)', author: 'Pranay Manocha; Donald Williamson; Adam Finkelstein', display:{Lore:['[{"text": "arXiv:2310.09388", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCORN: Co-Trained Full- And No-Reference Speech Quality Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oPranay Manocha\\nDonald Williamson\\nAdam Finkelstein\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.09388\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 8 Jan 2024 21:59:52 GMT)\\u00a7r"}']}
{title:'Cho et al. (§72024§r)', author: 'Cheol Jun Cho; Abdelrahman Mohamed; Alan W Black; Gopala K. Anumanchipalli', display:{Lore:['[{"text": "arXiv:2310.10788", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-Supervised Models of Speech Infer Universal Articulatory Kinematics\\u00a7r\\n\\n\\u00a78\\u00a7oCheol Jun Cho\\nAbdelrahman Mohamed\\nAlan W Black\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.10788\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 Jan 2024 08:09:15 GMT)\\u00a7r"}']}
{title:'Yao et al. (§72024§r)', author: 'Zengwei Yao; Liyong Guo; Xiaoyu Yang; Wei Kang; Fangjun Kuang; Yifan Yang; Zengrui Jin; Long Lin; Daniel Povey', display:{Lore:['[{"text": "arXiv:2310.11230", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZipformer: A faster and better encoder for automatic speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZengwei Yao\\nLiyong Guo\\nXiaoyu Yang\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.11230\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 10 Apr 2024 02:35:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished as a conference paper at ICLR 2024\\u00a7r"}']}
{title:'Huang et al. (§72024§r)', author: 'Weiming Huang; Qinghua Huang; Liyan Ma; Chuan Wang', display:{Lore:['[{"text": "arXiv:2310.14016", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSwG-former: A Sliding-Window Graph Convolutional Network for Simultaneous Spatial-Temporal Information Extraction in Sound Event Localization and Detection\\u00a7r\\n\\n\\u00a78\\u00a7oWeiming Huang\\nQinghua Huang\\nLiyan Ma\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.14016\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 20 Mar 2024 07:53:04 GMT)\\u00a7r"}']}
{title:'Jiang et al. (§72024§r)', author: 'Yidi Jiang; Zhengyang Chen; Ruijie Tao; Liqun Deng; Yanmin Qian; Haizhou Li', display:{Lore:['[{"text": "arXiv:2310.14823", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPrompt-driven Target Speech Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oYidi Jiang\\nZhengyang Chen\\nRuijie Tao\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.14823\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 8 Jan 2024 14:13:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Yamamoto et al. (§72024§r)', author: 'Ayako Yamamoto; Toshio Irino; Fuki Miyazaki; Honoka Tamaru', display:{Lore:['[{"text": "arXiv:2310.15399", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGESI: Gammachirp Envelope Similarity Index for Predicting Intelligibility of Simulated Hearing Loss Sounds\\u00a7r\\n\\n\\u00a78\\u00a7oAyako Yamamoto\\nToshio Irino\\nFuki Miyazaki\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.15399\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 14 Mar 2024 02:14:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis paper was submitted to JASA on March 14, 2024\\u00a7r"}']}
{title:'Garoufis et al. (§72024§r)', author: 'Christos Garoufis; Athanasia Zlatintsi; Petros Maragos', display:{Lore:['[{"text": "arXiv:2310.15845", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPre-training Music Classification Models via Music Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oChristos Garoufis\\nAthanasia Zlatintsi\\nPetros Maragos\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.15845\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 23 Apr 2024 13:08:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures/2 tables. EUSIPCO-24 submission (changes from v1: experiments with AST - classificationbackbone pre-training - correct FMA subset)\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Alexander H. Liu; Matt Le; Apoorv Vyas; Bowen Shi; Andros Tjandra; Wei-Ning Hsu', display:{Lore:['[{"text": "arXiv:2310.16338", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGenerative Pre-training for Speech with Flow Matching\\u00a7r\\n\\n\\u00a78\\u00a7oAlexander H. Liu\\nMatt Le\\nApoorv Vyas\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.16338\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 25 Mar 2024 18:18:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICLR 2024\\u00a7r"}']}
{title:'Zhu et al. (§72024§r)', author: 'Xinfa Zhu; Yuke Li; Yi Lei; Ning Jiang; Guoqing Zhao; Lei Xie', display:{Lore:['[{"text": "arXiv:2310.17101", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBoosting Multi-Speaker Expressive Speech Synthesis with Semi-supervised Contrastive Learning\\u00a7r\\n\\n\\u00a78\\u00a7oXinfa Zhu\\nYuke Li\\nYi Lei\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2310.17101\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 Apr 2024 14:41:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 4 figures; Accepted by ICME 2024\\u00a7r"}']}
{title:'Gui et al. (§72024§r)', author: 'Azalea Gui; Hannes Gamper; Sebastian Braun; Dimitra Emmanouilidou', display:{Lore:['[{"text": "arXiv:2311.01616", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdapting Frechet Audio Distance for Generative Music Evaluation\\u00a7r\\n\\n\\u00a78\\u00a7oAzalea Gui\\nHannes Gamper\\nSebastian Braun\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.01616\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 5 Mar 2024 22:14:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEEICASSP 2024\\u00a7r"}']}
{title:'Xu et al. (§72024§r)', author: 'Xinmeng Xu; Yuhong Yang; Weiping Tu', display:{Lore:['[{"text": "arXiv:2311.01679", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSE Territory: Monaural Speech Enhancement Meets the Fixed Virtual Perceptual Space Mapping\\u00a7r\\n\\n\\u00a78\\u00a7oXinmeng Xu\\nYuhong Yang\\nWeiping Tu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.01679\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 4 Mar 2024 04:05:52 GMT)\\u00a7r"}']}
{title:'Melechovsky et al. (§72024§r)', author: 'Jan Melechovsky; Zixun Guo; Deepanway Ghosal; Navonil Majumder; Dorien Herremans; Soujanya Poria', display:{Lore:['[{"text": "arXiv:2311.08355", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMustango: Toward Controllable Text-to-Music Generation\\u00a7r\\n\\n\\u00a78\\u00a7oJan Melechovsky\\nZixun Guo\\nDeepanway Ghosal\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.08355\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 16 Mar 2024 01:58:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oNAACL 2024\\u00a7r"}']}
{title:'Feng et al. (§72024§r)', author: 'Linfeng Feng; Xiao-Lei Zhang; Xuelong Li', display:{Lore:['[{"text": "arXiv:2311.12305", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEliminating Quantization Errors in Classification-Based Sound Source Localization\\u00a7r\\n\\n\\u00a78\\u00a7oLinfeng Feng\\nXiao-Lei Zhang\\nXuelong Li\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.12305\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 28 Jan 2024 14:46:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o12 pages\\u00a7r"}']}
{title:'Bai et al. (§72024§r)', author: 'Jisheng Bai; Han Yin; Mou Wang; Dongyuan Shi; Woon-Seng Gan; Jianfeng Chen; Susanto Rahardja', display:{Lore:['[{"text": "arXiv:2311.12371", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudioLog: LLMs-Powered Long Audio Logging with Hybrid Token-Semantic Contrastive Learning\\u00a7r\\n\\n\\u00a78\\u00a7oJisheng Bai\\nHan Yin\\nMou Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.12371\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 4 Jan 2024 16:04:03 GMT)\\u00a7r"}']}
{title:'Baghel et al. (§72024§r)', author: 'Shikha Baghel; Shreyas Ramoji; Somil Jain; Pratik Roy Chowdhuri; Prachi Singh; Deepu Vijayasenan; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:2311.12564", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSummary of the DISPLACE Challenge 2023 - DIarization of SPeaker and LAnguage in Conversational Environments\\u00a7r\\n\\n\\u00a78\\u00a7oShikha Baghel\\nShreyas Ramoji\\nSomil Jain\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.12564\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 3 Jan 2024 05:57:32 GMT)\\u00a7r"}']}
{title:'Zhou et al. (§72024§r)', author: 'Enting Zhou; You Zhang; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2311.14816", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Arousal-Valence Representation from Categorical Emotion Labels of Speech\\u00a7r\\n\\n\\u00a78\\u00a7oEnting Zhou\\nYou Zhang\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.14816\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 6 Feb 2024 06:21:29 GMT)\\u00a7r"}']}
{title:'Benazir et al. (§72024§r)', author: 'Afsara Benazir; Zhiming Xu; Felix Xiaozhu Lin', display:{Lore:['[{"text": "arXiv:2311.18188", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Understanding on Tiny Devices with A Learning Cache\\u00a7r\\n\\n\\u00a78\\u00a7oAfsara Benazir\\nZhiming Xu\\nFelix Xiaozhu Lin\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2311.18188\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 8 May 2024 17:08:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at MobiSys\'24\\u00a7r"}']}
{title:'Gonzalez et al. (§72024§r)', author: 'Philippe Gonzalez; Zheng-Hua Tan; Jan Østergaard; Jesper Jensen; Tommy Sonne Alstrøm; Tobias May', display:{Lore:['[{"text": "arXiv:2312.02683", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffusion-Based Speech Enhancement in Matched and Mismatched Conditions Using a Heun-Based Sampler\\u00a7r\\n\\n\\u00a78\\u00a7oPhilippe Gonzalez\\nZheng-Hua Tan\\nJan \\u00d8stergaard\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.02683\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 Jan 2024 16:17:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Tianchi Liu; Kong Aik Lee; Qiongqiong Wang; Haizhou Li', display:{Lore:['[{"text": "arXiv:2312.03620", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGolden Gemini is All You Need: Finding the Sweet Spots for Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oTianchi Liu\\nKong Aik Lee\\nQiongqiong Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.03620\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 24 Apr 2024 17:29:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEE/ACM Transactionson Audio, Speech, and Language Processing. Open Access: https://ieeexplore.ieee.org/abstract/document/10497864\\u00a7r"}']}
{title:'Cappellazzo et al. (§72024§r)', author: 'Umberto Cappellazzo; Daniele Falavigna; Alessio Brutti; Mirco Ravanelli', display:{Lore:['[{"text": "arXiv:2312.03694", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParameter-Efficient Transfer Learning of Audio Spectrogram Transformers\\u00a7r\\n\\n\\u00a78\\u00a7oUmberto Cappellazzo\\nDaniele Falavigna\\nAlessio Brutti\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.03694\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 11 Jan 2024 10:53:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe code is available at: https://github.com/umbertocappellazzo/PETL_AST\\u00a7r"}']}
{title:'Guo et al. (§72024§r)', author: 'Yinlin Guo; Haofan Huang; Xi Chen; He Zhao; Yuehai Wang', display:{Lore:['[{"text": "arXiv:2312.08089", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio Deepfake Detection with Self-Supervised WavLM and Multi-Fusion Attentive Classifier\\u00a7r\\n\\n\\u00a78\\u00a7oYinlin Guo\\nHaofan Huang\\nXi Chen\\nHe Zhao\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.08089\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 10 Jan 2024 03:01:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024. 5 pages, 1 figure\\u00a7r"}']}
{title:'Ding et al. (§72024§r)', author: 'Shaojin Ding; David Qiu; David Rim; Yanzhang He; Oleg Rybakov; Bo Li; Rohit Prabhavalkar; Weiran Wang; Tara N. Sainath; Zhonglin Han; Jian Li; Amir Yazdanbakhsh; Shivani Agrawal', display:{Lore:['[{"text": "arXiv:2312.08553", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUSM-Lite: Quantization and Sparsity Aware Fine-tuning for Speech Recognition with Universal Speech Models\\u00a7r\\n\\n\\u00a78\\u00a7oShaojin Ding\\nDavid Qiu\\nDavid Rim\\n+ 9 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.08553\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 16 Jan 2024 05:13:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024. Preprint\\u00a7r"}']}
{title:'Miotello et al. (§72024§r)', author: 'Federico Miotello; Luca Comanducci; Mirco Pezzoli; Alberto Bernardini; Fabio Antonacci; Augusto Sarti', display:{Lore:['[{"text": "arXiv:2312.08821", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReconstruction of Sound Field through Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oFederico Miotello\\nLuca Comanducci\\nMirco Pezzoli\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.08821\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 21 Feb 2024 16:15:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at ICASSP 2024\\u00a7r"}']}
{title:'Aditya et al. (§72024§r)', author: 'Bobbi Aditya; Mahdin Rohmatillah; Liang-Hsuan Tai; Jen-Tzung Chien', display:{Lore:['[{"text": "arXiv:2312.08856", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention-Guided Adaptation for Code-Switching Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oBobbi Aditya\\nMahdin Rohmatillah\\nLiang-Hsuan Tai\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.08856\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 12 Jan 2024 07:55:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Mezza et al. (§72024§r)', author: 'Alessandro Ilic Mezza; Riccardo Giampiccolo; Alberto Bernardini; Augusto Sarti', display:{Lore:['[{"text": "arXiv:2312.09663", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lToward Deep Drum Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oAlessandro Ilic Mezza\\nRiccardo Giampiccolo\\nAlberto Bernardini\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.09663\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.patrec.2024.04.026\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nPattern Recognit. Lett. 183 (2024) 86-91\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 20 May 2024 07:30:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 2 figures, 3 tables. Published in Pattern Recognition Letters, vol. 183, pp. 86-91, 2024\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Ziqian Wang; Xinfa Zhu; Zihan Zhang; YuanJun Lv; Ning Jiang; Guoqing Zhao; Lei Xie', display:{Lore:['[{"text": "arXiv:2312.09747", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSELM: Speech Enhancement Using Discrete Tokens and Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oZiqian Wang\\nXinfa Zhu\\nZihan Zhang\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.09747\\u00a7r\\n\\nVersion:\\u00a77v2 (Sun, 7 Jan 2024 09:02:52 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Guan et al. (§72024§r)', author: 'Wenhao Guan; Yishuang Li; Tao Li; Hukai Huang; Feng Wang; Jiayan Lin; Lingyan Huang; Lin Li; Qingyang Hong', display:{Lore:['[{"text": "arXiv:2312.10687", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMM-TTS: Multi-modal Prompt based Style Transfer for Expressive Text-to-Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oWenhao Guan\\nYishuang Li\\nTao Li\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.10687\\u00a7r\\n\\nVersion:\\u00a77v4 (Wed, 31 Jan 2024 09:53:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at AAAI2024\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Yu Zhang; Rongjie Huang; Ruiqi Li; JinZheng He; Yan Xia; Feiyang Chen; Xinyu Duan; Baoxing Huai; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2312.10741", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStyleSinger: Style Transfer for Out-of-Domain Singing Voice Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oYu Zhang\\nRongjie Huang\\nRuiqi Li\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.10741\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 2 Jan 2024 12:59:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by AAAI 2024\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Hongyu Wang; Hui Li; Bo Li', display:{Lore:['[{"text": "arXiv:2312.16826", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVOT: Revolutionizing Speaker Verification with Memory and Attention Mechanisms\\u00a7r\\n\\n\\u00a78\\u00a7oHongyu Wang\\nHui Li\\nBo Li\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2312.16826\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 19 Jan 2024 16:17:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages,4 figures,6 tables\\u00a7r"}']}
{title:'Zotter (§72024§r)', author: 'Franz Zotter', display:{Lore:['[{"text": "arXiv:2401.00813", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUltraspherical/Gegenbauer polynomials to unify 2D/3D Ambisonic directivity designs\\u00a7r\\n\\n\\u00a78\\u00a7oFranz Zotter\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.00813\\u00a7r\\n\\nVersion:\\u00a77v5 (Sat, 4 May 2024 23:08:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o56 pages, 9 figures\\u00a7r"}']}
{title:'Miller et al. (§72024§r)', author: 'Eran Miller; Boaz Rafaely', display:{Lore:['[{"text": "arXiv:2401.00936", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe role of direct sound spherical harmonics representation in externalization using binaural reproduction\\u00a7r\\n\\n\\u00a78\\u00a7oEran Miller\\nBoaz Rafaely\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.00936\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.apacoust.2018.12.011\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nApplied Acoustics, Volume 148, 2019, Pages 40-45\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Jan 2024 19:01:10 GMT)\\u00a7r"}']}
{title:'Jeong et al. (§72024§r)', author: 'Myeonghun Jeong; Minchan Kim; Joun Yeop Lee; Nam Soo Kim', display:{Lore:['[{"text": "arXiv:2401.01099", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Parallel Audio Generation using Group Masked Language Modeling\\u00a7r\\n\\n\\u00a78\\u00a7oMyeonghun Jeong\\nMinchan Kim\\nJoun Yeop Lee\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.01099\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Jan 2024 08:42:48 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\\u00a7r"}']}
{title:'Wisnu et al. (§72024§r)', author: 'Dyah A. M. G. Wisnu; Epri W. Pratiwi; Stefano Rini; Ryandhimas E. Zezario; Hsin-Min Wang; Yu Tsao', display:{Lore:['[{"text": "arXiv:2401.01145", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHAAQI-Net: A non-intrusive neural music quality assessment model for hearing aids\\u00a7r\\n\\n\\u00a78\\u00a7oDyah A. M. G. Wisnu\\nEpri W. Pratiwi\\nStefano Rini\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.01145\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 31 Jan 2024 05:50:05 GMT)\\u00a7r"}']}
{title:'Karakonstantis et al. (§72024§r)', author: 'Xenofon Karakonstantis; Diego Caviedes-Nozal; Antoine Richard; Efren Fernandez-Grande', display:{Lore:['[{"text": "arXiv:2401.01206", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRoom impulse response reconstruction with physics-informed deep learning\\u00a7r\\n\\n\\u00a78\\u00a7oXenofon Karakonstantis\\nDiego Caviedes-Nozal\\nAntoine Richard\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.01206\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Jan 2024 13:26:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Journal of Acoustical Society of America (JASA)\\u00a7r"}']}
{title:'Kafentzis (§72024§r)', author: 'George P. Kafentzis', display:{Lore:['[{"text": "arXiv:2401.01255", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Parameter Estimation of Sinusoidal Models for Speech and Audio Signals\\u00a7r\\n\\n\\u00a78\\u00a7oGeorge P. Kafentzis\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.01255\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Jan 2024 15:46:19 GMT)\\u00a7r"}']}
{title:'Cai et al. (§72024§r)', author: 'Danwei Cai; Zexin Cai; Ming Li', display:{Lore:['[{"text": "arXiv:2401.01473", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSelf-supervised Reflective Learning through Self-distillation and Online Clustering for Speaker Representation Learning\\u00a7r\\n\\n\\u00a78\\u00a7oDanwei Cai\\nZexin Cai\\nMing Li\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.01473\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Jan 2024 00:17:39 GMT)\\u00a7r"}']}
{title:'Kim et al. (§72024§r)', author: 'Minchan Kim; Myeonghun Jeong; Byoung Jin Choi; Semin Kim; Joun Yeop Lee; Nam Soo Kim', display:{Lore:['[{"text": "arXiv:2401.01498", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUtilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oMinchan Kim\\nMyeonghun Jeong\\nByoung Jin Choi\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.01498\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Jan 2024 02:03:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\\u00a7r"}']}
{title:'Lu et al. (§72024§r)', author: 'Yiwen Lu; Zhen Ye; Wei Xue; Xu Tan; Qifeng Liu; Yike Guo', display:{Lore:['[{"text": "arXiv:2401.01792", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCoMoSVC: Consistency Model-based Singing Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oYiwen Lu\\nZhen Ye\\nWei Xue\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.01792\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Jan 2024 15:47:17 GMT)\\u00a7r"}']}
{title:'Hou et al. (§72024§r)', author: 'Junfeng Hou; Peiyao Wang; Jincheng Zhang; Meng Yang; Minwei Feng; Jingcheng Yin', display:{Lore:['[{"text": "arXiv:2401.02046", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCTC Blank Triggered Dynamic Layer-Skipping for Efficient CTC-based Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJunfeng Hou\\nPeiyao Wang\\nJincheng Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.02046\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Jan 2024 03:25:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by ASRU 2023\\u00a7r"}']}
{title:'Millot et al. (§72024§r)', author: 'Laurent Millot; Antoine Valette; Manuel Lopes; Gérard Pelé; Mohammed Elliq; Dominique Lambert', display:{Lore:['[{"text": "arXiv:2401.02164", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lListening broadband physical model for microphones: a first step\\u00a7r\\n\\n\\u00a78\\u00a7oLaurent Millot\\nAntoine Valette\\nManuel Lopes\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.02164\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n120th Convention of the Audio Engineering Society, Audio\\n  Engineering Society, May 2006, Paris, France\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Jan 2024 09:22:14 GMT)\\u00a7r"}']}
{title:'Tourbabin et al. (§72024§r)', author: 'V. Tourbabin; M. Agmon; B. Rafaely; J. Tabrikian', display:{Lore:['[{"text": "arXiv:2401.02285", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOptimal Real-Weighted Beamforming With Application to Linear and Spherical Arrays\\u00a7r\\n\\n\\u00a78\\u00a7oV. Tourbabin\\nM. Agmon\\nB. Rafaely\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.02285\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASL.2012.2208626\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nn IEEE Transactions on Audio, Speech, and Language Processing,\\n  vol. 20, no. 9, pp. 2575-2585, Nov. 2012\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Jan 2024 14:06:43 GMT)\\u00a7r"}']}
{title:'Tourbabin et al. (§72024§r)', author: 'Vladimir Tourbabin; Boaz Rafaely', display:{Lore:['[{"text": "arXiv:2401.02386", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDirection of Arrival Estimation Using Microphone Array Processing for Moving Humanoid Robots\\u00a7r\\n\\n\\u00a78\\u00a7oVladimir Tourbabin\\nBoaz Rafaely\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.02386\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2015.2464671\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nin IEEE/ACM Transactions on Audio, Speech, and Language\\n  Processing, vol. 23, no. 11, pp. 2046-2058, Nov. 2015\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Jan 2024 17:55:17 GMT)\\u00a7r"}']}
{title:'Chan et al. (§72024§r)', author: 'David M. Chan; Shalini Ghosh; Hitesh Tulsiani; Ariya Rastrow; Björn Hoffmeister', display:{Lore:['[{"text": "arXiv:2401.02417", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTask Oriented Dialogue as a Catalyst for Self-Supervised Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oDavid M. Chan\\nShalini Ghosh\\nHitesh Tulsiani\\nAriya Rastrow\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.02417\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Jan 2024 18:59:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP 2024\\u00a7r"}']}
{title:'Millot (§72024§r)', author: 'Laurent Millot', display:{Lore:['[{"text": "arXiv:2401.02463", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSome clues to build a sound analysis relevant to hearing\\u00a7r\\n\\n\\u00a78\\u00a7oLaurent Millot\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.02463\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n116th Convention of the Audio Engineering Society,, Audio\\n  Engineering Society, May 2004, Berlin (Germany), Germany\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 4 Jan 2024 09:02:59 GMT)\\u00a7r"}']}
{title:'Zhao et al. (§72024§r)', author: 'Dongdi Zhao; Jianbo Ma; Lu Lu; Jinke Li; Xuan Ji; Lei Zhu; Fuming Fang; Ming Liu; Feijun Jiang', display:{Lore:['[{"text": "arXiv:2401.02673", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA unified multichannel far-field speech recognition system: combining neural beamforming with attention based end-to-end model\\u00a7r\\n\\n\\u00a78\\u00a7oDongdi Zhao\\nJianbo Ma\\nLu Lu\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.02673\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Jan 2024 07:11:13 GMT)\\u00a7r"}']}
{title:'Budzianowski et al. (§72024§r)', author: 'Paweł Budzianowski; Taras Sereda; Tomasz Cichy; Ivan Vulić', display:{Lore:['[{"text": "arXiv:2401.02839", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPheme: Efficient and Conversational Speech Generation\\u00a7r\\n\\n\\u00a78\\u00a7oPawe\\u0142 Budzianowski\\nTaras Sereda\\nTomasz Cichy\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.02839\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Jan 2024 14:47:20 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72024§r)', author: 'Yang Yang; Yury Kartynnik; Yunpeng Li; Jiuqiang Tang; Xing Li; George Sung; Matthias Grundmann', display:{Lore:['[{"text": "arXiv:2401.03078", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreamVC: Real-Time Low-Latency Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oYang Yang\\nYury Kartynnik\\nYunpeng Li\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03078\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Jan 2024 22:37:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Ravi et al. (§72024§r)', author: 'Nagarathna Ravi; Thishyan Raj T; Vipul Arora', display:{Lore:['[{"text": "arXiv:2401.03251", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTeLeS: Temporal Lexeme Similarity Score to Estimate Confidence in End-to-End ASR\\u00a7r\\n\\n\\u00a78\\u00a7oNagarathna Ravi\\nThishyan Raj T\\nVipul Arora\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03251\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 6 Jan 2024 16:29:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEE/ACM Transactionson Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Tourbabin et al. (§72024§r)', author: 'Vladimir Tourbabin; Boaz Rafaely', display:{Lore:['[{"text": "arXiv:2401.03286", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.RO\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTheoretical Framework for the Optimization of Microphone Array Configuration for Humanoid Robot Audition\\u00a7r\\n\\n\\u00a78\\u00a7oVladimir Tourbabin\\nBoaz Rafaely\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03286\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2014.2351133\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nin IEEE/ACM Transactions on Audio, Speech, and Language\\n  Processing, vol. 22, no. 12, 1803-1814, 2014\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 6 Jan 2024 19:21:44 GMT)\\u00a7r"}']}
{title:'Morgenstern et al. (§72024§r)', author: 'Hai Morgenstern; Boaz Rafaely; Markus Noisternig', display:{Lore:['[{"text": "arXiv:2401.03291", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDesign framework for spherical microphone and loudspeaker arrays in a multiple-input multiple-output system\\u00a7r\\n\\n\\u00a78\\u00a7oHai Morgenstern\\nBoaz Rafaely\\nMarkus Noisternig\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03291\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/1.4978660\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJ. Acoust. Soc. Am. 2017, vol 141, no 3, 2024-2038\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 6 Jan 2024 20:08:56 GMT)\\u00a7r"}']}
{title:'Morgenstern et al. (§72024§r)', author: 'Hai Morgenstern; Boaz Rafaely', display:{Lore:['[{"text": "arXiv:2401.03441", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatial Reverberation and Dereverberation using an Acoustic Multiple-Input Multiple-Output System\\u00a7r\\n\\n\\u00a78\\u00a7oHai Morgenstern\\nBoaz Rafaely\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03441\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.17743/jaes.2016.0063\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJ. Audio Eng. Soc, vol. 65, no. 1/2, pp. 42-55, 2017\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 7 Jan 2024 09:59:11 GMT)\\u00a7r"}']}
{title:'Opochinsky et al. (§72024§r)', author: 'Renana Opochinsky; Mordehay Moradi; Sharon Gannot', display:{Lore:['[{"text": "arXiv:2401.03448", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSingle-Microphone Speaker Separation and Voice Activity Detection in Noisy and Reverberant Environments\\u00a7r\\n\\n\\u00a78\\u00a7oRenana Opochinsky\\nMordehay Moradi\\nSharon Gannot\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03448\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 7 Jan 2024 10:46:17 GMT)\\u00a7r"}']}
{title:'Morgenstern et al. (§72024§r)', author: 'Hai Morgenstern; Boaz Rafaely', display:{Lore:['[{"text": "arXiv:2401.03458", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lModal smoothing for analysis of room reflections measured with spherical microphone and loudspeaker arrays\\u00a7r\\n\\n\\u00a78\\u00a7oHai Morgenstern\\nBoaz Rafaely\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03458\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/1.5024234\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJ. Acoust. Soc. Am., vol. 143, no. 2, pp. 1008-1018, 2018\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 7 Jan 2024 11:38:02 GMT)\\u00a7r"}']}
{title:'Zhu et al. (§72024§r)', author: 'Qiushi Zhu; Jie Zhang; Yu Gu; Yuchen Hu; Lirong Dai', display:{Lore:['[{"text": "arXiv:2401.03468", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultichannel AV-wav2vec2: A Framework for Learning Multichannel Multi-Modal Speech Representation\\u00a7r\\n\\n\\u00a78\\u00a7oQiushi Zhu\\nJie Zhang\\nYu Gu\\nYuchen Hu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03468\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 7 Jan 2024 12:27:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by AAAI 2024\\u00a7r"}']}
{title:'Morgenstern et al. (§72024§r)', author: 'Hai Morgenstern; Boaz Rafaely; Franz Zotter', display:{Lore:['[{"text": "arXiv:2401.03493", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTheory and investigation of acoustic multiple-input multiple-output systems based on spherical arrays in a room\\u00a7r\\n\\n\\u00a78\\u00a7oHai Morgenstern\\nBoaz Rafaely\\nFranz Zotter\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03493\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1121/1.4934555\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nJ. Acoust. Soc. Am., vol. 138, no. 5, pp. 2998-3009, November 2015\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 7 Jan 2024 14:15:07 GMT)\\u00a7r"}']}
{title:'Chen et al. (§72024§r)', author: 'Wenxi Chen; Yuzhe Liang; Ziyang Ma; Zhisheng Zheng; Xie Chen', display:{Lore:['[{"text": "arXiv:2401.03497", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEAT: Self-Supervised Pre-Training with Efficient Audio Transformer\\u00a7r\\n\\n\\u00a78\\u00a7oWenxi Chen\\nYuzhe Liang\\nZiyang Ma\\nZhisheng Zheng\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03497\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 7 Jan 2024 14:31:27 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Quan Wang; Yiling Huang; Guanlong Zhao; Evan Clark; Wei Xia; Hank Liao', display:{Lore:['[{"text": "arXiv:2401.03506", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiarizationLM: Speaker Diarization Post-Processing with Large Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oQuan Wang\\nYiling Huang\\nGuanlong Zhao\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03506\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 6 Feb 2024 22:38:24 GMT)\\u00a7r"}']}
{title:'Petermann et al. (§72024§r)', author: 'Darius Petermann; Minje Kim', display:{Lore:['[{"text": "arXiv:2401.03567", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHyperbolic Distance-Based Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oDarius Petermann\\nMinje Kim\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03567\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 7 Jan 2024 18:26:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo be published at ICASSP2024, 14th of April 2024, Seoul, South Korea. Copyright (c) 2023 IEEE. 5 pages, 2 figures, 3 tables\\u00a7r"}']}
{title:'Yi et al. (§72024§r)', author: 'Jayeon Yi; Junghyun Koo; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2401.03650", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDDD: A Perceptually Superior Low-Response-Time DNN-based Declipper\\u00a7r\\n\\n\\u00a78\\u00a7oJayeon Yi\\nJunghyun Koo\\nKyogu Lee\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03650\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Jan 2024 03:43:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear, ICASSP 2024. Demo samples at https://stet-stet.github.io/DDD, repo at https://github.com/stet-stet/DDD\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Zihan Zhang; Jiayao Sun; Xianjun Xia; Chuanzeng Huang; Yijian Xiao; Lei Xie', display:{Lore:['[{"text": "arXiv:2401.03687", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBS-PLCNet: Band-split Packet Loss Concealment Network with Multi-task Learning Framework and Multi-discriminators\\u00a7r\\n\\n\\u00a78\\u00a7oZihan Zhang\\nJiayao Sun\\nXianjun Xia\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03687\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Jan 2024 06:34:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Wei Liu; Jingyong Hou; Dong Yang; Muyong Cao; Tan Lee', display:{Lore:['[{"text": "arXiv:2401.03689", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLUPET: Incorporating Hierarchical Information Path into Multilingual ASR\\u00a7r\\n\\n\\u00a78\\u00a7oWei Liu\\nJingyong Hou\\nDong Yang\\nMuyong Cao\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03689\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Jan 2024 06:38:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInitial paper draft\\u00a7r"}']}
{title:'Tian et al. (§72024§r)', author: 'Yusheng Tian; Jingyu Li; Tan Lee', display:{Lore:['[{"text": "arXiv:2401.03816", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCreating Personalized Synthetic Voices from Articulation Impaired Speech Using Augmented Reconstruction Loss\\u00a7r\\n\\n\\u00a78\\u00a7oYusheng Tian\\nJingyu Li\\nTan Lee\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03816\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Jan 2024 11:10:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Lee et al. (§72024§r)', author: 'Jin Woo Lee; Gwang Seok An; Jeong-Yun Sun; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2401.03850", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInverse Nonlinearity Compensation of Hyperelastic Deformation in Dielectric Elastomer for Acoustic Actuation\\u00a7r\\n\\n\\u00a78\\u00a7oJin Woo Lee\\nGwang Seok An\\nJeong-Yun Sun\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03850\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Jan 2024 12:28:44 GMT)\\u00a7r"}']}
{title:'Williams et al. (§72024§r)', author: 'Jennifer Williams; Karla Pizzi; Paul-Gauthier Noe; Sneha Das', display:{Lore:['[{"text": "arXiv:2401.03936", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploratory Evaluation of Speech Content Masking\\u00a7r\\n\\n\\u00a78\\u00a7oJennifer Williams\\nKarla Pizzi\\nPaul-Gauthier Noe\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03936\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Jan 2024 14:56:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ITG Speech Conference 2023\\u00a7r"}']}
{title:'Cord-Landwehr et al. (§72024§r)', author: 'Tobias Cord-Landwehr; Christoph Boeddeker; Cătălin Zorilă; Rama Doddipatla; Reinhold Haeb-Umbach', display:{Lore:['[{"text": "arXiv:2401.03963", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGeodesic interpolation of frame-wise speaker embeddings for the diarization of meeting scenarios\\u00a7r\\n\\n\\u00a78\\u00a7oTobias Cord-Landwehr\\nChristoph Boeddeker\\nC\\u0103t\\u0103lin Zoril\\u0103\\nRama Doddipatla\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.03963\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Jan 2024 15:34:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2024\\u00a7r"}']}
{title:'Millot et al. (§72024§r)', author: 'Laurent Millot; Gérard Pelé; Mohammed Elliq', display:{Lore:['[{"text": "arXiv:2401.04127", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a75physics.class-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUsing perceptive subbands analysis to perform audio scenes cartography\\u00a7r\\n\\n\\u00a78\\u00a7oLaurent Millot\\nG\\u00e9rard Pel\\u00e9\\nMohammed Elliq\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.04127\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n118th Convention of the Audio Engineering Society, Audio\\n  Engineering Society, May 2005, Barcelone (Espagne), Spain\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Jan 2024 10:03:47 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Yang Liu; Li Wan; Yun Li; Yiteng Huang; Ming Sun; James Luan; Yangyang Shi; Xin Lei', display:{Lore:['[{"text": "arXiv:2401.04283", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFADI-AEC: Fast Score Based Diffusion Model Guided by Far-end Signal for Acoustic Echo Cancellation\\u00a7r\\n\\n\\u00a78\\u00a7oYang Liu\\nLi Wan\\nYun Li\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.04283\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Jan 2024 23:38:04 GMT)\\u00a7r"}']}
{title:'Mulimani et al. (§72024§r)', author: 'Manjunath Mulimani; Annamaria Mesaros', display:{Lore:['[{"text": "arXiv:2401.04447", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lClass-Incremental Learning for Multi-Label Audio Classification\\u00a7r\\n\\n\\u00a78\\u00a7oManjunath Mulimani\\nAnnamaria Mesaros\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.04447\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Jan 2024 09:25:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Dutta et al. (§72024§r)', author: 'Soumya Dutta; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:2401.04511", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lZero Shot Audio to Audio Emotion Transfer With Speaker Disentanglement\\u00a7r\\n\\n\\u00a78\\u00a7oSoumya Dutta\\nSriram Ganapathy\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.04511\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Jan 2024 12:10:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, accepted at ICASSP 2024\\u00a7r"}']}
{title:'Yue et al. (§72024§r)', author: 'Haobo Yue; Zhicheng Zhang; Da Mu; Yonghao Dang; Jianqin Yin; Jin Tang', display:{Lore:['[{"text": "arXiv:2401.04976", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFull-frequency dynamic convolution: a physical frequency-dependent convolution for sound event detection\\u00a7r\\n\\n\\u00a78\\u00a7oHaobo Yue\\nZhicheng Zhang\\nDa Mu\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.04976\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Jan 2024 07:43:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 4 figures, submitted to ICME2024\\u00a7r"}']}
{title:'Thornton et al. (§72024§r)', author: 'Mike Thornton; Danilo Mandic; Tobias Reichenbach', display:{Lore:['[{"text": "arXiv:2401.05187", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDecoding of Selective Attention to Speech From Ear-EEG Recordings\\u00a7r\\n\\n\\u00a78\\u00a7oMike Thornton\\nDanilo Mandic\\nTobias Reichenbach\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.05187\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Jan 2024 14:35:14 GMT)\\u00a7r"}']}
{title:'Cai et al. (§72024§r)', author: 'Kevin Cai; Chonghua Liu; David M. Chan', display:{Lore:['[{"text": "arXiv:2401.05314", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lANIM-400K: A Large-Scale Dataset for Automated End-To-End Dubbing of Video\\u00a7r\\n\\n\\u00a78\\u00a7oKevin Cai\\nChonghua Liu\\nDavid M. Chan\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.05314\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Jan 2024 18:32:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP 2024\\u00a7r"}']}
{title:'Salvi (§72024§r)', author: 'Giampiero Salvi', display:{Lore:['[{"text": "arXiv:2401.05717", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.IT\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a72math.IT\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSegment Boundary Detection via Class Entropy Measurements in Connectionist Phoneme Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGiampiero Salvi\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.05717\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.specom.2006.07.009\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nSpeech Communication Volume 48, Issue 12, December 2006, Pages\\n  1666-1676\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 12 Jan 2024 07:36:51 GMT)\\u00a7r"}']}
{title:'Tomita et al. (§72024§r)', author: 'Yoshihide Tomita; Shoichi Koyama; Hiroshi Saruwatari', display:{Lore:['[{"text": "arXiv:2401.05809", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLocalizing Acoustic Energy in Sound Field Synthesis by Directionally Weighted Exterior Radiation Suppression\\u00a7r\\n\\n\\u00a78\\u00a7oYoshihide Tomita\\nShoichi Koyama\\nHiroshi Saruwatari\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.05809\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Jan 2024 10:21:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2024\\u00a7r"}']}
{title:'Heikkinen et al. (§72024§r)', author: 'Mikko Heikkinen; Archontis Politis; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2401.05916", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Ambisonics encoding for compact irregular microphone arrays\\u00a7r\\n\\n\\u00a78\\u00a7oMikko Heikkinen\\nArchontis Politis\\nTuomas Virtanen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.05916\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Jan 2024 13:49:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in Proceedings of the 2024 IEEE InternationalConference on Acoustics, Speech and Signal Processing\\u00a7r"}']}
{title:'Tathe et al. (§72024§r)', author: 'Aniket Tathe; Anand Kamble; Suyash Kumbharkar; Atharva Bhandare; Anirban C. Mitra', display:{Lore:['[{"text": "arXiv:2401.06183", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnd to end Hindi to English speech conversion using Bark, mBART and a finetuned XLSR Wav2Vec2\\u00a7r\\n\\n\\u00a78\\u00a7oAniket Tathe\\nAnand Kamble\\nSuyash Kumbharkar\\nAtharva Bhandare\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.06183\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Jan 2024 04:26:21 GMT)\\u00a7r"}']}
{title:'Daly (§72024§r)', author: 'Matthew Daly', display:{Lore:['[{"text": "arXiv:2401.06203", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRemixing Music for Hearing Aids Using Ensemble of Fine-Tuned Source Separators\\u00a7r\\n\\n\\u00a78\\u00a7oMatthew Daly\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.06203\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 1 Feb 2024 00:09:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2 pages, ICASSP 2024, Cadenza Grand Challenge\\u00a7r"}']}
{title:'Lu et al. (§72024§r)', author: 'Ye-Xin Lu; Yang Ai; Hui-Peng Du; Zhen-Hua Ling', display:{Lore:['[{"text": "arXiv:2401.06387", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards High-Quality and Efficient Speech Bandwidth Extension with Parallel Amplitude and Phase Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oYe-Xin Lu\\nYang Ai\\nHui-Peng Du\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.06387\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Jan 2024 05:40:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEE/ACM Transactionson Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Xi et al. (§72024§r)', author: 'Yu Xi; Baochen Yang; Hao Li; Jiaqi Guo; Kai Yu', display:{Lore:['[{"text": "arXiv:2401.06485", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContrastive Learning With Audio Discrimination For Customizable Keyword Spotting In Continuous Speech\\u00a7r\\n\\n\\u00a78\\u00a7oYu Xi\\nBaochen Yang\\nHao Li\\nJiaqi Guo\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.06485\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Jan 2024 10:08:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2024\\u00a7r"}']}
{title:'Salvi (§72024§r)', author: 'Giampiero Salvi', display:{Lore:['[{"text": "arXiv:2401.06588", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDynamic Behaviour of Connectionist Speech Recognition with Strong Latency Constraints\\u00a7r\\n\\n\\u00a78\\u00a7oGiampiero Salvi\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.06588\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.specom.2005.05.005\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nSpeech Communication Volume 48, Issue 7, July 2006, Pages 802-818\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Jan 2024 14:10:28 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'He Wang; Pengcheng Guo; Wei Chen; Pan Zhou; Lei Xie', display:{Lore:['[{"text": "arXiv:2401.06788", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe NPU-ASLP-LiAuto System Description for Visual Speech Recognition in CNVSRC 2023\\u00a7r\\n\\n\\u00a78\\u00a7oHe Wang\\nPengcheng Guo\\nWei Chen\\nPan Zhou\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.06788\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 29 Feb 2024 18:09:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIncluded in CNVSRC Workshop 2023,NCMMSC 2023\\u00a7r"}']}
{title:'Ye et al. (§72024§r)', author: 'Zuzhao Ye; Gregory Ciccarelli; Brian Kulis', display:{Lore:['[{"text": "arXiv:2401.06897", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMaximum-Entropy Adversarial Audio Augmentation for Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oZuzhao Ye\\nGregory Ciccarelli\\nBrian Kulis\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.06897\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Jan 2024 21:27:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures\\u00a7r"}']}
{title:'Ting et al. (§72024§r)', author: 'Zhu Ting; Li Liangqi; Duan Shufei; Zhang Xueying; Xiao Zhongzhe; Jia Hairng; Liang Huizhi', display:{Lore:['[{"text": "arXiv:2401.07336", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConstruction and Evaluation of Mandarin Multimodal Emotional Speech Database\\u00a7r\\n\\n\\u00a78\\u00a7oZhu Ting\\nLi Liangqi\\nDuan Shufei\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.07336\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 14 Jan 2024 17:56:36 GMT)\\u00a7r"}']}
{title:'Sun et al. (§72024§r)', author: 'Anchen Sun; Juan J Londono; Batya Elbaum; Luis Estrada; Roberto Jose Lazo; Laura Vitale; Hugo Gonzalez Villasanti; Riccardo Fusaroli; Lynn K Perry; Daniel S Messinger', display:{Lore:['[{"text": "arXiv:2401.07342", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWho Said What? An Automated Approach to Analyzing Speech in Preschool Classrooms\\u00a7r\\n\\n\\u00a78\\u00a7oAnchen Sun\\nJuan J Londono\\nBatya Elbaum\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.07342\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 10 Apr 2024 21:02:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 4 figures, 3 tables, The paper has been accepted to 2024 IEEE International Conference on Development and Learning (ICDL) as a full oral presentation and will appear in the IEEE ICDL proceedings\\u00a7r"}']}
{title:'Sasindran et al. (§72024§r)', author: 'Zitha Sasindran; Harsha Yelchuri; T. V. Prabhakar', display:{Lore:['[{"text": "arXiv:2401.07506", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSeMaScore : a new evaluation metric for automatic speech recognition tasks\\u00a7r\\n\\n\\u00a78\\u00a7oZitha Sasindran\\nHarsha Yelchuri\\nT. V. Prabhakar\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.07506\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Jan 2024 07:13:43 GMT)\\u00a7r"}']}
{title:'Xiao et al. (§72024§r)', author: 'Tong Xiao; Simon Doclo', display:{Lore:['[{"text": "arXiv:2401.07681", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SY\\u00a7r, \\u00a7eeess.SY\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEffect of target signals and delays on spatially selective active noise control for open-fitting hearables\\u00a7r\\n\\n\\u00a78\\u00a7oTong Xiao\\nSimon Doclo\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.07681\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Jan 2024 13:48:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2024 (c) 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for "}','{"text": "advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted componentof this work in other works\\u00a7r"}']}
{title:'Fejgin et al. (§72024§r)', author: 'Daniel Fejgin; Elior Hadad; Sharon Gannot; Zbyněk Koldovský; Simon Doclo', display:{Lore:['[{"text": "arXiv:2401.07849", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparison of Frequency-Fusion Mechanisms for Binaural Direction-of-Arrival Estimation for Multiple Speakers\\u00a7r\\n\\n\\u00a78\\u00a7oDaniel Fejgin\\nElior Hadad\\nSharon Gannot\\nZbyn\\u011bk Koldovsk\\u00fd\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.07849\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 15 Jan 2024 17:25:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP 2024\\u00a7r"}']}
{title:'Cheng et al. (§72024§r)', author: 'Ming Cheng; Ming Li', display:{Lore:['[{"text": "arXiv:2401.08052", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Input Multi-Output Target-Speaker Voice Activity Detection For Unified, Flexible, and Robust Audio-Visual Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oMing Cheng\\nMing Li\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.08052\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 1 Mar 2024 02:22:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review of IEEE/ACM Transactions on Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Tang et al. (§72024§r)', author: 'Haobin Tang; Xulong Zhang; Ning Cheng; Jing Xiao; Jianzong Wang', display:{Lore:['[{"text": "arXiv:2401.08166", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lED-TTS: Multi-Scale Emotion Modeling using Cross-Domain Emotion Diarization for Emotional Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oHaobin Tang\\nXulong Zhang\\nNing Cheng\\nJing Xiao\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.08166\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Jan 2024 07:13:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by 2024 IEEE InternationalConference on Acoustics, Speech, and Signal Processing (ICASSP2024)\\u00a7r"}']}
{title:'Mariotte et al. (§72024§r)', author: 'Théo Mariotte; Antonio Almudévar; Marie Tahon; Alfonso Ortega', display:{Lore:['[{"text": "arXiv:2401.08268", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Explainable Proxy Model for Multiabel Audio Segmentation\\u00a7r\\n\\n\\u00a78\\u00a7oTh\\u00e9o Mariotte\\nAntonio Almud\\u00e9var\\nMarie Tahon\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.08268\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 17 Jan 2024 13:28:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2024\\u00a7r"}']}
{title:'Thienpondt et al. (§72024§r)', author: 'Jenthe Thienpondt; Kris Demuynck', display:{Lore:['[{"text": "arXiv:2401.08342", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lECAPA2: A Hybrid Neural Network Architecture and Training Strategy for Robust Speaker Embeddings\\u00a7r\\n\\n\\u00a78\\u00a7oJenthe Thienpondt\\nKris Demuynck\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.08342\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Jan 2024 13:17:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oproceedings of ASRU 2023\\u00a7r"}']}
{title:'Lohmann et al. (§72024§r)', author: 'Anselm Lohmann; Toon van Waterschoot; Joerg Bitzer; Simon Doclo', display:{Lore:['[{"text": "arXiv:2401.08486", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMicrophone Subset Selection for the Weighted Prediction Error Algorithm using a Group Sparsity Penalty\\u00a7r\\n\\n\\u00a78\\u00a7oAnselm Lohmann\\nToon van Waterschoot\\nJoerg Bitzer\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.08486\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Jan 2024 16:45:12 GMT)\\u00a7r"}']}
{title:'Yin et al. (§72024§r)', author: 'Han Yin; Mou Wang; Jisheng Bai; Dongyuan Shi; Woon-Seng Gan; Jianfeng Chen', display:{Lore:['[{"text": "arXiv:2401.08678", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSub-band and Full-band Interactive U-Net with DPRNN for Demixing Cross-talk Stereo Music\\u00a7r\\n\\n\\u00a78\\u00a7oHan Yin\\nMou Wang\\nJisheng Bai\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.08678\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Jan 2024 02:57:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to ICASSP 2024\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Alexander H. Liu; Sung-Lin Yeh; James Glass', display:{Lore:['[{"text": "arXiv:2401.08833", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRevisiting Self-supervised Learning of Speech Representation from a Mutual Information Perspective\\u00a7r\\n\\n\\u00a78\\u00a7oAlexander H. Liu\\nSung-Lin Yeh\\nJames Glass\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.08833\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Jan 2024 21:13:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2024\\u00a7r"}']}
{title:'Yang et al. (§72024§r)', author: 'Yang Yang; George Sung; Shao-Fu Shih; Hakan Erdogan; Chehung Lee; Matthias Grundmann', display:{Lore:['[{"text": "arXiv:2401.08864", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBinaural Angular Separation Network\\u00a7r\\n\\n\\u00a78\\u00a7oYang Yang\\nGeorge Sung\\nShao-Fu Shih\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.08864\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Jan 2024 22:36:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Raju et al. (§72024§r)', author: 'Anirudh Raju; Aparna Khare; Di He; Ilya Sklyar; Long Chen; Sam Alptekin; Viet Anh Trinh; Zhe Zhang; Colin Vaz; Venkatesh Ravichandran; Roland Maas; Ariya Rastrow', display:{Lore:['[{"text": "arXiv:2401.08916", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTwo-pass Endpoint Detection for Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oAnirudh Raju\\nAparna Khare\\nDi He\\n+ 8 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.08916\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ASRU57964.2023.10389743\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Jan 2024 02:00:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oASRU 2023\\u00a7r"}']}
{title:'Damiano et al. (§72024§r)', author: 'Stefano Damiano; Luca Bondi; Shabnam Ghaffarzadegan; Andre Guntoro; Toon van Waterschoot', display:{Lore:['[{"text": "arXiv:2401.09308", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCan Synthetic Data Boost the Training of Deep Acoustic Vehicle Counting Networks?\\u00a7r\\n\\n\\u00a78\\u00a7oStefano Damiano\\nLuca Bondi\\nShabnam Ghaffarzadegan\\nAndre Guntoro\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.09308\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Jan 2024 16:18:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted paper: 2024 IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP 2024)\\u00a7r"}']}
{title:'López-Espejo et al. (§72024§r)', author: 'Iván López-Espejo; Aditya Joglekar; Antonio M. Peinado; Jesper Jensen', display:{Lore:['[{"text": "arXiv:2401.09315", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Speech Pre-emphasis as a Simple and Inexpensive Method to Boost Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oIv\\u00e1n L\\u00f3pez-Espejo\\nAditya Joglekar\\nAntonio M. Peinado\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.09315\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Jan 2024 16:41:20 GMT)\\u00a7r"}']}
{title:'Khan et al. (§72024§r)', author: 'Hania Khan; Aleena Fatima Khalid; Zaryab Hassan', display:{Lore:['[{"text": "arXiv:2401.09354", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTranscending Controlled Environments Assessing the Transferability of ASRRobust NLU Models to Real-World Applications\\u00a7r\\n\\n\\u00a78\\u00a7oHania Khan\\nAleena Fatima Khalid\\nZaryab Hassan\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.09354\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 12 Jan 2024 16:10:04 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Qiquan Zhang; Meng Ge; Hongxu Zhu; Eliathamby Ambikairajah; Qi Song; Zhaoheng Ni; Haizhou Li', display:{Lore:['[{"text": "arXiv:2401.09686", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Empirical Study on the Impact of Positional Encoding in Transformer-based Monaural Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oQiquan Zhang\\nMeng Ge\\nHongxu Zhu\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.09686\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 13 Feb 2024 20:36:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Chowdhury et al. (§72024§r)', author: 'Tahiya Chowdhury; Veronica Romero; Amanda Stent', display:{Lore:['[{"text": "arXiv:2401.09717", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParameter Selection for Analyzing Conversations with Autism Spectrum Disorder\\u00a7r\\n\\n\\u00a78\\u00a7oTahiya Chowdhury\\nVeronica Romero\\nAmanda Stent\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.09717\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1885\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Jan 2024 04:28:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 tables, Proceedings of INTERSPEECH 2023\\u00a7r"}']}
{title:'Kim et al. (§72024§r)', author: 'Minsu Kim; Jeong Hun Yeo; Jeongsoo Choi; Se Jin Park; Yong Man Ro', display:{Lore:['[{"text": "arXiv:2401.09802", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultilingual Visual Speech Recognition with a Single Model by Learning with Discrete Visual Speech Units\\u00a7r\\n\\n\\u00a78\\u00a7oMinsu Kim\\nJeong Hun Yeo\\nJeongsoo Choi\\nSe Jin Park\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.09802\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Jan 2024 08:46:02 GMT)\\u00a7r"}']}
{title:'Nguyen et al. (§72024§r)', author: 'Tan Dat Nguyen; Ji-Hoon Kim; Youngjoon Jang; Jaehun Kim; Joon Son Chung', display:{Lore:['[{"text": "arXiv:2401.10032", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFreGrad: Lightweight and Fast Frequency-aware Diffusion Vocoder\\u00a7r\\n\\n\\u00a78\\u00a7oTan Dat Nguyen\\nJi-Hoon Kim\\nYoungjoon Jang\\nJaehun Kim\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.10032\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Jan 2024 14:57:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Lin et al. (§72024§r)', author: 'Ju Lin; Niko Moritz; Yiteng Huang; Ruiming Xie; Ming Sun; Christian Fuegen; Frank Seide', display:{Lore:['[{"text": "arXiv:2401.10411", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAGADIR: Towards Array-Geometry Agnostic Directional Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oJu Lin\\nNiko Moritz\\nYiteng Huang\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.10411\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Jan 2024 22:45:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Sudo et al. (§72024§r)', author: 'Yui Sudo; Muhammad Shakeel; Yosuke Fukumoto; Yifan Peng; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2401.10449", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContextualized Automatic Speech Recognition with Attention-Based Bias Phrase Boosted Beam Search\\u00a7r\\n\\n\\u00a78\\u00a7oYui Sudo\\nMuhammad Shakeel\\nYosuke Fukumoto\\nYifan Peng\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.10449\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Jan 2024 01:36:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted by ICASSP20224\\u00a7r"}']}
{title:'Yeon et al. (§72024§r)', author: 'Inmo Yeon; Jung-Woo Choi', display:{Lore:['[{"text": "arXiv:2401.10453", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l3D Room Geometry Inference from Multichannel Room Impulse Response using Deep Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oInmo Yeon\\nJung-Woo Choi\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.10453\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 24th International Congress on Acoustics, ICA\\n  2022\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Jan 2024 01:50:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, Proceedings of the 24th International Congress on Acoustics\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Yuewei Zhang; Huanbin Zou; Jie Zhu', display:{Lore:['[{"text": "arXiv:2401.10494", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Two-Stage Framework in Cross-Spectrum Domain for Real-Time Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oYuewei Zhang\\nHuanbin Zou\\nJie Zhu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.10494\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Jan 2024 05:12:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Jacobs (§72024§r)', author: 'Christiaan Jacobs', display:{Lore:['[{"text": "arXiv:2401.10543", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultilingual acoustic word embeddings for zero-resource languages\\u00a7r\\n\\n\\u00a78\\u00a7oChristiaan Jacobs\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.10543\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 23 Jan 2024 14:46:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPhD thesis\\u00a7r"}']}
{title:'Ulgen et al. (§72024§r)', author: 'Ismail Rasim Ulgen; Zongyang Du; Carlos Busso; Berrak Sisman', display:{Lore:['[{"text": "arXiv:2401.11017", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRevealing Emotional Clusters in Speaker Embeddings: A Contrastive Learning Strategy for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oIsmail Rasim Ulgen\\nZongyang Du\\nCarlos Busso\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.11017\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Jan 2024 20:31:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Zhichao Wang; Yuanzhe Chen; Xinsheng Wang; Zhuo Chen; Lei Xie; Yuping Wang; Yuxuan Wang', display:{Lore:['[{"text": "arXiv:2401.11053", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreamVoice: Streamable Context-Aware Language Modeling for Real-time Zero-Shot Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oZhichao Wang\\nYuanzhe Chen\\nXinsheng Wang\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.11053\\u00a7r\\n\\nVersion:\\u00a77v3 (Wed, 7 Feb 2024 03:10:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThere is an error in the submitted version. The author and institution information needs to bemodified, and the company requires re-examination before submission\\u00a7r"}']}
{title:'Patil et al. (§72024§r)', author: 'Aditya Patil; Vikas Joshi; Purvi Agrawal; Rupesh Mehta', display:{Lore:['[{"text": "arXiv:2401.11645", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lStreaming Bilingual End-to-End ASR model using Attention over Multiple Softmax\\u00a7r\\n\\n\\u00a78\\u00a7oAditya Patil\\nVikas Joshi\\nPurvi Agrawal\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.11645\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/SLT54892.2023.10022475\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n2022 IEEE Spoken Language Technology Workshop (SLT), Doha, Qatar,\\n  2023, pp. 252-259\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Jan 2024 01:44:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in IEEE\'s Spoken Language Technology (SLT) 2022, 8 pages (6 +2 for references), 5 figures\\u00a7r"}']}
{title:'R et al. (§72024§r)', author: 'Vinotha R; Hepsiba D; L. D. Vijay Anand; Deepak John Reji', display:{Lore:['[{"text": "arXiv:2401.11771", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEmpowering Communication: Speech Technology for Indian and Western Accents through AI-powered Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oVinotha R\\nHepsiba D\\nL. D. Vijay Anand\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.11771\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 16 Feb 2024 06:08:00 GMT)\\u00a7r"}']}
{title:'Queiroz et al. (§72024§r)', author: 'A. Queiroz; R. Coelho', display:{Lore:['[{"text": "arXiv:2401.11829", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHarmonic Detection from Noisy Speech with Auditory Frame Gain for Intelligibility Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oA. Queiroz\\nR. Coelho\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.11829\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Jan 2024 10:41:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 6 figures, 4 tables\\u00a7r"}']}
{title:'Pillonetto et al. (§72024§r)', author: 'M. Pillonetto; A. Queiroz; R. Coelho', display:{Lore:['[{"text": "arXiv:2401.11832", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntelligibility Enhancement of Acoustic Noisy Speech for Autism Spectrum Disorder Condition\\u00a7r\\n\\n\\u00a78\\u00a7oM. Pillonetto\\nA. Queiroz\\nR. Coelho\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.11832\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Jan 2024 10:48:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figues, 2 tables\\u00a7r"}']}
{title:'Chen et al. (§72024§r)', author: 'Shihao Chen; Liping Chen; Jie Zhang; KongAik Lee; Zhenhua Ling; Lirong Dai', display:{Lore:['[{"text": "arXiv:2401.11857", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdversarial speech for voice privacy protection from Personalized Speech generation\\u00a7r\\n\\n\\u00a78\\u00a7oShihao Chen\\nLiping Chen\\nJie Zhang\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.11857\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Jan 2024 11:26:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by icassp 2024\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Jisi Zhang; Vandana Rajan; Haaris Mehmood; David Tuckey; Pablo Peso Parada; Md Asif Jalal; Karthikeyan Saravanan; Gil Ho Lee; Jungin Lee; Seokyeong Jung', display:{Lore:['[{"text": "arXiv:2401.12085", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConsistency Based Unsupervised Self-training For ASR Personalisation\\u00a7r\\n\\n\\u00a78\\u00a7oJisi Zhang\\nVandana Rajan\\nHaaris Mehmood\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.12085\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Jan 2024 16:23:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for IEEE ASRU 2023\\u00a7r"}']}
{title:'Wu et al. (§72024§r)', author: 'Yi-Chiao Wu; Dejan Marković; Steven Krenn; Israel D. Gebru; Alexander Richard', display:{Lore:['[{"text": "arXiv:2401.12160", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScoreDec: A Phase-preserving High-Fidelity Audio Codec with A Generalized Score-based Diffusion Post-filter\\u00a7r\\n\\n\\u00a78\\u00a7oYi-Chiao Wu\\nDejan Markovi\\u0107\\nSteven Krenn\\nIsrael D. Gebru\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.12160\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Jan 2024 17:54:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 2 tables. Proc. ICASSP, 2024\\u00a7r"}']}
{title:'Roman et al. (§72024§r)', author: 'Iran R. Roman; Christopher Ick; Sivan Ding; Adrian S. Roman; Brian McFee; Juan P. Bello', display:{Lore:['[{"text": "arXiv:2401.12238", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatial Scaper: A Library to Simulate and Augment Soundscapes for Sound Event Localization and Detection in Realistic Rooms\\u00a7r\\n\\n\\u00a78\\u00a7oIran R. Roman\\nChristopher Ick\\nSivan Ding\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.12238\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 19 Jan 2024 19:01:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, 1 table, to be presented at ICASSP 2024 in Seoul,South Korea\\u00a7r"}']}
{title:'Yue et al. (§72024§r)', author: 'Xianghu Yue; Xiaohai Tian; Lu Lu; Malu Zhang; Zhizheng Wu; Haizhou Li', display:{Lore:['[{"text": "arXiv:2401.12264", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCoAVT: A Cognition-Inspired Unified Audio-Visual-Text Pre-Training Model for Multimodal Processing\\u00a7r\\n\\n\\u00a78\\u00a7oXianghu Yue\\nXiaohai Tian\\nLu Lu\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.12264\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 21 Feb 2024 08:54:29 GMT)\\u00a7r"}']}
{title:'Gao et al. (§72024§r)', author: 'Chenyang Gao; Brecht Desplanques; Chelsea J. -T. Ju; Aman Chadha; Andreas Stolcke', display:{Lore:['[{"text": "arXiv:2401.12440", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPost-Training Embedding Alignment for Decoupling Enrollment and Runtime Speaker Recognition Models\\u00a7r\\n\\n\\u00a78\\u00a7oChenyang Gao\\nBrecht Desplanques\\nChelsea J. -T. Ju\\nAman Chadha\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.12440\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Jan 2024 02:19:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Lee et al. (§72024§r)', author: 'Younglo Lee; Shukjae Choi; Byeong-Yeol Kim; Zhong-Qiu Wang; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2401.12473", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBoosting Unknown-number Speaker Separation with Transformer Decoder-based Attractor\\u00a7r\\n\\n\\u00a78\\u00a7oYounglo Lee\\nShukjae Choi\\nByeong-Yeol Kim\\nZhong-Qiu Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.12473\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Jan 2024 03:55:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, accepted by ICASSP 2024\\u00a7r"}']}
{title:'Uzrad et al. (§72024§r)', author: 'Noy Uzrad; Oren Barkan; Almog Elharar; Shlomi Shvartzman; Moshe Laufer; Lior Wolf; Noam Koenigstein', display:{Lore:['[{"text": "arXiv:2401.12570", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffMoog: a Differentiable Modular Synthesizer for Sound Matching\\u00a7r\\n\\n\\u00a78\\u00a7oNoy Uzrad\\nOren Barkan\\nAlmog Elharar\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.12570\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Jan 2024 08:59:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 7 figures, 1 table, Our code is released at https://github.com/aisynth/diffmoog\\u00a7r"}']}
{title:'Singh et al. (§72024§r)', author: 'Prachi Singh; Sriram Ganapathy', display:{Lore:['[{"text": "arXiv:2401.12850", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOverlap-aware End-to-End Supervised Hierarchical Graph Clustering for Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oPrachi Singh\\nSriram Ganapathy\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.12850\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Jan 2024 15:35:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages\\u00a7r"}']}
{title:'Jalal et al. (§72024§r)', author: 'Md Asif Jalal; Pablo Peso Parada; George Pavlidis; Vasileios Moschopoulos; Karthikeyan Saravanan; Chrysovalantis-Giorgos Kontoulis; Jisi Zhang; Anastasios Drosou; Gil Ho Lee; Jungin Lee; Seokyeong Jung', display:{Lore:['[{"text": "arXiv:2401.13146", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLocality enhanced dynamic biasing and sampling strategies for contextual ASR\\u00a7r\\n\\n\\u00a78\\u00a7oMd Asif Jalal\\nPablo Peso Parada\\nGeorge Pavlidis\\n+ 7 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.13146\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Jan 2024 23:46:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for IEEE ASRU 2023\\u00a7r"}']}
{title:'Zhou et al. (§72024§r)', author: 'Wangjin Zhou; Zhengdong Yang; Chenhui Chu; Sheng Li; Raj Dabre; Yi Zhao; Tatsuya Kawahara', display:{Lore:['[{"text": "arXiv:2401.13249", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMOS-FAD: Improving Fake Audio Detection Via Automatic Mean Opinion Score Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oWangjin Zhou\\nZhengdong Yang\\nChenhui Chu\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.13249\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 Jan 2024 02:24:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in ICASSP2024\\u00a7r"}']}
{title:'Tong et al. (§72024§r)', author: 'Weinan Tong; Jiaxu Zhu; Jun Chen; Shiyin Kang; Tao Jiang; Yang Li; Zhiyong Wu; Helen Meng', display:{Lore:['[{"text": "arXiv:2401.13276", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSCNet: Sparse Compression Network for Music Source Separation\\u00a7r\\n\\n\\u00a78\\u00a7oWeinan Tong\\nJiaxu Zhu\\nJun Chen\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.13276\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Jan 2024 07:35:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Hold et al. (§72024§r)', author: 'Christoph Hold; Leo McCormack; Archontis Politis; Ville Pulkki', display:{Lore:['[{"text": "arXiv:2401.13401", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPerceptually-motivated Spatial Audio Codec for Higher-Order Ambisonics Compression\\u00a7r\\n\\n\\u00a78\\u00a7oChristoph Hold\\nLeo McCormack\\nArchontis Politis\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.13401\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Jan 2024 11:55:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in Proceedings of the 2024 IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP 2024)\\u00a7r"}']}
{title:'Hu et al. (§72024§r)', author: 'Hu Hu; Sabato Marco Siniscalchi; Chin-Hui Lee', display:{Lore:['[{"text": "arXiv:2401.13766", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBayesian adaptive learning to latent variables via Variational Bayes and Maximum a Posteriori\\u00a7r\\n\\n\\u00a78\\u00a7oHu Hu\\nSabato Marco Siniscalchi\\nChin-Hui Lee\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.13766\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Jan 2024 19:27:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oASRU2023 Bayesian Symposium. arXiv admin note: text overlap with arXiv:2110.08598\\u00a7r"}']}
{title:'Jung et al. (§72024§r)', author: 'Sunghee Jung; Won Jang; Jaesam Yoon; Bongwan Kim', display:{Lore:['[{"text": "arXiv:2401.13921", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntelli-Z: Toward Intelligible Zero-Shot TTS\\u00a7r\\n\\n\\u00a78\\u00a7oSunghee Jung\\nWon Jang\\nJaesam Yoon\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.13921\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 Jan 2024 03:37:23 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Heming Wang; Eric W. Healy; DeLiang Wang', display:{Lore:['[{"text": "arXiv:2401.14269", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCombined Generative and Predictive Modeling for Speech Super-resolution\\u00a7r\\n\\n\\u00a78\\u00a7oHeming Wang\\nEric W. Healy\\nDeLiang Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.14269\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 Jan 2024 16:04:51 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Wangyou Zhang; Jee-weon Jung; Shinji Watanabe; Yanmin Qian', display:{Lore:['[{"text": "arXiv:2401.14271", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lImproving Design of Input Condition Invariant Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oWangyou Zhang\\nJee-weon Jung\\nShinji Watanabe\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.14271\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 16 Feb 2024 04:50:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024, 5 pages, 2 figures, 3 tables (corrected the resultsof no processing on CHiME-4 (Simu) in Table 2)\\u00a7r"}']}
{title:'Du et al. (§72024§r)', author: 'Chenpeng Du; Yiwei Guo; Hankun Wang; Yifan Yang; Zhikang Niu; Shuai Wang; Hui Zhang; Xie Chen; Kai Yu', display:{Lore:['[{"text": "arXiv:2401.14321", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVALL-T: Decoder-Only Generative Transducer for Robust and Decoding-Controllable Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oChenpeng Du\\nYiwei Guo\\nHankun Wang\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.14321\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 30 Jan 2024 02:48:31 GMT)\\u00a7r"}']}
{title:'Millot et al. (§72024§r)', author: 'Laurent Millot; Mohammed Elliq; Manuel Lopes; Gérard Pelé; Dominique Lambert', display:{Lore:['[{"text": "arXiv:2401.14410", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRevisiting proximity effect using broadband signals\\u00a7r\\n\\n\\u00a78\\u00a7oLaurent Millot\\nMohammed Elliq\\nManuel Lopes\\nG\\u00e9rard Pel\\u00e9\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.14410\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7n122th Convention of the Audio Engineering Society, Audio\\n  Engineering Society, May 2007, Vienne (Autriche), Austria\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Jan 2024 08:42:23 GMT)\\u00a7r"}']}
{title:'Deloche et al. (§72024§r)', author: 'François Deloche; Laurent Bonnasse-Gahot; Judit Gervain', display:{Lore:['[{"text": "arXiv:2401.14416", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic characterization of speech rhythm: going beyond metrics with recurrent neural networks\\u00a7r\\n\\n\\u00a78\\u00a7oFran\\u00e7ois Deloche\\nLaurent Bonnasse-Gahot\\nJudit Gervain\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.14416\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Jan 2024 09:49:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 7 figures\\u00a7r"}']}
{title:'Abdalmalak et al. (§72024§r)', author: "Kerlos Atia Abdalmalak; Ascensión Gallardo-Antol'in", display:{Lore:['[{"text": "arXiv:2401.15018", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancement of a Text-Independent Speaker Verification System by using Feature Combination and Parallel-Structure Classifiers\\u00a7r\\n\\n\\u00a78\\u00a7oKerlos Atia Abdalmalak\\nAscensi\\u00f3n Gallardo-Antol\'in\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.15018\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1007/s00521-016-2470-x\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nNeural Computing and Applications 29 (2018) 637-651\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Jan 2024 17:19:59 GMT)\\u00a7r"}']}
{title:'Pawlak et al. (§72024§r)', author: 'Alan Pawlak; Hyunkook Lee; Aki Mäkivirta; Thomas Lund', display:{Lore:['[{"text": "arXiv:2401.15023", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatial Analysis and Synthesis Methods: Subjective and Objective Evaluations Using Various Microphone Arrays in the Auralization of a Critical Listening Room\\u00a7r\\n\\n\\u00a78\\u00a7oAlan Pawlak\\nHyunkook Lee\\nAki M\\u00e4kivirta\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.15023\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Jan 2024 17:29:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 6 figures\\u00a7r"}']}
{title:'Raj et al. (§72024§r)', author: 'Desh Raj; Matthew Wiesner; Matthew Maciejewski; Leibny Paola Garcia-Perera; Daniel Povey; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:2401.15676", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn Speaker Attribution with SURT\\u00a7r\\n\\n\\u00a78\\u00a7oDesh Raj\\nMatthew Wiesner\\nMatthew Maciejewski\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.15676\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 28 Jan 2024 14:56:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 6 figures, 6 tables. Submitted to Odyssey 2024\\u00a7r"}']}
{title:'Kasess et al. (§72024§r)', author: 'Christian H. Kasess; Wolfgang Kreuzer; Prateek Soni; Holger Waubke', display:{Lore:['[{"text": "arXiv:2401.16819", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLocalizing uniformly moving mono-frequent sources using an inverse 2.5D approach\\u00a7r\\n\\n\\u00a78\\u00a7oChristian H. Kasess\\nWolfgang Kreuzer\\nPrateek Soni\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.16819\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Jan 2024 08:45:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o27 pages, 15 figures\\u00a7r"}']}
{title:'Hsu et al. (§72024§r)', author: 'Yicheng Hsu; Ssuhan Chen; Mingsian R. Bai', display:{Lore:['[{"text": "arXiv:2401.16850", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpatial-Temporal Activity-Informed Diarization and Separation\\u00a7r\\n\\n\\u00a78\\u00a7oYicheng Hsu\\nSsuhan Chen\\nMingsian R. Bai\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.16850\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Jan 2024 09:50:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages\\u00a7r"}']}
{title:'Thornton et al. (§72024§r)', author: 'Mike Thornton; Jonas Auernheimer; Constantin Jehn; Danilo Mandic; Tobias Reichenbach', display:{Lore:['[{"text": "arXiv:2401.17380", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDetecting gamma-band responses to the speech envelope for the ICASSP 2024 Auditory EEG Decoding Signal Processing Grand Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oMike Thornton\\nJonas Auernheimer\\nConstantin Jehn\\nDanilo Mandic\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.17380\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Jan 2024 19:07:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP 2024 (challengetrack)\\u00a7r"}']}
{title:'Kim et al. (§72024§r)', author: 'Jaeyeon Kim; Jaeyoon Jung; Jinjoo Lee; Sang Hoon Woo', display:{Lore:['[{"text": "arXiv:2401.17690", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnCLAP: Combining Neural Audio Codec and Audio-Text Joint Embedding for Automated Audio Captioning\\u00a7r\\n\\n\\u00a78\\u00a7oJaeyeon Kim\\nJaeyoon Jung\\nJinjoo Lee\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.17690\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 Jan 2024 09:23:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Kamper et al. (§72024§r)', author: 'Herman Kamper; Benjamin van Niekerk', display:{Lore:['[{"text": "arXiv:2401.17902", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRevisiting speech segmentation and lexicon learning with better features\\u00a7r\\n\\n\\u00a78\\u00a7oHerman Kamper\\nBenjamin van Niekerk\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2401.17902\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 31 Jan 2024 15:06:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2 pages\\u00a7r"}']}
{title:'Gruttadauria et al. (§72024§r)', author: 'Elio Gruttadauria; Mathieu Fontaine; Slim Essid', display:{Lore:['[{"text": "arXiv:2402.00067", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOnline speaker diarization of meetings guided by speech separation\\u00a7r\\n\\n\\u00a78\\u00a7oElio Gruttadauria\\nMathieu Fontaine\\nSlim Essid\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.00067\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nIEEE International Conference on Acoustics, Speech, and Signal\\n  Processing, Apr 2024, Seoul (Korea), South Korea\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Jan 2024 09:09:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at ICASSP 2024\\u00a7r"}']}
{title:'Deshmukh et al. (§72024§r)', author: 'Soham Deshmukh; Dareen Alharthi; Benjamin Elizalde; Hannes Gamper; Mahmoud Al Ismail; Rita Singh; Bhiksha Raj; Huaming Wang', display:{Lore:['[{"text": "arXiv:2402.00282", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPAM: Prompting Audio-Language Models for Audio Quality Assessment\\u00a7r\\n\\n\\u00a78\\u00a7oSoham Deshmukh\\nDareen Alharthi\\nBenjamin Elizalde\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.00282\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Feb 2024 02:15:59 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72024§r)', author: 'Dong Yang; Tomoki Koriyama; Yuki Saito', display:{Lore:['[{"text": "arXiv:2402.00288", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFrame-Wise Breath Detection with Self-Training: An Exploration of Enhancing Breath Naturalness in Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oDong Yang\\nTomoki Koriyama\\nYuki Saito\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.00288\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Feb 2024 02:39:19 GMT)\\u00a7r"}']}
{title:'Togami et al. (§72024§r)', author: 'Masahito Togami; Jean-Marc Valin; Karim Helwani; Ritwik Giri; Umut Isik; Michael M. Goodwin', display:{Lore:['[{"text": "arXiv:2402.00337", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-time Stereo Speech Enhancement with Spatial-Cue Preservation based on Dual-Path Structure\\u00a7r\\n\\n\\u00a78\\u00a7oMasahito Togami\\nJean-Marc Valin\\nKarim Helwani\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.00337\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Feb 2024 04:47:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for ICASSP 2024, 5 pages\\u00a7r"}']}
{title:'Lay et al. (§72024§r)', author: 'Bunlong Lay; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2402.00811", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Analysis of the Variance of Diffusion-based Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oBunlong Lay\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.00811\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Feb 2024 17:46:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 1 table\\u00a7r"}']}
{title:'Wang (§72024§r)', author: 'Zhong-Qiu Wang', display:{Lore:['[{"text": "arXiv:2402.00820", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUSDnet: Unsupervised Speech Dereverberation via Neural Forward Filtering\\u00a7r\\n\\n\\u00a78\\u00a7oZhong-Qiu Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.00820\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Feb 2024 18:02:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oin submission\\u00a7r"}']}
{title:'Cappellazzo et al. (§72024§r)', author: 'Umberto Cappellazzo; Daniele Falavigna; Alessio Brutti', display:{Lore:['[{"text": "arXiv:2402.00828", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters\\u00a7r\\n\\n\\u00a78\\u00a7oUmberto Cappellazzo\\nDaniele Falavigna\\nAlessio Brutti\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.00828\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Feb 2024 18:16:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe code will be released ad: <https://github.com/umbertocappellazzo/PETL_AST>\\u00a7r"}']}
{title:'Lin et al. (§72024§r)', author: 'Jackie Lin; Georg Götz; Sebastian J. Schlecht', display:{Lore:['[{"text": "arXiv:2402.00859", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep Room Impulse Response Completion\\u00a7r\\n\\n\\u00a78\\u00a7oJackie Lin\\nGeorg G\\u00f6tz\\nSebastian J. Schlecht\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.00859\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Feb 2024 18:55:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe following article has been submitted to the EURASIP Journal on Audio, Speech, and Music Processing\\u00a7r"}']}
{title:'Xu et al. (§72024§r)', author: 'Linping Xu; Jiawei Jiang; Dejun Zhang; Xianjun Xia; Li Chen; Yijian Xiao; Piao Ding; Shenyi Song; Sixing Yin; Ferdous Sohel', display:{Lore:['[{"text": "arXiv:2402.01271", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Intra-BRNN and GB-RVQ Based END-TO-END Neural Audio Codec\\u00a7r\\n\\n\\u00a78\\u00a7oLinping Xu\\nJiawei Jiang\\nDejun Zhang\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.01271\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Feb 2024 09:55:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oINTERSPEECH 2023\\u00a7r"}']}
{title:'Kim et al. (§72024§r)', author: 'Jaeyeon Kim; Injune Hwang; Kyogu Lee', display:{Lore:['[{"text": "arXiv:2402.01298", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLearning Semantic Information from Raw Audio Signal Using Both Contextual and Phonetic Representations\\u00a7r\\n\\n\\u00a78\\u00a7oJaeyeon Kim\\nInjune Hwang\\nKyogu Lee\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.01298\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Feb 2024 10:39:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Sánchez et al. (§72024§r)', author: 'María Sánchez; Laura Fernández; Julián Arias; Mateo Cámara; Giulia Comini; Adam Gabrys; José Luis Blanco; Juan Ignacio Godino; Luis Alfonso Hernández', display:{Lore:['[{"text": "arXiv:2402.01385", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDel Visual al Auditivo: Sonorizaci\\u00f3n de Escenas Guiada por Imagen\\u00a7r\\n\\n\\u00a78\\u00a7oMar\\u00eda S\\u00e1nchez\\nLaura Fern\\u00e1ndez\\nJuli\\u00e1n Arias\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.01385\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Feb 2024 13:09:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, in Spanish, Tecniac\\u00fastica\\u00a7r"}']}
{title:'Phukan et al. (§72024§r)', author: 'Orchid Chetia Phukan; Gautam Siddharth Kashyap; Arun Balaji Buduru; Rajesh Sharma', display:{Lore:['[{"text": "arXiv:2402.01579", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHow Paralingual are Paralinguistic Representations? A Case Study in Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oOrchid Chetia Phukan\\nGautam Siddharth Kashyap\\nArun Balaji Buduru\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.01579\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Feb 2024 17:17:42 GMT)\\u00a7r"}']}
{title:'Zheng et al. (§72024§r)', author: 'Zhisheng Zheng; Puyuan Peng; Ziyang Ma; Xie Chen; Eunsol Choi; David Harwath', display:{Lore:['[{"text": "arXiv:2402.01591", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBAT: Learning to Reason about Spatial Sounds with Large Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oZhisheng Zheng\\nPuyuan Peng\\nZiyang Ma\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.01591\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 25 May 2024 17:05:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICML2024. Our demo, dataset, code and modelweights are available at: https://zhishengzheng.com/BAT\\u00a7r"}']}
{title:'Wickramaarachchi et al. (§72024§r)', author: 'W. A. K. M. Wickramaarachchi; Sameeri Sathsara Subasinghe; K. K. Rashani Tharushika Wijerathna; A. Sahashra Udani Athukorala; Lakmini Abeywardhana; A. Karunasena', display:{Lore:['[{"text": "arXiv:2402.01752", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIdentifying False Content and Hate Speech in Sinhala YouTube Videos by Analyzing the Audio\\u00a7r\\n\\n\\u00a78\\u00a7oW. A. K. M. Wickramaarachchi\\nSameeri Sathsara Subasinghe\\nK. K. Rashani Tharushika Wijerathna\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.01752\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Jan 2024 08:08:34 GMT)\\u00a7r"}']}
{title:'Dauphin (§72024§r)', author: 'Gabriel Dauphin', display:{Lore:['[{"text": "arXiv:2402.01778", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntroduction to speech recognition\\u00a7r\\n\\n\\u00a78\\u00a7oGabriel Dauphin\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.01778\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 1 Feb 2024 17:54:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oin French language\\u00a7r"}']}
{title:'Wiepert et al. (§72024§r)', author: 'Daniela A. Wiepert; Rene L. Utianski; Joseph R. Duffy; John L. Stricker; Leland R. Barnard; David T. Jones; Hugo Botha', display:{Lore:['[{"text": "arXiv:2402.01796", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring transfer learning for pathological speech feature prediction: Impact of layer selection\\u00a7r\\n\\n\\u00a78\\u00a7oDaniela A. Wiepert\\nRene L. Utianski\\nJoseph R. Duffy\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.01796\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Feb 2024 05:09:42 GMT)\\u00a7r"}']}
{title:'Yuan et al. (§72024§r)', author: 'Kuang Yuan; Mohamed Ibrahim; Yiwen Song; Guoxiang Deng; Suvendra Vijayan; Robert Nerone; Akshay Gadre; Swarun Kumar', display:{Lore:['[{"text": "arXiv:2402.01933", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lToMoBrush: Exploring Dental Health Sensing using a Sonic Toothbrush\\u00a7r\\n\\n\\u00a78\\u00a7oKuang Yuan\\nMohamed Ibrahim\\nYiwen Song\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.01933\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Feb 2024 22:05:57 GMT)\\u00a7r"}']}
{title:'San et al. (§72024§r)', author: 'Nay San; Georgios Paraskevopoulos; Aryaman Arora; Xiluo He; Prabhjot Kaur; Oliver Adams; Dan Jurafsky', display:{Lore:['[{"text": "arXiv:2402.02302", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPredicting positive transfer for improved low-resource speech recognition using acoustic pseudo-tokens\\u00a7r\\n\\n\\u00a78\\u00a7oNay San\\nGeorgios Paraskevopoulos\\nAryaman Arora\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.02302\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 3 Feb 2024 23:54:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for SIGTYP2024\\u00a7r"}']}
{title:'Bai et al. (§72024§r)', author: 'Jisheng Bai; Mou Wang; Haohe Liu; Han Yin; Yafei Jia; Siwei Huang; Yutong Du; Dongzhe Zhang; Dongyuan Shi; Woon-Seng Gan; Mark D. Plumbley; Susanto Rahardja; Bin Xiang; Jianfeng Chen', display:{Lore:['[{"text": "arXiv:2402.02694", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDescription on IEEE ICME 2024 Grand Challenge: Semi-supervised Acoustic Scene Classification under Domain Shift\\u00a7r\\n\\n\\u00a78\\u00a7oJisheng Bai\\nMou Wang\\nHaohe Liu\\n+ 10 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.02694\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 29 Feb 2024 02:35:54 GMT)\\u00a7r"}']}
{title:'Vázquez-Romero et al. (§72024§r)', author: 'Adrián Vázquez-Romero; Ascensión Gallardo-Antolín', display:{Lore:['[{"text": "arXiv:2402.02830", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Detection of Depression in Speech Using Ensemble Convolutional Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oAdri\\u00e1n V\\u00e1zquez-Romero\\nAscensi\\u00f3n Gallardo-Antol\\u00edn\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.02830\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3390/e22060688\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nA. Vazquez-Romero and A. Gallardo-Antolin Entropy 22, no. 6: 688\\n  (2020)\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Feb 2024 09:36:21 GMT)\\u00a7r"}']}
{title:'Fernández-Díaz et al. (§72024§r)', author: 'Miguel Fernández-Díaz; Ascensión Gallardo-Antolín', display:{Lore:['[{"text": "arXiv:2402.02850", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Attention Long Short-Term Memory based system for automatic classification of speech intelligibility\\u00a7r\\n\\n\\u00a78\\u00a7oMiguel Fern\\u00e1ndez-D\\u00edaz\\nAscensi\\u00f3n Gallardo-Antol\\u00edn\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.02850\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.engappai.2020.103976\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nMiguel Fernandez-Diaz and Ascension Gallardo-Antolin Engineering\\n  Applications of Artificial Intelligence 96 (2020) 103976\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Feb 2024 10:03:28 GMT)\\u00a7r"}']}
{title:'Gallardo-Antolín et al. (§72024§r)', author: 'Ascensión Gallardo-Antolín; Juan M. Montero', display:{Lore:['[{"text": "arXiv:2402.02865", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn combining acoustic and modulation spectrograms in an attention LSTM-based system for speech intelligibility level classification\\u00a7r\\n\\n\\u00a78\\u00a7oAscensi\\u00f3n Gallardo-Antol\\u00edn\\nJuan M. Montero\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.02865\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.neucom.2021.05.065\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nAscension Gallardo-Antolin and Juan M. Montero Neurocomputing 456\\n  (2021) 49-60\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Feb 2024 10:26:28 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Shanshan Wang; Soumya Tripathy; Toni Heittola; Annamaria Mesaros', display:{Lore:['[{"text": "arXiv:2402.02899", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPositive and negative sampling strategies for self-supervised learning on audio-video data\\u00a7r\\n\\n\\u00a78\\u00a7oShanshan Wang\\nSoumya Tripathy\\nToni Heittola\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.02899\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Feb 2024 11:11:38 GMT)\\u00a7r"}']}
{title:'Tammen et al. (§72024§r)', author: 'Marvin Tammen; Tsubasa Ochiai; Marc Delcroix; Tomohiro Nakatani; Shoko Araki; Simon Doclo', display:{Lore:['[{"text": "arXiv:2402.03058", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lArray Geometry-Robust Attention-Based Neural Beamformer for Moving Speakers\\u00a7r\\n\\n\\u00a78\\u00a7oMarvin Tammen\\nTsubasa Ochiai\\nMarc Delcroix\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.03058\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Feb 2024 14:48:07 GMT)\\u00a7r"}']}
{title:'Uddin et al. (§72024§r)', author: 'Majbah Uddin; Nathan Huynh; Jose M Vidal; Kevin M Taaffe; Lawrence D Fredendall; Joel S Greenstein', display:{Lore:['[{"text": "arXiv:2402.03369", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluation of Google\'s Voice Recognition and Sentence Classification for Health Care Applications\\u00a7r\\n\\n\\u00a78\\u00a7oMajbah Uddin\\nNathan Huynh\\nJose M Vidal\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.03369\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1080/10429247.2015.1054752\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nEngineering Management Journal, 27:3, 152-162, 2015\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 2 Feb 2024 03:13:09 GMT)\\u00a7r"}']}
{title:'Martín-Cortinas et al. (§72024§r)', author: 'Álvaro Martín-Cortinas; Daniel Sáez-Trigueros; Iván Vallés-Pérez; Biel Tura-Vecino; Piotr Biliński; Mateusz Lajszczak; Grzegorz Beringer; Roberto Barra-Chicote; Jaime Lorenzo-Trueba', display:{Lore:['[{"text": "arXiv:2402.03407", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations\\u00a7r\\n\\n\\u00a78\\u00a7o\\u00c1lvaro Mart\\u00edn-Cortinas\\nDaniel S\\u00e1ez-Trigueros\\nIv\\u00e1n Vall\\u00e9s-P\\u00e9rez\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.03407\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 5 Feb 2024 15:08:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 1 figure, 3 tables\\u00a7r"}']}
{title:'Jiang et al. (§72024§r)', author: 'Xilin Jiang; Cong Han; Yinghao Aaron Li; Nima Mesgarani', display:{Lore:['[{"text": "arXiv:2402.03710", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lListen, Chat, and Edit: Text-Guided Soundscape Modification for Enhanced Auditory Experience\\u00a7r\\n\\n\\u00a78\\u00a7oXilin Jiang\\nCong Han\\nYinghao Aaron Li\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.03710\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 6 Feb 2024 05:05:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7opreprint\\u00a7r"}']}
{title:'Tseng et al. (§72024§r)', author: 'Liang-Hsuan Tseng; En-Pei Hu; Cheng-Han Chiang; Yuan Tseng; Hung-yi Lee; Lin-shan Lee; Shao-Hua Sun', display:{Lore:['[{"text": "arXiv:2402.03988", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lREBORN: Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR\\u00a7r\\n\\n\\u00a78\\u00a7oLiang-Hsuan Tseng\\nEn-Pei Hu\\nCheng-Han Chiang\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.03988\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 28 May 2024 17:19:59 GMT)\\u00a7r"}']}
{title:'Ahmad et al. (§72024§r)', author: 'Rehan Ahmad; Muhammad Umar Farooq; Thomas Hain', display:{Lore:['[{"text": "arXiv:2402.04805", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProgressive unsupervised domain adaptation for ASR using ensemble models and multi-stage training\\u00a7r\\n\\n\\u00a78\\u00a7oRehan Ahmad\\nMuhammad Umar Farooq\\nThomas Hain\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.04805\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 7 Feb 2024 12:52:22 GMT)\\u00a7r"}']}
{title:'Ronchini et al. (§72024§r)', author: 'Francesca Ronchini; Luca Comanducci; Mirco Pezzoli; Fabio Antonacci; Augusto Sarti', display:{Lore:['[{"text": "arXiv:2402.04866", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRoom Transfer Function Reconstruction Using Complex-valued Neural Networks and Irregularly Distributed Microphones\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesca Ronchini\\nLuca Comanducci\\nMirco Pezzoli\\nFabio Antonacci\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.04866\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 26 Mar 2024 16:57:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to EUSIPCO 2024\\u00a7r"}']}
{title:'Fang et al. (§72024§r)', author: 'Hung-Chieh Fang; Nai-Xuan Ye; Yi-Jen Shih; Puyuan Peng; Hsuan-Fu Wang; Layne Berry; Hung-yi Lee; David Harwath', display:{Lore:['[{"text": "arXiv:2402.05819", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIntegrating Self-supervised Speech Model with Pseudo Word-level Targets from Visually-grounded Speech Model\\u00a7r\\n\\n\\u00a78\\u00a7oHung-Chieh Fang\\nNai-Xuan Ye\\nYi-Jen Shih\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.05819\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Feb 2024 16:55:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024 workshop on Self-supervision in Audio, Speech, and Beyond (SASB)\\u00a7r"}']}
{title:'Bicer et al. (§72024§r)', author: 'H. Nazim Bicer; Cagdas Tuna; Andreas Walther; Emanuël A. P. Habets', display:{Lore:['[{"text": "arXiv:2402.06246", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData-driven Joint Detection and Localization of Acoustic Reflectors\\u00a7r\\n\\n\\u00a78\\u00a7oH. Nazim Bicer\\nCagdas Tuna\\nAndreas Walther\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.06246\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Feb 2024 08:40:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4+1(bib) Pages. Accepted to ICASSP Satellite Workshop - HSCMA 2024\\u00a7r"}']}
{title:'Rodriguez-Perez et al. (§72024§r)', author: 'Pablo Rodriguez-Perez; Ruben Fraile; Miguel Garcia-Escrig; Nicolas Saenz-Lechon; Juana M. Gutierrez-Arriola; Victor Osma-Ruiz', display:{Lore:['[{"text": "arXiv:2402.06387", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Transversal Study of Fundamental Frequency Contours in Parkinsonian Voices\\u00a7r\\n\\n\\u00a78\\u00a7oPablo Rodriguez-Perez\\nRuben Fraile\\nMiguel Garcia-Escrig\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.06387\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1016/j.bspc.2019.02.021\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 9 Feb 2024 13:08:58 GMT)\\u00a7r"}']}
{title:'Helwani et al. (§72024§r)', author: 'Karim Helwani; Masahito Togami; Paris Smaragdis; Michael M. Goodwin', display:{Lore:['[{"text": "arXiv:2402.06683", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Source Separation Using Latent Variational Block-Wise Disentanglement\\u00a7r\\n\\n\\u00a78\\u00a7oKarim Helwani\\nMasahito Togami\\nParis Smaragdis\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.06683\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 8 Feb 2024 07:22:39 GMT)\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Jialu Li; Mark Hasegawa-Johnson; Nancy L. McElwain', display:{Lore:['[{"text": "arXiv:2402.06888", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnalysis of Self-Supervised Speech Models on Children\'s Speech and Infant Vocalizations\\u00a7r\\n\\n\\u00a78\\u00a7oJialu Li\\nMark Hasegawa-Johnson\\nNancy L. McElwain\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.06888\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 10 Feb 2024 05:20:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to 2024 ICASSP Workshop of Self-supervision in Audio, Speech and Beyond (SASB)\\u00a7r"}']}
{title:'Ziogas et al. (§72024§r)', author: 'Ioannis Ziogas; Hessa Alfalahi; Ahsan H. Khandoker; Leontios J. Hadjileontiadis', display:{Lore:['[{"text": "arXiv:2402.06923", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r, \\u00a7cstat.ML\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCochCeps-Augment: A Novel Self-Supervised Contrastive Learning Using Cochlear Cepstrum-based Masking for Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oIoannis Ziogas\\nHessa Alfalahi\\nAhsan H. Khandoker\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.06923\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 10 Feb 2024 11:13:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure Accepted in IEEE ICASSP 2024 Workshops- Self-Supervision in Audio, Speech, and Beyond\\u00a7r"}']}
{title:'Kanda et al. (§72024§r)', author: 'Naoyuki Kanda; Xiaofei Wang; Sefik Emre Eskimez; Manthan Thakker; Hemin Yang; Zirun Zhu; Min Tang; Canrun Li; Chung-Hsien Tsai; Zhen Xiao; Yufei Xia; Jinzhu Li; Yanqing Liu; Sheng Zhao; Michael Zeng', display:{Lore:['[{"text": "arXiv:2402.07383", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMaking Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like\\u00a7r\\n\\n\\u00a78\\u00a7oNaoyuki Kanda\\nXiaofei Wang\\nSefik Emre Eskimez\\n+ 11 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.07383\\u00a7r\\n\\nVersion:\\u00a77v2 (Mon, 4 Mar 2024 19:15:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSee https://aka.ms/elate/ for demo samples, v2: subjective evaluation has been added\\u00a7r"}']}
{title:'Saxena et al. (§72024§r)', author: 'Kavya Ranjan Saxena; Vipul Arora', display:{Lore:['[{"text": "arXiv:2402.07599", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInteractive singing melody extraction based on active adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oKavya Ranjan Saxena\\nVipul Arora\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.07599\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 12 Feb 2024 11:58:41 GMT)\\u00a7r"}']}
{title:'Yang et al. (§72024§r)', author: 'Qian Yang; Jin Xu; Wenrui Liu; Yunfei Chu; Ziyue Jiang; Xiaohuan Zhou; Yichong Leng; Yuanjun Lv; Zhou Zhao; Chang Zhou; Jingren Zhou', display:{Lore:['[{"text": "arXiv:2402.07729", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension\\u00a7r\\n\\n\\u00a78\\u00a7oQian Yang\\nJin Xu\\nWenrui Liu\\n+ 7 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.07729\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 12 Feb 2024 15:41:22 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Shiqi Zhang; Zheng Qiu; Daiki Takeuchi; Noboru Harada; Shoji Makino', display:{Lore:['[{"text": "arXiv:2402.08252", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUnrestricted Global Phase Bias-Aware Single-channel Speech Enhancement with Conformer-based Metric GAN\\u00a7r\\n\\n\\u00a78\\u00a7oShiqi Zhang\\nZheng Qiu\\nDaiki Takeuchi\\nNoboru Harada\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.08252\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Feb 2024 06:47:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Mariotte et al. (§72024§r)', author: 'Théo Mariotte; Anthony Larcher; Silvio Montrésor; Jean-Hugh Thomas', display:{Lore:['[{"text": "arXiv:2402.08312", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChannel-Combination Algorithms for Robust Distant Voice Activity and Overlapped Speech Detection\\u00a7r\\n\\n\\u00a78\\u00a7oTh\\u00e9o Mariotte\\nAnthony Larcher\\nSilvio Montr\\u00e9sor\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.08312\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Feb 2024 09:16:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages, 5 figures, accepted at IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)\\u00a7r"}']}
{title:'Philip et al. (§72024§r)', author: 'Alexander Philip; Sanya Chawla; Lola Jover; George P. Kafentzis; Joe Brew; Vishakh Saraf; Shibu Vijayan; Peter Small; Carlos Chaccour', display:{Lore:['[{"text": "arXiv:2402.08789", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7bq-bio.QM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLeveraging cough sounds to optimize chest x-ray usage in low-resource settings\\u00a7r\\n\\n\\u00a78\\u00a7oAlexander Philip\\nSanya Chawla\\nLola Jover\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.08789\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 13 Feb 2024 20:54:55 GMT)\\u00a7r"}']}
{title:'Fan et al. (§72024§r)', author: 'Ruchao Fan; Natarajan Balaji Shanka; Abeer Alwan', display:{Lore:['[{"text": "arXiv:2402.08898", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUniEnc-CASSNAT: An Encoder-only Non-autoregressive ASR for Speech SSL Models\\u00a7r\\n\\n\\u00a78\\u00a7oRuchao Fan\\nNatarajan Balaji Shanka\\nAbeer Alwan\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.08898\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/LSP.2024.3365036\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Feb 2024 02:11:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in IEEESignal Processing Letters\\u00a7r"}']}
{title:'Ma et al. (§72024§r)', author: 'Fei Ma; Sipei Zhao; Ian S. Burnett', display:{Lore:['[{"text": "arXiv:2402.08904", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSound Field Reconstruction Using a Compact Acoustics-informed Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oFei Ma\\nSipei Zhao\\nIan S. Burnett\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.08904\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Feb 2024 02:32:18 GMT)\\u00a7r"}']}
{title:'Raj (§72024§r)', author: 'Desh Raj', display:{Lore:['[{"text": "arXiv:2402.08932", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lListening to Multi-talker Conversations: Modular and End-to-end Perspectives\\u00a7r\\n\\n\\u00a78\\u00a7oDesh Raj\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.08932\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Feb 2024 04:21:11 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPh.D. dissertation\\u00a7r"}']}
{title:'Marinoni et al. (§72024§r)', author: 'Christian Marinoni; Riccardo Fosco Gramaccioni; Changan Chen; Aurelio Uncini; Danilo Comminiello', display:{Lore:['[{"text": "arXiv:2402.09245", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOverview of the L3DAS23 Challenge on Audio-Visual Extended Reality\\u00a7r\\n\\n\\u00a78\\u00a7oChristian Marinoni\\nRiccardo Fosco Gramaccioni\\nChangan Chen\\nAurelio Uncini\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.09245\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Feb 2024 15:34:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to 2023 IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP 2023)\\u00a7r"}']}
{title:'Wang (§72024§r)', author: 'Zhong-Qiu Wang', display:{Lore:['[{"text": "arXiv:2402.09313", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMixture to Mixture: Leveraging Close-talk Mixtures as Weak-supervision for Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oZhong-Qiu Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.09313\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Feb 2024 17:03:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oin submission\\u00a7r"}']}
{title:'Ji et al. (§72024§r)', author: 'Shengpeng Ji; Ziyue Jiang; Hanting Wang; Jialong Zuo; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2402.09378", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oShengpeng Ji\\nZiyue Jiang\\nHanting Wang\\nJialong Zuo\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.09378\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 14 Feb 2024 18:24:41 GMT)\\u00a7r"}']}
{title:'Lemercier et al. (§72024§r)', author: 'Jean-Marie Lemercier; Julius Richter; Simon Welker; Eloi Moliner; Vesa Välimäki; Timo Gerkmann', display:{Lore:['[{"text": "arXiv:2402.09821", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffusion Models for Audio Restoration\\u00a7r\\n\\n\\u00a78\\u00a7oJean-Marie Lemercier\\nJulius Richter\\nSimon Welker\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.09821\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 15 Feb 2024 09:36:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oFull paper invited to the IEEE Signal Processing Magazine Special Issue \\"Model-based and Data-Driven Audio Signal Processing\\"\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Xiangyu Zhang; Daijiao Liu; Hexin Liu; Qiquan Zhang; Hanyu Meng; Leibny Paola Garcia; Eng Siong Chng; Lina Yao', display:{Lore:['[{"text": "arXiv:2402.10642", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaking in Wavelet Domain: A Simple and Efficient Approach to Speed up Speech Diffusion Model\\u00a7r\\n\\n\\u00a78\\u00a7oXiangyu Zhang\\nDaijiao Liu\\nHexin Liu\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.10642\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 16 Feb 2024 12:43:01 GMT)\\u00a7r"}']}
{title:'Santo et al. (§72024§r)', author: 'Gloria Dal Santo; Karolina Prawda; Sebastian J. Schlecht; Vesa Välimäki', display:{Lore:['[{"text": "arXiv:2402.11216", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFeedback Delay Network Optimization\\u00a7r\\n\\n\\u00a78\\u00a7oGloria Dal Santo\\nKarolina Prawda\\nSebastian J. Schlecht\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.11216\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 17 Feb 2024 07:52:21 GMT)\\u00a7r"}']}
{title:'Zotter et al. (§72024§r)', author: 'Franz Zotter; Stefan Riedel; Lukas Gölles; Matthias Frank', display:{Lore:['[{"text": "arXiv:2402.11330", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiffuse Sound Field Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oFranz Zotter\\nStefan Riedel\\nLukas G\\u00f6lles\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.11330\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 21 Feb 2024 07:24:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o27 pages, 17 figures, submitted to acta acustica, including jan/feb 2024 upgrades while awaitingthe reviews\\u00a7r"}']}
{title:'Lashkarashvili et al. (§72024§r)', author: 'Nineli Lashkarashvili; Wen Wu; Guangzhi Sun; Philip C. Woodland', display:{Lore:['[{"text": "arXiv:2402.11747", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lParameter Efficient Finetuning for Speech Emotion Recognition and Domain Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oNineli Lashkarashvili\\nWen Wu\\nGuangzhi Sun\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.11747\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP48485.2024.10446272\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2024 - 2024 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP), Seoul, Korea, Republic of, 2024, pp.\\n  10986-10990\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Feb 2024 00:21:07 GMT)\\u00a7r"}']}
{title:'Umesh et al. (§72024§r)', author: 'Srinivasan Umesh; Leon Cohen; Douglas Nelson', display:{Lore:['[{"text": "arXiv:2402.12094", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the relationship between speech and hearing\\u00a7r\\n\\n\\u00a78\\u00a7oSrinivasan Umesh\\nLeon Cohen\\nDouglas Nelson\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.12094\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Feb 2024 12:20:56 GMT)\\u00a7r"}']}
{title:'Ji et al. (§72024§r)', author: 'Shengpeng Ji; Minghui Fang; Ziyue Jiang; Siqi Zheng; Qian Chen; Rongjie Huang; Jialung Zuo; Shulei Wang; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2402.12208", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLanguage-Codec: Reducing the Gaps Between Discrete Codec Representation and Speech Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oShengpeng Ji\\nMinghui Fang\\nZiyue Jiang\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.12208\\u00a7r\\n\\nVersion:\\u00a77v3 (Sat, 27 Apr 2024 15:17:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oWe release a more powerful checkpoint in Language-Codec v3\\u00a7r"}']}
{title:'Chen et al. (§72024§r)', author: 'Haolin Chen; Philip N. Garner', display:{Lore:['[{"text": "arXiv:2402.12220", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting\\u00a7r\\n\\n\\u00a78\\u00a7oHaolin Chen\\nPhilip N. Garner\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.12220\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 19 Feb 2024 15:26:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\\u00a7r"}']}
{title:'Chen et al. (§72024§r)', author: 'Yanan Chen; Zihao Cui; Yingying Gao; Junlan Feng; Chao Deng; Shilei Zhang', display:{Lore:['[{"text": "arXiv:2402.12746", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPlugin Speech Enhancement: A Universal Speech Enhancement Framework Inspired by Dynamic Neural Network\\u00a7r\\n\\n\\u00a78\\u00a7oYanan Chen\\nZihao Cui\\nYingying Gao\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.12746\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Feb 2024 06:24:38 GMT)\\u00a7r"}']}
{title:'Wu et al. (§72024§r)', author: 'Haibin Wu; Huang-Cheng Chou; Kai-Wei Chang; Lucas Goncalves; Jiawei Du; Jyh-Shing Roger Jang; Chi-Chun Lee; Hung-Yi Lee', display:{Lore:['[{"text": "arXiv:2402.13018", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEMO-SUPERB: An In-depth Look at Speech Emotion Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHaibin Wu\\nHuang-Cheng Chou\\nKai-Wei Chang\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.13018\\u00a7r\\n\\nVersion:\\u00a77v4 (Tue, 12 Mar 2024 07:13:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7owebpage: https://emosuperb.github.io/\\u00a7r"}']}
{title:'Wu et al. (§72024§r)', author: 'Haibin Wu; Ho-Lam Chung; Yi-Cheng Lin; Yuan-Kuei Wu; Xuanjun Chen; Yu-Chi Pai; Hsiu-Hsuan Wang; Kai-Wei Chang; Alexander H. Liu; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2402.13071", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCodec-SUPERB: An In-Depth Analysis of Sound Codec Models\\u00a7r\\n\\n\\u00a78\\u00a7oHaibin Wu\\nHo-Lam Chung\\nYi-Cheng Lin\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.13071\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Feb 2024 15:13:38 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oGithub:https://github.com/voidful/Codec-SUPERB\\u00a7r"}']}
{title:'Peng et al. (§72024§r)', author: 'Junyi Peng; Marc Delcroix; Tsubasa Ochiai; Oldrich Plchot; Shoko Araki; Jan Cernocky', display:{Lore:['[{"text": "arXiv:2402.13199", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTarget Speech Extraction with Pre-trained Self-supervised Learning Models\\u00a7r\\n\\n\\u00a78\\u00a7oJunyi Peng\\nMarc Delcroix\\nTsubasa Ochiai\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.13199\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 17 Feb 2024 13:45:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Peng et al. (§72024§r)', author: 'Junyi Peng; Marc Delcroix; Tsubasa Ochiai; Oldrich Plchot; Takanori Ashihara; Shoko Araki; Jan Cernocky', display:{Lore:['[{"text": "arXiv:2402.13200", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lProbing Self-supervised Learning Models with Target Speech Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oJunyi Peng\\nMarc Delcroix\\nTsubasa Ochiai\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.13200\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 17 Feb 2024 13:37:22 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024, Self-supervision in Audio, Speech, and Beyond (SASB) workshop\\u00a7r"}']}
{title:'Wu et al. (§72024§r)', author: 'Haibin Wu; Xuanjun Chen; Yi-Cheng Lin; Kai-wei Chang; Ho-Lam Chung; Alexander H. Liu; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2402.13236", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards audio language modeling \\u2013 an overview\\u00a7r\\n\\n\\u00a78\\u00a7oHaibin Wu\\nXuanjun Chen\\nYi-Cheng Lin\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.13236\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 20 Feb 2024 18:50:25 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Xiangyu Zhang; Hexin Liu; Kaishuai Xu; Qiquan Zhang; Daijiao Liu; Beena Ahmed; Julien Epps', display:{Lore:['[{"text": "arXiv:2402.13276", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhen LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection\\u00a7r\\n\\n\\u00a78\\u00a7oXiangyu Zhang\\nHexin Liu\\nKaishuai Xu\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.13276\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 17 Feb 2024 09:39:46 GMT)\\u00a7r"}']}
{title:'Zhou et al. (§72024§r)', author: 'Rui Zhou; Xian Li; Ying Fang; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2402.13511", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMel-FullSubNet: Mel-Spectrogram Enhancement for Improving Both Speech Quality and ASR\\u00a7r\\n\\n\\u00a78\\u00a7oRui Zhou\\nXian Li\\nYing Fang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.13511\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 22 Feb 2024 02:33:22 GMT)\\u00a7r"}']}
{title:'Miotello et al. (§72024§r)', author: 'Federico Miotello; Paolo Ostan; Mirco Pezzoli; Luca Comanducci; Alberto Bernardini; Fabio Antonacci; Augusto Sarti', display:{Lore:['[{"text": "arXiv:2402.13896", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHOMULA-RIR: A Room Impulse Response Dataset for Teleconferencing and Spatial Audio Applications Acquired Through Higher-Order Microphones and Uniform Linear Microphone Arrays\\u00a7r\\n\\n\\u00a78\\u00a7oFederico Miotello\\nPaolo Ostan\\nMirco Pezzoli\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.13896\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 21 Feb 2024 16:09:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication at ICASSP 2024 - HSCMA Workshop\\u00a7r"}']}
{title:'Zhao et al. (§72024§r)', author: 'Changjiang Zhao; Shulin He; Xueliang Zhang', display:{Lore:['[{"text": "arXiv:2402.14225", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSICRN: Advancing Speech Enhancement through State Space Model and Inplace Convolution Techniques\\u00a7r\\n\\n\\u00a78\\u00a7oChangjiang Zhao\\nShulin He\\nXueliang Zhang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.14225\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Feb 2024 02:08:35 GMT)\\u00a7r"}']}
{title:'Hono et al. (§72024§r)', author: 'Yukiya Hono; Kei Hashimoto; Yoshihiko Nankaku; Keiichi Tokuda', display:{Lore:['[{"text": "arXiv:2402.14692", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPeriodGrad: Towards Pitch-Controllable Neural Vocoder Based on a Diffusion Probabilistic Model\\u00a7r\\n\\n\\u00a78\\u00a7oYukiya Hono\\nKei Hashimoto\\nYoshihiko Nankaku\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.14692\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 22 Feb 2024 16:47:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures, To appear in ICASSP 2024. Audio samples: https://www.sp.nitech.ac.jp/ hono/demos/icassp2024/\\u00a7r"}']}
{title:'Singh et al. (§72024§r)', author: 'Vishwanath Pratap Singh; Md Sahidullah; Tomi Kinnunen', display:{Lore:['[{"text": "arXiv:2402.15214", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lChildAugment: Data Augmentation Methods for Zero-Resource Children\'s Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oVishwanath Pratap Singh\\nMd Sahidullah\\nTomi Kinnunen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.15214\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Feb 2024 09:19:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe following article has been accepted by The Journal of the Acoustical Society of America (JASA). After it is published, it will be found at https://pubs.aip.org/asa/jasa\\u00a7r"}']}
{title:'Riley et al. (§72024§r)', author: 'Xavier Riley; Drew Edwards; Simon Dixon', display:{Lore:['[{"text": "arXiv:2402.15258", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHigh Resolution Guitar Transcription via Domain Adaptation\\u00a7r\\n\\n\\u00a78\\u00a7oXavier Riley\\nDrew Edwards\\nSimon Dixon\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.15258\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Feb 2024 10:56:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Lee et al. (§72024§r)', author: 'Seonwoo Lee; Jihyun Mun; Sunhee Kim; Minhwa Chung', display:{Lore:['[{"text": "arXiv:2402.15539", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Corpus for Korean Children with Autism Spectrum Disorder: Towards Automatic Assessment Systems\\u00a7r\\n\\n\\u00a78\\u00a7oSeonwoo Lee\\nJihyun Mun\\nSunhee Kim\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.15539\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Feb 2024 07:32:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, Accepted for LREC-COLING 2024\\u00a7r"}']}
{title:'Cwitkowitz et al. (§72024§r)', author: 'Frank Cwitkowitz; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2402.15569", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lToward Fully Self-Supervised Multi-Pitch Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oFrank Cwitkowitz\\nZhiyao Duan\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.15569\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 23 Feb 2024 19:12:41 GMT)\\u00a7r"}']}
{title:'Ma et al. (§72024§r)', author: 'Duo Ma; Xianghu Yue; Junyi Ao; Xiaoxue Gao; Haizhou Li', display:{Lore:['[{"text": "arXiv:2402.15725", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lText-guided HuBERT: Self-Supervised Speech Pre-training via Generative Adversarial Networks\\u00a7r\\n\\n\\u00a78\\u00a7oDuo Ma\\nXianghu Yue\\nJunyi Ao\\nXiaoxue Gao\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.15725\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 28 Feb 2024 07:51:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figures,5 tables, submit to IEEE Signal Processing Letters(SPL)\\u00a7r"}']}
{title:'Zhao et al. (§72024§r)', author: 'Sipei Zhao; Fei Ma', display:{Lore:['[{"text": "arXiv:2402.15735", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA circular microphone array with virtual microphones based on acoustics-informed neural networks\\u00a7r\\n\\n\\u00a78\\u00a7oSipei Zhao\\nFei Ma\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.15735\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 24 Feb 2024 06:29:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to JASA on 24/02/2024\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Chunxi Wang; Maoshen Jia; Meiran Li; Changchun Bao; Wenyu Jin', display:{Lore:['[{"text": "arXiv:2402.16003", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring the Power of Pure Attention Mechanisms in Blind Room Parameter Estimation\\u00a7r\\n\\n\\u00a78\\u00a7oChunxi Wang\\nMaoshen Jia\\nMeiran Li\\nChangchun Bao\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.16003\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 Apr 2024 14:43:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o28 pages, 9 figures, accepted for publishing to EURASIP Journal On Audio Speech And Music Processing\\u00a7r"}']}
{title:'Gunduz et al. (§72024§r)', author: 'Ahmet Gunduz; Kamer Ali Yuksel; Kareem Darwish; Golara Javadi; Fabio Minazzi; Nicola Sobieski; Sebastien Bratieres', display:{Lore:['[{"text": "arXiv:2402.16380", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Automated End-to-End Open-Source Software for High-Quality Text-to-Speech Dataset Generation\\u00a7r\\n\\n\\u00a78\\u00a7oAhmet Gunduz\\nKamer Ali Yuksel\\nKareem Darwish\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.16380\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Feb 2024 07:58:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 Pages, 6 Figures, 4 Tables, LREC-COLING 2024\\u00a7r"}']}
{title:'Hussain et al. (§72024§r)', author: 'Tassadaq Hussain; Kia Dashtipour; Yu Tsao; Amir Hussain', display:{Lore:['[{"text": "arXiv:2402.16394", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-Visual Speech Enhancement in Noisy Environments via Emotion-Based Contextual Cues\\u00a7r\\n\\n\\u00a78\\u00a7oTassadaq Hussain\\nKia Dashtipour\\nYu Tsao\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.16394\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Feb 2024 08:38:32 GMT)\\u00a7r"}']}
{title:'Zampierin et al. (§72024§r)', author: 'Luca Zampierin; Ghouthi Boukli Hacene; Bac Nguyen; Mirco Ravanelli', display:{Lore:['[{"text": "arXiv:2402.16830", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSKILL: Similarity-aware Knowledge distILLation for Speech Self-Supervised Learning\\u00a7r\\n\\n\\u00a78\\u00a7oLuca Zampierin\\nGhouthi Boukli Hacene\\nBac Nguyen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.16830\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 26 Feb 2024 18:56:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at the Self-supervision in Audio, Speech and Beyond (SASB) Workshop at ICASSP 2024\\u00a7r"}']}
{title:'Yang et al. (§72024§r)', author: 'Xue Yang; Changchun Bao; Jing Zhou; Xianhong Chen', display:{Lore:['[{"text": "arXiv:2402.17146", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTarget Speaker Extraction by Directly Exploiting Contextual Information in the Time-Frequency Domain\\u00a7r\\n\\n\\u00a78\\u00a7oXue Yang\\nChangchun Bao\\nJing Zhou\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.17146\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Feb 2024 02:18:55 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP 2024\\u00a7r"}']}
{title:'Gayer et al. (§72024§r)', author: 'Yhonatan Gayer; Vladimir Tourbabin; Zamir Ben-Hur; Jacob Donley; Boaz Rafaely', display:{Lore:['[{"text": "arXiv:2402.17362", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAmbisonics Encoding For Arbitrary Microphone Arrays Incorporating Residual Channels For Binaural Reproduction\\u00a7r\\n\\n\\u00a78\\u00a7oYhonatan Gayer\\nVladimir Tourbabin\\nZamir Ben-Hur\\nJacob Donley\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.17362\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Feb 2024 09:54:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for presentation at HSCMA 2024\\u00a7r"}']}
{title:'Ma et al. (§72024§r)', author: 'Hao Ma; Zhiyuan Peng; Xu Li; Mingjie Shao; Xixin Wu; Ju Liu', display:{Lore:['[{"text": "arXiv:2402.17455", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCLAPSep: Leveraging Contrastive Pre-trained Model for Multi-Modal Query-Conditioned Target Sound Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oHao Ma\\nZhiyuan Peng\\nXu Li\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.17455\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 8 May 2024 05:07:29 GMT)\\u00a7r"}']}
{title:'Venkatesh et al. (§72024§r)', author: 'Satvik Venkatesh; Arthur Benilov; Philip Coleman; Frederic Roskam', display:{Lore:['[{"text": "arXiv:2402.17701", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-time Low-latency Music Source Separation using Hybrid Spectrogram-TasNet\\u00a7r\\n\\n\\u00a78\\u00a7oSatvik Venkatesh\\nArthur Benilov\\nPhilip Coleman\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.17701\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Feb 2024 17:26:33 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Churchwell et al. (§72024§r)', author: 'Cameron Churchwell; Max Morrison; Bryan Pardo', display:{Lore:['[{"text": "arXiv:2402.17735", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHigh-Fidelity Neural Phonetic Posteriorgrams\\u00a7r\\n\\n\\u00a78\\u00a7oCameron Churchwell\\nMax Morrison\\nBryan Pardo\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.17735\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Feb 2024 18:12:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024 Workshop on Explainable Machine Learning for Speech and Audio\\u00a7r"}']}
{title:'Masuyama et al. (§72024§r)', author: 'Yoshiki Masuyama; Gordon Wichern; François G. Germain; Zexu Pan; Sameer Khurana; Chiori Hori; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2402.17907", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNIIRF: Neural IIR Filter Field for HRTF Upsampling and Personalization\\u00a7r\\n\\n\\u00a78\\u00a7oYoshiki Masuyama\\nGordon Wichern\\nFran\\u00e7ois G. Germain\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.17907\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 27 Feb 2024 21:48:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Jeon et al. (§72024§r)', author: 'Chang-Bin Jeon; Gordon Wichern; François G. Germain; Jonathan Le Roux', display:{Lore:['[{"text": "arXiv:2402.18407", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhy does music source separation benefit from cacophony?\\u00a7r\\n\\n\\u00a78\\u00a7oChang-Bin Jeon\\nGordon Wichern\\nFran\\u00e7ois G. Germain\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.18407\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 28 Feb 2024 15:27:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP 2024 Workshop on Explainable AI for Speech and Audio\\u00a7r"}']}
{title:'Saeki et al. (§72024§r)', author: 'Takaaki Saeki; Gary Wang; Nobuyuki Morioka; Isaac Elias; Kyle Kastner; Andrew Rosenberg; Bhuvana Ramabhadran; Heiga Zen; Françoise Beaufays; Hadar Shemtov', display:{Lore:['[{"text": "arXiv:2402.18932", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExtending Multilingual Speech Synthesis to 100+ Languages without Transcribed Data\\u00a7r\\n\\n\\u00a78\\u00a7oTakaaki Saeki\\nGary Wang\\nNobuyuki Morioka\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.18932\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 29 Feb 2024 07:49:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in ICASSP 2024\\u00a7r"}']}
{title:'Shaybet et al. (§72024§r)', author: 'Bar Shaybet; Anurag Kumar; Vladimir Tourbabin; Boaz Rafaely', display:{Lore:['[{"text": "arXiv:2402.18968", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAmbisonics Networks \\u2013 The Effect Of Radial Functions Regularization\\u00a7r\\n\\n\\u00a78\\u00a7oBar Shaybet\\nAnurag Kumar\\nVladimir Tourbabin\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.18968\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 29 Feb 2024 09:12:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oto be published in Icassp 2024\\u00a7r"}']}
{title:'Oncescu et al. (§72024§r)', author: 'Andreea-Maria Oncescu; João F. Henriques; Andrew Zisserman; Samuel Albanie; A. Sophia Koepke', display:{Lore:['[{"text": "arXiv:2402.19106", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA SOUND APPROACH: Using Large Language Models to generate audio descriptions for egocentric text-audio retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oAndreea-Maria Oncescu\\nJo\\u00e3o F. Henriques\\nAndrew Zisserman\\nSamuel Albanie\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2402.19106\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 29 Feb 2024 12:38:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o9 pages, 2 figures, 9 tables, Accepted at ICASSP 2024\\u00a7r"}']}
{title:'Sang et al. (§72024§r)', author: 'Mufan Sang; John H. L. Hansen', display:{Lore:['[{"text": "arXiv:2403.00293", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Adapter Tuning of Pre-trained Speech Models for Automatic Speaker Verification\\u00a7r\\n\\n\\u00a78\\u00a7oMufan Sang\\nJohn H. L. Hansen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.00293\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Mar 2024 05:32:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Nguyen et al. (§72024§r)', author: 'Tin Nguyen; Lam Pham; Phat Lam; Dat Ngo; Hieu Tang; Alexander Schindler', display:{Lore:['[{"text": "arXiv:2403.00379", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Impact of Frequency Bands on Acoustic Anomaly Detection of Machines using Deep Learning Based Model\\u00a7r\\n\\n\\u00a78\\u00a7oTin Nguyen\\nLam Pham\\nPhat Lam\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.00379\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Mar 2024 09:05:55 GMT)\\u00a7r"}']}
{title:'R et al. (§72024§r)', author: 'Aron R; Indra Sigicharla; Chirag Periwal; Mohanaprasad K; Nithya Darisini P S; Sourabh Tiwari; Shivani Arora', display:{Lore:['[{"text": "arXiv:2403.00887", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSEGAA: A Unified Approach to Predicting Age, Gender, and Emotion in Speech\\u00a7r\\n\\n\\u00a78\\u00a7oAron R\\nIndra Sigicharla\\nChirag Periwal\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.00887\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 1 Mar 2024 11:28:37 GMT)\\u00a7r"}']}
{title:'Lee (§72024§r)', author: 'Shih-Kuang Lee', display:{Lore:['[{"text": "arXiv:2403.01130", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lArbitrary Discrete Fourier Analysis and Its Application in Replayed Speech Detection\\u00a7r\\n\\n\\u00a78\\u00a7oShih-Kuang Lee\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.01130\\u00a7r\\n\\nVersion:\\u00a77v2 (Sat, 23 Mar 2024 10:42:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7ohttps://github.com/shihkuanglee/ADFA\\u00a7r"}']}
{title:'Shim et al. (§72024§r)', author: 'Hye-jin Shim; Jee-weon Jung; Tomi Kinnunen; Nicholas Evans; Jean-Francois Bonastre; Itshak Lapidot', display:{Lore:['[{"text": "arXiv:2403.01355", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7la-DCF: an architecture agnostic metric with application to spoofing-robust speaker verification\\u00a7r\\n\\n\\u00a78\\u00a7oHye-jin Shim\\nJee-weon Jung\\nTomi Kinnunen\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.01355\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 3 Mar 2024 00:58:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, submitted to Speaker Odyssey 2024\\u00a7r"}']}
{title:'Shankar et al. (§72024§r)', author: 'Ravi Shankar; Ke Tan; Buye Xu; Anurag Kumar', display:{Lore:['[{"text": "arXiv:2403.01369", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Closer Look at Wav2Vec2 Embeddings for On-Device Single-Channel Speech Enhancement\\u00a7r\\n\\n\\u00a78\\u00a7oRavi Shankar\\nKe Tan\\nBuye Xu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.01369\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 3 Mar 2024 02:05:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages; Shorter form accepted in ICASSP 2024\\u00a7r"}']}
{title:'Qi et al. (§72024§r)', author: 'Tianhua Qi; Wenming Zheng; Cheng Lu; Yuan Zong; Hailun Lian', display:{Lore:['[{"text": "arXiv:2403.01494", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPAVITS: Exploring Prosody-aware VITS for End-to-End Emotional Voice Conversion\\u00a7r\\n\\n\\u00a78\\u00a7oTianhua Qi\\nWenming Zheng\\nCheng Lu\\nYuan Zong\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.01494\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 3 Mar 2024 12:07:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP2024\\u00a7r"}']}
{title:'Yasuda et al. (§72024§r)', author: 'Masahiro Yasuda; Shoichiro Saito; Akira Nakayama; Noboru Harada', display:{Lore:['[{"text": "arXiv:2403.01670", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l6DoF SELD: Sound Event Localization and Detection Using Microphones and Motion Tracking Sensors on self-motioning human\\u00a7r\\n\\n\\u00a78\\u00a7oMasahiro Yasuda\\nShoichiro Saito\\nAkira Nakayama\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.01670\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Mar 2024 01:45:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICASSP2024 accepted\\u00a7r"}']}
{title:'Gómez-Zaragozá et al. (§72024§r)', author: 'Lucía Gómez-Zaragozá; Óscar Valls; Rocío del Amor; María José Castro-Bleda; Valery Naranjo; Mariano Alcañiz Raya; Javier Marín-Morales', display:{Lore:['[{"text": "arXiv:2403.02167", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech emotion recognition from voice messages recorded in the wild\\u00a7r\\n\\n\\u00a78\\u00a7oLuc\\u00eda G\\u00f3mez-Zaragoz\\u00e1\\n\\u00d3scar Valls\\nRoc\\u00edo del Amor\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.02167\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Mar 2024 16:13:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\\u00a7r"}']}
{title:'Kalda et al. (§72024§r)', author: 'Joonas Kalda; Clément Pagés; Ricard Marxer; Tanel Alumäe; Hervé Bredin', display:{Lore:['[{"text": "arXiv:2403.02288", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPixIT: Joint Training of Speaker Diarization and Speech Separation from Real-world Multi-speaker Recordings\\u00a7r\\n\\n\\u00a78\\u00a7oJoonas Kalda\\nCl\\u00e9ment Pag\\u00e9s\\nRicard Marxer\\nTanel Alum\\u00e4e\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.02288\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 4 Mar 2024 18:18:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to Speaker Odyssey 2024\\u00a7r"}']}
{title:'Mendes-Laureano et al. (§72024§r)', author: 'Janaína Mendes-Laureano; Jorge A. Gómez-García; Alejandro Guerrero-López; Elisa Luque-Buzo; Julián D. Arias-Londoño; Francisco J. Grandas-Pérez; Juan I. Godino-Llorente', display:{Lore:['[{"text": "arXiv:2403.02371", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeuroVoz: a Castillian Spanish corpus of parkinsonian speech\\u00a7r\\n\\n\\u00a78\\u00a7oJana\\u00edna Mendes-Laureano\\nJorge A. G\\u00f3mez-Garc\\u00eda\\nAlejandro Guerrero-L\\u00f3pez\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.02371\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 6 Mar 2024 11:08:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPreprint version\\u00a7r"}']}
{title:'Ju et al. (§72024§r)', author: 'Zeqian Ju; Yuancheng Wang; Kai Shen; Xu Tan; Detai Xin; Dongchao Yang; Yanqing Liu; Yichong Leng; Kaitao Song; Siliang Tang; Zhizheng Wu; Tao Qin; Xiang-Yang Li; Wei Ye; Shikun Zhang; Jiang Bian; Lei He; Jinyu Li; Sheng Zhao', display:{Lore:['[{"text": "arXiv:2403.03100", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oZeqian Ju\\nYuancheng Wang\\nKai Shen\\n+ 15 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.03100\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 23 Apr 2024 08:38:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAchieving human-level quality andnaturalness on multi-speaker datasets(e.g., LibriSpeech) in a zero-shot way\\u00a7r"}']}
{title:'Phan (§72024§r)', author: 'Dang Thoai Phan', display:{Lore:['[{"text": "arXiv:2403.03611", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lComparison Performance of Spectrogram and Scalogram as Input of Acoustic Recognition Task\\u00a7r\\n\\n\\u00a78\\u00a7oDang Thoai Phan\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.03611\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 26 Apr 2024 07:06:26 GMT)\\u00a7r"}']}
{title:'Mokrý et al. (§72024§r)', author: 'Ondřej Mokrý; Pavel Rajmic', display:{Lore:['[{"text": "arXiv:2403.04433", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn the Use of Autoregressive Methods for Audio Inpainting\\u00a7r\\n\\n\\u00a78\\u00a7oOnd\\u0159ej Mokr\\u00fd\\nPavel Rajmic\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.04433\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Mar 2024 12:06:19 GMT)\\u00a7r"}']}
{title:'Tang et al. (§72024§r)', author: 'Xiaoyu Tang; Yixin Lin; Ting Dang; Yuanfang Zhang; Jintao Cheng', display:{Lore:['[{"text": "arXiv:2403.04743", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoyu Tang\\nYixin Lin\\nTing Dang\\nYuanfang Zhang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.04743\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 7 Mar 2024 18:49:29 GMT)\\u00a7r"}']}
{title:'Easthope (§72024§r)', author: 'Eric Easthope', display:{Lore:['[{"text": "arXiv:2403.04800", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.GR\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l(Un)paired signal-to-signal translation with 1D conditional GANs\\u00a7r\\n\\n\\u00a78\\u00a7oEric Easthope\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.04800\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Mar 2024 18:52:50 GMT)\\u00a7r"}']}
{title:'Alexos et al. (§72024§r)', author: 'Antonios Alexos; Pierre Baldi', display:{Lore:['[{"text": "arXiv:2403.04804", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.MM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttentionStitch: How Attention Solves the Speech Editing Problem\\u00a7r\\n\\n\\u00a78\\u00a7oAntonios Alexos\\nPierre Baldi\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.04804\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 5 Mar 2024 22:09:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted in Machine Learning for Audio workship in NeurIPS 2023\\u00a7r"}']}
{title:'Weng et al. (§72024§r)', author: 'Zhenzi Weng; Zhijin Qin', display:{Lore:['[{"text": "arXiv:2403.05187", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Semantic Communications for Speech Transmission\\u00a7r\\n\\n\\u00a78\\u00a7oZhenzi Weng\\nZhijin Qin\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.05187\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 25 Apr 2024 12:40:32 GMT)\\u00a7r"}']}
{title:'Tokala et al. (§72024§r)', author: 'Vikas Tokala; Eric Grinstein; Mike Brookes; Simon Doclo; Jesper Jensen; Patrick A. Naylor', display:{Lore:['[{"text": "arXiv:2403.05393", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBinaural Speech Enhancement Using Deep Complex Convolutional Transformer Networks\\u00a7r\\n\\n\\u00a78\\u00a7oVikas Tokala\\nEric Grinstein\\nMike Brookes\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.05393\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Mar 2024 15:42:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Chengjie Zhang; Jiang Wang; He Kong', display:{Lore:['[{"text": "arXiv:2403.05791", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAsynchronous Microphone Array Calibration using Hybrid TDOA Information\\u00a7r\\n\\n\\u00a78\\u00a7oChengjie Zhang\\nJiang Wang\\nHe Kong\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.05791\\u00a7r\\n\\nVersion:\\u00a77v3 (Tue, 19 Mar 2024 12:22:04 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Hexin Liu; Xiangyu Zhang; Leibny Paola Garcia; Andy W. H. Khong; Eng Siong Chng; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2403.05887", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAligning Speech to Languages to Enhance Code-switching Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oHexin Liu\\nXiangyu Zhang\\nLeibny Paola Garcia\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.05887\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 9 Mar 2024 11:59:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oManuscript submitted to IEEE/ACM Transactions on Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Jansen et al. (§72024§r)', author: 'Wouter Jansen; Jan Steckel', display:{Lore:['[{"text": "arXiv:2403.06847", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSonoTraceLab \\u2013 A Raytracing-Based Acoustic Modelling System for Simulating Echolocation Behavior of Bats\\u00a7r\\n\\n\\u00a78\\u00a7oWouter Jansen\\nJan Steckel\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.06847\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Mar 2024 16:04:18 GMT)\\u00a7r"}']}
{title:'Eliav et al. (§72024§r)', author: 'Amit Eliav; Sharon Gannot', display:{Lore:['[{"text": "arXiv:2403.06856", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConcurrent Speaker Detection: A multi-microphone Transformer-Based Approach\\u00a7r\\n\\n\\u00a78\\u00a7oAmit Eliav\\nSharon Gannot\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.06856\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Mar 2024 16:12:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 6 tables, 2 figures\\u00a7r"}']}
{title:'Arbel et al. (§72024§r)', author: 'Lior Arbel; Ishwarya Ananthabhotla; Zamir Ben-Hur; David Lou Alon; Boaz Rafaely', display:{Lore:['[{"text": "arXiv:2403.07579", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lOn HRTF Notch Frequency Prediction Using Anthropometric Features and Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oLior Arbel\\nIshwarya Ananthabhotla\\nZamir Ben-Hur\\nDavid Lou Alon\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.07579\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Mar 2024 12:07:56 GMT)\\u00a7r"}']}
{title:'Koutsogiannaki et al. (§72024§r)', author: 'Maria Koutsogiannaki; Shafel Mc Dowall; Ioannis Agiomyrgiannakis', display:{Lore:['[{"text": "arXiv:2403.07661", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGender-ambiguous voice generation through feminine speaking style transfer in male voices\\u00a7r\\n\\n\\u00a78\\u00a7oMaria Koutsogiannaki\\nShafel Mc Dowall\\nIoannis Agiomyrgiannakis\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.07661\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 15 Mar 2024 13:21:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7osubmitted to Interspeech\\u00a7r"}']}
{title:'Pešán et al. (§72024§r)', author: "Jan Pešán; Santosh Kesiraju; Lukáš Burget; Jan ''Honza'' Černocký", display:{Lore:['[{"text": "arXiv:2403.07767", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBeyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech Recognition Datasets\\u00a7r\\n\\n\\u00a78\\u00a7oJan Pe\\u0161\\u00e1n\\nSantosh Kesiraju\\nLuk\\u00e1\\u0161 Burget\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.07767\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Mar 2024 15:54:32 GMT)\\u00a7r"}']}
{title:'Shah et al. (§72024§r)', author: 'Muhammad A. Shah; David Solans Noguero; Mikko A. Heikkila; Nicolas Kourtellis', display:{Lore:['[{"text": "arXiv:2403.07937", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Robust Bench: A Robustness Benchmark For Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oMuhammad A. Shah\\nDavid Solans Noguero\\nMikko A. Heikkila\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.07937\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 8 Mar 2024 08:10:29 GMT)\\u00a7r"}']}
{title:'Phaladi et al. (§72024§r)', author: 'Amanda Phaladi; Thipe Modipa', display:{Lore:['[{"text": "arXiv:2403.07947", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe evaluation of a code-switched Sepedi-English automatic speech recognition system\\u00a7r\\n\\n\\u00a78\\u00a7oAmanda Phaladi\\nThipe Modipa\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.07947\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 11 Mar 2024 15:11:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages,2 figures,2nd International Conference on NLP AI (NLPAI 2024)\\u00a7r"}']}
{title:'Guimarães et al. (§72024§r)', author: 'Heitor R. Guimarães; Arthur Pimentel; Anderson R. Avila; Mehdi Rezagholizadeh; Boxing Chen; Tiago H. Falk', display:{Lore:['[{"text": "arXiv:2403.08654", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAn Efficient End-to-End Approach to Noise Invariant Speech Features via Multi-Task Learning\\u00a7r\\n\\n\\u00a78\\u00a7oHeitor R. Guimar\\u00e3es\\nArthur Pimentel\\nAnderson R. Avila\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.08654\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 13 Mar 2024 16:08:59 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oUnder review on IEEE Transactions on Audio, Speech, and Language Processing (2024)\\u00a7r"}']}
{title:'Olivieri et al. (§72024§r)', author: 'Marco Olivieri; Xenofon Karakonstantis; Mirco Pezzoli; Fabio Antonacci; Augusto Sarti; Efren Fernandez-Grande', display:{Lore:['[{"text": "arXiv:2403.09524", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPhysics-Informed Neural Network for Volumetric Sound field Reconstruction of Speech Signals\\u00a7r\\n\\n\\u00a78\\u00a7oMarco Olivieri\\nXenofon Karakonstantis\\nMirco Pezzoli\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.09524\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 23 Apr 2024 11:34:22 GMT)\\u00a7r"}']}
{title:'Liang et al. (§72024§r)', author: 'Jinhua Liang; Huan Zhang; Haohe Liu; Yin Cao; Qiuqiang Kong; Xubo Liu; Wenwu Wang; Mark D. Plumbley; Huy Phan; Emmanouil Benetos', display:{Lore:['[{"text": "arXiv:2403.09527", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWavCraft: Audio Editing and Generation with Large Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oJinhua Liang\\nHuan Zhang\\nHaohe Liu\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.09527\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 10 May 2024 07:54:17 GMT)\\u00a7r"}']}
{title:'Shu et al. (§72024§r)', author: 'Nicolas Shu; David V. Anderson', display:{Lore:['[{"text": "arXiv:2403.09789", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudiosockets: A Python socket package for Real-Time Audio Processing\\u00a7r\\n\\n\\u00a78\\u00a7oNicolas Shu\\nDavid V. Anderson\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.09789\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Mar 2024 18:19:29 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 2 figures\\u00a7r"}']}
{title:'Wang (§72024§r)', author: 'Zhong-Qiu Wang', display:{Lore:['[{"text": "arXiv:2403.10271", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSuperME: Supervised and Mixture-to-Mixture Co-Learning for Speech Enhancement and Robust ASR\\u00a7r\\n\\n\\u00a78\\u00a7oZhong-Qiu Wang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.10271\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Mar 2024 13:03:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oin submission\\u00a7r"}']}
{title:'Leer et al. (§72024§r)', author: 'Peter Leer; Jesper Jensen; Laurel Carney; Zheng-Hua Tan; Jan Østergaard; Lars Bramsløw', display:{Lore:['[{"text": "arXiv:2403.10420", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNeural Networks Hear You Loud And Clear: Hearing Loss Compensation Using Deep Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Leer\\nJesper Jensen\\nLaurel Carney\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.10420\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Mar 2024 15:55:19 GMT)\\u00a7r"}']}
{title:'Leer et al. (§72024§r)', author: 'Peter Leer; Jesper Jensen; Zheng-Hua Tan; Jan Østergaard; Lars Bramsløw', display:{Lore:['[{"text": "arXiv:2403.10428", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHow to train your ears: Auditory-model emulation for large-dynamic-range inputs and mild-to-severe hearing losses\\u00a7r\\n\\n\\u00a78\\u00a7oPeter Leer\\nJesper Jensen\\nZheng-Hua Tan\\nJan \\u00d8stergaard\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.10428\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2024.3378099\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 15 Mar 2024 16:00:27 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE/ACM Transactionson Audio, Speech and Language Processing. This version is the authors\' version and may vary from the final publication in details\\u00a7r"}']}
{title:'Chen et al. (§72024§r)', author: 'Ao Chen; Xin Zhang', display:{Lore:['[{"text": "arXiv:2403.10548", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTwo-sided Acoustic Metascreen for Broadband and Individual Reflection and Transmission Control\\u00a7r\\n\\n\\u00a78\\u00a7oAo Chen\\nXin Zhang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.10548\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 12 Mar 2024 16:46:57 GMT)\\u00a7r"}']}
{title:'Nguyen-Phuoc et al. (§72024§r)', author: 'Long Nguyen-Phuoc; Renald Gaboriau; Dimitri Delacroix; Laurent Navarro', display:{Lore:['[{"text": "arXiv:2403.10565", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r, \\u00a7bq-bio.NC\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lPTSD-MDNN : Fusion tardive de r\\u00e9seaux de neurones profonds multimodaux pour la d\\u00e9tection du trouble de stress post-traumatique\\u00a7r\\n\\n\\u00a78\\u00a7oLong Nguyen-Phuoc\\nRenald Gaboriau\\nDimitri Delacroix\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.10565\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Mar 2024 14:57:16 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oin French language. GRETSI 2023\\u00a7r"}']}
{title:'Tsubaki et al. (§72024§r)', author: 'Shunsuke Tsubaki; Daisuke Niizumi; Daiki Takeuchi; Yasunori Ohishi; Noboru Harada; Keisuke Imoto', display:{Lore:['[{"text": "arXiv:2403.10756", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRefining Knowledge Transfer on Audio-Image Temporal Agreement for Audio-Text Cross Retrieval\\u00a7r\\n\\n\\u00a78\\u00a7oShunsuke Tsubaki\\nDaisuke Niizumi\\nDaiki Takeuchi\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.10756\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 16 Mar 2024 01:38:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to EUSIPCO2024\\u00a7r"}']}
{title:'Murthy et al. (§72024§r)', author: 'Savitha Murthy; Dinkar Sitaram', display:{Lore:['[{"text": "arXiv:2403.10937", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInitial Decoding with Minimally Augmented Language Model for Improved Lattice Rescoring in Low Resource ASR\\u00a7r\\n\\n\\u00a78\\u00a7oSavitha Murthy\\nDinkar Sitaram\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.10937\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 16 Mar 2024 14:34:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o14 pages, 7 figures, Accepted in Sadhana Journal\\u00a7r"}']}
{title:'Fedorishin et al. (§72024§r)', author: 'Dennis Fedorishin; III Livio Forte; Philip Schneider; Srirangaraj Setlur; Venu Govindaraju', display:{Lore:['[{"text": "arXiv:2403.11037", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFine-Grained Engine Fault Sound Event Detection Using Multimodal Signals\\u00a7r\\n\\n\\u00a78\\u00a7oDennis Fedorishin\\nIII Livio Forte\\nPhilip Schneider\\nSrirangaraj Setlur\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.11037\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 16 Mar 2024 22:51:02 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Fujimura et al. (§72024§r)', author: 'Takuya Fujimura; Keisuke Imoto; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2403.11508", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDiscriminative Neighborhood Smoothing for Generative Anomalous Sound Detection\\u00a7r\\n\\n\\u00a78\\u00a7oTakuya Fujimura\\nKeisuke Imoto\\nTomoki Toda\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.11508\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Mar 2024 06:26:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to EUSIPCO 2024\\u00a7r"}']}
{title:'Eom et al. (§72024§r)', author: 'SooHwan Eom; Eunseop Yoon; Hee Suk Yoon; Chanwoo Kim; Mark Hasegawa-Johnson; Chang D. Yoo', display:{Lore:['[{"text": "arXiv:2403.11578", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaMER-CTC: Connectionist Temporal Classification with Adaptive Maximum Entropy Regularization for Automatic Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oSooHwan Eom\\nEunseop Yoon\\nHee Suk Yoon\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.11578\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Mar 2024 08:53:04 GMT)\\u00a7r"}']}
{title:'Karchkhadze et al. (§72024§r)', author: 'Tornike Karchkhadze; Hassan Salami Kavaki; Mohammad Rasool Izadi; Bryce Irvin; Mikolaj Kegler; Ari Hertz; Shuo Zhang; Marko Stamenovic', display:{Lore:['[{"text": "arXiv:2403.12182", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLatent CLAP Loss for Better Foley Sound Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oTornike Karchkhadze\\nHassan Salami Kavaki\\nMohammad Rasool Izadi\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.12182\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Mar 2024 18:55:37 GMT)\\u00a7r"}']}
{title:'Qiao et al. (§72024§r)', author: 'Yue Qiao; Ryan Miguel Gonzales; Edgar Choueiri', display:{Lore:['[{"text": "arXiv:2403.12258", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Multi-loudspeaker Binaural Room Impulse Response Dataset with High-Resolution Translational and Rotational Head Coordinates in a Listening Room\\u00a7r\\n\\n\\u00a78\\u00a7oYue Qiao\\nRyan Miguel Gonzales\\nEdgar Choueiri\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.12258\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.3389/frsip.2024.1380060\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 18 Mar 2024 21:14:18 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to Frontiers in Signal Processing\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Jiarui Wang; Thushara Abhayapala; Jihui Aimee Zhang; Prasanga Samarasinghe', display:{Lore:['[{"text": "arXiv:2403.12630", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReproducing the Acoustic Velocity Vectors in a Circular Listening Area\\u00a7r\\n\\n\\u00a78\\u00a7oJiarui Wang\\nThushara Abhayapala\\nJihui Aimee Zhang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.12630\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 19 Mar 2024 10:57:17 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to EUSIPCO 2024\\u00a7r"}']}
{title:'Xi et al. (§72024§r)', author: 'Yu Xi; Hao Li; Baochen Yang; Haoyu Li; Hainan Xu; Kai Yu', display:{Lore:['[{"text": "arXiv:2403.13332", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTDT-KWS: Fast And Accurate Keyword Spotting Using Token-and-duration Transducer\\u00a7r\\n\\n\\u00a78\\u00a7oYu Xi\\nHao Li\\nBaochen Yang\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.13332\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Mar 2024 06:24:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by ICASSP2024\\u00a7r"}']}
{title:'Zhou et al. (§72024§r)', author: 'Huali Zhou; Yuke Lin; Dong Liu; Ming Li', display:{Lore:['[{"text": "arXiv:2403.13356", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKunquDB: An Attempt for Speaker Verification in the Chinese Opera Scenario\\u00a7r\\n\\n\\u00a78\\u00a7oHuali Zhou\\nYuke Lin\\nDong Liu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.13356\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Mar 2024 07:34:21 GMT)\\u00a7r"}']}
{title:'Mohammad et al. (§72024§r)', author: 'Mir Sayeed Mohammad; Azizul Zahid; Md Asif Iqbal', display:{Lore:['[{"text": "arXiv:2403.13465", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBanglaNum \\u2013 A Public Dataset for Bengali Digit Recognition from Speech\\u00a7r\\n\\n\\u00a78\\u00a7oMir Sayeed Mohammad\\nAzizul Zahid\\nMd Asif Iqbal\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.13465\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Mar 2024 10:16:33 GMT)\\u00a7r"}']}
{title:"Doyon-D'Amour et al. (§72024§r)", author: "Francis Doyon-D'Amour; Carly Stalder; Timothy Hodges; Michel Stephan; Lixiue Wu; Triantafillos Koukoulas; Stephane Leahy; Raphael St-Gelais", display:{Lore:['[{"text": "arXiv:2403.13643", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVibration Sensitivity of one-port and two-port MEMS microphones\\u00a7r\\n\\n\\u00a78\\u00a7oFrancis Doyon-D\'Amour\\nCarly Stalder\\nTimothy Hodges\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.13643\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 20 Mar 2024 14:50:00 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 14 figures\\u00a7r"}']}
{title:'Wilkinghoff (§72024§r)', author: 'Kevin Wilkinghoff', display:{Lore:['[{"text": "arXiv:2403.14179", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdaProj: Adaptively Scaled Angular Margin Subspace Projections for Anomalous Sound Detection with Auxiliary Classification Tasks\\u00a7r\\n\\n\\u00a78\\u00a7oKevin Wilkinghoff\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.14179\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 8 May 2024 09:18:53 GMT)\\u00a7r"}']}
{title:'Baligar et al. (§72024§r)', author: 'Shrishail Baligar; Mikolaj Kegler; Bryce Irvin; Marko Stamenovic; Shawn Newsam', display:{Lore:['[{"text": "arXiv:2403.14246", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCATSE: A Context-Aware Framework for Causal Target Sound Extraction\\u00a7r\\n\\n\\u00a78\\u00a7oShrishail Baligar\\nMikolaj Kegler\\nBryce Irvin\\nMarko Stamenovic\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.14246\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Mar 2024 09:06:28 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to EUSIPCO 2024\\u00a7r"}']}
{title:'Lee et al. (§72024§r)', author: 'PeiYing Lee; HauYun Guo; Berlin Chen', display:{Lore:['[{"text": "arXiv:2403.14268", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech-Aware Neural Diarization with Encoder-Decoder Attractor Guided by Attention Constraints\\u00a7r\\n\\n\\u00a78\\u00a7oPeiYing Lee\\nHauYun Guo\\nBerlin Chen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.14268\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Mar 2024 10:09:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to The 28th International Conference on Technologies and Applications of Artificial Intelligence (TAAI), in Chinese language\\u00a7r"}']}
{title:'Lechler et al. (§72024§r)', author: 'Laura Lechler; Kamil Wojcicki', display:{Lore:['[{"text": "arXiv:2403.14817", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCrowdsourced Multilingual Speech Intelligibility Testing\\u00a7r\\n\\n\\u00a78\\u00a7oLaura Lechler\\nKamil Wojcicki\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.14817\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/ICASSP48485.2024.10447869\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 21 Mar 2024 20:14:53 GMT)\\u00a7r"}']}
{title:'Martinez et al. (§72024§r)', author: 'Helard Becerra Martinez; Alessandro Ragano; Diptasree Debnath; Asad Ullah; Crisron Rudolf Lucas; Martin Walsh; Andrew Hines', display:{Lore:['[{"text": "arXiv:2403.15336", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.MM\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDialogue Understandability: Why are we streaming movies with subtitles?\\u00a7r\\n\\n\\u00a78\\u00a7oHelard Becerra Martinez\\nAlessandro Ragano\\nDiptasree Debnath\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.15336\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 22 Mar 2024 16:41:45 GMT)\\u00a7r"}']}
{title:'Essaid et al. (§72024§r)', author: 'Billel Essaid; Hamza Kheddar; Noureddine Batel; Abderrahmane Lakas; Muhammad E. H. Chowdhury', display:{Lore:['[{"text": "arXiv:2403.15442", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdvanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives\\u00a7r\\n\\n\\u00a78\\u00a7oBillel Essaid\\nHamza Kheddar\\nNoureddine Batel\\nAbderrahmane Lakas\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.15442\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 17 Mar 2024 11:28:23 GMT)\\u00a7r"}']}
{title:'Dohi et al. (§72024§r)', author: 'Kota Dohi; Yohei Kawaguchi', display:{Lore:['[{"text": "arXiv:2403.16610", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDistributed collaborative anomalous sound detection by embedding sharing\\u00a7r\\n\\n\\u00a78\\u00a7oKota Dohi\\nYohei Kawaguchi\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.16610\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Mar 2024 10:40:04 GMT)\\u00a7r"}']}
{title:'Peng et al. (§72024§r)', author: 'Puyuan Peng; Po-Yao Huang; Abdelrahman Mohamed; David Harwath', display:{Lore:['[{"text": "arXiv:2403.16973", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild\\u00a7r\\n\\n\\u00a78\\u00a7oPuyuan Peng\\nPo-Yao Huang\\nAbdelrahman Mohamed\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.16973\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 19 Apr 2024 19:33:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oData, code, and model weightsare available at https://github.com/jasonppy/VoiceCraft\\u00a7r"}']}
{title:'Ogiso et al. (§72024§r)', author: 'Satoki Ogiso; Yoshiaki Bando; Takeshi Kurata; Takashi Okuma', display:{Lore:['[{"text": "arXiv:2403.17402", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInfrastructure-less Localization from Indoor Environmental Sounds Based on Spectral Decomposition and Spatial Likelihood Model\\u00a7r\\n\\n\\u00a78\\u00a7oSatoki Ogiso\\nYoshiaki Bando\\nTakeshi Kurata\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.17402\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/SII55687.2023.10039378\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Mar 2024 05:41:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o6 pages, 6 figures, accepted to IEEE/SICE SII 2023\\u00a7r"}']}
{title:'Neri et al. (§72024§r)', author: 'Michael Neri; Archontis Politis; Daniel Krause; Marco Carli; Tuomas Virtanen', display:{Lore:['[{"text": "arXiv:2403.17514", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Distance Estimation in Enclosures from Single-Channel Audio\\u00a7r\\n\\n\\u00a78\\u00a7oMichael Neri\\nArchontis Politis\\nDaniel Krause\\nMarco Carli\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.17514\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.1109/TASLP.2024.3382504\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Mar 2024 09:16:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in IEEE/ACM Transactions on Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Ronchini et al. (§72024§r)', author: 'Francesca Ronchini; Luca Comanducci; Fabio Antonacci', display:{Lore:['[{"text": "arXiv:2403.17864", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSynthesizing Soundscapes: Leveraging Text-to-Audio Models for Environmental Sound Classification\\u00a7r\\n\\n\\u00a78\\u00a7oFrancesca Ronchini\\nLuca Comanducci\\nFabio Antonacci\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.17864\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 26 Mar 2024 16:51:21 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to EUSIPCO 2024\\u00a7r"}']}
{title:'Jiang et al. (§72024§r)', author: 'Xilin Jiang; Cong Han; Nima Mesgarani', display:{Lore:['[{"text": "arXiv:2403.18257", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDual-path Mamba: Short and Long-term Bidirectional Selective Structured State Space Models for Speech Separation\\u00a7r\\n\\n\\u00a78\\u00a7oXilin Jiang\\nCong Han\\nNima Mesgarani\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.18257\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 1 May 2024 00:36:13 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7owork inprogress\\u00a7r"}']}
{title:'Mørk et al. (§72024§r)', author: 'Jacob Mørk; Holger Severin Bovbjerg; Gergely Kiss; Zheng-Hua Tan', display:{Lore:['[{"text": "arXiv:2403.18560", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNoise-Robust Keyword Spotting through Self-supervised Pretraining\\u00a7r\\n\\n\\u00a78\\u00a7oJacob M\\u00f8rk\\nHolger Severin Bovbjerg\\nGergely Kiss\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.18560\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Mar 2024 13:42:14 GMT)\\u00a7r"}']}
{title:'Moliner et al. (§72024§r)', author: 'Eloi Moliner; Maija Turunen; Filip Elvander; Vesa Välimäki', display:{Lore:['[{"text": "arXiv:2403.18636", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Diffusion-Based Generative Equalizer for Music Restoration\\u00a7r\\n\\n\\u00a78\\u00a7oEloi Moliner\\nMaija Turunen\\nFilip Elvander\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.18636\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Mar 2024 14:41:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to DAFx24. Historical musicrestoration examples are available at: http://research.spa.aalto.fi/publications/papers/dafx-babe2/\\u00a7r"}']}
{title:'Liang et al. (§72024§r)', author: 'Jinhua Liang; Ines Nolasco; Burooj Ghani; Huy Phan; Emmanouil Benetos; Dan Stowell', display:{Lore:['[{"text": "arXiv:2403.18638", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMind the Domain Gap: a Systematic Analysis on Bioacoustic Sound Event Detection\\u00a7r\\n\\n\\u00a78\\u00a7oJinhua Liang\\nInes Nolasco\\nBurooj Ghani\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.18638\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 27 Mar 2024 14:44:24 GMT)\\u00a7r"}']}
{title:'Fujita et al. (§72024§r)', author: 'Yuya Fujita; Shinji Watanabe; Xuankai Chang; Takashi Maekaku', display:{Lore:['[{"text": "arXiv:2403.19207", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lLV-CTC: Non-autoregressive ASR with CTC and latent variable models\\u00a7r\\n\\n\\u00a78\\u00a7oYuya Fujita\\nShinji Watanabe\\nXuankai Chang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.19207\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Mar 2024 08:08:53 GMT)\\u00a7r"}']}
{title:'Deppisch et al. (§72024§r)', author: 'Thomas Deppisch; Nils Meyer-Kahlen; Sebastià V. Amengual Garí', display:{Lore:['[{"text": "arXiv:2403.19217", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBlind Identification of Binaural Room Impulse Responses from Smart Glasses\\u00a7r\\n\\n\\u00a78\\u00a7oThomas Deppisch\\nNils Meyer-Kahlen\\nSebasti\\u00e0 V. Amengual Gar\\u00ed\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.19217\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 28 Mar 2024 08:30:14 GMT)\\u00a7r"}']}
{title:'Munkhdalai et al. (§72024§r)', author: 'Tsendsuren Munkhdalai; Youzheng Chen; Khe Chai Sim; Fadi Biadsy; Tara Sainath; Pedro Moreno Mengibar', display:{Lore:['[{"text": "arXiv:2403.19709", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.NE\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models\\u00a7r\\n\\n\\u00a78\\u00a7oTsendsuren Munkhdalai\\nYouzheng Chen\\nKhe Chai Sim\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.19709\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Mar 2024 17:21:56 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures, 5 tables\\u00a7r"}']}
{title:'Chen et al. (§72024§r)', author: 'Yafeng Chen; Siqi Zheng; Hui Wang; Luyao Cheng; Tinglong Zhu; Changhe Song; Rongjie Huang; Ziyang Ma; Qian Chen; Shiliang Zhang; Xihao Li', display:{Lore:['[{"text": "arXiv:2403.19971", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l3D-Speaker-Toolkit: An Open Source Toolkit for Multi-modal Speaker Verification and Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oYafeng Chen\\nSiqi Zheng\\nHui Wang\\n+ 7 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.19971\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Mar 2024 04:42:12 GMT)\\u00a7r"}']}
{title:'Fagerström et al. (§72024§r)', author: 'Jon Fagerström; Sebastian J. Schlecht; Vesa Välimäki', display:{Lore:['[{"text": "arXiv:2403.20090", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lNon-Exponential Reverberation Modeling Using Dark Velvet Noise\\u00a7r\\n\\n\\u00a78\\u00a7oJon Fagerstr\\u00f6m\\nSebastian J. Schlecht\\nVesa V\\u00e4lim\\u00e4ki\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.20090\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Mar 2024 09:54:37 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for publication in the Journal of Audio Engineering Society\\u00a7r"}']}
{title:'Nguyen et al. (§72024§r)', author: 'Tuan Nguyen; Corinne Fredouille; Alain Ghio; Mathieu Balaguer; Virginie Woisard', display:{Lore:['[{"text": "arXiv:2403.20184", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Pathological Speech Quality Assessment with ASR-Powered Wav2Vec2 in Data-Scarce Context\\u00a7r\\n\\n\\u00a78\\u00a7oTuan Nguyen\\nCorinne Fredouille\\nAlain Ghio\\nMathieu Balaguer\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2403.20184\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 29 Mar 2024 13:59:34 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at LREC-COLING 2024\\u00a7r"}']}
{title:'Mezza et al. (§72024§r)', author: 'Alessandro Ilic Mezza; Riccardo Giampiccolo; Enzo De Sena; Alberto Bernardini', display:{Lore:['[{"text": "arXiv:2404.00082", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData-Driven Room Acoustic Modeling Via Differentiable Feedback Delay Networks With Learnable Delay Lines\\u00a7r\\n\\n\\u00a78\\u00a7oAlessandro Ilic Mezza\\nRiccardo Giampiccolo\\nEnzo De Sena\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.00082\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 17 May 2024 11:56:07 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe article has been submitted to EURASIP Journal on Audio, Speech, and Music Processing on Jan 02, 2024 and is currently under review\\u00a7r"}']}
{title:'Cuervo et al. (§72024§r)', author: 'Santiago Cuervo; Ricard Marxer', display:{Lore:['[{"text": "arXiv:2404.00685", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.NE\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lScaling Properties of Speech Language Models\\u00a7r\\n\\n\\u00a78\\u00a7oSantiago Cuervo\\nRicard Marxer\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.00685\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 Apr 2024 06:46:18 GMT)\\u00a7r"}']}
{title:'Phukan et al. (§72024§r)', author: 'Orchid Chetia Phukan; Gautam Siddharth Kashyap; Arun Balaji Buduru; Rajesh Sharma', display:{Lore:['[{"text": "arXiv:2404.00809", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHeterogeneity over Homogeneity: Investigating Multilingual Speech Pre-Trained Models for Detecting Audio Deepfake\\u00a7r\\n\\n\\u00a78\\u00a7oOrchid Chetia Phukan\\nGautam Siddharth Kashyap\\nArun Balaji Buduru\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.00809\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 31 Mar 2024 21:48:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to NAACL (Findings) 2024\\u00a7r"}']}
{title:'Tao et al. (§72024§r)', author: 'Ruijie Tao; Xinyuan Qian; Rohan Kumar Das; Xiaoxue Gao; Jiadong Wang; Haizhou Li', display:{Lore:['[{"text": "arXiv:2404.00861", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7eeess.IV\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEnhancing Real-World Active Speaker Detection with Multi-Modal Extraction Pre-Training\\u00a7r\\n\\n\\u00a78\\u00a7oRuijie Tao\\nXinyuan Qian\\nRohan Kumar Das\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.00861\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Apr 2024 02:01:49 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages\\u00a7r"}']}
{title:'Tao et al. (§72024§r)', author: 'Ruijie Tao; Zhan Shi; Yidi Jiang; Tianchi Liu; Haizhou Li', display:{Lore:['[{"text": "arXiv:2404.00863", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Conversion Augmentation for Speaker Recognition on Defective Datasets\\u00a7r\\n\\n\\u00a78\\u00a7oRuijie Tao\\nZhan Shi\\nYidi Jiang\\nTianchi Liu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.00863\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 1 Apr 2024 02:05:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages\\u00a7r"}']}
{title:'Abilbekov et al. (§72024§r)', author: 'Adal Abilbekov; Saida Mussakhojayeva; Rustem Yeshpanov; Huseyin Atakan Varol', display:{Lore:['[{"text": "arXiv:2404.01033", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lKazEmoTTS: A Dataset for Kazakh Emotional Text-to-Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oAdal Abilbekov\\nSaida Mussakhojayeva\\nRustem Yeshpanov\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.01033\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 9 Apr 2024 21:01:54 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTo appear in Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)\\u00a7r"}']}
{title:'Guo et al. (§72024§r)', author: 'Jinxi Guo; Niko Moritz; Yingyi Ma; Frank Seide; Chunyang Wu; Jay Mahadeokar; Ozlem Kalinli; Christian Fuegen; Mike Seltzer', display:{Lore:['[{"text": "arXiv:2404.01716", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEffective internal language model training and fusion for factorized transducer model\\u00a7r\\n\\n\\u00a78\\u00a7oJinxi Guo\\nNiko Moritz\\nYingyi Ma\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.01716\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Apr 2024 08:01:05 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to ICASSP 2024\\u00a7r"}']}
{title:'Best et al. (§72024§r)', author: 'Paul Best; Santiago Cuervo; Ricard Marxer', display:{Lore:['[{"text": "arXiv:2404.01737", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTransfer Learning from Whisper for Microscopic Intelligibility Prediction\\u00a7r\\n\\n\\u00a78\\u00a7oPaul Best\\nSantiago Cuervo\\nRicard Marxer\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.01737\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 2 Apr 2024 08:53:51 GMT)\\u00a7r"}']}
{title:'Tomashenko et al. (§72024§r)', author: 'Natalia Tomashenko; Xiaoxiao Miao; Pierre Champion; Sarina Meyer; Xin Wang; Emmanuel Vincent; Michele Panariello; Nicholas Evans; Junichi Yamagishi; Massimiliano Todisco', display:{Lore:['[{"text": "arXiv:2404.02677", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.CR\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe VoicePrivacy 2024 Challenge Evaluation Plan\\u00a7r\\n\\n\\u00a78\\u00a7oNatalia Tomashenko\\nXiaoxiao Miao\\nPierre Champion\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.02677\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Apr 2024 12:20:51 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oarXiv admin note: substantial text overlap with arXiv:2203.12468\\u00a7r"}']}
{title:'Kim et al. (§72024§r)', author: 'Jaehyeon Kim; Keon Lee; Seungjun Chung; Jaewoong Cho', display:{Lore:['[{"text": "arXiv:2404.02781", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oJaehyeon Kim\\nKeon Lee\\nSeungjun Chung\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.02781\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 3 Apr 2024 14:52:20 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oICLR 2024\\u00a7r"}']}
{title:'Xin et al. (§72024§r)', author: 'Detai Xin; Xu Tan; Kai Shen; Zeqian Ju; Dongchao Yang; Yuancheng Wang; Shinnosuke Takamichi; Hiroshi Saruwatari; Shujie Liu; Jinyu Li; Sheng Zhao', display:{Lore:['[{"text": "arXiv:2404.03204", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oDetai Xin\\nXu Tan\\nKai Shen\\n+ 7 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.03204\\u00a7r\\n\\nVersion:\\u00a77v3 (Sun, 19 May 2024 21:34:28 GMT)\\u00a7r"}']}
{title:'Comanducci et al. (§72024§r)', author: 'Luca Comanducci; Fabio Antonacci; Augusto Sarti', display:{Lore:['[{"text": "arXiv:2404.03436", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lInterpreting End-to-End Deep Learning Models for Speech Source Localization Using Layer-wise Relevance Propagation\\u00a7r\\n\\n\\u00a78\\u00a7oLuca Comanducci\\nFabio Antonacci\\nAugusto Sarti\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.03436\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 26 Apr 2024 08:42:34 GMT)\\u00a7r"}']}
{title:'Shuvo et al. (§72024§r)', author: 'Samiul Based Shuvo; Syed Samiul Alam; Taufiq Hasan', display:{Lore:['[{"text": "arXiv:2404.04365", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lUformer: A UNet-Transformer fused robust end-to-end deep learning framework for real-time denoising of lung sounds\\u00a7r\\n\\n\\u00a78\\u00a7oSamiul Based Shuvo\\nSyed Samiul Alam\\nTaufiq Hasan\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.04365\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Apr 2024 19:24:39 GMT)\\u00a7r"}']}
{title:'Subramani et al. (§72024§r)', author: 'Krishna Subramani; Paris Smaragdis; Takuya Higuchi; Mehrez Souden', display:{Lore:['[{"text": "arXiv:2404.04439", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRethinking Non-Negative Matrix Factorization with Implicit Neural Representations\\u00a7r\\n\\n\\u00a78\\u00a7oKrishna Subramani\\nParis Smaragdis\\nTakuya Higuchi\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.04439\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 5 Apr 2024 22:48:57 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IEEESPL, Code: https://github.com/SubramaniKrishna/in-nmf\\u00a7r"}']}
{title:'Luo et al. (§72024§r)', author: 'Yi Luo; Jianwei Yu; Hangting Chen; Rongzhi Gu; Chao Weng', display:{Lore:['[{"text": "arXiv:2404.04947", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGull: A Generative Multifunctional Audio Codec\\u00a7r\\n\\n\\u00a78\\u00a7oYi Luo\\nJianwei Yu\\nHangting Chen\\nRongzhi Gu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.04947\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 7 Apr 2024 12:57:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oDemo page: https://yluo42.github.io/Gull/\\u00a7r"}']}
{title:'Guo et al. (§72024§r)', author: 'Yiwei Guo; Chenrun Wang; Yifan Yang; Hankun Wang; Ziyang Ma; Chenpeng Du; Shuai Wang; Hanzheng Li; Shuai Fan; Hui Zhang; Xie Chen; Kai Yu', display:{Lore:['[{"text": "arXiv:2404.06079", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oYiwei Guo\\nChenrun Wang\\nYifan Yang\\n+ 8 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.06079\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 10 Apr 2024 00:33:25 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 3 figures. Report of a challenge\\u00a7r"}']}
{title:'Niizumi et al. (§72024§r)', author: 'Daisuke Niizumi; Daiki Takeuchi; Yasunori Ohishi; Noboru Harada; Kunio Kashino', display:{Lore:['[{"text": "arXiv:2404.06095", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMasked Modeling Duo: Towards a Universal Audio Pre-training Framework\\u00a7r\\n\\n\\u00a78\\u00a7oDaisuke Niizumi\\nDaiki Takeuchi\\nYasunori Ohishi\\nNoboru Harada\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.06095\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 9 Apr 2024 07:57:08 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o15 pages, 6 figures, 15 tables. Accepted by TASLP\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Leying Zhang; Yao Qian; Long Zhou; Shujie Liu; Dongmei Wang; Xiaofei Wang; Midia Yousefi; Yanmin Qian; Jinyu Li; Lei He; Sheng Zhao; Michael Zeng', display:{Lore:['[{"text": "arXiv:2404.06690", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCoVoMix: Advancing Zero-Shot Speech Generation for Human-like Multi-talker Conversations\\u00a7r\\n\\n\\u00a78\\u00a7oLeying Zhang\\nYao Qian\\nLong Zhou\\n+ 8 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.06690\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 29 May 2024 07:30:20 GMT)\\u00a7r"}']}
{title:'Meng et al. (§72024§r)', author: 'Hanyu Meng; Vidhyasaharan Sethu; Eliathamby Ambikairajah', display:{Lore:['[{"text": "arXiv:2404.06702", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWhat is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions\\u00a7r\\n\\n\\u00a78\\u00a7oHanyu Meng\\nVidhyasaharan Sethu\\nEliathamby Ambikairajah\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.06702\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1617\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nInterspeech 2023\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Apr 2024 03:19:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oInterspeech 2023 Proceeding\\u00a7r"}']}
{title:'Kwon et al. (§72024§r)', author: 'Taegyun Kwon; Dasaem Jeong; Juhan Nam', display:{Lore:['[{"text": "arXiv:2404.06818", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTowards Efficient and Real-Time Piano Transcription Using Neural Autoregressive Models\\u00a7r\\n\\n\\u00a78\\u00a7oTaegyun Kwon\\nDasaem Jeong\\nJuhan Nam\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.06818\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Apr 2024 08:06:15 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 8 figures, preprint\\u00a7r"}']}
{title:'Karakonstantis et al. (§72024§r)', author: 'Xenofon Karakonstantis; Efren Fernandez-Grande; Peter Gerstoft', display:{Lore:['[{"text": "arXiv:2404.06928", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient Sound Field Reconstruction with Conditional Invertible Neural Networks\\u00a7r\\n\\n\\u00a78\\u00a7oXenofon Karakonstantis\\nEfren Fernandez-Grande\\nPeter Gerstoft\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.06928\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 10 Apr 2024 11:27:06 GMT)\\u00a7r"}']}
{title:'Koudounas et al. (§72024§r)', author: 'Alkis Koudounas; Flavio Giobergia', display:{Lore:['[{"text": "arXiv:2404.07226", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHouston we have a Divergence: A Subgroup Performance Analysis of ASR Models\\u00a7r\\n\\n\\u00a78\\u00a7oAlkis Koudounas\\nFlavio Giobergia\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.07226\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 31 Mar 2024 10:06:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2 pages\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Kevin Zhang; Luka Chkhetiani; Francis McCann Ramirez; Yash Khare; Andrea Vanzo; Michael Liang; Sergio Ramirez Martin; Gabriel Oexle; Ruben Bousbib; Taufiquzzaman Peyash; Michael Nguyen; Dillon Pulliam; Domenic Donato', display:{Lore:['[{"text": "arXiv:2404.07341", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConformer-1: Robust ASR via Large-Scale Semisupervised Bootstrapping\\u00a7r\\n\\n\\u00a78\\u00a7oKevin Zhang\\nLuka Chkhetiani\\nFrancis McCann Ramirez\\n+ 9 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.07341\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 12 Apr 2024 18:23:35 GMT)\\u00a7r"}']}
{title:'Yu et al. (§72024§r)', author: 'Chin-Yun Yu; Christopher Mitcheltree; Alistair Carson; Stefan Bilbao; Joshua D. Reiss; György Fazekas', display:{Lore:['[{"text": "arXiv:2404.07970", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDifferentiable All-pole Filters for Time-varying Audio Systems\\u00a7r\\n\\n\\u00a78\\u00a7oChin-Yun Yu\\nChristopher Mitcheltree\\nAlistair Carson\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.07970\\u00a7r\\n\\nVersion:\\u00a77v2 (Fri, 12 Apr 2024 09:58:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to DAFx 2024\\u00a7r"}']}
{title:'Arasteh et al. (§72024§r)', author: 'Soroosh Tayebi Arasteh; Tomas Arias-Vergara; Paula Andrea Perez-Toro; Tobias Weise; Kai Packhaeuser; Maria Schuster; Elmar Noeth; Andreas Maier; Seung Hee Yang', display:{Lore:['[{"text": "arXiv:2404.08064", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe Impact of Speech Anonymization on Pathology and Its Limits\\u00a7r\\n\\n\\u00a78\\u00a7oSoroosh Tayebi Arasteh\\nTomas Arias-Vergara\\nPaula Andrea Perez-Toro\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.08064\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 11 Apr 2024 18:06:35 GMT)\\u00a7r"}']}
{title:'Hong et al. (§72024§r)', author: 'Zhiqing Hong; Rongjie Huang; Xize Cheng; Yongqi Wang; Ruiqi Li; Fuming You; Zhou Zhao; Zhimeng Zhang', display:{Lore:['[{"text": "arXiv:2404.09313", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lText-to-Song: Towards Controllable Music Generation Incorporating Vocals and Accompaniment\\u00a7r\\n\\n\\u00a78\\u00a7oZhiqing Hong\\nRongjie Huang\\nXize Cheng\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.09313\\u00a7r\\n\\nVersion:\\u00a77v3 (Mon, 20 May 2024 05:50:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oACL 2024 Main\\u00a7r"}']}
{title:'Yang et al. (§72024§r)', author: 'Shu-wen Yang; Heng-Jui Chang; Zili Huang; Andy T. Liu; Cheng-I Lai; Haibin Wu; Jiatong Shi; Xuankai Chang; Hsiang-Sheng Tsai; Wen-Chin Huang; Tzu-hsun Feng; Po-Han Chi; Yist Y. Lin; Yung-Sung Chuang; Tzu-Hsien Huang; Wei-Cheng Tseng; Kushal Lakhotia; Shang-Wen Li; Abdelrahman Mohamed; Shinji Watanabe; Hung-yi Lee', display:{Lore:['[{"text": "arXiv:2404.09385", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Large-Scale Evaluation of Speech Foundation Models\\u00a7r\\n\\n\\u00a78\\u00a7oShu-wen Yang\\nHeng-Jui Chang\\nZili Huang\\n+ 17 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.09385\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 29 May 2024 21:24:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThe extended journal version for SUPERB andSUPERB-SG. Published inIEEE/ACM TASLP. The Arxiv version is preferred\\u00a7r"}']}
{title:'Ramirez et al. (§72024§r)', author: 'Francis McCann Ramirez; Luka Chkhetiani; Andrew Ehrenberg; Robert McHardy; Rami Botros; Yash Khare; Andrea Vanzo; Taufiquzzaman Peyash; Gabriel Oexle; Michael Liang; Ilya Sklyar; Enver Fakhan; Ahmed Etefy; Daniel McCrystal; Sam Flamini; Domenic Donato; Takuya Yoshioka', display:{Lore:['[{"text": "arXiv:2404.09841", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAnatomy of Industrial Scale Multilingual ASR\\u00a7r\\n\\n\\u00a78\\u00a7oFrancis McCann Ramirez\\nLuka Chkhetiani\\nAndrew Ehrenberg\\n+ 13 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.09841\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 16 Apr 2024 14:55:13 GMT)\\u00a7r"}']}
{title:'Wazir et al. (§72024§r)', author: 'Hassam Khan Wazir; Zaid Waghoo; Vikram Kapila', display:{Lore:['[{"text": "arXiv:2404.10310", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lWireless Earphone-based Real-Time Monitoring of Breathing Exercises: A Deep Learning Approach\\u00a7r\\n\\n\\u00a78\\u00a7oHassam Khan Wazir\\nZaid Waghoo\\nVikram Kapila\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.10310\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Apr 2024 06:37:19 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 2 figures. Paperaccepted at IEEE International Conference on Engineering in Medicine Biology Society, 2024\\u00a7r"}']}
{title:'Futeral et al. (§72024§r)', author: 'Matthieu Futeral; Andrea Agostinelli; Marco Tagliasacchi; Neil Zeghidour; Eugene Kharitonov', display:{Lore:['[{"text": "arXiv:2404.10419", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMAD Speech: Measures of Acoustic Diversity of Speech\\u00a7r\\n\\n\\u00a78\\u00a7oMatthieu Futeral\\nAndrea Agostinelli\\nMarco Tagliasacchi\\nNeil Zeghidour\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.10419\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 16 Apr 2024 09:35:27 GMT)\\u00a7r"}']}
{title:'Brandao et al. (§72024§r)', author: 'Eric Brandao; William Fonseca; Paulo Mareze; Carlos Resende; Gabriel Azzuz; Joao Pontalti; Efren Fernandez-Grande', display:{Lore:['[{"text": "arXiv:2404.11399", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a75physics.class-ph\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIn situ sound absorption estimation with the discrete complex image source method\\u00a7r\\n\\n\\u00a78\\u00a7oEric Brandao\\nWilliam Fonseca\\nPaulo Mareze\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.11399\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 17 Apr 2024 14:03:42 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o37 pages, 12 figures, original manuscript to be submitted to the Journal of Sound and Vibration\\u00a7r"}']}
{title:'Wotherspoon et al. (§72024§r)', author: 'Shannon Wotherspoon; William Hartmann; Matthew Snover', display:{Lore:['[{"text": "arXiv:2404.11619", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdvancing Speech Translation: A Corpus of Mandarin-English Conversational Telephone Speech\\u00a7r\\n\\n\\u00a78\\u00a7oShannon Wotherspoon\\nWilliam Hartmann\\nMatthew Snover\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.11619\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 25 Mar 2024 21:08:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o2 pages\\u00a7r"}']}
{title:'Seidel et al. (§72024§r)', author: 'Ernst Seidel; Pejman Mowlaee; Tim Fingscheidt', display:{Lore:['[{"text": "arXiv:2404.11621", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEfficient High-Performance Bark-Scale Neural Network for Residual Echo and Noise Suppression\\u00a7r\\n\\n\\u00a78\\u00a7oErnst Seidel\\nPejman Mowlaee\\nTim Fingscheidt\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.11621\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 8 Apr 2024 08:47:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted to ICASSP 2024; 5 pages, 3 figures\\u00a7r"}']}
{title:'Santos et al. (§72024§r)', author: 'Arthur N. dos Santos; Bruno S. Masiero; Túlio C. L. Mateus', display:{Lore:['[{"text": "arXiv:2404.14564", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring the Potential of Data-Driven Spatial Audio Enhancement Using a Single-Channel Model\\u00a7r\\n\\n\\u00a78\\u00a7oArthur N. dos Santos\\nBruno S. Masiero\\nT\\u00falio C. L. Mateus\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.14564\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 22 Apr 2024 20:19:01 GMT)\\u00a7r"}']}
{title:'Ye et al. (§72024§r)', author: 'Zhen Ye; Zeqian Ju; Haohe Liu; Xu Tan; Jianyi Chen; Yiwen Lu; Peiwen Sun; Jiahao Pan; Weizhen Bian; Shulin He; Qifeng Liu; Yike Guo; Wei Xue', display:{Lore:['[{"text": "arXiv:2404.14700", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFlashSpeech: Efficient Zero-Shot Speech Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oZhen Ye\\nZeqian Ju\\nHaohe Liu\\n+ 9 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.14700\\u00a7r\\n\\nVersion:\\u00a77v3 (Thu, 25 Apr 2024 03:38:46 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oEfficient zero-shot speech synthesis\\u00a7r"}']}
{title:'Ochiai et al. (§72024§r)', author: 'Tsubasa Ochiai; Kazuma Iwamoto; Marc Delcroix; Rintaro Ikeshita; Hiroshi Sato; Shoko Araki; Shigeru Katagiri', display:{Lore:['[{"text": "arXiv:2404.14860", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRethinking Processing Distortions: Disentangling the Impact of Speech Enhancement Errors on Speech Recognition Performance\\u00a7r\\n\\n\\u00a78\\u00a7oTsubasa Ochiai\\nKazuma Iwamoto\\nMarc Delcroix\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.14860\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Apr 2024 09:30:23 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13 pages, 6 figures, Submitted to IEEE/ACM Trans. Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Wilkinghoff et al. (§72024§r)', author: 'Kevin Wilkinghoff; Alessia Cornaggia-Urrigshardt', display:{Lore:['[{"text": "arXiv:2404.14903", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.IR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-Sample Dynamic Time Warping for Few-Shot Keyword Spotting\\u00a7r\\n\\n\\u00a78\\u00a7oKevin Wilkinghoff\\nAlessia Cornaggia-Urrigshardt\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.14903\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Apr 2024 10:36:23 GMT)\\u00a7r"}']}
{title:'Lepage et al. (§72024§r)', author: 'Theo Lepage; Reda Dehak', display:{Lore:['[{"text": "arXiv:2404.14913", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAdditive Margin in Contrastive Self-Supervised Frameworks to Learn Discriminative Speaker Representations\\u00a7r\\n\\n\\u00a78\\u00a7oTheo Lepage\\nReda Dehak\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.14913\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Apr 2024 10:56:58 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oaccepted at Odyssey 2024: The Speaker and Language Recognition Workshop. arXiv admin note: text overlap with arXiv:2306.03664\\u00a7r"}']}
{title:'Ali et al. (§72024§r)', author: 'Hasmot Ali; Md. Fahad Hossain; Md. Mehedi Hasan; Sheikh Abujar; Sheak Rashed Haider Noori', display:{Lore:['[{"text": "arXiv:2404.15168", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lArtificial Neural Networks to Recognize Speakers Division from Continuous Bengali Speech\\u00a7r\\n\\n\\u00a78\\u00a7oHasmot Ali\\nMd. Fahad Hossain\\nMd. Mehedi Hasan\\nSheikh Abujar\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.15168\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 18 Apr 2024 10:17:20 GMT)\\u00a7r"}']}
{title:'Doukhan et al. (§72024§r)', author: 'David Doukhan; Simon Devauchelle; Lucile Girard-Monneron; Mía Chávez Ruz; V. Chaddouk; Isabelle Wagner; Albert Rilliard', display:{Lore:['[{"text": "arXiv:2404.15176", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.HC\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lVoice Passing : a Non-Binary Voice Gender Prediction System for evaluating Transgender voice transition\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Doukhan\\nSimon Devauchelle\\nLucile Girard-Monneron\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.15176\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/Interspeech.2023-1835\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. INTERSPEECH 2023, 5207-5211\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 23 Apr 2024 16:15:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 1 figure, keywords: Transgender voice, Gender perception, Speaker gender classification, CNN, X-Vector\\u00a7r"}']}
{title:'Rilliard et al. (§72024§r)', author: 'Albert Rilliard; David Doukhan; Rémi Uro; Simon Devauchelle', display:{Lore:['[{"text": "arXiv:2404.16104", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvolution of Voices in French Audiovisual Media Across Genders and Age in a Diachronic Perspective\\u00a7r\\n\\n\\u00a78\\u00a7oAlbert Rilliard\\nDavid Doukhan\\nR\\u00e9mi Uro\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.16104\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nRadek Skarnitzl & Jan Vol\\\\\'in (Eds.), Proceedings of the 20th\\n  International Congress of Phonetic Sciences (ICPhS), Prague 2023, pp.\\n  753-757. Guarant International. ISBN 978-80-908 114-2-3\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 24 Apr 2024 18:00:06 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 2 figures, keywords:, Gender, Diachrony, Vocal Tract Resonance, Vocal register, Broadcast speech\\u00a7r"}']}
{title:'Salvi (§72024§r)', author: 'Giampiero Salvi', display:{Lore:['[{"text": "arXiv:2404.16547", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeveloping Acoustic Models for Automatic Speech Recognition in Swedish\\u00a7r\\n\\n\\u00a78\\u00a7oGiampiero Salvi\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.16547\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nEuropean Student Journal of Language and Speech, 1999\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 25 Apr 2024 12:03:14 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o16 pages, 7 figures\\u00a7r"}']}
{title:'Niizumi et al. (§72024§r)', author: 'Daisuke Niizumi; Daiki Takeuchi; Yasunori Ohishi; Noboru Harada; Kunio Kashino', display:{Lore:['[{"text": "arXiv:2404.17107", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring Pre-trained General-purpose Audio Representations for Heart Murmur Detection\\u00a7r\\n\\n\\u00a78\\u00a7oDaisuke Niizumi\\nDaiki Takeuchi\\nYasunori Ohishi\\nNoboru Harada\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.17107\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Apr 2024 01:52:50 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o4 pages, 1 figure, and 4 tables. Accepted by IEEE EMBC 2024\\u00a7r"}']}
{title:'Lyon et al. (§72024§r)', author: 'Richard F. Lyon; Rob Schonberger; Malcolm Slaney; Mihajlo Velimirović; Honglin Yu', display:{Lore:['[{"text": "arXiv:2404.17490", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lThe CARFAC v2 Cochlear Model in Matlab, NumPy, and JAX\\u00a7r\\n\\n\\u00a78\\u00a7oRichard F. Lyon\\nRob Schonberger\\nMalcolm Slaney\\nMihajlo Velimirovi\\u0107\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.17490\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Apr 2024 15:45:31 GMT)\\u00a7r"}']}
{title:'Uro et al. (§72024§r)', author: 'Rémi Uro; David Doukhan; Albert Rilliard; Laëtitia Larcher; Anissa-Claire Adgharouamane; Marie Tahon; Antoine Laurent', display:{Lore:['[{"text": "arXiv:2404.17552", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.DL\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Semi-Automatic Approach to Create Large Gender- and Age-Balanced Speaker Corpora: Usefulness of Speaker Diarization     Identification\\u00a7r\\n\\n\\u00a78\\u00a7oR\\u00e9mi Uro\\nDavid Doukhan\\nAlbert Rilliard\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.17552\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProceedings of the 13th Conference on Language Resources and\\n  Evaluation (LREC 2022), pages 3271-3280, Marseille, 20-25 June 2022. European\\n  Language Resources Association (ELRA)\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 26 Apr 2024 17:30:36 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oKeywords:, semi-automatic processing, corpus creation, diarization, speaker identification, gender-balanced, age-balanced, speakercorpus, diachrony\\u00a7r"}']}
{title:'Chouchane et al. (§72024§r)', author: 'Oubaida Chouchane; Christoph Busch; Chiara Galdi; Nicholas Evans; Massimiliano Todisco', display:{Lore:['[{"text": "arXiv:2404.17810", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Comparison of Differential Performance Metrics for the Evaluation of Automatic Speaker Verification Fairness\\u00a7r\\n\\n\\u00a78\\u00a7oOubaida Chouchane\\nChristoph Busch\\nChiara Galdi\\nNicholas Evans\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.17810\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 27 Apr 2024 07:28:10 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o8 pages, 7 figures\\u00a7r"}']}
{title:'Tao et al. (§72024§r)', author: 'Ruijie Tao; Xinyuan Qian; Yidi Jiang; Junjie Li; Jiadong Wang; Haizhou Li', display:{Lore:['[{"text": "arXiv:2404.18501", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudio-Visual Target Speaker Extraction with Reverse Selective Auditory Attention\\u00a7r\\n\\n\\u00a78\\u00a7oRuijie Tao\\nXinyuan Qian\\nYidi Jiang\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.18501\\u00a7r\\n\\nVersion:\\u00a77v2 (Wed, 8 May 2024 08:05:22 GMT)\\u00a7r"}']}
{title:'Bokaei et al. (§72024§r)', author: 'Mohammad Bokaei; Jesper Jensen; Simon Doclo; Jan Østergaard', display:{Lore:['[{"text": "arXiv:2404.19375", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDeep low-latency joint speech transmission and enhancement over a gaussian channel\\u00a7r\\n\\n\\u00a78\\u00a7oMohammad Bokaei\\nJesper Jensen\\nSimon Doclo\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.19375\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Apr 2024 09:06:32 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Hankun Wang; Chenpeng Du; Yiwei Guo; Shuai Wang; Xie Chen; Kai Yu', display:{Lore:['[{"text": "arXiv:2404.19723", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAttention-Constrained Inference for Robust Decoder-Only Text-to-Speech\\u00a7r\\n\\n\\u00a78\\u00a7oHankun Wang\\nChenpeng Du\\nYiwei Guo\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2404.19723\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 30 Apr 2024 17:17:07 GMT)\\u00a7r"}']}
{title:'La Quatra et al. (§72024§r)', author: 'Moreno La Quatra; Alkis Koudounas; Lorenzo Vaiani; Elena Baralis; Luca Cagliero; Paolo Garza; Sabato Marco Siniscalchi', display:{Lore:['[{"text": "arXiv:2405.00934", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBenchmarking Representations for Speech, Music, and Acoustic Events\\u00a7r\\n\\n\\u00a78\\u00a7oMoreno La Quatra\\nAlkis Koudounas\\nLorenzo Vaiani\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.00934\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 May 2024 01:24:53 GMT)\\u00a7r"}']}
{title:'Du et al. (§72024§r)', author: 'Zongyang Du; Junchen Lu; Kun Zhou; Lakshmish Kaushik; Berrak Sisman', display:{Lore:['[{"text": "arXiv:2405.01730", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lConverting Anyone\'s Voice: End-to-End Expressive Voice Conversion with a Conditional Diffusion Model\\u00a7r\\n\\n\\u00a78\\u00a7oZongyang Du\\nJunchen Lu\\nKun Zhou\\nLakshmish Kaushik\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.01730\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 2 May 2024 20:51:53 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by Speaker Odyssey 2024\\u00a7r"}']}
{title:'Westhausen et al. (§72024§r)', author: 'Nils L. Westhausen; Hendrik Kayser; Theresa Jansen; Bernd T. Meyer', display:{Lore:['[{"text": "arXiv:2405.01967", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-time multichannel deep speech enhancement in hearing aids: Comparing monaural and binaural processing in complex acoustic scenarios\\u00a7r\\n\\n\\u00a78\\u00a7oNils L. Westhausen\\nHendrik Kayser\\nTheresa Jansen\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.01967\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 3 May 2024 09:45:09 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\\u00a7r"}']}
{title:'Tits et al. (§72024§r)', author: 'Noé Tits; Prernna Bhatnagar; Thierry Dutoit', display:{Lore:['[{"text": "arXiv:2405.02124", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lTIPAA-SSL: Text Independent Phone-to-Audio Alignment based on Self-Supervised Learning and Knowledge Transfer\\u00a7r\\n\\n\\u00a78\\u00a7oNo\\u00e9 Tits\\nPrernna Bhatnagar\\nThierry Dutoit\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.02124\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 3 May 2024 14:25:21 GMT)\\u00a7r"}']}
{title:'Mu et al. (§72024§r)', author: 'Bingshen Mu; Yangze Li; Qijie Shao; Kun Wei; Xucheng Wan; Naijun Zheng; Huan Zhou; Lei Xie', display:{Lore:['[{"text": "arXiv:2405.03152", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMMGER: Multi-modal and Multi-granularity Generative Error Correction with LLM for Joint Accent and Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oBingshen Mu\\nYangze Li\\nQijie Shao\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.03152\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 6 May 2024 04:05:19 GMT)\\u00a7r"}']}
{title:'Liu et al. (§72024§r)', author: 'Xiaokang Liu; Xiaoxia Du; Juan Liu; Rongfeng Su; Manwa Lawrence Ng; Yumei Zhang; Yudong Yang; Shaofeng Zhao; Lan Wang; Nan Yan', display:{Lore:['[{"text": "arXiv:2405.03254", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAutomatic Assessment of Dysarthria Using Audio-visual Vowel Graph Attention Network\\u00a7r\\n\\n\\u00a78\\u00a7oXiaokang Liu\\nXiaoxia Du\\nJuan Liu\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.03254\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 7 May 2024 02:49:26 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o10 pages, 7 figures, 7 tables\\u00a7r"}']}
{title:'Costa et al. (§72024§r)', author: 'Federico Costa; Miquel India; Javier Hernando', display:{Lore:['[{"text": "arXiv:2405.04096", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Characterization by means of Attention Pooling\\u00a7r\\n\\n\\u00a78\\u00a7oFederico Costa\\nMiquel India\\nJavier Hernando\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.04096\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n10.21437/IberSPEECH.2022-34\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nProc. IberSPEECH 2022, 166-170\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 May 2024 07:56:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oIberSpeech 2022\\u00a7r"}']}
{title:'Moliner et al. (§72024§r)', author: 'Eloi Moliner; Jean-Marie Lemercier; Simon Welker; Timo Gerkmann; Vesa Välimäki', display:{Lore:['[{"text": "arXiv:2405.04272", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBUDDy: Single-Channel Blind Unsupervised Dereverberation with Diffusion Models\\u00a7r\\n\\n\\u00a78\\u00a7oEloi Moliner\\nJean-Marie Lemercier\\nSimon Welker\\nTimo Gerkmann\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.04272\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 May 2024 12:41:31 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IWAENC 2024\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Lijun Wang; Yixian Lu; Ziyan Gao; Kai Li; Jianqiang Huang; Yuntao Kong; Shogo Okada', display:{Lore:['[{"text": "arXiv:2405.04476", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lBERP: A Blind Estimator of Room Acoustic and Physical Parameters for Single-Channel Noisy Speech Signals\\u00a7r\\n\\n\\u00a78\\u00a7oLijun Wang\\nYixian Lu\\nZiyan Gao\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.04476\\u00a7r\\n\\nVersion:\\u00a77v2 (Thu, 16 May 2024 10:17:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o13-page, Submitted to IEEE/ACM Transaction on Audio Speech and Language Processing (TASLP)\\u00a7r"}']}
{title:'Eliav et al. (§72024§r)', author: 'Amit Eliav; Aaron Taub; Renana Opochinsky; Sharon Gannot', display:{Lore:['[{"text": "arXiv:2405.04627", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSingIt! Singer Voice Transformation\\u00a7r\\n\\n\\u00a78\\u00a7oAmit Eliav\\nAaron Taub\\nRenana Opochinsky\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.04627\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 7 May 2024 19:14:57 GMT)\\u00a7r"}']}
{title:'Ahn et al. (§72024§r)', author: 'Sunghwan Ahn; Beom Jun Woo; Min Hyun Han; Chanyeong Moon; Nam Soo Kim', display:{Lore:['[{"text": "arXiv:2405.04752", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lHILCodec: High Fidelity and Lightweight Neural Audio Codec\\u00a7r\\n\\n\\u00a78\\u00a7oSunghwan Ahn\\nBeom Jun Woo\\nMin Hyun Han\\nChanyeong Moon\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.04752\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 8 May 2024 01:40:13 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'You Zhang; Yongyi Zang; Jiatong Shi; Ryuichi Yamamoto; Jionghao Han; Yuxun Tang; Tomoki Toda; Zhiyao Duan', display:{Lore:['[{"text": "arXiv:2405.05244", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSVDD Challenge 2024: A Singing Voice Deepfake Detection Challenge Evaluation Plan\\u00a7r\\n\\n\\u00a78\\u00a7oYou Zhang\\nYongyi Zang\\nJiatong Shi\\n+ 4 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.05244\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 8 May 2024 17:40:12 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oEvaluation plan of the SVDD Challenge @SLT 2024\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Yabo Wang; Bing Yang; Xiaofei Li', display:{Lore:['[{"text": "arXiv:2405.07021", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lIPDnet: A Universal Direct-Path IPD Estimation Network for Sound Source Localization\\u00a7r\\n\\n\\u00a78\\u00a7oYabo Wang\\nBing Yang\\nXiaofei Li\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.07021\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 11 May 2024 14:02:15 GMT)\\u00a7r"}']}
{title:'Gelderblom et al. (§72024§r)', author: 'Femke B. Gelderblom; Tron V. Tronstad; Iván López-Espejo', display:{Lore:['[{"text": "arXiv:2405.07641", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluating Speech Enhancement Systems Through Listening Effort\\u00a7r\\n\\n\\u00a78\\u00a7oFemke B. Gelderblom\\nTron V. Tronstad\\nIv\\u00e1n L\\u00f3pez-Espejo\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.07641\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 May 2024 11:00:27 GMT)\\u00a7r"}']}
{title:'Weng et al. (§72024§r)', author: 'Zhenzi Weng; Zhijin Qin; Huiqiang Xie; Xiaoming Tao; Khaled B. Letaief', display:{Lore:['[{"text": "arXiv:2405.08096", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSemantic MIMO Systems for Speech-to-Text Transmission\\u00a7r\\n\\n\\u00a78\\u00a7oZhenzi Weng\\nZhijin Qin\\nHuiqiang Xie\\nXiaoming Tao\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.08096\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 13 May 2024 18:22:02 GMT)\\u00a7r"}']}
{title:'Brendel et al. (§72024§r)', author: 'Andreas Brendel; Nicola Pia; Kishan Gupta; Guillaume Fuchs; Markus Multrus', display:{Lore:['[{"text": "arXiv:2405.08417", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSimple and Efficient Quantization Techniques for Neural Speech Coding\\u00a7r\\n\\n\\u00a78\\u00a7oAndreas Brendel\\nNicola Pia\\nKishan Gupta\\nGuillaume Fuchs\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.08417\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 May 2024 08:23:30 GMT)\\u00a7r"}']}
{title:'Hsu et al. (§72024§r)', author: 'Yicheng Hsu; Mingsian R. Bai', display:{Lore:['[{"text": "arXiv:2405.08742", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA tunable binaural audio telepresence system capable of balancing immersive and enhanced modes\\u00a7r\\n\\n\\u00a78\\u00a7oYicheng Hsu\\nMingsian R. Bai\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.08742\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 14 May 2024 16:30:32 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages, 4 figures\\u00a7r"}']}
{title:'Thienpondt et al. (§72024§r)', author: 'Jenthe Thienpondt; Kris Demuynck', display:{Lore:['[{"text": "arXiv:2405.09142", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeaker Embeddings With Weakly Supervised Voice Activity Detection For Efficient Speaker Diarization\\u00a7r\\n\\n\\u00a78\\u00a7oJenthe Thienpondt\\nKris Demuynck\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.09142\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 15 May 2024 07:13:24 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oProceedings of Odyssey 2024: The Speaker and Language Recognition Workshop\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Siyang Wang; Éva Székely', display:{Lore:['[{"text": "arXiv:2405.09768", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lEvaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model\\u00a7r\\n\\n\\u00a78\\u00a7oSiyang Wang\\n\\u00c9va Sz\\u00e9kely\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.09768\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 May 2024 02:18:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o11 pages, 4 figures. Language Resources and Evaluation Conference (LREC) 2024. demo: https://swatsw.github.io/lrec24_eval_slm/\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Ruiqi Li; Yu Zhang; Yongqi Wang; Zhiqing Hong; Rongjie Huang; Zhou Zhao', display:{Lore:['[{"text": "arXiv:2405.09940", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRobust Singing Voice Transcription Serves Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oRuiqi Li\\nYu Zhang\\nYongqi Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.09940\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 May 2024 09:43:40 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oACL 2024\\u00a7r"}']}
{title:'Schmid et al. (§72024§r)', author: 'Florian Schmid; Paul Primus; Toni Heittola; Annamaria Mesaros; Irene Martín-Morató; Khaled Koutini; Gerhard Widmer', display:{Lore:['[{"text": "arXiv:2405.10018", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lData-Efficient Low-Complexity Acoustic Scene Classification in the DCASE 2024 Challenge\\u00a7r\\n\\n\\u00a78\\u00a7oFlorian Schmid\\nPaul Primus\\nToni Heittola\\n+ 3 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.10018\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 May 2024 12:00:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oTask Description Page: https://dcase.community/challenge2024/task-data-efficient-low-complexity-acoustic-scene-classification\\u00a7r"}']}
{title:'Chen et al. (§72024§r)', author: 'Xingyu Chen; Hanwen Bi; Wei-Ting Lai; Fei Ma', display:{Lore:['[{"text": "arXiv:2405.10022", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMonaural speech enhancement on drone via Adapter based transfer learning\\u00a7r\\n\\n\\u00a78\\u00a7oXingyu Chen\\nHanwen Bi\\nWei-Ting Lai\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.10022\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 May 2024 12:03:58 GMT)\\u00a7r"}']}
{title:'Luong et al. (§72024§r)', author: 'Manh Luong; Khai Nguyen; Nhat Ho; Reza Haf; Dinh Phung; Lizhen Qu', display:{Lore:['[{"text": "arXiv:2405.10084", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lRevisiting Deep Audio-Text Retrieval Through the Lens of Transportation\\u00a7r\\n\\n\\u00a78\\u00a7oManh Luong\\nKhai Nguyen\\nNhat Ho\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.10084\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 16 May 2024 13:28:10 GMT)\\u00a7r"}']}
{title:'Yao et al. (§72024§r)', author: 'Jixun Yao; Qing Wang; Pengcheng Guo; Ziqian Ning; Lei Xie', display:{Lore:['[{"text": "arXiv:2405.10786", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lDistinctive and Natural Speaker Anonymization via Singular Value Transformation-assisted Matrix\\u00a7r\\n\\n\\u00a78\\u00a7oJixun Yao\\nQing Wang\\nPengcheng Guo\\nZiqian Ning\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.10786\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 17 May 2024 13:48:43 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted by IEEE/ACM Transactionson Audio, Speech, and Language Processing\\u00a7r"}']}
{title:'Manohar et al. (§72024§r)', author: 'Vimal Manohar; Szu-Jui Chen; Zhiqi Wang; Yusuke Fujita; Shinji Watanabe; Sanjeev Khudanpur', display:{Lore:['[{"text": "arXiv:2405.11078", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAcoustic modeling for Overlapping Speech Recognition: JHU Chime-5 Challenge System\\u00a7r\\n\\n\\u00a78\\u00a7oVimal Manohar\\nSzu-Jui Chen\\nZhiqi Wang\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.11078\\u00a7r\\n\\nDOI:\\u00a76\\u00a7n0.1109/ICASSP.2019.8682556\\u00a7r\\n\\nJournal reference:\\u00a71\\u00a7nICASSP 2019 - 2019 IEEE International Conference on Acoustics,\\n  Speech and Signal Processing (ICASSP), Brighton, UK, 2019, pp. 6665-6669\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 17 May 2024 20:20:41 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oPublished in: ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\\u00a7r"}']}
{title:'Xu (§72024§r)', author: 'David Xu', display:{Lore:['[{"text": "arXiv:2405.11093", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lAudioSetMix: Enhancing Audio-Language Datasets with LLM-Assisted Augmentations\\u00a7r\\n\\n\\u00a78\\u00a7oDavid Xu\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.11093\\u00a7r\\n\\nVersion:\\u00a77v1 (Fri, 17 May 2024 21:08:58 GMT)\\u00a7r"}']}
{title:'Chandra et al. (§72024§r)', author: 'Shreeram Suresh Chandra; Zongyang Du; Berrak Sisman', display:{Lore:['[{"text": "arXiv:2405.11413", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lExploring speech style spaces with language models: Emotional TTS without emotion labels\\u00a7r\\n\\n\\u00a78\\u00a7oShreeram Suresh Chandra\\nZongyang Du\\nBerrak Sisman\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.11413\\u00a7r\\n\\nVersion:\\u00a77v1 (Sat, 18 May 2024 23:21:39 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted at Speaker Odyssey 2024\\u00a7r"}']}
{title:'Ohlenbusch et al. (§72024§r)', author: 'Mattes Ohlenbusch; Christian Rollwage; Simon Doclo', display:{Lore:['[{"text": "arXiv:2405.11592", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech-dependent Data Augmentation for Own Voice Reconstruction with Hearable Microphones in Noisy Environments\\u00a7r\\n\\n\\u00a78\\u00a7oMattes Ohlenbusch\\nChristian Rollwage\\nSimon Doclo\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.11592\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 19 May 2024 15:56:03 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o19 pages, 6 figures\\u00a7r"}']}
{title:'Huang et al. (§72024§r)', author: 'Wen-Chin Huang; Yi-Chiao Wu; Tomoki Toda', display:{Lore:['[{"text": "arXiv:2405.11767", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CR\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMulti-speaker Text-to-speech Training with Speaker Anonymized Data\\u00a7r\\n\\n\\u00a78\\u00a7oWen-Chin Huang\\nYi-Chiao Wu\\nTomoki Toda\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.11767\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 May 2024 03:55:44 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages. Submitted to Signal Processing Letters. Audio sample page: https://unilight.github.io/Publication-Demos/publications/sa-tts-spl/index.html\\u00a7r"}']}
{title:'Lai et al. (§72024§r)', author: 'Wei-Ting Lai; Lachlan Birnie; Xingyu Chen; Amy Bastine; Thushara D. Abhayapala; Prasanga N. Samarasinghe', display:{Lore:['[{"text": "arXiv:2405.11792", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSource Localization by Multidimensional Steered Response Power Mapping with Sparse Bayesian Learning\\u00a7r\\n\\n\\u00a78\\u00a7oWei-Ting Lai\\nLachlan Birnie\\nXingyu Chen\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.11792\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 May 2024 05:18:47 GMT)\\u00a7r"}']}
{title:'Shams et al. (§72024§r)', author: 'Siavash Shams; Sukru Samet Dindar; Xilin Jiang; Nima Mesgarani', display:{Lore:['[{"text": "arXiv:2405.11831", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSSAMBA: Self-Supervised Audio Representation Learning with Mamba State Space Model\\u00a7r\\n\\n\\u00a78\\u00a7oSiavash Shams\\nSukru Samet Dindar\\nXilin Jiang\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.11831\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 20 May 2024 06:58:47 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oCode at https://github.com/SiavashShams/ssamba\\u00a7r"}']}
{title:'Shen et al. (§72024§r)', author: 'Xiaoyi Shen; Dongyuan Shi; Zhengding Luo; Junwei Ji; Woon-Seng Gan', display:{Lore:['[{"text": "arXiv:2405.12496", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.NI\\u00a7r, \\u00a7acs.SD\\u00a7r, \\u00a7eeess.SP\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Survey of Integrating Wireless Technology into Active Noise Control\\u00a7r\\n\\n\\u00a78\\u00a7oXiaoyi Shen\\nDongyuan Shi\\nZhengding Luo\\nJunwei Ji\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.12496\\u00a7r\\n\\nVersion:\\u00a77v1 (Tue, 21 May 2024 04:53:39 GMT)\\u00a7r"}']}
{title:'Zhang et al. (§72024§r)', author: 'Xiangyu Zhang; Qiquan Zhang; Hexin Liu; Tianyi Xiao; Xinyuan Qian; Beena Ahmed; Eliathamby Ambikairajah; Haizhou Li; Julien Epps', display:{Lore:['[{"text": "arXiv:2405.12609", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMamba in Speech: Towards an Alternative to Self-Attention\\u00a7r\\n\\n\\u00a78\\u00a7oXiangyu Zhang\\nQiquan Zhang\\nHexin Liu\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.12609\\u00a7r\\n\\nVersion:\\u00a77v3 (Fri, 24 May 2024 02:23:41 GMT)\\u00a7r"}']}
{title:'Burchi et al. (§72024§r)', author: 'Maxime Burchi; Krishna C. Puvvada; Jagadeesh Balam; Boris Ginsburg; Radu Timofte', display:{Lore:['[{"text": "arXiv:2405.12983", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CV\\u00a7r, \\u00a7acs.MM\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lMultilingual Audio-Visual Speech Recognition with Hybrid CTC/RNN-T Fast Conformer\\u00a7r\\n\\n\\u00a78\\u00a7oMaxime Burchi\\nKrishna C. Puvvada\\nJagadeesh Balam\\nBoris Ginsburg\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.12983\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 14 Mar 2024 01:16:32 GMT)\\u00a7r"}']}
{title:'Wang et al. (§72024§r)', author: 'Yicheng Wang; Mark Cusick; Mohamed Laila; Kate Puech; Zhengping Ji; Xia Hu; Michael Wilson; Noah Spitzer-Williams; Bryan Wheeler; Yasser Ibrahim', display:{Lore:['[{"text": "arXiv:2405.13166", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.AI\\u00a7r, \\u00a7acs.CY\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lFairLENS: Assessing Fairness in Law Enforcement Speech Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oYicheng Wang\\nMark Cusick\\nMohamed Laila\\n+ 6 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.13166\\u00a7r\\n\\nVersion:\\u00a77v2 (Tue, 28 May 2024 19:10:30 GMT)\\u00a7r"}']}
{title:'Sudo et al. (§72024§r)', author: 'Yui Sudo; Yosuke Fukumoto; Muhammad Shakeel; Yifan Peng; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2405.13344", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lContextualized Automatic Speech Recognition with Dynamic Vocabulary\\u00a7r\\n\\n\\u00a78\\u00a7oYui Sudo\\nYosuke Fukumoto\\nMuhammad Shakeel\\nYifan Peng\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.13344\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 May 2024 05:03:39 GMT)\\u00a7r"}']}
{title:'Shakeel et al. (§72024§r)', author: 'Muhammad Shakeel; Yui Sudo; Yifan Peng; Shinji Watanabe', display:{Lore:['[{"text": "arXiv:2405.13514", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lJoint Optimization of Streaming and Non-Streaming Automatic Speech Recognition with Multi-Decoder and Knowledge Distillation\\u00a7r\\n\\n\\u00a78\\u00a7oMuhammad Shakeel\\nYui Sudo\\nYifan Peng\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.13514\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 22 May 2024 10:17:30 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted to IEEEICASSP 2024 workshop Hands-free Speech Communication and Microphone Arrays (HSCMA 2024)\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Hui Li; Hongyu Wang; Zhijin Chen; Bohan Sun; Bo Li', display:{Lore:['[{"text": "arXiv:2405.15093", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lReal-Time and Accurate: Zero-shot High-Fidelity Singing Voice Conversion with Multi-Condition Flow Synthesis\\u00a7r\\n\\n\\u00a78\\u00a7oHui Li\\nHongyu Wang\\nZhijin Chen\\nBohan Sun\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.15093\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 23 May 2024 22:51:04 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7o5 pages,4 figures\\u00a7r"}']}
{title:'Li et al. (§72024§r)', author: 'Yuanchao Li; Pinzhen Chen; Peter Bell; Catherine Lai', display:{Lore:['[{"text": "arXiv:2405.16677", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.CL\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lCrossmodal ASR Error Correction with Discrete Speech Units\\u00a7r\\n\\n\\u00a78\\u00a7oYuanchao Li\\nPinzhen Chen\\nPeter Bell\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.16677\\u00a7r\\n\\nVersion:\\u00a77v1 (Sun, 26 May 2024 19:58:38 GMT)\\u00a7r"}']}
{title:'Pal et al. (§72024§r)', author: 'Monisankha Pal; Arvind Ramanathan; Ted Wada; Ashutosh Pandey', display:{Lore:['[{"text": "arXiv:2405.16834", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech enhancement deep-learning architecture for efficient edge processing\\u00a7r\\n\\n\\u00a78\\u00a7oMonisankha Pal\\nArvind Ramanathan\\nTed Wada\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.16834\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 May 2024 05:04:09 GMT)\\u00a7r"}']}
{title:'Guo et al. (§72024§r)', author: 'Zilu Guo; Qing Wang; Jun Du; Jia Pan; Qing-Feng Liu; Chin-Hui', display:{Lore:['[{"text": "arXiv:2405.16952", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lA Variance-Preserving Interpolation Approach for Diffusion Models with Applications to Single Channel Speech Enhancement and Recognition\\u00a7r\\n\\n\\u00a78\\u00a7oZilu Guo\\nQing Wang\\nJun Du\\n+ 2 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.16952\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 May 2024 08:44:19 GMT)\\u00a7r"}']}
{title:'Torcoli et al. (§72024§r)', author: 'Matteo Torcoli; Mhd Modar Halimeh; Thomas Leitz; Yannik Grewe; Michael Kratschmer; Bernhard Neugebauer; Adrian Murtaza; Harald Fuchs; Emanuël A. P. Habets', display:{Lore:['[{"text": "arXiv:2405.17364", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lSpeech Loudness in Broadcasting and Streaming\\u00a7r\\n\\n\\u00a78\\u00a7oMatteo Torcoli\\nMhd Modar Halimeh\\nThomas Leitz\\n+ 5 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.17364\\u00a7r\\n\\nVersion:\\u00a77v1 (Mon, 27 May 2024 17:14:45 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oAccepted for presentation at the Audio Engineering Society (AES) 156th Convention, June 2024, Madrid, Spain\\u00a7r"}']}
{title:'Moliner et al. (§72024§r)', author: 'Eloi Moliner; Sebastian Braun; Hannes Gamper', display:{Lore:['[{"text": "arXiv:2405.19497", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.LG\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7lGaussian Flow Bridges for Audio Domain Transfer with Unpaired Data\\u00a7r\\n\\n\\u00a78\\u00a7oEloi Moliner\\nSebastian Braun\\nHannes Gamper\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.19497\\u00a7r\\n\\nVersion:\\u00a77v1 (Wed, 29 May 2024 20:23:01 GMT)\\u00a7r\\n\\n"}','{"text": "Comments: \\u00a77\\u00a7oSubmitted to IWAENC 2024\\u00a7r"}']}
{title:'Chen et al. (§72024§r)', author: 'Mingjie Chen; Hezhao Zhang; Yuanchao Li; Jiachen Luo; Wen Wu; Ziyang Ma; Peter Bell; Catherine Lai; Joshua Reiss; Lin Wang; Philip C. Woodland; Xie Chen; Huy Phan; Thomas Hain', display:{Lore:['[{"text": "arXiv:2405.20064", "color": "red"}]']}, pages:['{"text": "\\u00a7n\\u00a7eeess.AS\\u00a7r, \\u00a7acs.SD\\u00a7r\\u00a7r\\n\\u00a76\\u00a7l1st Place Solution to Odyssey Emotion Recognition Challenge Task1: Tackling Class Imbalance Problem\\u00a7r\\n\\n\\u00a78\\u00a7oMingjie Chen\\nHezhao Zhang\\nYuanchao Li\\n+ 10 others\\u00a7r\\n\\n\\u00a772024\\u00a7r"}','{"text": "arXiv:\\u00a7c\\u00a7n2405.20064\\u00a7r\\n\\nVersion:\\u00a77v1 (Thu, 30 May 2024 13:55:43 GMT)\\u00a7r"}']}
